paperId,uses dataset or derivative,dataset(s) / model(s) used,unable to disambiguate,cites D9,cites D10,title,abstract,year,venue,arxivId,doi,pdfUrl
00281325c4b0a662a2b7e15eabb647923f65dda2,0,,,0,1,MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets,"When there is a mismatch between the target identity and the driver identity, face reenactment suffers severe degradation in the quality of the result, especially in a few-shot setting. The identity preservation problem, where the model loses the detailed information of the target leading to a defective output, is the most common failure mode. The problem has several potential sources such as the identity of the driver leaking due to the identity mismatch, or dealing with unseen large poses. To overcome such problems, we introduce components that address the mentioned problem: image attention block, target feature alignment, and landmark transformer. Through attending and warping the relevant features, the proposed architecture, called MarioNETte, produces high-quality reenactments of unseen identities in a few-shot setting. In addition, the landmark transformer dramatically alleviates the identity preservation problem by isolating the expression geometry through landmark disentanglement. Comprehensive experiments are performed to verify that the proposed framework can generate highly realistic faces, outperforming all other baselines, even under a significant mismatch of facial characteristics between the target and the driver.",2020,AAAI,1911.08139,10.1609/AAAI.V34I07.6721,https://arxiv.org/pdf/1911.08139.pdf
004c7dbd5578865ac72cfa7b6ebc51c7fa7cda31,0,,,1,0,Two-Branch Recurrent Network for Isolating Deepfakes in Videos,"The current spike of hyper-realistic faces artificially generated using deepfakes calls for media forensics solutions that are tailored to video streams and work reliably with a low 0 alarm rate at the video level. We present a method for deepfake detection based on a two-branch network structure that isolates digitally manipulated faces by learning to amplify artifacts while suppressing the high-level face content. Unlike current methods that extract spatial frequencies as a preprocessing step, we propose a two-branch structure: one branch propagates the original information, while the other branch suppresses the face content yet amplifies multi-band frequencies using a Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate manipulated faces, we derive a novel cost function that, unlike regular classification, compresses the variability of natural faces and pushes away the unrealistic facial samples in the feature space. Our two novel components show promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview benchmarks, when compared to prior work. We then offer a full, detailed ablation study of our network architecture and cost function. Finally, although the bar is still high to get very remarkable figures at a very low 0 alarm rate, our study shows that we can achieve good video-level performance when cross-testing in terms of video-level AUC.",2020,ECCV,2008.03412,10.1007/978-3-030-58571-6_39,https://arxiv.org/pdf/2008.03412.pdf
019bd8a33a5ac7d548b26eaa3c83605f2e9cb07f,1,[M5],,1,0,"Mitigating Bias in Gender, Age and Ethnicity Classification: A Multi-task Convolution Neural Network Approach","This work explores joint classification of gender, age and race. Specifically, we here propose a Multi-Task Convolution Neural Network (MTCNN) employing joint dynamic loss weight adjustment towards classification of named soft biometrics, as well as towards mitigation of soft biometrics related bias. The proposed algorithm achieves promising results on the UTKFace and the Bias Estimation in Face Analytics (BEFA) datasets and was ranked first in the BEFA Challenge of the European Conference of Computer Vision (ECCV) 2018.",2018,ECCV Workshops,,10.1007/978-3-030-11009-3_35,https://hal.inria.fr/hal-01892103/file/DasDantchevaBremond_ECCVW_18.pdf
01a5a65451f5343bca8f3c75e583b7c4c89187dd,0,,,0,1,MMA Regularization: Decorrelating Weights of Neural Networks by Maximizing the Minimal Angles,"The strong correlation between neurons or filters can significantly weaken the generalization ability of neural networks. Inspired by the well-known Tammes problem, we propose a novel diversity regularization method to address this issue, which makes the normalized weight vectors of neurons or filters distributed on a hypersphere as uniformly as possible, through maximizing the minimal pairwise angles (MMA). This method can easily exert its effect by plugging the MMA regularization term into the loss function with negligible computational overhead. The MMA regularization is simple, efficient, and effective. Therefore, it can be used as a basic regularization method in neural network training. Extensive experiments demonstrate that MMA regularization is able to enhance the generalization ability of various modern models and achieves considerable performance improvements on CIFAR100 and TinyImageNet datasets. In addition, experiments on face verification show that MMA regularization is also effective for feature learning.",2020,ArXiv,2006.06527,,https://arxiv.org/pdf/2006.06527.pdf
0260782796630906e3e24563498f9c18d8ac90df,0,,,0,1,Speaker Embeddings Incorporating Acoustic Conditions for Diarization,"We present our work on training speaker embeddings, especially effective for speaker diarization. For various speaker recognition tasks, extracting speaker embeddings using Deep Neural Networks (DNNs) has become major methods. These embeddings are generally trained to be discriminate speakers and be robust with respect to different acoustic conditions. In speaker diarization, however, the acoustic conditions can be used as consistent information for discriminating speakers. Such information can include the distances to a microphone in a meeting, or the channels for each speaker in telephone conversation recorded in monaural. Hence, the proposed speaker-embedding network leverages differences in acoustic conditions to train effective speaker embeddings for speaker diarization. The information on acoustic conditions can be anything that contributes to distinguishing between recording environments; for example, we explore using i-vectors. Experiments conducted on a practical diarization system demonstrated that the proposed embeddings significantly improve performance over embeddings without information on acoustic conditions.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9054273,
0263c0b6ceae59b8cea0acff6ccaf1d6dbffbf29,0,,,0,1,Statistical insights into deep neural network learning in subspace classification,"Correspondence Hao Wu, Department of Mathematics, Dornsife College of Letters, Arts and Sciences, University of Southern California, Los Angeles, CA 90089-0001, USA. Email: hwu409@usc.edu Deep learning has benefited almost every aspect of modern big data applications. Yet its statistical properties still remain largely unexplored. It is commonly believed nowadays that deep neural networks (DNNs) benefit from representational learning. To gain some statistical insights into this, we design a simple simulation setting where we generate data from some latent subspace structure with each subspace regarded as a cluster. We empirically demonstrate that the performance of DNN is very similar to that of the two-step procedure of clustering followed by classification (unsupervised plus supervised). This motivates us to ask: Does DNN indeed mimic the two-step procedure statistically? That is, do bottom layers in DNN try to cluster first and then top layers classify within each cluster? To answer this question, we conduct a series of simulation studies, and to our surprise, none of the hidden layers in DNN conduct successful clustering. In some sense, our results provide an important complement to the common belief of representational learning, suggesting that at least in some model settings, although the performance of DNN is comparable with that of the ideal two-step procedure knowing the 1 latent cluster information a priori, it does not really do clustering in any of its layers. We also provide some statistical insights and heuristic arguments to support our empirical discoveries and further demonstrate the revealed phenomenon on the real data application of traffic sign recognition.",2020,,,10.1002/sta4.273,http://faculty.marshall.usc.edu/jinchi-lv/publications/Stat-WFL20.pdf
03878f5348ed24fcb20538efa30089ee9b699f8d,0,,,1,0,Learning Neural Bag-of-Features for Large-Scale Image Retrieval,"In this paper, the well-known bag-of-features (BoFs) model is generalized and formulated as a neural network that is composed of three layers: 1) a radial basis function (RBF) layer; 2) an accumulation layer; and 3) a fully connected layer. This formulation allows for decoupling the representation size from the number of used codewords, as well as for better modeling the feature distribution using a separate trainable scaling parameter for each RBF neuron. The resulting network, called retrieval-oriented neural BoF (RN-BoF), is trained using regular back propagation and allows for fast extraction of compact image representations. It is demonstrated that the RN-BoF model is capable of: 1) increasing the object encoding and retrieval speed; 2) reducing the extracted representation size; and 3) increasing the retrieval precision. A symmetry-aware spatial segmentation technique is also proposed to further reduce the encoding time and the storage requirements and allows the method to efficiently scale to large datasets. The proposed method is evaluated and compared to other state-of-the-art techniques using five different image datasets, including the large-scale YouTube Faces database.",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,10.1109/TSMC.2017.2680404,http://poseidon.csd.auth.gr/papers/PUBLISHED/JOURNAL/pdf/2017/fast-bof-retrieval.pdf
039be37ad95d009c8114b7bfa963b09624cd427b,0,,,1,0,Semantic-Aware Makeup Cleanser,"Face verification aims at determining whether a pair of face images belongs to the same identity. Recent studies have revealed the negative impact of facial makeup on the verification performance. With the rapid development of deep generative models, this paper proposes a semantic-aware makeup cleanser (SAMC) to remove facial makeup under different poses and expressions and achieve verification via generation. The intuition lies in the fact that makeup is a combined effect of multiple cosmetics and tailored treatments should be imposed on different cosmetic regions. To this end, we present both unsupervised and supervised semantic-aware learning strategies in SAMC. At image level, an unsupervised attention module is jointly learned with the generator to locate cosmetic regions and estimate the degree. At feature level, we resort to the effort of face parsing merely in training phase and design a localized texture loss to serve complements and pursue superior synthetic quality. The experimental results on four makeup-related datasets verify that SAMC not only produces appealing de-makeup outputs at a resolution of 256 × 256, but also facilitates makeup-invariant face verification through image generation.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",,10.1109/BTAS46853.2019.9186001,
0417722d4005efbf0e7b81ea8b70176863c5348e,0,,,0,1,Norm-Aware Embedding for Efficient Person Search,"Person Search is a practically relevant task that aims to jointly solve Person Detection and Person Re-identification (re-ID). Specifically, it requires to find and locate all instances with the same identity as the query person in a set of panoramic gallery images. One major challenge comes from the contradictory goals of the two sub-tasks, i.e., person detection focuses on finding the commonness of all persons while person re-ID handles the differences among multiple identities. Therefore, it is crucial to reconcile the relationship between the two sub-tasks in a joint person search model. To this end, We present a novel approach called Norm-Aware Embedding to disentangle the person embedding into norm and angle for detection and re-ID respectively, allowing for both effective and efficient multi-task training. We further extend the proposal-level person embedding to pixel-level, whose discrimination ability is less affected by mis-alignment. We outperform other one-step methods by a large margin and achieve comparable performance to two-step methods on both CUHK-SYSU and PRW. Also, Our method is easy to train and resource-friendly, running at 12 fps on a single GPU.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.01263,http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf
04d050642c11c1744607926e36c35cb99d6f5205,0,,,0,1,Angular Margin Centroid Loss for Text-Independent Speaker Recognition,"Speaker recognition for unseen speakers out of the training dataset relies on the discrimination of speaker embedding. Recent studies use the angular softmax losses with angular margin penalties to enhance the intra-class compactness of speaker embedding, which achieve obvious performance improvement. However, the classification layer encounters the problem of dimension explosion in these losses with the growth of training speakers. In this paper, like the prototype network loss in the few-short learning and the generalized end-to-end loss, we optimize the cosine distances between speaker embeddings and their corresponding centroids rather than the weight vectors in the classification layer. For the intra-class compactness, we impose the additive angular margin to shorten the cosine distance between speaker embeddings belonging to the same speaker. Meanwhile, we also explicitly improve the inter-class separability by enlarging the cosine distance between different speaker centroids. Experiments show that our loss achieves comparable performance with the stat-of-the-art angular margin softmax loss in both verification and identification tasks and markedly reduces the training iterations.",2020,INTERSPEECH,,10.21437/interspeech.2020-2538,https://isca-speech.org/archive/Interspeech_2020/pdfs/2538.pdf
05fbc7ebc416a75f0a3cbccae902e83fbae35e24,1,[D10],,0,1,Fast Face Recognition Model without Pruning,"This paper proposes a real-time high accuracy face recognition model which can be deployed on performanceconstrained embedded platforms or mobile terminals without pruning. In the proposed model, we optimize the network structure by balancing the memory access cost (MAC) and Floatingpoint Operations (FLOPs) and propose a new factor to measure the efficiency of learning parameters. Based on these two metrics, we designed two structures called Simple SqueezeFaceNet (SSN) and Channel-Split Network (CSN) to achieve a good balance between the high precision and high FPS (Frames per Second). After being trained with the refined MS1M-refine-v2 dataset, our architectures achieve 0.992 (or 0.990) accuracy on the Labeled Faces in the Wild (LFW) with 155 (or 180) FPS. This indicates that our models can also be applied to real-time multi-face recognition in video.",2019,2019 IEEE Symposium Series on Computational Intelligence (SSCI),,10.1109/SSCI44817.2019.9002922,
07929d975c492d68261b27ff6c4b6b41ca1097e4,0,,,0,1,Heatmap Regression via Randomized Rounding,"Heatmap regression has become the mainstream methodology for deep learning-based semantic landmark localization, including in facial landmark localization and human pose estimation. Though heatmap regression is robust to large variations in pose, illumination, and occlusion in unconstrained settings, it usually suffers from a sub-pixel localization problem. Specifically, considering that the activation point indices in heatmaps are always integers, quantization error thus appears when using heatmaps as the representation of numerical coordinates. Previous methods to overcome the sub-pixel localization problem usually rely on high-resolution heatmaps. As a result, there is always a trade-off between achieving localization accuracy and computational cost, where the computational complexity of heatmap regression depends on the heatmap resolution in a quadratic manner. In this paper, we formally analyze the quantization error of vanilla heatmap regression and propose a simple yet effective quantization system to address the sub-pixel localization problem. The proposed quantization system induced by the randomized rounding operation 1) encodes the fractional part of numerical coordinates into the ground truth heatmap using a probabilistic approach during training; and 2) decodes the predicted numerical coordinates from a set of activation points during testing. We prove that the proposed quantization system for heatmap regression is unbiased and lossless. Experimental results on four popular facial landmark localization datasets (WFLW, 300W, COFW, and AFLW) demonstrate the effectiveness of the proposed method for efficient and accurate semantic landmark localization. Code is available at this http URL.",2020,ArXiv,2009.00225,,https://arxiv.org/pdf/2009.00225.pdf
08402ffdecdb76bcba75d23910635fb371c6db7b,0,,,1,0,Improving face representation learning with center invariant loss,"Abstract In this paper, we address on the deep face representation learning with imbalanced data. With a large number of available face images of different people for training, Convolutional Neural Networks could learn deep face representation through classifying these people. However, uniformed distributed data for all people are hard to get. Some people come with more images but some come with less. In learning the deep face representation, the imbalanced images between people introduce the bias towards these people that have more images. Existing methods focus on the intra-class and inter-class variations but not well address the imbalanced data problem. To generate a robust and discriminative face representation for all people, we propose a center invariant loss which adds penalty to the differences between each center of classes. The center invariant loss could align the center of each person to the mean of all centers, which could force the deeply learned face features to have a good representation for all people with better generalization ability. Extensive experiments well demonstrate the effectiveness of the proposed approach. Many existing methods in learning deep face representation are further improved after adding the proposed center invariant loss.",2018,Image Vis. Comput.,,10.1016/j.imavis.2018.09.010,
089c4e170b114a5b3f8564e02eaae767f30ebed8,0,,,0,1,An a-contrario Biometric Fusion Approach,"Fusion is a key component in many biometric systems: it is one of the most widely used techniques to improve their accuracy. Each time we need to combine the output of systems that use different biometric traits, or different samples of the same biometric trait, or even different algorithms, we need to define a fusion strategy. Independently of the fusion method used, there is always a decision step, in which it is decided if the traits being compared correspond to the same individual or not. In this work, we present a statistical decision criterion based on the a-contrario framework, which has already proven to be useful in biometric applications. The proposed method and its theoretical background is described in detail, and its application to biometric fusion is illustrated with simulated and real data.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00419,
0c641b46a0863788186234632b33ac9831dc21bb,0,,,0,1,A Novel Deep Multi-Modal Feature Fusion Method for Celebrity Video Identification,"In this paper, we develop a novel multi-modal feature fusion method for the 2019 iQIYI Celebrity Video Identification Challenge, which is held in conjunction with ACM MM 2019. The purpose of this challenge is to retrieve all the video clips of a given identity in the testing set. In this challenge, the multi-modal features of a celebrity are encouraged to be combined for a promising performance, such as face features, head features, body features, and audio features. As we know, the features from different modalities usually have their own influences on the results. To achieve better results, a novel weighted multi-modal feature fusion method is designed to obtain the final feature representation. After many experimental verification, we found that different feature fusion weights for training and testing make the method robust to multi-modal person identification. Experiments on the iQIYI-VID-2019 dataset show that our multi-modal feature fusion strategy effectively improves the accuracy of person identification. Specifically, for competition, we use a single model to get the result of 0.8952 in mAP, which ranks TOP-5 among all the competitive results.",2019,ACM Multimedia,,10.1145/3343031.3356067,
0d31ec5153fb805e1d632b6e8ee0f08985f71fdb,0,,,0,1,A Study on Angular Based Embedding Learning for Text-independent Speaker Verification,"Learning a good speaker embedding is important for many automatic speaker recognition tasks, including verification, identification and diarization. The embeddings learned by softmax are not discriminative enough for open-set verification tasks. Angular based embedding learning target can achieve such discriminativeness by optimizing angular distance and adding margin penalty. We apply several different popular angular margin embedding learning strategies in this work and explicitly compare their performance on Voxceleb speaker recognition dataset. Observing the fact that encouraging inter-class separability is important when applying angular based embedding learning, we propose an exclusive inter-class regularization as a complement for angular based loss. We verify the effectiveness of these methods for learning a discriminative embedding space on ASV task with several experiments. These methods together, we manage to achieve an impressive result with 16.5% improvement on equal error rate (EER) and 18.2% improvement on minimum detection cost function comparing with baseline softmax systems.",2019,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),1908.0399,10.1109/APSIPAASC47483.2019.9023165,https://arxiv.org/pdf/1908.03990.pdf
0dfd73ad08c9a10fe5628257e2170c728e78a357,0,,,0,1,FH-GAN: Face Hallucination and Recognition using Generative Adversarial Network,"There are many factors affecting visual face recognition, such as low resolution images, aging, illumination and pose variance, etc. One of the most important problem is low resolution face images which can result in bad performance on face recognition. Most of the general face recognition algorithms usually assume a sufficient resolution for the face images. However, in practice many applications often do not have sufficient image resolutions. The modern face hallucination models demonstrate reasonable performance to reconstruct high-resolution images from its corresponding low resolution images. However, they do not consider identity level information during hallucination which directly affects results of the recognition of low resolution faces. To address this issue, we propose a Face Hallucination Generative Adversarial Network (FH-GAN) which improves the quality of low resolution face images and accurately recognize those low quality images. Concretely, we make the following contributions: 1) we propose FH-GAN network, an end-to-end system, that improves both face hallucination and face recognition simultaneously. The novelty of this proposed network depends on incorporating identity information in a GAN-based face hallucination algorithm via combining a face recognition network for identity preserving. 2) We also propose a new face hallucination network, namely Dense Sparse Network (DSNet), which improves upon the state-of-art in face hallucination. 3) We demonstrate benefits of training the face recognition and GAN-based DSNet jointly by reporting good result on face hallucination and recognition.",2019,ICONIP,1905.06537,10.1007/978-3-030-36708-4_1,https://arxiv.org/pdf/1905.06537.pdf
10915b826bfd5eb720b5bc35eee162ef32846759,0,,,0,1,ModelHub.AI: Dissemination Platform for Deep Learning Models,"Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present this http URL (this http URL), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on this http URL and allows both researchers and developers to utilize models out-of-the-box. this http URL is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.",2019,ArXiv,1911.13218,,https://arxiv.org/pdf/1911.13218.pdf
11f7ca99836c0151dae9d22df5cff874f42992e2,0,,,1,0,Deep learning with noisy labels: exploring techniques and remedies in medical image analysis,"Supervised training of deep learning models requires large labeled datasets. There is a growing interest in obtaining such datasets for medical image analysis applications. However, the impact of label noise has not received sufficient attention. Recent studies have shown that label noise can significantly impact the performance of deep learning models in many machine learning and computer vision applications. This is especially concerning for medical applications, where datasets are typically small, labeling requires domain expertise and suffers from high inter- and intra-observer variability, and erroneous predictions may influence decisions that directly impact human health. In this paper, we first review the state-of-the-art in handling label noise in deep learning. Then, we review studies that have dealt with label noise in deep learning for medical image analysis. Our review shows that recent progress on handling label noise in deep learning has gone largely unnoticed by the medical image analysis community. To help achieve a better understanding of the extent of the problem and its potential remedies, we conducted experiments with three medical imaging datasets with different types of label noise, where we investigated several existing strategies and developed new methods to combat the negative effect of label noise. Based on the results of these experiments and our review of the literature, we have made recommendations on methods that can be used to alleviate the effects of different types of label noise on deep models trained for medical image analysis. We hope that this article helps the medical image analysis researchers and developers in choosing and devising new techniques that effectively handle label noise in deep learning.",2020,Medical Image Anal.,1912.02911,10.1016/j.media.2020.101759,https://arxiv.org/pdf/1912.02911.pdf
123bbc9e6987c1374442bfebaf195409ec6c2e4e,0,,,0,1,ByeGlassesGAN: Identity Preserving Eyeglasses Removal for Face Images,"In this paper, we propose a novel image-to-image GAN framework for eyeglasses removal, called ByeGlassesGAN, which is used to automatically detect the position of eyeglasses and then remove them from face images. Our ByeGlassesGAN consists of an encoder, a face decoder, and a segmentation decoder. The encoder is responsible for extracting information from the source face image, and the face decoder utilizes this information to generate glasses-removed images. The segmentation decoder is included to predict the segmentation mask of eyeglasses and completed face region. The feature vectors generated by the segmentation decoder are shared with the face decoder, which facilitates better reconstruction results. Our experiments show that ByeGlassesGAN can provide visually appealing results in the eyeglasses-removed face images even for semi-transparent color eyeglasses or glasses with glare. Furthermore, we demonstrate significant improvement in face recognition accuracy for face images with glasses by applying our method as a pre-processing step in our face recognition experiment.",2020,ECCV,2008.11042,10.1007/978-3-030-58526-6_15,https://arxiv.org/pdf/2008.11042.pdf
124f6992202777c09169343d191c254592e4428c,1,[M5],,1,0,Visual Psychophysics for Making Face Recognition Algorithms More Explainable,"Scientific fields that are interested in faces have developed their own sets of concepts and procedures for understanding how a target model system (be it a person or algorithm) perceives a face under varying conditions. In computer vision, this has largely been in the form of dataset evaluation for recognition tasks where summary statistics are used to measure progress. While aggregate performance has continued to improve, understanding individual causes of failure has been difficult, as it is not always clear why a particular face fails to be recognized, or why an impostor is recognized by an algorithm. Importantly, other fields studying vision have addressed this via the use of visual psychophysics: the controlled manipulation of stimuli and careful study of the responses they evoke in a model system. In this paper, we suggest that visual psychophysics is a viable methodology for making face recognition algorithms more explainable. A comprehensive set of procedures is developed for assessing face recognition algorithm behavior, which is then deployed over state-of-the-art convolutional neural networks and more basic, yet still widely used, shallow and handcrafted feature-based approaches.",2018,ECCV,1803.0714,10.1007/978-3-030-01267-0_16,https://arxiv.org/pdf/1803.07140.pdf
1299ecacc8facfb20c68b3fc6b36021f47013608,1,[M1],,1,0,Cast Search via Two-Stream Label Propagation,"We address the problem of Cast Search by Portraits (CSP) where a facial portrait of a cast member is provided to retrieve from a given video clip those frames containing the query target. The underlying CSP formulation is related to the task of person re-identification. However, CSP is more challenging in that the provided query image is only a portrait of a certain cast member, and the instances of the target in the candidate video could have a very different visual appearance. Such drastic visual variations are not common in addressing the problem of person re-id. We propose a two-stream network architecture for tackling the CSP challenge and also participate in the public CSP competition. The overall outcome in the competition is promising and worth further effort to improve our proposed model.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00231,http://openaccess.thecvf.com/content_ICCVW_2019/papers/WIDER/Wu_Cast_Search_via_Two-Stream_Label_Propagation_ICCVW_2019_paper.pdf
12fcb2637f241d515827fb193a650b7e6bf6b1f7,1,,1,1,0,Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations,"We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.",2018,AAAI,1705.08841,,https://arxiv.org/pdf/1705.08841.pdf
1344317f255a9d338fb80f276126951b9644f7e3,1,[M5],1,1,0,M-VAD names: a dataset for video captioning with naming,"Current movie captioning architectures are not capable of mentioning characters with their proper name, replacing them with a generic “someone” tag. The lack of movie description datasets with characters’ visual annotations surely plays a relevant role in this shortage. Recently, we proposed to extend the M-VAD dataset by introducing such information. In this paper, we present an improved version of the dataset, namely M-VAD Names, and its semi-automatic annotation procedure. The resulting dataset contains 63 k visual tracks and 34 k textual mentions, all associated with character identities. To showcase the features of the dataset and quantify the complexity of the naming task, we investigate multimodal architectures to replace the “someone” tags with proper character names in existing video captions. The evaluation is further extended by testing this application on videos outside of the M-VAD Names dataset.",2018,Multimedia Tools and Applications,1903.01489,10.1007/s11042-018-7040-z,https://arxiv.org/pdf/1903.01489.pdf
13f91a455c89890e75470d507b0eafc57cd83041,1,,1,1,1,SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing,"Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee ""subtle perturbation"" by limiting the $L_p$ norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate ""unrestricted adversarial examples"".  In particular, we propose an algorithm \emph{SemanticAdv} which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various ""adversarial"" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against \emph{real-world black-box} services such as Azure face verification service based on transferability.  To further demonstrate the applicability of \emph{SemanticAdv} beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.",2019,ArXiv,1906.07927,,https://arxiv.org/pdf/1906.07927.pdf
14ac5016a3f1df12418933f167c8b1c625669e06,0,,,1,0,Visualizing Important Areas for Facial Verification,iii,2017,,,,https://pdfs.semanticscholar.org/14ac/5016a3f1df12418933f167c8b1c625669e06.pdf
1584003eda0efca3aef4ce37089dc6ada1062fe4,0,,,1,1,Driver Face Verification with Depth Maps,"Face verification is the task of checking if two provided images contain the face of the same person or not. In this work, we propose a fully-convolutional Siamese architecture to tackle this task, achieving state-of-the-art results on three publicly-released datasets, namely Pandora, High-Resolution Range-based Face Database (HRRFaceD), and CurtinFaces. The proposed method takes depth maps as the input, since depth cameras have been proven to be more reliable in different illumination conditions. Thus, the system is able to work even in the case of the total or partial absence of external light sources, which is a key feature for automotive applications. From the algorithmic point of view, we propose a fully-convolutional architecture with a limited number of parameters, capable of dealing with the small amount of depth data available for training and able to run in real time even on a CPU and embedded boards. The experimental results show acceptable accuracy to allow exploitation in real-world applications with in-board cameras. Finally, exploiting the presence of faces occluded by various head garments and extreme head poses available in the Pandora dataset, we successfully test the proposed system also during strong visual occlusions. The excellent results obtained confirm the efficacy of the proposed method.",2019,Sensors,,10.3390/s19153361,https://pdfs.semanticscholar.org/c9b7/8803a9f44bf87a63f62d3d4977601d0078b1.pdf
1586725207bab1345a50ea3d8ee8464248ba80ea,0,,,0,1,A Metric Learning Approach to Misogyny Categorization,"The task of automatic misogyny identification and categorization has not received as much attention as other natural language tasks have, even though it is crucial for identifying hate speech in social Internet interactions. In this work, we address this sentence classification task from a representation learning perspective, using both a bidirectional LSTM and BERT optimized with the following metric learning loss functions: contrastive loss, triplet loss, center loss, congenerous cosine loss and additive angular margin loss. We set new state-of-the-art for the task with our fine-tuned BERT, whose sentence embeddings can be compared with a simple cosine distance, and we release all our code as open source for easy reproducibility. Moreover, we find that almost every loss function performs equally well in this setting, matching the regular cross entropy loss.",2020,RepL4NLP@ACL,,10.18653/v1/2020.repl4nlp-1.12,https://pdfs.semanticscholar.org/37cc/5b7729679cfabc66ce0d8fa5b40515a16720.pdf
16c10d0242739d0edfe076bcac58b507d46da79a,1,[D14],,1,1,DBLFace: Domain-Based Labels for NIR-VIS Heterogeneous Face Recognition,"Deep learning-based domain-invariant feature learning methods are advancing in near-infrared and visible (NIR-VIS) heterogeneous face recognition. However, these methods are prone to overfitting due to the large intra-class variation and the lack of NIR images for training. In this paper, we introduce Domain-Based Label Face (DBLFace), a learning approach based on the assumption that a subject is not represented by a single label but by a set of labels. Each label represents images of a specific domain. In particular, a set of two labels per subject, one for the NIR images and one for the VIS images, are used for training a NIR-VIS face recognition model. The classification of images into different domains reduces the intra-class variation and lessens the negative impact of data imbalance in training. To train a network with sets of labels, we introduce a domain-based angular margin loss and a maximum angular loss to maintain the inter-class discrepancy and to enforce the close relationship of labels in a set. Quantitative experiments confirm that DBLFace significantly improves the rank-1 identification rate by 6.7% on the EDGE20 dataset and achieves state-of-the-art performance on the CASIA NIR-VIS 2.0 dataset.",2020,ArXiv,2010.03771,,https://arxiv.org/pdf/2010.03771.pdf
173a413d286fd204c245497b135ecf2311a9c9f1,1,[D10],,1,1,A New Deep Neural Architecture Search Pipeline for Face Recognition,"With the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature with simple detection methods and good recognition accuracy. Face recognition is one of the main functions of electronic equipment propaganda. The previous work in this field mainly focused on converting loss function in traditional deep convolution neural networks without changing the network structure. With the development of AutoML, neural architecture search (NAS) has shown remarkable performance in image classification tasks. In this paper, we first propose a new deep neural architecture search pipeline combined with NAS technology and reinforcement learning strategy into face recognition. We quote the framework of NAS, which trains the child and controller networks alternately. At the same time, we optimize NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize the policy gradient algorithm to search the architecture automatically with the cross-entropy loss. The network architectures we searched out have achieved state-of-the-art accuracy in the large-scale face dataset, which achieved 98.77% top-1 in the MS-Celeb-1M dataset and 99.89% in LFW dataset with relatively small network size.",2020,IEEE Access,,10.1109/ACCESS.2020.2994207,
17a51f44b43098f629f80f9481fb1875ccc1604e,1,[D9],,1,0,Addressing Model Vulnerability to Distributional Shifts Over Image Transformation Sets,"We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1903.119,10.1109/ICCV.2019.00807,https://arxiv.org/pdf/1903.11900.pdf
195a124a5e21bf72fb096005aff72a55daee0f7a,0,,,0,1,Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing,"Face anti-spoofing aims to prevent 0 authentications of face recognition systems by distinguishing whether an image is originated from a human face or a spoof medium. We propose a novel method called Doubly Adversarial Suppression Network (DASN) for domain-agnostic face antispoofing; DASN improves the generalization ability to unseen domains by learning to effectively suppress spoofirrelevant factors (SiFs) (e.g., camera sensors, illuminations). To achieve our goal, we introduce two types of adversarial learning schemes. In the first adversarial learning scheme, multiple SiFs are suppressed by deploying multiple discrimination heads that are trained against an encoder. In the second adversarial learning scheme, each of the discrimination heads is also adversarially trained to suppress a spoof factor, and the group of the secondary spoof classifier and the encoder aims to intensify the spoof factor by overcoming the suppression. We evaluate the proposed method on four public benchmark datasets, and achieve remarkable evaluation results. The results demonstrate the effectiveness of the proposed method.",2020,ArXiv,2012.01271,,https://arxiv.org/pdf/2012.01271.pdf
19fdaeff329c54bcffaf2e858d5a2584c229cb64,1,[D9],,1,1,Voice-Face Cross-modal Matching and Retrieval: A Benchmark,"Cross-modal associations between voice and face from a person can be learnt algorithmically, which can benefit a lot of applications. The problem can be defined as voice-face matching and retrieval tasks. Much research attention has been paid on these tasks recently. However, this research is still in the early stage. Test schemes based on random tuple mining tend to have low test confidence. Generalization ability of models can not be evaluated by small scale datasets. Performance metrics on various tasks are scarce. A benchmark for this problem needs to be established. In this paper, first, a framework based on comprehensive studies is proposed for voice-face matching and retrieval. It achieves state-of-the-art performance with various performance metrics on different tasks and with high test confidence on large scale datasets, which can be taken as a baseline for the follow-up research. In this framework, a voice anchored L2-Norm constrained metric space is proposed, and cross-modal embeddings are learned with CNN-based networks and triplet loss in the metric space. The embedding learning process can be more effective and efficient with this strategy. Different network structures of the framework and the cross language transfer abilities of the model are also analyzed. Second, a voice-face dataset (with 1.15M face data and 0.29M audio data) from Chinese speakers is constructed, and a convenient and quality controllable dataset collection tool is developed. The dataset and source code of the paper will be published together with this paper.",2019,ArXiv,1911.09338,,https://arxiv.org/pdf/1911.09338.pdf
1bab0f5df6fc9506e922dcf6e2dee3cc8ad8a427,1,[D9],,1,1,Automatic Quality Assessment for Audio-Visual Verification Systems. The LOVe submission to NIST SRE Challenge 2019,"Fusion of scores is a cornerstone of multimodal biometric systems composed of independent unimodal parts. In this work, we focus on quality-dependent fusion for speaker-face verification. To this end, we propose a universal model which can be trained for automatic quality assessment of both face and speaker modalities. This model estimates the quality of representations produced by unimodal systems which are then used to enhance the score-level fusion of speaker and face verification modules. We demonstrate the improvements brought by this quality-dependent fusion on the recent NIST SRE19 Audio-Visual Challenge dataset.",2020,INTERSPEECH,2008.05889,10.21437/interspeech.2020-1434,https://arxiv.org/pdf/2008.05889.pdf
1bd2fba7083829042d0ba0765dbed8ec692cb335,1,"[D9], [M3]",,1,1,Towards Gender-Neutral Face Descriptors for Mitigating Bias in Face Recognition,"State-of-the-art deep networks implicitly encode gender information while being trained for face recognition. Gender is often viewed as an important attribute with respect to identifying faces. However, the implicit encoding of gender information in face descriptors has two major issues: (a.) It makes the descriptors susceptible to privacy leakage, i.e. a malicious agent can be trained to predict the face gender from such descriptors. (b.) It appears to contribute to gender bias in face recognition, i.e. we find a significant difference in the recognition accuracy of DCNNs on male and female faces. Therefore, we present a novel `Adversarial Gender De-biasing algorithm (AGENDA)' to reduce the gender information present in face descriptors obtained from previously trained face recognition networks. We show that AGENDA significantly reduces gender predictability of face descriptors. Consequently, we are also able to reduce gender bias in face verification while maintaining reasonable recognition performance.",2020,,2006.07845,,https://arxiv.org/pdf/2006.07845.pdf
1bf011a0a5c471d98c26134a2e67d573b8601c8f,0,,,0,1,Transfer of Pretrained Model Weights Substantially Improves Semi-supervised Image Classification,"Deep neural networks produce state-of-the-art results when trained on a large number of labeled examples but tend to overfit when small amounts of labeled examples are used for training. Creating a large number of labeled examples requires considerable resources, time, and effort. If labeling new data is not feasible, so-called semi-supervised learning can achieve better generalisation than purely supervised learning by employing unlabeled instances as well as labeled ones. The work presented in this paper is motivated by the observation that transfer learning provides the opportunity to potentially further improve performance by exploiting models pretrained on a similar domain. More specifically, we explore the use of transfer learning when performing semi-supervised learning using self-learning. The main contribution is an empirical evaluation of transfer learning using different combinations of similarity metric learning methods and label propagation algorithms in semi-supervised learning. We find that transfer learning always substantially improves the model’s accuracy when few labeled examples are available, regardless of the type of loss used for training the neural network. This finding is obtained by performing extensive experiments on the SVHN, CIFAR10, and Plant Village image classification datasets and applying pretrained weights from Imagenet for transfer learning.",2020,Australasian Conference on Artificial Intelligence,,10.1007/978-3-030-64984-5_34,https://www.cs.waikato.ac.nz/~eibe/pubs/Transfer_Learning_camera_ready.pdf
1ca63ee80e1d8a94a0440c2d777f67bfde4c95cd,0,,,1,0,Compact Web Video Summarization Via Supervised Learning,"Ever growing consumption of online videos from search, recommendation, and sharing has generated a strong demand on compact summarization, to allow users to quickly understand the video content and make the whether-to-watch decision. This paper explores achieving this using a compact set of four thumbnails, via supervised learning methods. Due to the ubiquitous disagreements among the thumbnail sets preferred by different labelers, there exists no unique ground truth set. To address this problem, we propose the pair wise ranking method, which trains the model to best predict the user preference over each pair of thumbnail set candidates. Experimental results on a large video dataset showed that the proposed method outperforms the existing schemes by a large margin.",2018,2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),,10.1109/ICMEW.2018.8551530,
1e987075566eb86f8fabab2664fd12ca0fef78aa,1,[D9],,1,0,Comparison of Face Recognition Loss Functions,"Significant progresses have been made to face recognition algorithms in recent years. The progresses include the improvements of the solutions and the availability of more challenging databases. As the performance on previous benchmark databases, such as MPIE and LFW, saturates, more challenging databases are emerging and keep driving the advancements of face recognition solutions. The loss function considered in the solution usually plays the most critical role. We compare a few latest loss functions with the same feature embedding network by evaluating their performance on two recently-released databases, the IARPA Janus BenchmarkB (IJB-B) [1], and IARPA Janus BenchmarkC (IJB-C) [2], and highlight the directions for the next phase research.",2019,2019 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS),,10.1109/ISPACS48206.2019.8986257,
1ee0da73fb2c576686630b01fffa475e5ec2fbae,0,,,1,0,Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning,"To address the problem of data inconsistencies among different facial expression recognition (FER) datasets, many cross-domain FER methods (CD-FERs) have been extensively devised in recent years. Although each declares to achieve superior performance, fair comparisons are lacking due to the inconsistent choices of the source/target datasets and feature extractors. In this work, we first analyze the performance effect caused by these inconsistent choices, and then re-implement some well-performing CD-FER and recently published domain adaptation algorithms. We ensure that all these algorithms adopt the same source datasets and feature extractors for fair CD-FER evaluations. We find that most of the current leading algorithms use adversarial learning to learn holistic domain-invariant features to mitigate domain shifts. However, these algorithms ignore local features, which are more transferable across different datasets and carry more detailed content for fine-grained adaptation. To address these issues, we integrate graph representation propagation with adversarial learning for cross-domain holistic-local feature co-adaptation by developing a novel adversarial graph representation adaptation (AGRA) framework. Specifically, it first builds two graphs to correlate holistic and local regions within each domain and across different domains, respectively. Then, it extracts holistic-local features from the input image and uses learnable per-class statistical distributions to initialize the corresponding graph nodes. Finally, two stacked graph convolution networks (GCNs) are adopted to propagate holistic-local features within each domain to explore their interaction and across different domains for holistic-local feature co-adaptation. We conduct extensive and fair evaluations on several popular benchmarks and show that the proposed AGRA framework outperforms previous state-of-the-art methods.",2020,,2008.00923,,https://arxiv.org/pdf/2008.00923.pdf
22b52d3f18eaf43993a3a91053f5efe6267144e7,0,,,0,1,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification,"Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.",2020,ICLR,2001.01526,,https://arxiv.org/pdf/2001.01526.pdf
23082d047f38cc3c17bf24c761aa052d70ae899d,1,[D9],,1,0,SEMANTICADV: GENERATING ADVERSARIAL EXAM-,"Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation"" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate “unrestricted adversarial examples"". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various “adversarial"" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.",2019,,,,https://pdfs.semanticscholar.org/7723/2266b32fdd1c6ad33129bf0e04cfd59a2d88.pdf
2391b8cf30f1ec99798672d20203c791b2d0a622,1,[D9],,1,1,Deep class-skewed learning for face recognition,"Abstract Face datasets often exhibit highly-skewed class distribution, i.e., rich classes contain a plenty amount of instances, while only few images belong to poor classes. To mitigate this issue, we explore deep class-skewed learning from two aspects in this paper: feature augmentation and feature normalization. To deal with the imbalance distribution problem, we put forward a novel feature augmentation method termed Large Margin Feature Augmentation (LMFA) to augment hard features and equalize class distribution, leading to balanced classification boundaries between rich and poor classes. By considering the distribution gap between training and testing features, A novel feature normalization called Transferable Domain Normalization (TDN) is proposed to normalize domain-specific features to obey an identical Gaussian distribution, and enhance the feature generalization. Extensive experiments are conducted on five popular face recognition datasets including LFW, YTF, CFP, AgeDB and MegaFace. We achieve remarkable results on par with or better than the state-of-the-art methods, which demonstrate the effectiveness of our proposed learning class-balanced features.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.04.085,
23fc6825255d9dae460848cea213c0988a964096,1,,1,1,1,On Improving the Generalization of Face Recognition in the Presence of Occlusions,"In this paper, we address a key limitation of existing 2D face recognition methods: robustness to occlusions. To accomplish this task, we systematically analyzed the impact of facial attributes on the performance of a state-of-the-art face recognition method and through extensive experimentation, quantitatively analyzed the performance degradation under different types of occlusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach learned discriminative facial templates despite the presence of such occlusions. First, an attention mechanism was proposed that extracted local identity-related region. The local features were then aggregated with the global representations to form a single template. Second, a simple, yet effective, training strategy was introduced to balance the non-occluded and occluded facial images. Extensive experiments demonstrated that OREO improved the generalization ability of face recognition under occlusions by 10.17% in a single-image-based setting and outperformed the baseline by approximately 2% in terms of rank-1 accuracy in an image-set-based scenario.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2006.06787,10.1109/CVPRW50498.2020.00407,https://arxiv.org/pdf/2006.06787.pdf
24f2e1316c5d6c2042956d19063b4491014307c2,0,,,0,1,BSF-RCNN-VFR: Background Subtracted Faster RCNN for Video based Face Recognition,"Surveillance systems are widely deployed in various organization and public palaces to monitor suspicious activities and reduce the crime rate. Recently, visual surveillance systems has gained huge attraction from research community due to their significant impact on monitoring application. Several techniques have been developed which are based on the still image which do not provide efficient solution for real-time application. Hence, video based face recognition is considered as a tedious task. Recently, deep learning based schemes have been adopted widely for video face recognition but these techniques suffer from well-known challenges such as pose and illumination variation. Hence, we present a Convolutional Neural Network (CNN) based approach for video face recognition. According to the proposed approach, we employ background subtraction scheme which helps to reduce the scene complexity and improves the feature extraction process. Later CNN based face detection scheme is developed which uses region proposal generation networks. In this phase, we incorporate bounding box regression model to reduce the face detection error. Finally, RCNN based learning model is applied which uses Joint Bayesian learning to discriminate the classes of detected faces. Based on these stages, the proposed model is named as Background subtracted Faster RCNN for Video based Face Recognition (BSF-RCNN-VFR). An experimental study is carried out based on the proposed method where we use publically available datasets such as YouTube celebrity dataset, Buffy dataset and YouTube face dataset. The experimental study shows a significant improvement in the detection and recognition accuracy process. Keywords— Face recognition, object detection, deep learning,",2019,,,,
25a199c6544b48120911c5bceaa54068ca1bd51c,0,,,1,0,Images as Data for Social Science Research: An Introduction to Convolutional Neural Nets for Image Classification,,2020,,,10.1017/9781108860741,
26d5be8308e3d138dbdbb3adb39bfd4abce0c05f,1,[M3],,0,1,Diagonal Symmetric Pattern-Based Illumination Invariant Measure for Severe Illumination Variation Face Recognition,"The center symmetric pattern (CSP) was widely used in the local binary pattern based facial feature, whereas never used to develop the illumination invariant measure in the literature. This paper proposes a novel diagonal symmetric pattern (DSP) to develop the illumination invariant measure for severe illumination variation face recognition. Firstly, the subtraction of two diagonal symmetric pixels is defined as the DSP unit in the face local region, which may be positive or negative. The DSP model is obtained by combining the positive and negative DSP units in the even <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> even block region. Then, the DSP model can be used to generate several DSP images based on the <inline-formula> <tex-math notation=""LaTeX"">$2\times 2$ </tex-math></inline-formula> block or the <inline-formula> <tex-math notation=""LaTeX"">$4\times 4$ </tex-math></inline-formula> block by controlling the proportions of positive and negative DSP units, which results in the DSP2 image or the DSP4 image. The single DSP2 or DSP4 image with the arctangent function can develop the DSP2-face or the DSP4-face. Multi DSP2 or DSP4 images employ the extended sparse representation classification (ESRC) as the classifier that can form the DSP2 images based classification (DSP2C) or the DSP4 images based classification (DSP4C). Further, the DSP model is integrated with the pre-trained deep learning (PDL) model to construct the DSP-PDL model. Finally, the experimental results on the Extended Yale B, CMU PIE, AR, and VGGFace2 face databases indicate that the proposed methods are efficient to tackle severe illumination variations.",2020,IEEE Access,,10.1109/ACCESS.2020.2983837,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09049393.pdf
26d8293b6f94d951f8cd79e2dc6d6ebb8212fd2c,1,[D14],,1,1,Mitigating Bias in Face Recognition Using Skewness-Aware Reinforcement Learning,"Racial equality is an important theme of international human rights law, but it has been largely obscured when the overall face recognition accuracy is pursued blindly. More facts indicate racial bias indeed degrades the fairness of recognition system and the error rates on non-Caucasians are usually much higher than Caucasians. To encourage fairness, we introduce the idea of adaptive margin to learn balanced performance for different races based on large margin losses. A reinforcement learning based race balance network (RL-RBN) is proposed. We formulate the process of finding the optimal margins for non-Caucasians as a Markov decision process and employ deep Q-learning to learn policies for an agent to select appropriate margin by approximating the Q-value function. Guided by the agent, the skewness of feature scatter between races can be reduced. Besides, we provide two ethnicity aware training datasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be utilized to study racial bias from both data and algorithm aspects. Extensive experiments on RFW database show that RL-RBN successfully mitigates racial bias and learns more balanced performance.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1911.10692,10.1109/cvpr42600.2020.00934,https://arxiv.org/pdf/1911.10692.pdf
28f02e8668f5b9ed1e192c433506b61b0b53b7a9,0,,,0,1,Face Memorization Using AIM Model for Mobile Robot and Its Application to Name Calling Function,"We are developing a social mobile robot that has a name calling function using a face memorization system. It is said that it is an important function for a social robot to call to a person by her/his name, and the name calling can make a friendly impression of the robot on her/him. Our face memorization system has the following features: (1) When the robot detects a stranger, it stores her/his face images and name after getting her/his permission. (2) The robot can call to a person whose face it has memorized by her/his name. (3) The robot system has a sleep–wake function, and a face classifier is re-trained in a REM sleep state, or execution frequencies of information processes are reduced when it has nothing to do, for example, when there is no person around the robot. In this paper, we confirmed the performance of these functions and conducted an experiment to evaluate the impression of the name calling function with research participants. The experimental results revealed the validity and effectiveness of the proposed face memorization system.",2020,Sensors,,10.3390/s20226629,https://pdfs.semanticscholar.org/5e8a/1f07229a0e73a81cd48a0ac3a0d16b198d2a.pdf
2ad5f836e1d9876fa3fc53cb2c0a704b45988f0f,0,,,0,1,Negative Margin Matters: Understanding Margin in Few-shot Classification,"This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles. These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid 0ly mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at this https URL.",2020,ECCV,2003.1206,10.1007/978-3-030-58548-8_26,https://arxiv.org/pdf/2003.12060.pdf
2b2acf2de016f0fb3538ceaaf3a9ba869b466089,0,,,1,0,Finding your Lookalike: Measuring Face Similarity Rather than Face Identity,"Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1806.05252,10.1109/CVPRW.2018.00311,https://arxiv.org/pdf/1806.05252.pdf
2be87b5d2b61ea629e9ffd07a23c9ee8c4b0cc3e,0,,,0,1,A Backbone Replaceable Fine-tuning Network for Stable Face Alignment,"Heatmap regression based face alignment has achieved prominent performance on static images. However, the stability and accuracy are remarkably discounted when applying the existing methods on dynamic videos. We attribute the degradation to random noise and motion blur, which are common in videos. The temporal information is critical to address this issue yet not fully considered in the existing works. In this paper, we visit the video-oriented face alignment problem in two perspectives: detection accuracy prefers lower error for a single frame, and detection consistency forces better stability between adjacent frames. On this basis, we propose a Jitter loss function that leverages temporal information to suppress inaccurate as well as jittered landmarks. The Jitter loss is involved in a novel framework with a fine-tuning ConvLSTM structure over a backbone replaceable network. We further demonstrate that accurate and stable landmarks are associated with different regions with overlaps in a canonical coordinate, based on which the proposed Jitter loss facilitates the optimization process during training. The proposed framework achieves at least 40% improvement on stability evaluation metrics while enhancing detection accuracy versus state-of-the-art methods. Generally, it can swiftly convert a landmark detector for facial images to a better-performing one for videos without retraining the entire model.",2020,ArXiv,,,
2c2c31cf8ad0d68549a68e856c5ae723ad9482b3,1,"[D9], [D15]",,1,1,Learning Meta Face Recognition in Unseen Domains,"Face recognition systems are usually faced with unseen domains in real-world applications and show unsatisfactory performance due to their poor generalization. For example, a well-trained model on webface data cannot deal with the ID vs. Spot task in surveillance scenario. In this paper, we aim to learn a generalized model that can directly handle new unseen domains without any model updating. To this end, we propose a novel face recognition method via meta-learning named Meta Face Recognition (MFR). MFR synthesizes the source/target domain shift with a meta-optimization objective, which requires the model to learn effective representations not only on synthesized source domains but also on synthesized target domains. Specifically, we build domain-shift batches through a domain-level sampling strategy and get back-propagated gradients/meta-gradients on synthesized source/target domains by optimizing multi-domain distributions. The gradients and meta-gradients are further combined to update the model to improve generalization. Besides, we propose two benchmarks for generalized face recognition evaluation. Experiments on our benchmarks validate the generalization of our method compared to several baselines and other state-of-the-arts. The proposed benchmarks and code will be available at https://github.com/cleardusk/MFR.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.07733,10.1109/cvpr42600.2020.00620,https://arxiv.org/pdf/2003.07733.pdf
2d5829f085b55c7a5ea7878ffec44dcbe40188f2,0,,,1,0,Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition,"A recent trend to recognize facial expressions in the real-world scenario is to deploy attention based convolutional neural networks (CNNs) locally to signify the importance of facial regions and, combine it with global facial features and/or other complementary context information for performance gain. However, in the presence of occlusions and pose variations, different channels respond differently, and further that the response intensity of a channel differ across spatial locations. Also, modern facial expression recognition(FER) architectures rely on external sources like landmark detectors for defining attention. Failure of landmark detector will have a cascading effect on FER. Additionally, there is no emphasis laid on the relevance of features that are input to compute complementary context information. Leveraging on the aforementioned observations, an end-to-end architecture for FER is proposed in this work that obtains both local and global attention per channel per spatial location through a novel spatio-channel attention net (SCAN), without seeking any information from the landmark detectors. SCAN is complemented by a complementary context information (CCI) branch. Further, using efficient channel attention (ECA), the relevance of features input to CCI is also attended to. The representation learnt by the proposed architecture is robust to occlusions and pose variations. Robustness and superior performance of the proposed model is demonstrated on both in-lab and in-the-wild datasets (AffectNet, FERPlus, RAF-DB, FED-RO, SFEW, CK+, Oulu-CASIA and JAFFE) along with a couple of constructed face mask datasets resembling masked faces in COVID-19 scenario. Codes are publicly available at this https URL",2020,ArXiv,2007.10298,,https://arxiv.org/pdf/2007.10298.pdf
2e0d56794379c436b2d1be63e71a215dd67eb2ca,1,[D16],,1,0,Improving precision and recall of face recognition in SIPP with combination of modified mean search and LSH,"Although face recognition has been improved much as the development of Deep Neural Networks, SIPP(Single Image Per Person) problem in face recognition has not been better solved, especially in practical applications where searching over complicated database. In this paper, a combination of modified mean search and LSH method would be introduced orderly to improve the precision and recall of SIPP face recognition without retrain of the DNN model. First, a modified SVD based augmentation method would be introduced to get more intra-class variations even for person with only one image. Second, an unique rule based combination of modified mean search and LSH method was proposed the first time to help get the most similar personID in a complicated dataset, and some theoretical explaining followed. Third, we would like to emphasize, no need to retrain of the DNN model and would easy to be extended without much efforts. We do some practical testing in competition of Msceleb challenge-2 2017 which was hold by Microsoft Research, great improvement of coverage from 13.39% to 19.25%, 29.94%, 42.11%, 47.52% at precision 99%(P99) would be shown latter, coverage reach 94.2% and 100% at precision 97%(P97) and 95%(P95) respectively. As far as we known, this is the only paper who do not fine-tuning on competition dataset and ranked top-10. A similar test on CASIA WebFace dataset also demonstrated the same improvements on both precision and recall.",2017,,,,https://pdfs.semanticscholar.org/2e0d/56794379c436b2d1be63e71a215dd67eb2ca.pdf
2e71ead0ca8d5583c99f311e8bf5403995f468ac,0,,,1,1,Threat of Adversarial Attacks on Face Recognition: A Comprehensive Survey,"Face recognition (FR) systems have demonstrated outstanding verification performance, suggesting suitability for real-world applications, ranging from photo tagging in social media to automated border control (ABC). In an advanced FR system with deep learning-based architecture, however, promoting the recognition efficiency alone is not sufficient and the system should also withstand potential kinds of attacks designed to target its proficiency. Recent studies show that (deep) FR systems exhibit an intriguing vulnerability to imperceptible or perceptible but natural-looking adversarial input images that drive the model to incorrect output predictions. In this article, we present a comprehensive survey on adversarial attacks against FR systems and elaborate on the competence of new countermeasures against them. Further, we propose a taxonomy of existing attack and defense strategies according to different criteria. Finally, we compare the presented approaches according to techniques' characteristics.",2020,ArXiv,2007.11709,,https://arxiv.org/pdf/2007.11709.pdf
2f75f27ef8f239489437dfe48bc29afc3b24bd71,0,,,0,1,Simulation of Print-Scan Transformations for Face Images based on Conditional Adversarial Networks,"In many countries, printing and scanning of face images is frequently performed as part of the issuance process of electronic travel documents, e.g., ePassports. Image alterations induced by such print-scan transformations may negatively effect the performance of various biometric sub-systems, in particular image manipulation detection. Consequently, according training data is needed in order to achieve robustness towards said transformations. However, manual printing and scanning is time-consuming and costly.In this work, we propose a simulation of print-scan transformations for face images based on a Conditional Generative Adversarial Network (cGAN). To this end, subsets of two public face databases are manually printed and scanned using different printer-scanner combinations. A cGAN is then trained to perform an image-to-image translation which simulates the corresponding print-scan transformations. The goodness of simulation is evaluated with respect to image quality, biometric sample quality and performance, as well as human assessment.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
3002c16de1027be0911ba2642811c68d6059d37a,0,,,0,1,Boosting Network Weight Separability via Feed-Backward Reconstruction,"This paper proposes a new evaluation metric and a boosting method for weight separability in neural network design. In contrast to general visual recognition methods designed to encourage both intra-class compactness and inter-class separability of latent features, we focus on estimating linear independence of column vectors in weight matrix and improving the separability of weight vectors. To this end, we propose an evaluation metric for weight separability based on semi-orthogonality of a matrix, Frobenius distance, and the feed-backward reconstruction loss, which explicitly encourages weight separability between the column vectors in the weight matrix. The experimental results on image classification and face recognition demonstrate that the weight separability boosting via minimization of feed-backward reconstruction loss can improve the visual recognition performance, hence universally boosting the performance on various visual recognition tasks.",2020,IEEE Access,1910.09024,10.1109/ACCESS.2020.3041470,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09274406.pdf
32873f6111963607d3f768f4685fe8137fdd1253,1,[D10],,1,1,Learning to Cluster Faces on an Affinity Graph,"Face recognition sees remarkable progress in recent years, and its performance has reached a very high level. Taking it to a next level requires substantially larger data, which would involve prohibitive annotation cost. Hence, exploiting unlabeled data becomes an appealing alternative. Recent works have shown that clustering unlabeled faces is a promising approach, often leading to notable performance gains. Yet, how to effectively cluster, especially on a large-scale (i.e. million-level or above) dataset, remains an open question. A key challenge lies in the complex variations of cluster patterns, which make it difficult for conventional clustering methods to meet the needed accuracy. This work explores a novel approach, namely, learning to cluster instead of relying on hand-crafted criteria. Specifically, we propose a framework based on graph convolutional network, which combines a detection and a segmentation module to pinpoint face clusters. Experiments show that our method yields significantly more accurate face clusters, which, as a result, also lead to further performance gain in face recognition.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.02749,10.1109/CVPR.2019.00240,https://arxiv.org/pdf/1904.02749.pdf
32b2e1c54c9f13e21611f68241bcb613b5c715ed,0,,,1,1,Balanced Alignment for Face Recognition: A Joint Learning Approach,"Face alignment is crucial for face recognition and has been widely adopted. However, current practice is too simple and under-explored. There lacks an understanding of how important face alignment is and how it should be performed, for recognition. This work studies these problems and makes two contributions. First, it provides an in-depth and quantitative study of how alignment strength affects recognition accuracy. Our results show that excessive alignment is harmful and an optimal balanced point of alignment is in need. To strike the balance, our second contribution is a novel joint learning approach where alignment learning is controllable with respect to its strength and driven by recognition. Our proposed method is validated by comprehensive experiments on several benchmarks, especially the challenging ones with large pose.",2020,ArXiv,2003.10168,,https://arxiv.org/pdf/2003.10168.pdf
339d0690f88b01eba162ab1b3244a3db4924d258,0,,,0,1,Generating and Validating DSA Private Keys from Online Face Images for Digital Signatures,"Signing digital documents is attracting more attention in recent years, according to the rapidly growing number of digital documents being exchanged online. The digital signature proves the authenticity of the document and the sender’s approval on the contents of the document. However, storing the private keys of users for digital signing imposes threats toward gaining unauthorized access, which can result in producing 0 signatures. Thus, in this paper, a novel approach is proposed to extract the private component of the key used to produce the digital signature from online face image. Hence, this private component is never stored in any database, so that, 0 signatures cannot be produced and the sender’s approval cannot be denied. The proposed method uses a convolutional neural network that is trained using a semi-supervised approach, so that, the values used for the training are extracted based on the predictions of the neural network. To avoid the need for training a complex neural network, the proposed neural network makes use of existing pretrained neural networks, that already have the knowledge about the distinctive features in the faces. The use of the MTCNN for face detection and Facenet for face recognition, in addition to the proposed neural network, to achieved the best performance. The performance of the proposed method is evaluated using the Colored FERET Faces Database Version 2 and has achieved robustness rate of 13.48% and uniqueness of 100%.",2019,,,10.18517/IJASEIT.9.3.8950,
36cd62031d7cf516f89cf10b40220d33ca66c87a,0,,,0,1,DropClass and DropAdapt: Dropping classes for deep speaker representation learning,"Many recent works on deep speaker embeddings train their feature extraction networks on large classification tasks, distinguishing between all speakers in a training set. Empirically, this has been shown to produce speaker-discriminative embeddings, even for unseen speakers. However, it is not clear that this is the optimal means of training embeddings that generalize well. This work proposes two approaches to learning embeddings, based on the notion of dropping classes during training. We demonstrate that both approaches can yield performance gains in speaker verification tasks. The first proposed method, DropClass, works via periodically dropping a random subset of classes from the training data and the output layer throughout training, resulting in a feature extractor trained on many different classification tasks. Combined with an additive angular margin loss, this method can yield a 7.9% relative improvement in equal error rate (EER) over a strong baseline on VoxCeleb. The second proposed method, DropAdapt, is a means of adapting a trained model to a set of enrolment speakers in an unsupervised manner. This is performed by fine-tuning a model on only those classes which produce high probability predictions when the enrolment speakers are used as input, again also dropping the relevant rows from the output layer. This method yields a large 13.2% relative improvement in EER on VoxCeleb. The code for this paper has been made publicly available.",2020,ArXiv,2002.00453,,https://arxiv.org/pdf/2002.00453.pdf
3827f1cab643a57e3cd22fbffbf19dd5e8a298a8,1,[D16],,1,0,One-Shot Face Recognition via Generative Learning,"One-shot face recognition measures the ability to recognize persons with only seeing them once, which is a hallmark of human visual intelligence. It is challenging for existing machine learning approaches to mimic this way, since limited data cannot well represent the data variance. To this end, we propose to build a large-scale face recognizer, which is capable to fight off the data imbalance difficulty. To seek a more effective general classifier, we develop a novel generative model attempting to synthesize meaningful data for one-shot classes by adapting the data variances from other normal classes. Specifically, we formulate conditional generative adversarial networks and the general Softmax classifier into a unified framework. Such a two-player minimax optimization can guide the generation of more effective data, which benefit the classifier learning for one-shot classes. The experimental results on a large-scale face benchmark with 21K persons verify the effectiveness of our proposed algorithm in one-shot classification, as our generative model significantly improves the recognition coverage rate from 25:65% to 94:84% at the precision of 99% for the one-shot classes, while still keeps an overall Top-1 accuracy at 99:80% for the normal classes.",2018,2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018),,10.1109/FG.2018.00011,
3933e323653ff27e68c3458d245b47e3e37f52fd,1,[M5],,1,0,Evaluation of a 3 D-aided Pose Invariant 2 D Face Recognition System,"A few well-developed face recognition pipelines have been reported in recent years. Most of the face-related work focuses on a specific module or demonstrates a research idea. In this paper, we present a pose-invariant 3D-aided 2D face recognition system (3D2D-PIFR) that is robust to pose variations as large as 90◦ by leveraging deep learning technology. We describe the architecture and the interface of 3D2D-PIFR, and introduce each module in detail (Code for module algorithms are kindly provided for the authors and their institution). Experiments are conducted on the UHDB31 and IJB-A, demonstrating that 3D2D-PIFR outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on UHDB31 and 3% on IJB-A dataset in average. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques. A video demo of 3D2D-PIFR is available at http://cbl. uh.edu/index.php/pages/research/demos.",2017,,,,https://pdfs.semanticscholar.org/3933/e323653ff27e68c3458d245b47e3e37f52fd.pdf
397689cdbf66e87788d936be0c906fcdc669c409,0,,,0,1,THIN: THrowable Information Networks and Application for Facial Expression Recognition In The Wild,"For a number of tasks solved using deep learning techniques, an exogenous variable can be identified such that (a) it heavily influences the appearance of the different classes, and (b) an ideal classifier should be invariant to this variable. An example of such exogenous variable is identity if facial expression recognition (FER) is considered. In this paper, we propose a dual exogenous/endogenous representation. The former captures the exogenous variable whereas the second one models the task at hand (e.g. facial expression). We design a prediction layer that uses a deep ensemble conditioned by the exogenous representation. It employs a differential tree gate that learns an adaptive weak predictor weighting, therefore modeling a partition of the exogenous representation space, upon which the weak predictors specialize. This layer explicitly models the dependency between the exogenous variable and the predicted task (a). We also propose an exogenous dispelling loss to remove the exogenous information from the endogenous representation, enforcing (b). Thus, the exogenous information is used two times in a throwable fashion, first as a conditioning variable for the target task, and second to create invariance within the endogenous representation. We call this method THIN, standing for THrowable Information Networks. We experimentally validate THIN in several contexts where an exogenous information can be identified, such as digit recognition under large rotations and shape recognition at multiple scales. We also apply it to FER with identity as the exogenous variable. In particular, we demonstrate that THIN significantly outperforms state-of-the-art approaches on several challenging datasets.",2020,ArXiv,2010.07614,,https://arxiv.org/pdf/2010.07614.pdf
3998f7d6022f67a1cf6050bccd3131911fec5a8a,1,[D14],,1,1,Self Residual Attention Network for Deep Face Recognition,"Discriminative feature embedding is of essential importance in the field of large scale face recognition. In this paper, we propose a self residual attention-based convolutional neural network (SRANet) for discriminative face feature embedding, which aims to learn the long-range dependencies of face images by decreasing the information redundancy among channels and focusing on the most informative components of spatial feature maps. More specifically, the proposed attention module consists of the self channel attention (SCA) block and self spatial attention (SSA) block which adaptively aggregates the feature maps in both channel and spatial domains to learn the inter-channel relationship matrix and the inter-spatial relationship matrix; moreover, matrix multiplications are conducted for a refined and robust face feature. With the attention module we proposed, we can make standard convolutional neural networks (CNNs), such as ResNet-50 and ResNet-101, which have more discriminative power for deep face recognition. The experiments on Labelled Faces in the Wild (LFW), Age Database (AgeDB), Celebrities in Frontal Profile (CFP), and MegaFace Challenge 1 (MF1) show that our proposed SRANet structure consistently outperforms naive CNNs and achieves state-of-the-art performance.",2019,IEEE Access,,10.1109/ACCESS.2019.2913205,
39b615c73810e13998df3df9d5e73aebd3e67dab,0,,,1,0,A Compact Embedding for Facial Expression Similarity,"Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between expressions measured in the action unit space need not correspond to how humans perceive expression similarity. Different from previous work, our goal is to describe facial expressions in a continuous fashion using a compact embedding space that mimics human visual preferences. To achieve this goal, we collect a large-scale faces-in-the-wild dataset with human annotations in the form: Expressions A and B are visually more similar when compared to expression C, and use this dataset to train a neural network that produces a compact (16-dimensional) expression embedding. We experimentally demonstrate that the learned embedding can be successfully used for various applications such as expression retrieval, photo album summarization, and emotion recognition. We also show that the embedding learned using the proposed dataset performs better than several other embeddings learned using existing emotion or action unit datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1811.11283,10.1109/CVPR.2019.00583,https://arxiv.org/pdf/1811.11283.pdf
39f516aa3b575c9e457ac53ac8013e31f9010982,1,"[M5], [M3]",,1,1,Beyond Identity: What Information Is Stored in Biometric Face Templates?,"Deeply-learned face representations enable the success of current face recognition systems. Despite the ability of these representations to encode the identity of an individual, recent works have shown that more information is stored within, such as demographics, image characteristics, and social traits. This threatens the user's privacy, since for many applications these templates are expected to be solely used for recognition purposes. Knowing the encoded information in face templates helps to develop bias-mitigating and privacy-preserving face recognition technologies. This work aims to support the development of these two branches by analysing face templates regarding 113 attributes. Experiments were conducted on two publicly available face embeddings. For evaluating the predictability of the attributes, we trained a massive attribute classifier that is additionally able to accurately state its prediction confidence. This allows us to make more sophisticated statements about the attribute predictability. The results demonstrate that up to 74 attributes can be accurately predicted from face templates. Especially non-permanent attributes, such as age, hairstyles, haircolors, beards, and various accessories, found to be easily-predictable. Since face recognition systems aim to be robust against these variations, future research might build on this work to develop more understandable privacy preserving solutions and build robust and fair face templates.",2020,ArXiv,2009.09918,,https://arxiv.org/pdf/2009.09918.pdf
3beb1528ce4770ed5b63e424202acc981aeaf149,1,[D17],,1,0,Deep convolutional neural networks in the face of caricature,"Real-world face recognition requires us to perceive the uniqueness of a face across variable images. Deep convolutional neural networks (DCNNs) accomplish this feat by generating robust face representations that can be analysed in a multidimensional ‘face space’. We examined the organization of viewpoint, illumination, gender and identity in this space. We found that DCNNs create a highly organized face similarity structure in which identities and images coexist. Natural image variation is organized hierarchically, with face identity nested under gender, and illumination and viewpoint nested under identity. To examine identity, we caricatured faces and found that identification accuracy increased with the strength of identity information in a face, and caricature representations ‘resembled’ their veridical counterparts—mimicking human perception. DCNNs therefore offer a theoretical framework for reconciling decades of behavioural and neural results that emphasized either the image or the face in representations, without understanding how a neural code could seamlessly accommodate both.Human face recognition is robust to changes in viewpoint, illumination, facial expression and appearance. The authors investigated face recognition in deep convolutional neural networks by manipulating the strength of identity information in a face by caricaturing. They found that networks create a highly organized face similarity structure in which identities and images coexist.",2019,,1812.10902,10.1038/s42256-019-0111-7,https://arxiv.org/pdf/1812.10902.pdf
3f4a0bb3f031cbe7b0043243116e779c1e75d493,0,,,1,0,"FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age","Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups.",2019,ArXiv,1908.04913,,https://arxiv.org/pdf/1908.04913.pdf
3fcecdded1dad3385e06c0e82f21746c24deac89,0,,,1,0,Deeply-learned Hybrid Representations for Facial Age Estimation,"In this paper, we propose a novel unified network named Deep Hybrid-Aligned Architecture for facial age estimation. It contains global, local and global-local branches, which are jointly optimized and thus can capture multiple types of features with complementary information. In each sub-network of each branch, we employ a separate loss to extract the independent region features and use a recurrent fusion to explore correlations among them. Considering that pose variations may lead to misalignment in different regions, we design an Aligned Region Pooling operation to generate aligned region features. Moreover, a new large private age dataset named Web-FaceAge owning more than 120K samples is collected under diverse scenes and spanning a large age range. Experiments on five age benchmark datasets, including Web-FaceAge, Morph, FG-NET, CACD and Chalearn LAP 2015, show that the proposed method outperforms the state-of-the-art approaches significantly.",2019,IJCAI,,10.24963/ijcai.2019/492,https://pdfs.semanticscholar.org/4cda/13ec06fc64ff16f0e6bcd13335e20d84959b.pdf
3ff7839a184f0b5d5f98586067ca551540aba1a3,0,,,1,0,Are French Really That Different? Recognizing Europeans from Faces Using Data-Driven Learning,"Travel agents and retailers are curious about where their customers come from, which would help them increase their sales and optimize their marketing strategy. In this study, we present a system to predict where people come from in the European region only using their faces. The countries that have been chosen for the study are Russia, Italy, Germany, Spain, and France, based on diversity and representativeness. These countries have been well known for their economy, population, and political impact. First, we implement different neural network classifiers on the dataset of people's faces that we have collected from Twitter. Next, we investigate in more detail 11 different facial features that may help differentiate ethnic groups representative of those five countries. Our system achieves an accuracy of over 50%, more than twice as good as that of humans. Furthermore, we uncover and interpret using genetic anthropological evidences the various differences and similarities between people's faces across geographical distances among different contingents.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545887,http://vietduy.me/European.pdf
40e24eb322d19d4f8aa6b2756e9babf88162869a,1,[M1],,1,0,Video Face Clustering With Unknown Number of Clusters,"Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded. To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.03381,10.1109/ICCV.2019.00513,https://arxiv.org/pdf/1908.03381.pdf
412f7b504244e0843226d0e626691d09f10b8ec6,1,[M1],,1,0,Video Face Clustering With Self-Supervised Representation Learning,"Characters are a key component of understanding the story conveyed in TV series and movies. With the rise of advanced deep face models, identifying face images may seem like a solved problem. However, as face detectors get better, clustering and identification need to be revisited to address increasing diversity in facial appearance. In this paper, we propose unsupervised methods for feature refinement with application to video face clustering. Our emphasis is on distilling the essential information, identity, from the representations obtained using deep pre-trained face networks. We propose a self-supervised Siamese network that can be trained without the need for video/track based supervision, that can also be applied to image collections. We evaluate our methods on three video face clustering datasets. Thorough experiments including generalization studies show that our methods outperform current state-of-the-art methods on all datasets. The datasets and code are available at https://github.com/vivoutlaw/SSIAM.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2019.2947264,http://www.cs.toronto.edu/~makarand/papers/TBIOM2019_SelfSupervised.pdf
439aca3a3d4497f822b058ccabcbfc94a83c5ee9,0,,,0,1,On Learning Disentangled Representations for Gait Recognition,"Gait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and view angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as dynamic gait feature while canonical features are averaged as static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state of the art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/ lower resolutions, cross view angles.",2020,IEEE transactions on pattern analysis and machine intelligence,1909.03051,10.1109/tpami.2020.2998790,https://arxiv.org/pdf/1909.03051.pdf
43dab36cd9caae66ac259401f5f3e02ffa7c2097,0,,,1,1,Recognizing Families In the Wild (RFIW): The 4th Edition,"Recognizing Families In the Wild (RFIW): an annual large-scale, multi-track automatic kinship recognition evaluation that supports various visual kin-based problems on scales much higher than ever before. Organized in conjunction with the 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG) as a Challenge, RFIW provides a platform for publishing original work and the gathering of experts for a discussion of the next steps. This paper summarizes the supported tasks (i.e., kinship verification, tri-subject verification, and search & retrieval of missing children) in the evaluation protocols, which include the practical motivation, technical background, data splits, metrics, and benchmark results. Furthermore, top submissions (i.e., leader-board stats) are listed and reviewed as a high-level analysis on the state of the problem. In the end, the purpose of this paper is to describe the 2020 RFIW challenge, end-to-end, along with forecasts in promising future directions.",2020,ArXiv,,,
44b827df6c433ca49bcf44f9f3ebfdc0774ee952,1,,1,1,0,Deep Correlation Feature Learning for Face Verification in the Wild,"Convolutional neural networks (CNNs) commonly uses the softmax loss function as the supervision signal. In order to enhance the discriminative power of the deeply learned features, this letter proposes a new supervision signal, called correlation loss, for face verification task. Specifically, the correlation loss encourages the large correlation between the deep feature vectors and their corresponding weight vectors in softmax loss. With the joint supervision of softmax loss and correlation loss, the deep correlation feature learning (DCFL) network can learn the deep features with both the interclass separability and the intraclass compactness, which are highly discriminative for face verification. More importantly, by applying the weight vector of softmax function as the class prototype, the proposed correlation loss function is easy to be optimized during the backpropatation of CNN. Finally, the DCFL method achieves 99.55% and 96.06% face verification accuracy using a 64-layer ResNet on the labeled face in-the-Wild (LFW) and you-tube face (YTF) benchmark, respectively.",2017,IEEE Signal Processing Letters,,10.1109/LSP.2017.2726105,
45527d53a37e6f72eac363f70ba97a2b4073e6e8,0,,,0,1,FaceGuard: A Self-Supervised Defense Against Adversarial Face Images,"Prevailing defense schemes against adversarial face images tend to overfit to the perturbations in the training set and fail to generalize to unseen adversarial attacks. We propose a new self-supervised adversarial defense framework, namely FaceGuard, that can automatically detect, localize, and purify a wide variety of adversarial faces without utilizing pre-computed adversarial training samples. During training, FaceGuard automatically synthesizes challenging and diverse adversarial attacks, enabling a classifier to learn to distinguish them from real faces. Concurrently, a purifier attempts to remove the adversarial perturbations in the image space. Experimental results on LFW dataset show that FaceGuard can achieve 99.81% detection accuracy on six unseen adversarial attack types. In addition, the proposed method can enhance the face recognition performance of ArcFace from 34.27% TAR @ 0.1% FAR under no defense to 77.46% TAR @ 0.1% FAR. Code, pre-trained models and dataset will be publicly available.",2020,ArXiv,2011.14218,,https://arxiv.org/pdf/2011.14218.pdf
455a7e03a0c5ab618d0e86a06c9910ac179f0479,1,[D9],,1,0,Identity Preserving Face Completion for Large Ocular Region Occlusion,"We present a novel deep learning approach to synthesize complete face images in the presence of large ocular region occlusions. This is motivated by recent surge of VR/AR displays that hinder face-to-face communications. Different from the state-of-the-art face inpainting methods that have no control over the synthesized content and can only handle frontal face pose, our approach can faithfully recover the missing content under various head poses while preserving the identity. At the core of our method is a novel generative network with dedicated constraints to regularize the synthesis process. To preserve the identity, our network takes an arbitrary occlusion-free image of the target identity to infer the missing content, and its high-level CNN features as an identity prior to regularize the searching space of generator. Since the input reference image may have a different pose, a pose map and a novel pose discriminator are further adopted to supervise the learning of implicit pose transformations. Our method is capable of generating coherent facial inpainting with consistent identity over videos with large variations of head motions. Experiments on both synthesized and real data demonstrate that our method greatly outperforms the state-of-the-art methods in terms of both synthesis quality and robustness.",2018,BMVC,1807.08772,,https://arxiv.org/pdf/1807.08772.pdf
46702e0127e16a4d6a1feda3ffc5f0f123957e87,0,,,1,0,Revisit Multinomial Logistic Regression in Deep Learning: Data Dependent Model Initialization for Image Recognition,"We study in this paper how to initialize the parameters of multinomial logistic regression (a fully connected layer followed with softmax and cross entropy loss), which is widely used in deep neural network (DNN) models for classification problems. As logistic regression is widely known not having a closed-form solution, it is usually randomly initialized, leading to several deficiencies especially in transfer learning where all the layers except for the last task-specific layer are initialized using a pre-trained model. The deficiencies include slow convergence speed, possibility of stuck in local minimum, and the risk of over-fitting. To address those deficiencies, we first study the properties of logistic regression and propose a closed-form approximate solution named regularized Gaussian classifier (RGC). Then we adopt this approximate solution to initialize the task-specific linear layer and demonstrate superior performance over random initialization in terms of both accuracy and convergence speed on various tasks and datasets. For example, for image classification, our approach can reduce the training time by 10 times and achieve 3.2% gain in accuracy for Flickr-style classification. For object detection, our approach can also be 10 times faster in training for the same accuracy, or 5% better in terms of mAP for VOC 2007 with slightly longer training.",2018,ArXiv,1809.06131,,https://arxiv.org/pdf/1809.06131.pdf
46800f04189e5bb57c5ec825b95a2064ed2537df,0,,,1,0,Human Attribute Recognition: A Comprehensive Survey,"Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR’s main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.",2020,,,10.20944/preprints202007.0055.v1,https://pdfs.semanticscholar.org/9fc4/56fca45f3c51d32b8dc2b1b6e8e47d7a3dd6.pdf
46b968d82c4709e74419828b2767a9218a3ebcc8,0,,,0,1,Learning Invariant Representations of Social Media Users,"The evolution of social media users’ behavior over time complicates user-level comparison tasks such as verification, classification, clustering, and ranking. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a mapping from short episodes of user activity on social media to a vector space in which the distance between points captures the similarity of the corresponding users’ invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the mapping may be applied to users not seen at training time and enables efficient comparisons of users in the resulting vector space. We present a comprehensive evaluation to validate the benefits of the proposed approach using data from Reddit, Twitter, and Wikipedia.",2019,EMNLP/IJCNLP,1910.04979,10.18653/v1/D19-1178,https://arxiv.org/pdf/1910.04979.pdf
48499deeaa1e31ac22c901d115b8b9867f89f952,0,,,1,0,Interim Report of Final Year Project HKU-Face : A Large Scale Dataset for Deep Face Recognition,"Current development of face recognition usually encounters problems with its training dataset because of the small size and human labelling errors. This project proposes a general dataset construction and filtering process in order to tackle the problem efficiently. Several models in the literature are utilized but for the new purpose to filter the original dataset collected from the website. Current results show the impressive effectiveness of automatic filtering and purity enhancement after filtering. Subsequent research and experiment are needed for the further improvement of filtering process with lower 0 negative rate. After the completion of the project, facial dataset constructions are expected to accelerate with less human effort. Further studies based on it is expected to contribute more to the unsupervised learning in the general object recognition.",2018,,,,https://pdfs.semanticscholar.org/4849/9deeaa1e31ac22c901d115b8b9867f89f952.pdf
4851bcfc0e2ae7b417e38fcb00b7a44ffdd117d0,0,,,0,1,MagnifierNet: Towards Semantic Regularization and Fusion for Person Re-identification,"Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this paper, we propose MagnifierNet, a novel network which accurately mines details for each semantic region and selectively fuse all semantic feature representations. Apart from conventional global branch, our proposed network is composed of a Semantic Regularization Branch (SRB) as learning regularizer and a Semantic Fusion Branch (SFB) towards selectively semantic fusion. The SRB learns with limited number of semantic regions randomly sampled in each batch, which forces the network to learn detailed representation for each semantic region, and the SFB selectively fuses semantic region information in a sequential manner, focusing on beneficial information while neglecting irrelevant features or noises. In addition, we introduce a novel loss function ""Semantic Diversity Loss"" (SD Loss) to facilitate feature diversity and improves regularization among all semantic regions. State-of-the-art performance has been achieved on multiple datasets by large margins. Notably, we improve SOTA on CUHK03-Labeled Dataset by 12.6% in mAP and 8.9% in Rank-1. We also outperform existing works on CUHK03-Detected Dataset by 13.2% in mAP and 7.8% in Rank-1 respectively, which demonstrates the effectiveness of our method.",2020,ArXiv,2002.10979,,https://arxiv.org/pdf/2002.10979.pdf
489936641d9d6033ef4b5ee6524d56e29d8ff24f,1,[D10],,1,1,Hierarchical Pyramid Diverse Attention Networks for Face Recognition,"Deep learning has achieved a great success in face recognition (FR), however, few existing models take hierarchical multi-scale local features into consideration. In this work, we propose a hierarchical pyramid diverse attention (HPDA) network. First, it is observed that local patches would play important roles in FR when the global face appearance changes dramatically. Some recent works apply attention modules to locate local patches automatically without relying on face landmarks. Unfortunately, without considering diversity, some learned attentions tend to have redundant responses around some similar local patches, while neglecting other potential discriminative facial parts. Meanwhile, local patches may appear at different scales due to pose variations or large expression changes. To alleviate these challenges, we propose a pyramid diverse attention (PDA) to learn multi-scale diverse local representations automatically and adaptively. More specifically, a pyramid attention is developed to capture multi-scale features. Meanwhile, a diverse learning is developed to encourage models to focus on different local patches and generate diverse local features. Second, almost all existing models focus on extracting features from the last convolutional layer, lacking of local details or small-scale face parts in lower layers. Instead of simple concatenation or addition, we propose to use a hierarchical bilinear pooling (HBP) to fuse information from multiple layers effectively. Thus, the HPDA is developed by integrating the PDA into the HBP. Experimental results on several datasets show the effectiveness of the HPDA, compared to the state-of-the-art methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00835,http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Hierarchical_Pyramid_Diverse_Attention_Networks_for_Face_Recognition_CVPR_2020_paper.pdf
48cb52aa85e55ea00ae3f81f2d80ac75bbe5b6e3,1,[M5],,1,0,Multi-algorithmic Fusion for Reliable Age and Gender Estimation from Face Images,"Automated estimation of demographic attributes, such as gender and age, became of great importance for many potential applications ranging from forensics to social media. Although previous works reported performances that closely match human level. These solutions lack of human intuition that allows human beings to state the confidences of their predictions. While the human intuition subconsciously considers surrounding conditions or the lack of experience in a certain task, current algorithmic solutions tend to mispredict with high confidence scores. In this work, we propose a multi-algorithmic fusion approach for age and gender estimation that is able to accurately state the model's prediction reliability. Our solution is based on stochastic forward passes through a dropout-reduced neural network ensemble. By utilizing multiple stochastic forward passes combined from the neural network ensemble, the centrality and dispersion of these predictions are used to derive a confidence statement about the prediction. Our experiments were conducted on the Adience benchmark. We showed that the proposed solution reached and exceeded state-of-the-art performance for the age and gender estimation tasks. Further, we demonstrated that the reliability statements of the predictions of our proposed solution capture challenging conditions and underrepresented training samples.",2019,2019 22th International Conference on Information Fusion (FUSION),,,
4ae3d316748d5f18abc8b13a926ba765a24f647a,0,,,0,1,Deep Face Representations for Differential Morphing Attack Detection,"The vulnerability of facial recognition systems to face morphing attacks is well known. Many different approaches for morphing attack detection (MAD) have been proposed in the scientific literature. However, the MAD algorithms proposed so far have mostly been trained and tested on datasets whose distributions of image characteristics are either very limited (e.g., only created with a single morphing tool) or rather unrealistic (e.g., no print-scan transformation). As a consequence, these methods easily overfit on certain image types and the results presented cannot be expected to apply to real-world scenarios. For example, the results of the latest NIST FRVT MORPH show that the majority of submitted MAD algorithms lacks robustness and performance when considering unseen and challenging datasets. In this work, subsets of the FERET and FRGCv2 face databases are used to create a realistic database for training and testing of MAD algorithms, containing a large number of ICAO-compliant bona fide facial images, corresponding unconstrained probe images, and morphed images created with four different face morphing tools. Furthermore, multiple post-processings are applied on the reference images, e.g., print-scan and JPEG2000 compression. On this database, previously proposed differential morphing algorithms are evaluated and compared. In addition, the application of deep face representations for differential MAD algorithms is investigated. It is shown that algorithms based on deep face representations can achieve very high detection performance (less than 3% D-EER) and robustness with respect to various post-processings. Finally, the limitations of the developed methods are analyzed.",2020,IEEE Transactions on Information Forensics and Security,2001.01202,10.1109/TIFS.2020.2994750,https://ieeexplore.ieee.org/ielx7/10206/8833568/09093905.pdf
4b5d42b1f6e3a8924f595ca60a000a96ff047521,0,,,0,1,Siamese Networks: The Tale of Two Manifolds,"Siamese networks are non-linear deep models that have found their ways into a broad set of problems in learning theory, thanks to their embedding capabilities. In this paper, we study Siamese networks from a new perspective and question the validity of their training procedure. We show that in the majority of cases, the objective of a Siamese network is endowed with an invariance property. Neglecting the invariance property leads to a hindrance in training the Siamese networks. To alleviate this issue, we propose two Riemannian structures and generalize a well-established accelerated stochastic gradient descent method to take into account the proposed Riemannian structures. Our empirical evaluations suggest that by making use of the Riemannian geometry, we achieve state-of-the-art results against several algorithms for the challenging problem of fine-grained image classification.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00314,http://openaccess.thecvf.com/content_ICCV_2019/papers/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.pdf
4c2a6524b458e7f964a35bf3e888b31d6cf9bf46,0,,,1,1,Classical and modern face recognition approaches: a complete review,"Human face recognition have been an active research area for the last few decades. Especially, during the last five years, it has gained significant research attention from multiple domains like computer vision, machine learning and artificial intelligence due to its remarkable progress and broad social applications. The primary goal of any face recognition system is to recognize the human identity from the static images, video data, data-streams and the knowledge of the context in which these data components are being actively used. In this review, we have highlighted major applications, challenges and trends of face recognition systems in social and scientific domains. The prime objective of this research is to sum-up recent face recognition techniques and develop a broad understanding of how these techniques behave on different datasets. Moreover, we discuss some key challenges such as variability in illumination, pose, aging, cosmetics, scale, occlusion, and background. Along with classical face recognition techniques, most recent research directions are deeply investigated, i.e., deep learning, sparse models and fuzzy set theory. Additionally, basic methodologies are briefly discussed, while contemporary research contributions are examined in broader details. Finally, this research presents future aspects of face recognition technologies and its potential significance in the upcoming digital society.",2020,,,10.1007/S11042-020-09850-1,
4cb5780b87c44bde549420a93b2ae990d0d423b9,1,[D9],,1,0,Weighted Feature Pooling Network in Template-Based Recognition,"Many computer vision tasks are template-based learning tasks in which multiple instances of a specific concept (e.g. multiple images of a subject’s face) are available at once to the learning algorithm. The template structure of the input data provides an opportunity for generating a robust and discriminative unified template-level representation that effectively exploits the inherent diversity of feature-level information across instances within a template. In contrast to other feature aggregation methods, we propose a new technique to dynamically predict weights that consider factors such as noise and redundancy in assessing the importance of image-level features and use those weights to appropriately aggregate the features into a single template-level representation. We present extensive experimental results on the MNIST, CIFAR10, UCF101, IJB-A, IJB-B, and Janus CS4 datasets to show that the new technique outperforms statistical feature pooling methods as well as other neural-network-based aggregation mechanisms on a broad set of tasks.",2018,ACCV,,10.1007/978-3-030-20873-8_28,
4cc32db67ff82cf1aa160631c35bb315c5add749,0,,,0,1,Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation,"We present a generic image-to-image translation framework, Pixel2Style2Pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. We further introduce a dedicated identity loss which is shown to achieve improved performance in the reconstruction of an input image. We demonstrate pSp to be a simple architecture that, by leveraging a well-trained, fixed generator network, can be easily applied on a wide-range of image-to-image translation tasks. Solving these tasks through the style representation results in a global approach that does not rely on a local pixel-to-pixel correspondence and further supports multi-modal synthesis via the resampling of styles. Notably, we demonstrate that pSp can be trained to align a face image to a frontal pose without any labeled data, generate multi-modal results for ambiguous tasks such as conditional face generation from segmentation maps, and construct high-resolution images from corresponding low-resolution images.",2020,ArXiv,2008.00951,,https://arxiv.org/pdf/2008.00951.pdf
4cfe0b002f34328322f066d8fbe63f3d35898a27,1,[D9],,1,0,Conditional Dual-Agent GANs for Photorealistic and Annotation Preserving Image Synthesis,"Conditional and semi-supervised Generative Adversarial Networks (GANs) have been proven to be effective for image synthesis with preserved annotation information. However, learning from GAN generated images may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real images. To narrow this gap, we expand existing generative methods and propose a novel Conditional Dual-Agent GAN (CDA-GAN) model for photorealistic and annotation preserving image synthesis, which significantly benefits object classification and face recognition through Deep Convolutional Neural Networks (DCNNs) learned with such augmented data. Instead of merely distinguishing “real” or “fake” for the generated images, the proposed dual agents of the Discriminator are able to preserve both of realism and annotation information simultaneously through a standard adversarial loss and an auxiliary annotation perception loss. During the training process, the Generator is conditioned on the desired image features learned by a pre-trained CNN sharing the same architecture of the Discriminator yet different weights. Thus, CDA-GAN is flexible in terms of the scalability and able to generate photorealistic image with well preserved class labeling information for learning DCNNs in specific domains. We perform qualitative and quantitative experiments to verify the effectiveness of our proposed method, which outperforms other state-of-the-arts on MNIST hand written digits classification dataset and National Institute of Standards and Technology (NIST) IARPA Janus Benchmark A (IJB-A) face recognition dataset. c © 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. * indicate equal contributions. 2 WANG ET AL.: CONDITIONAL DUAL-AGENT GANS FOR IMAGE SYNTHESIS Furthermore, we also prove that the CDA-GAN generated data represent the distinct class relationships as well as the real data, so adding such data for training DCNN models ends up with impressive improvement in terms of overall accuracy, generalization capacity, and robustness.",2017,,,,https://pdfs.semanticscholar.org/4cfe/0b002f34328322f066d8fbe63f3d35898a27.pdf
4e19a1d2b2d28ed2d20cb097513df70e266ea308,0,,,1,0,Automatic Group Cohesiveness Detection With Multi-modal Features,"Group cohesiveness is a compelling and often studied composition in group dynamics and group performance. The enormous number of web images of groups of people can be used to develop an effective method to detect group cohesiveness. This paper introduces an automatic group cohesiveness prediction method for the 7th Emotion Recognition in the Wild (EmotiW 2019) Grand Challenge in the category of Group-based Cohesion Prediction. The task is to predict the cohesive level for a group of people in images. To tackle this problem, a hybrid network including regression models which are separately trained on face features, skeleton features, and scene features is proposed. Predicted regression values, corresponding to each feature, are fused for the final cohesive intensity. Experimental results demonstrate that the proposed hybrid network is effective and makes promising improvements. A mean squared error (MSE) of 0.444 is achieved on the testing sets which outperforms the baseline MSE of 0.5.",2019,ICMI '19,1910.01197,10.1145/3340555.3355716,https://arxiv.org/pdf/1910.01197.pdf
4f0b641860d90dfa4c185670bf636149a2b2b717,1,[D9],,1,1,Improve Cross-Domain Face Recognition with IBN-block,"Domain adaptation is one of the major challenges for face recognition (FR). Most large-scale FR training datasets are built from massive images crawled from the Internet, while in practical applications face images come from specific scenarios. Especially, for applications like ID card verification, registered face images are taken in controlled environments while probe face images are not. There are different distributions between source domain and target domain. In this paper, we propose to use Instance-Batch-Normalization (IBN) block to improve cross-domain FR performance. A million-scale cross-domain test set named IDCard-Scene-1M is used for evaluation. CNN models are trained with an improved loss function which we call L2-ASoftmax Loss. Without using any data from the target domain, IBN-block increased recall rates (@FPR = 10−6) on IDCard-Scene-1M by 1 percentage point for different CNN models. Besides, experiments show that the proposed IBN-CNN models trained with L2-ASoftmax Loss made state-of-the-art performance on MegaFace evaluation.",2018,2018 IEEE International Conference on Big Data (Big Data),,10.1109/BigData.2018.8622251,
4f842a92c110f40e9187bc9e6544b5dcb1daa5f3,0,,,0,1,Advancing High Fidelity Identity Swapping for Forgery Detection,"In this work, we study various existing benchmarks for deepfake detection researches. In particular, we examine a novel two-stage face swapping algorithm, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, FaceShifter generates the swapped face with high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. FaceShifter can handle facial occlusions with a second synthesis stage consisting of a Heuristic Error Acknowledging Refinement Network (HEAR-Net), which is trained to recover anomaly regions in a self-supervised way without any manual annotations. Experiments show that existing deepfake detection algorithm performs poorly with FaceShifter, since it achieves advantageous quality over all existing benchmarks. However, our newly developed Face X-Ray method can reliably detect forged images created by FaceShifter.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00512,https://pdfs.semanticscholar.org/4f84/2a92c110f40e9187bc9e6544b5dcb1daa5f3.pdf
5309dfee51cad8a280a66a14a7c0466ad161d251,1,[D9],,1,1,EagleEye: wearable camera-based person identification in crowded urban spaces,"We present EagleEye, an AR-based system that identifies missing person (or people) in large, crowded urban spaces. Designing EagleEye involves critical technical challenges for both accuracy and latency. Firstly, despite recent advances in Deep Neural Network (DNN)-based face identification, we observe that state-of-the-art models fail to accurately identify Low-Resolution (LR) faces. Accordingly, we design a novel Identity Clarification Network to recover missing details in the LR faces, which enhances 1 positives by 78% with only 14% 0 positives. Furthermore, designing EagleEye involves unique challenges compared to recent continuous mobile vision systems in that it requires running a series of complex DNNs multiple times on a high-resolution image. To tackle the challenge, we develop Content-Adaptive Parallel Execution to optimize complex multi-DNN face identification pipeline execution latency using heterogeneous processors on mobile and cloud. Our results show that EagleEye achieves 9.07X faster latency compared to naive execution, with only 108 KBytes of data offloaded.",2020,MobiCom,,10.1145/3372224.3380881,
5313367e0a350f3e0b632dfb2e588f63fc272b2b,1,"[D12], [D16]",,1,1,Feature Transfer Learning for Face Recognition With Under-Represented Data,"Despite the large volume of face recognition datasets, there is a significant portion of subjects, of which the samples are insufficient and thus under-represented. Ignoring such significant portion results in insufficient training data. Training with under-represented data leads to biased classifiers in conventionally-trained deep networks. In this paper, we propose a center-based feature transfer framework to augment the feature space of under-represented subjects from the regular subjects that have sufficiently diverse samples. A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones. This encourages the under-represented distribution to be closer to the regular distribution. Further, an alternating training regimen is proposed to simultaneously achieve less biased classifiers and a more discriminative feature representation. We conduct ablative study to mimic the under-represented datasets by varying the portion of under-represented classes on the MS-Celeb-1M dataset. Advantageous results on LFW, IJB-A and MS-Celeb-1M demonstrate the effectiveness of our feature transfer and training strategy, compared to both general baselines and state-of-the-art methods. Moreover, our feature transfer successfully presents smooth visual interpolation, which conducts disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations such as pose and lighting.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00585,http://cseweb.ucsd.edu/~mkchandraker/pdf/cvpr19_featuretransfer.pdf
53829cd71916382dd9dc8cbf1ec369ce2bd7f0a4,0,,,0,1,A New Discriminative Feature Learning for Person Re-Identification Using Additive Angular Margin Softmax Loss,"In this paper, a new end-to-end framework is proposed for person re-identification (re-ID) by combining metric learning and classification. In this new framework, the Additive Angular Margin Softmax is used which imposes an additive angular margin constraint to the target logit on hypersphere manifold. This is aimed to improve the similarity of the intra-class features and the dissimilarity of the inter-class features simultaneously. Compard with the three popular used softmax-based-loss methods, the experiments show that the proposed approach has achieved improved performance on Market1501 and DukeMTMC-reID datasets for person re-ID.",2019,2019 UK/ China Emerging Technologies (UCET),,10.1109/UCET.2019.8881838,
54a6da19283bf3a10bc518c52d8ae1dccc75585b,1,[M3],,1,1,"Is Face Recognition Sexist? No, Gendered Hairstyles and Biology Are","Recent news articles have accused face recognition of being ""biased"", ""sexist"" or ""racist"". There is consensus in the research literature that face recognition accuracy is lower for females, who often have both a higher 0 match rate and a higher 0 non-match rate. However, there is little published research aimed at identifying the cause of lower accuracy for females. For instance, the 2019 Face Recognition Vendor Test that documents lower female accuracy across a broad range of algorithms and datasets also lists ""Analyze cause and effect"" under the heading ""What we did not do"". We present the first experimental analysis to identify major causes of lower face recognition accuracy for females on datasets where previous research has observed this result. Controlling for equal amount of visible face in the test images reverses the apparent higher 0 non-match rate for females. Also, principal component analysis indicates that images of two different females are inherently more similar than of two different males, potentially accounting for a difference in 0 match rates.",2020,ArXiv,2008.06989,,https://arxiv.org/pdf/2008.06989.pdf
54ac0938649d70d3ce4a45028475b07fcdc9cbd5,0,,,1,0,Gotta Adapt 'Em All: Joint Pixel and Feature-Level Domain Adaptation for Recognition in the Wild,"Recent developments in deep domain adaptation have allowed knowledge transfer from a labeled source domain to an unlabeled target domain at the level of intermediate features or input pixels. We propose that advantages may be derived by combining them, in the form of different insights that lead to a novel design and complementary properties that result in better performance. At the feature level, inspired by insights from semi-supervised learning, we propose a classification-aware domain adversarial neural network that brings target examples into more classifiable regions of source domain. Next, we posit that computer vision insights are more amenable to injection at the pixel level. In particular, we use 3D geometry and image synthesis based on a generalized appearance flow to preserve identity across pose transformations, while using an attribute-conditioned CycleGAN to translate a single source into multiple target images that differ in lower-level properties such as lighting. Besides standard UDA benchmark, we validate on a novel and apt problem of car recognition in unlabeled surveillance images using labeled images from the web, handling explicitly specified, nameable factors of variation through pixel-level and implicit, unspecified factors through feature-level adaptation.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00278,
54e1daa2c01ef8907796867a1d23b52acb2573ef,1,[D9],,1,0,Finding Person Relations in Image Data of the Internet Archive,"The amount of multimedia content in the World Wide Web is rapidly growing and contains valuable information for many applications in different domains. The Internet Archive initiative has gathered billions of time-versioned web pages since the mid-nineties. However, the huge amount of data is rarely labeled with appropriate metadata and automatic approaches are required to enable semantic search. Normally, the textual content of the Internet Archive is used to extract entities and their possible relations across domains such as politics and entertainment, whereas image and video content is usually disregarded. In this paper, we introduce a system for person recognition in image content of web news stored in the Internet Archive. Thus, the system complements entity recognition in text and allows researchers and analysts to track media coverage and relations of persons more precisely. Based on a deep learning face recognition approach, we suggest a system that detects persons of interest and gathers sample material, which is subsequently used to identify them in the image data of the Internet Archive. We evaluate the performance of the face recognition system on an appropriate standard benchmark dataset and demonstrate the feasibility of the approach with two use cases.",2018,TPDL,1806.08246,10.1007/978-3-030-00066-0_20,https://arxiv.org/pdf/1806.08246.pdf
568435e4a2d161c68808a35250be147202867ce4,0,,,1,1,Deep Learning For Face Recognition: A Critical Analysis,"Face recognition is a rapidly developing and widely applied aspect of biometric technologies. Its applications are broad, ranging from law enforcement to consumer applications, and industry efficiency and monitoring solutions. The recent advent of affordable, powerful GPUs and the creation of huge face databases has drawn research focus primarily on the development of increasingly deep neural networks designed for all aspects of face recognition tasks, ranging from detection and preprocessing to feature representation and classification in verification and identification solutions. However, despite these improvements, real-time, accurate face recognition is still a challenge, primarily due to the high computational cost associated with the use of Deep Convolutions Neural Networks (DCNN), and the need to balance accuracy requirements with time and resource constraints. Other significant issues affecting face recognition relate to occlusion, illumination and pose invariance, which causes a notable decline in accuracy in both traditional handcrafted solutions and deep neural networks. This survey will provide a critical analysis and comparison of modern state of the art methodologies, their benefits, and their limitations. It provides a comprehensive coverage of both deep and shallow solutions, as they stand today, and highlight areas requiring future development and improvement. This review is aimed at facilitating research into novel approaches, and further development of current methodologies by scientists and engineers, whilst imparting an informative and analytical perspective on currently available solutions to end users in industry, government and consumer contexts.",2019,ArXiv,1907.12739,,https://arxiv.org/pdf/1907.12739.pdf
5697ae2b337a771956684b6f8b26e0d6f13097cc,0,,,0,1,Real-time detection tracking and recognition algorithm based on multi-target faces,"At present, face recognition algorithms are facing some problems with poor face tracking and low real-time performance in multi-target recognition scenarios. This paper details a multi-target face real-time detection tracking and recognition algorithm, including three methods of fast-tracking, fast detection, and quick recognition. The first step offers a new network based on GOTURN for achieving fast face tracking. The prior information of the previous frame image used to predict the position of the face boxes at the current frame. The second step is based on MTCNN for face detection, using the prior information of the present structure to avoid generating massive of invalid candidate boxes, thereby achieving rapid detection of faces. Finally, fast face recognition realized by reduced MobileFaceNet. By avoiding repeated exposure and repeated identification of the same target, the algorithm successfully transforms a multi-target scene into a single-target scene. On the OTB2015 and 300_VW test sets, the evaluation trackers tracked faces with an accuracy rate of 92.2% and 99.6% respectively. On the Xiph test set, multi-target detection and tracking face speed reached 102fps on the CPU. Compared with the original MobileFaceNet, the streamlined network has an accuracy rate of 99.1% on LFW, the feature extraction speed increased by 25%, and the model size reduced by 45%. Experimental results show that the algorithm has high recognition accuracy and real-time performance in multi-target recognition scenes.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09601-2,
5714183a5ef9fa40a5ad0bdff4908fb447a144a6,0,,,1,1,NPCFace: A Negative-Positive Cooperation Supervision for Training Large-scale Face Recognition,"Deep face recognition has made remarkable advances in the last few years, while the training scheme still remains challenging in the large-scale data situation where many hard cases occur. Especially in the range of low 0 accept rate (FAR), there are various hard cases in both positives ($\textit{i.e.}$ intra-class) and negatives ($\textit{i.e.}$ inter-class). In this paper, we study how to make better use of these hard samples for improving the training. The existing training methods deal with the challenge by adding margins in either the positive logit (such as SphereFace, CosFace, ArcFace) or the negative logit (such as MV-softmax, ArcNegFace, CurricularFace). However, the correlation between hard positive and hard negative is overlooked, as well as the relation between the margin in positive logit and the margin in negative logit. We find such correlation is significant, especially in the large-scale dataset, and one can take advantage from it to boost the training via relating the positive and negative margins for each training sample. To this end, we propose an explicit cooperation between positive and negative margins sample-wisely. Given a batch of hard samples, a novel Negative-Positive Cooperation loss, named NPCFace, is formulated, which emphasizes the training on both the negative and positive hard cases via a cooperative-margin mechanism in the softmax logits, and also brings better interpretation of negative-positive hardness correlation. Besides, the negative emphasis is implemented with an improved formulation to achieve stable convergence and flexible parameter setting.We validate the effectiveness of our approach on various benchmarks of large-scale face recognition and outperform the previous methods especially in the low FAR range.",2020,ArXiv,2007.10172,,https://arxiv.org/pdf/2007.10172.pdf
590b2c92f59a90bd17e4dbcaf417d6a6eaf81a5f,0,,,0,1,Deep Representation Learning With Full Center Loss for Credit Card Fraud Detection,"Credit card fraud detection is an important study in the current era of mobile payment. Improving the performance of a fraud detection model and keeping its stability are very challenging because users’ payment behaviors and criminals’ fraud behaviors are often changing. In this article, we focus on obtaining deep feature representations of legal and fraud transactions from the aspect of the loss function of a deep neural network. Our purpose is to obtain better separability and discrimination of features so that it can improve the performance of our fraud detection model and keep its stability. We propose a new kind of loss function, full center loss (FCL), which considers both distances and angles among features and, thus, can comprehensively supervise the deep representation learning. We conduct lots of experiments on two big data sets of credit card transactions, one is private and another is public, to demonstrate the detection performance of our model by comparing FCL with other state-of-the-art loss functions. The results illustrate that FCL outperforms others. We also conduct experiments to show that FCL can ensure a more stable model than others.",2020,IEEE Transactions on Computational Social Systems,,10.1109/TCSS.2020.2970805,
59c47e49d8211953b1acd68984650b807ce69a71,1,[D9],,1,1,Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network,"Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00078,
5a8fb050f3127fd33292a720878ff0d38567caae,0,,,0,1,Margin-Based Deep Learning Networks for Human Activity Recognition,"Human activity recognition (HAR) is a popular and challenging research topic, driven by a variety of applications. More recently, with significant progress in the development of deep learning networks for classification tasks, many researchers have made use of such models to recognise human activities in a sensor-based manner, which have achieved good performance. However, sensor-based HAR still faces challenges; in particular, recognising similar activities that only have a different sequentiality and similarly classifying activities with large inter-personal variability. This means that some human activities have large intra-class scatter and small inter-class separation. To deal with this problem, we introduce a margin mechanism to enhance the discriminative power of deep learning networks. We modified four kinds of common neural networks with our margin mechanism to test the effectiveness of our proposed method. The experimental results demonstrate that the margin-based models outperform the unmodified models on the OPPORTUNITY, UniMiB-SHAR, and PAMAP2 datasets. We also extend our research to the problem of open-set human activity recognition and evaluate the proposed method’s performance in recognising new human activities.",2020,Sensors,,10.3390/s20071871,https://pdfs.semanticscholar.org/c63f/d596503061fc6de92c50dadb8fe5db88b271.pdf
5b9c6ca84268cb283941ae28b73989c0cf7e2ac2,1,"[D9], [M5]",,1,0,A Pipeline to Improve Face Recognition Datasets and Applications,"Face recognition has a wide practical applicability in various contexts, for example, detecting students attending a lecture at university, identifying members in a gym or monitoring people in an airport. Recent methods based on Convolutional Neural Network (CNN), such as FaceNet, achieved state-of-the-art performance in face recognition. Inspired from this work, we propose a pipeline to improve face recognition systems based on Center loss. The main advantage is that our approach does not suffer from data expansion as in Triplet loss. Our pipeline is capable of cleaning an existing face dataset to improve the recognition performance or creating one from scratch. We present detailed experiments to show characteristics and performance of the pipeline. In addition, a small-scale application for face recognition that makes use of the proposed cleaning process is presented.",2018,2018 International Conference on Image and Vision Computing New Zealand (IVCNZ),,10.1109/IVCNZ.2018.8634724,
5c54e0f46330787c4fac48aecced9a8f8e37658a,1,[D9],,1,0,Simple Triplet Loss Based on Intra/Inter-Class Metric Learning for Face Verification,"Recently, benefiting from the advances of the deep convolution neural networks (CNNs), significant progress has been made in the field of the face verification and face recognition. Specially, the performance of the FaceNet has overpassed the human level performance in terms of the accuracy on the datasets ""Labeled Faces in the Wild (LFW)"" and ""Youtube Faces in the Wild (YTF)"". The triplet loss used in the FaceNet has proved its effectiveness for face verification. However, the number of the possible triplets is explosive when using a large scale dataset to train the model. In this paper, we propose a simple class-wise triplet loss based on the intra/inter-class distance metric learning which can largely reduce the number of the possible triplets to be learned. However the simplification of the classic triplet loss function has not degraded the performance of the proposed approach. The experimental evaluations on the most widely used benchmarks LFW and YTF show that the model with the proposed class-wise simple triplet loss can reach the state-of-the-art performance. And the visualization of the distribution of the learned features based on the MNIST dataset has also shown the effectiveness of the proposed method to better separate the classes and make the features more discriminative in comparison with the other state-of-the-art loss function.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.194,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w23/Ming_Simple_Triplet_Loss_ICCV_2017_paper.pdf
5cf3b29a998e12542584560d253012808345ad7f,0,,,1,0,Trustworthy Face Recognition: Improving Generalization of Deep Face Presentation Attack Detection,"The extremely high recognition accuracy achieved by modern, convolutional neural network (CNN) based face recognition (FR) systems has contributed significantly to the adoption of such systems in a variety of applications, from mundane activities like unlocking phones to high-security applications such as border-control. Nonetheless, they have been shown to be highly vulnerable to presentation attacks (PA), also known as spoof-attacks. A face PA is said to have occurred when a face biometric-sample is presented to the camera of an FR system with the intention of interfering with the operation of biometric recognition. An example PA is when someone tries to illicitly access an FR system by presenting a printed face photo of an authorized person to the camera. State-of-the-art face presentation attack detection (PAD) systems which are based on CNNs as well offer counter-measures to PAs. Over the past decade, several datasets have been collected and publicly shared by different research groups, for face PAD experiments. It has been shown that most face PAD systems do not generalize well. That is, PAD systems show satisfactory classification performance when they are trained and evaluated on disjoint subsets of a dataset (known as an intradataset evaluation). However, their performance degrades significantly when they are trained using data from one dataset and evaluated using data from another dataset (a cross-dataset evaluation). The poor generalization of PAD systems precludes FR systems from deployment in many real-world applications. In this thesis, I address generalization issues in face PAD systems in three ways: 1. Although many CNN architectures have been proposed for face PAD, no systematic evaluation of their classification performance has been done before. Here, I evaluate six different CNN architectures on four face PAD datasets in terms of both intra-dataset and cross-dataset performance, and show that patch-based CNN architectures generalize better. Moreover, I propose a novel CNN that analyzes the face images at different scales. This multi-scale analysis allows the proposed CNN to generalize better compared to baseline CNNs. 2. I formulate the low cross-dataset performance of PAD as a domain shift problem and investigate domain adaptation methods as a solution. I propose a novel domain adaptation method based on the hypothesis that some learned filters in CNNs are domain",2020,,,10.5075/EPFL-THESIS-7635,http://publications.idiap.ch/downloads/papers/2020/Mohammadi_THESIS_2020.pdf
5d01486a097ed38d033338089dbd9be85d6c4d17,1,[M4],,1,0,Domain Private and Agnostic Feature for Modality Adaptive Face Recognition,"Heterogeneous face recognition is a challenging task due to the large modality discrepancy and insufficient cross-modal samples. Most existing works focus on discriminative feature transformation, metric learning and cross-modal face synthesis. However, the fact that cross-modal faces are always coupled by domain (modality) and identity information has received little attention. Therefore, how to learn and utilize the domain-private feature and domain-agnostic feature for modality adaptive face recognition is the focus of this work. Specifically, this paper proposes a Feature Aggregation Network (FAN), which includes disentangled representation module (DRM), feature fusion module (FFM) and adaptive penalty metric (APM) learning session. First, in DRM, two subnetworks, i.e. domain-private network and domain-agnostic network are specially designed for learning modality features and identity features, respectively. Second, in FFM, the identity features are fused with domain features to achieve cross-modal bi-directional identity feature transformation, which, to a large extent, further disentangles the modality information and identity information. Third, considering that the distribution imbalance between easy and hard pairs exists in cross-modal datasets, which increases the risk of model bias, the identity preserving guided metric learning with adaptive hard pairs penalization is proposed in our FAN. The proposed APM also guarantees the cross-modality intra-class compactness and inter-class separation. Extensive experiments on benchmark cross-modal face datasets show that our FAN outperforms SOTA methods.",2020,,2008.03848,,https://arxiv.org/pdf/2008.03848.pdf
5e127324c272c2dd3ac3f11f91da7716380eadf0,0,,,0,1,Template-Instance Loss for Offline Handwritten Chinese Character Recognition,"The long-standing challenges for offline handwritten Chinese character recognition (HCCR) are twofold: Chinese characters can be very diverse and complicated while similarly looking, and cursive handwriting (due to increased writing speed and infrequent pen lifting) makes strokes and even characters connected together in a flowing manner. In this paper, we propose the template and instance loss functions for the relevant machine learning tasks in offline handwritten Chinese character recognition. First, the character template is designed to deal with the intrinsic similarities among Chinese characters. Second, the instance loss can reduce category variance according to classification difficulty, giving a large penalty to the outlier instance of handwritten Chinese character. Trained with the new loss functions using our deep network architecture HCCR14Layer model consisting of simple layers, our extensive experiments show that it yields state-of-the-art performance and beyond for offline HCCR.",2019,2019 International Conference on Document Analysis and Recognition (ICDAR),1910.05545,10.1109/icdar.2019.00058,https://arxiv.org/pdf/1910.05545.pdf
605dd24753c9cad5e45e1a9415583c9e6188b293,1,,1,1,1,Margin based knowledge distillation for mobile face recognition,"With the rapid progress of face recognition it has more and more applications in everyday life. Although its backbone, very deep neural networks, also show improvement both in terms of accuracy and efficiency their computational cost and memory usage is still a limiting factor for deploying these models on a hardware with limited computational and power resources, such as mobile or embedded devices. Here arises the task of learning fast and compact deep neural networks which have a comparable accuracy to the complex model as requirement of real-life applications. Another issue is that sometimes face recognition system may run models of different complexity depending of the devices used for biometric template extraction (i.e. desktop with GPU or mobile phone), so the compatibility between the face descriptors is desirable. Our paper considers both this cases: we propose a new method for learning fast and compact face recognition model which has a similar performance to a much more complex model used for transferring its knowledge and we also show that both these models can be used for verification in a single face recognition system. To the best of our knowledge such evaluation of a compatibility between 2 different models for face recognition was never done before our work.",2020,International Conference on Machine Vision,,10.1117/12.2557244,
606d159ca979d352cecd9343faefa4ad5a2a7f24,1,[D10],,1,1,Diving into Optimization of Topology in Neural Networks,"Seeking effective networks has become one of the most crucial and practical areas in deep learning. The architecture of a neural network can be represented as a directed acyclic graph, whose nodes denote the transformation of layers and edges represent information flow. Despite the selection of micro node operations, macro connections among the whole network, noted as topology, largely affect the optimization process. We first rethink the residual connections via a new topological view and observe the benefits provided by dense connections to the optimization. Motivated by this, we propose an novel method to optimize the topology of a neural network. The optimization space is defined as a complete graph, and through assigning learnable weights which reflect the importance of connections, the optimization of topology is transformed into learning a set of continuous variables of edges. To extend the optimization to larger search spaces, a new series of networks, called TopoNet, are designed. To further focus on critical edges and promote generalization ability in dense topologies, an auxiliary sparsity constraint is adopted to constrain the distribution of edges. Experiments on classical networks prove the effectiveness of the optimization of topology. Experiments with TopoNets further verify both availability and transferability of the proposed method in different tasks e.g. image classification, object detection, and face recognition.",2019,,,,https://pdfs.semanticscholar.org/606d/159ca979d352cecd9343faefa4ad5a2a7f24.pdf
60b992139e5cf9a1683ea640d43aa7b1f24bdf3c,1,"[D9], [M5]",,1,0,Fairness in Biometrics: a figure of merit to assess biometric verification systems,"Machine learning-based (ML) systems are being largely deployed since the last decade in a myriad of scenarios impacting several instances in our daily lives. With this vast sort of applications, aspects of fairness start to rise in the spotlight due to the social impact that this can get in minorities. In this work aspects of fairness in biometrics are addressed. First, we introduce the first figure of merit that is able to evaluate and compare fairness aspects between multiple biometric verification systems, the so-called Fairness Discrepancy Rate (FDR). A use case with two synthetic biometric systems is introduced and demonstrates the potential of this figure of merit in extreme cases of fair and unfair behavior. Second, a use case using face biometrics is presented where several systems are evaluated compared with this new figure of merit using three public datasets exploring gender and race demographics.",2020,ArXiv,2011.02395,,https://arxiv.org/pdf/2011.02395.pdf
621db5ccdcdd9bc58fab2e256f74bb295bb568d3,0,,,0,1,Advancing Deep Learning for Automatic Autonomous Vision-based Power Line Inspection,"Electricity is fundamental to the ability to function of almost all modern-day societies. To maintain the reliability, availability, and sustainability of electricity supply, electric utilities are usually required to perform visual inspections on their electrical grids regularly. These inspections have been typically carried out using a combination of airborne surveys via low-flying helicopters and field surveys via foot patrol and tower climb. The primary purpose of these visual inspections is to plan for necessary repair or replacement works before any major damage that may lead to a power outage. These traditional inspection methods are not only slow and expensive but also potentially dangerous. In the past few years, numerous efforts have been made to automate these visual inspections. However, due to the high accuracy requirements of the task and its unique challenges, automatic vision-based inspection has not yet been widely adopted in this field. In this dissertation, we exploit recent advances in Deep Learning (DL), especially deep Convolutional Neural Networks (CNNs), and Unmanned Aerial Vehicle (UAV) technologies for facilitating automatic autonomous vision-based power line inspection. We propose a novel automatic autonomous vision-based power line inspection concept that uses UAV inspection as the main inspection method, optical images as the primary data source, and deep learning as the backbone of data analysis. Next, we conduct an extensive literature review on automatic vision-based power line inspection. Based on that, we identify the possibilities and six main challenges of DL vision-based UAV inspection: (i) the lack of training data; (ii) class imbalance; (iii) the detection of small power line components and defects; (iv) the detection of power lines in cluttered backgrounds; (v) the detection of previously unseen power line components and defects; and (vi) the lack of metrics for evaluating inspection performance. We address the first three challenges by creating four medium-sized datasets for training component detection and classification models, by applying a series of effective data augmentation techniques to balance out the imbalanced classes, and by utilizing multistage component detection and classification based on Single Shot multibox Detector (SDD) and deep Residual Networks (ResNets) to detect small power line components",2019,,,,
628a3f027b7646f398c68a680add48c7969ab1d9,0,,,1,0,Plan for Final Year Project : HKU-Face : A Large Scale Dataset for Deep Face Recognition,"Face recognition has been one of the most successful techniques in the field of artificial intelligence because of its surpassing human-level performance in academic experiments and broad application in the industrial world. Gaussian-face[1] and Facenet[2] hold state-of-the-art record using statistical method and deep-learning method respectively. What’s more, face recognition has been applied in various areas like authority checking and recording, fostering a large number of start-ups like Face.",2017,,,,https://pdfs.semanticscholar.org/628a/3f027b7646f398c68a680add48c7969ab1d9.pdf
65251e9715c740fb5f1a86a98615867e9eb634af,1,[M6],,0,1,Video Based Face Recognition by Using Discriminatively Learned Convex Models,"A majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual’s face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generatively learned convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on six of the eight tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX, IJB-C and ESOGU video datasets.",2020,International Journal of Computer Vision,,10.1007/s11263-020-01356-5,
6556b3a45c7627586fc1e2d3db449ce742648993,0,,,0,1,Investigation of Large-Margin Softmax in Neural Language Modeling,"To encourage intra-class compactness and inter-class separability among trainable feature vectors, large-margin softmax methods are developed and widely applied in the face recognition community. The introduction of the large-margin concept into the softmax is reported to have good properties such as enhanced discriminative power, less overfitting and well-defined geometric intuitions. Nowadays, language modeling is commonly approached with neural networks using softmax and cross entropy. In this work, we are curious to see if introducing large-margins to neural language models would improve the perplexity and consequently word error rate in automatic speech recognition. Specifically, we first implement and test various types of conventional margins following the previous works in face recognition. To address the distribution of natural language data, we then compare different strategies for word vector norm-scaling. After that, we apply the best norm-scaling setup in combination with various margins and conduct neural language models rescoring experiments in automatic speech recognition. We find that although perplexity is slightly deteriorated, neural language models with large-margin softmax can yield word error rate similar to that of the standard softmax baseline. Finally, expected margins are analyzed through visualization of word vectors, showing that the syntactic and semantic relationships are also preserved.",2020,INTERSPEECH,2005.10089,10.21437/interspeech.2020-1849,https://arxiv.org/pdf/2005.10089.pdf
659815bc880a2fa6d617cf1545ab5f0bcbcb5eda,1,[D9],,1,0,Defending Black Box Facial Recognition Classifiers Against Adversarial Attacks,"Defending adversarial attacks is a critical step towards reliable deployment of deep learning empowered solutions for biometrics verification. Current approaches for defending Black box models use the classification accuracy of the Black box as a performance metric for validating their defense. However, classification accuracy by itself is not a reliable metric to determine if the resulting image is ""adversarial-free"". This is a serious problem for online biometrics verification applications where the ground-truth of the incoming image is not known and hence we cannot compute the accuracy of the classifier or know if the image is ""adversarial-free"" or not. This paper proposes a novel framework for defending Black box systems from adversarial attacks using an ensemble of iterative adversarial image purifiers whose performance is continuously validated in a loop using Bayesian uncertainties. The proposed approach is (i) model agnostic, (ii) can convert single step black box defenses into an iterative defense and (iii) has the ability to reject adversarial examples. This paper uses facial recognition as a test case for validating the defense and experimental results on the MS-Celeb dataset show that the proposed approach can consistently detect adversarial examples and purify/reject them against a variety of adversarial attacks with different ranges of perturbations.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00414,https://www.vislab.ucr.edu/PUBLICATIONS/pubs/Journal%20and%20Conference%20Papers/after10-1-1997/Conference/2020/defending%20black%20box%20facial.pdf
661ee1178a17631313774023a5b0c9ea7d918474,0,,,0,1,Reverse-auction-based crowdsourced labeling for active learning,"In the past few years, Machine Learning (ML) has aroused great interest in both academic and industrial societies. ML is booming because of its huge application potential in many areas, such as facial recognition, natural language processing, self-driving car, and so on. Nevertheless, one of the key problems is the scarcity of labeled data. Fortunately, mobile crowdsourcing makes it possible to recruit mobile workers to label large-scale data by offering them small payments. In this paper, we use crowdsourcing to tackle the scarcity of training data in active learning, and then propose an approximately truthful, individually rational, privacy-preserving incentive mechanism with a guaranteed approximate performance , based on the single-minded reverse auction for data labeling in active learning. Different from prior works, we take crowd workers’ reliability into consideration when selecting data to be labeled which can improve the labeling quality and the model performance. In addition, we employ differential privacy to preserve workers’ bid privacy because a worker’s bid usually contains sensitive information. The simulation results demonstrate that the learning model is much accurate compared with the traditional active learning without the consideration of reliability in the case of the same number of iterations.",2019,World Wide Web,,10.1007/s11280-019-00744-3,
67a9659de0bf671fafccd7f39b7587f85fb6dfbd,1,[D9],,1,0,Ring Loss: Convex Feature Normalization for Face Recognition,"We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.0013,10.1109/CVPR.2018.00534,https://arxiv.org/pdf/1803.00130.pdf
69af7e9c522e327c835917bc9f03756cb8f1989a,1,"[M2], [M3]",,0,1,ChildFace: Gender Aware Child Face Aging,"Child face aging and rejuvenation has amassed considerable active research interest due to its immense impact on monitoring applications especially for finding lost/abducted children with childhood photos and hence protect children. Prior studies are primarily motivated to enhance the generation quality and aging of face images, rather than quantifying face recognition performance. To address this challenge we propose ChildFace model. Our model does child face aging and rejuvenation while using gender as condition. Our model uses Conditional Generative Adversarial Nets (cGANs), VGG19 based perceptual loss and LightCNN29 age classifier and produces impressive results. Intense quantitative study based on verification, identification and age estimation proves that our model is competent to existing state-of-art models and can make a significant contribution in identifying missing children.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
6a6535dda813b9b72dfe82ca652c8eacdf6eb196,0,,,0,1,A Technical Report for VIPriors Image Classification Challenge,"Image classification has always been a hot and challenging task. This paper is a brief report to our submission to the VIPriors Image Classification Challenge. In this challenge, the difficulty is how to train the model from scratch without any pretrained weight. In our method, several strong backbones and multiple loss functions are used to learn more representative features. To improve the models' generalization and robustness, efficient image augmentation strategies are utilized, like autoaugment and cutmix. Finally, ensemble learning is used to increase the performance of the models. The final Top-1 accuracy of our team DeepBlueAI is 0.7015, ranking second in the leaderboard.",2020,ArXiv,2007.08722,,https://arxiv.org/pdf/2007.08722.pdf
6b2e5f18e890d3ef905c09f5b649478b73df5a65,0,,,0,1,A Deep Learning Approach for Dog Face Verification and Recognition,"Recently, deep learning methods for biometrics identification have mainly focused on human face identification and have proven their efficiency. However, little research have been performed on animal biometrics identification. In this paper, a deep learning approach for dog face verification and recognition is proposed and evaluated. Due to the lack of available datasets and the complexity of dog face shapes this problem is harder than human identification. The first publicly available dataset is thus composed, and a deep convolutional neural network coupled with the triplet loss is trained on this dataset. The model is then evaluated on a verification problem, on a recognition problem and on clustering dog faces. For an open-set of 48 different dogs, it reaches an accuracy of 92% on a verification task and a rank-5 accuracy of 88% on a one-shot recognition task. The model can additionally cluster pictures of these unknown dogs. This work could push zoologists to further investigate these new kinds of techniques for animal identification or could help pet owners to find their lost animal. The code and the dataset of this project are publicly available (https://github.com/GuillaumeMougeot/DogFaceNet).",2019,PRICAI,,10.1007/978-3-030-29894-4_34,
6b8472d2c77c4879c87640eedbc301f6ee4fee2e,0,,,1,0,Survey on the Analysis and Modeling of Visual Kinship: A Decade in the Making.,"Kinship recognition is a challenging problem with many practical applications. With much progress and milestones having been reached after ten years - we are now able to survey the research and create new milestones. We review the public resources and data challenges that enabled and inspired many to hone-in on the views of automatic kinship recognition in the visual domain. The different tasks are described in technical terms and syntax consistent across the problem domain and the practical value of each discussed and measured. State-of-the-art methods for visual kinship recognition problems, whether to discriminate between or generate from, are examined. As part of such, we review systems proposed as part of a recent data challenge held in conjunction with the 2020 IEEE Conference on Automatic Face and Gesture Recognition. We establish a stronghold for the state of progress for the different problems in a consistent manner. This survey will serve as the central resource for the work of the next decade to build upon. For the tenth anniversary, the demo code is provided for the various kin-based tasks. Detecting relatives with visual recognition and classifying the relationship is an area with high potential for impact in research and practice.",2020,,2006.16033,,https://arxiv.org/pdf/2006.16033.pdf
6ca6ade6c9acb833790b1b4e7ee8842a04c607f7,0,,,1,0,Deep Transfer Network for Unconstrained Face Verification,"Face verification has achieved remarkable success on constrained settings where faces have frontal bias because of the limitation of face detection algorithms. However, unconstrained face verification is still a challenging problem and IJB-A dataset is an important benchmark in this setting. In this paper, we propose the Deep Transfer Network (DTN) which integrates transfer learning and attention-based feature aggregation mechanism together. First, we train a resnet with all its convolutions replaced by shufflenet-like convolutions supervised by A-softmax loss function. Then, we apply metric learning, feature aggregation and template adaptation successively to improve the performance. Our DTN is more efficient than previous methods thanks to its compact representation (256-d). And our DTN can produce results comparable to the state-of-the-art in the challenging face dataset, IJB-A.",2018,ICDLT '18,,10.1145/3234804.3234805,
6d91da37627c05150cb40cac323ca12a91965759,0,,,1,0,Gender Politics in the 2016 U.S. Presidential Election: A Computer Vision Approach,"Gender plays an important role in the 2016 U.S. presidential election, especially with Hillary Clinton becoming the first female presidential nominee and Donald Trump being frequently accused of sexism. In this paper, we introduce computer vision to the study of gender politics and present an image-driven method that can measure the effects of gender in an accurate and timely manner. We first collect all the profile images of the candidates’ Twitter followers. Then we train a convolutional neural network using images that contain gender labels. Lastly, we classify all the follower and unfollower images. Through a case study of the ‘woman card’ controversy, we demonstrate how gender is informing the 2016 presidential election. Our framework of analysis can be readily generalized to other case studies and elections.",2017,SBP-BRiMS,1611.02806,10.1007/978-3-319-60240-0_4,https://arxiv.org/pdf/1611.02806.pdf
6ef4245b61572b3fcee18a8e6cbe4f78f6103a41,0,,,0,1,A Novel Soft Margin Loss Function for Deep Discriminative Embedding Learning,"Deep embedding learning aims to learn discriminative feature representations through a deep convolutional neural network model. Commonly, such a model contains a network architecture and a loss function. The architecture is responsible for hierarchical feature extraction, while the loss function supervises the training procedure with the purpose of maximizing inter-class separability and intra-class compactness. By considering that loss function is crucial for the feature performance, in this article we propose a new loss function called soft margin loss (SML) based on a classification framework for deep embedding learning. Specifically, we first normalize the learned features and the classification weights to map them into the hypersphere. After that, we construct our loss with the difference between the maximum intra-class distance and minimum inter-class distance. By constraining the distance difference with a soft margin that is inherent in the proposed loss, both the inter-class discrepancy and intra-class compactness of learned features can be effectively improved. Finally, under the joint training with an improved softmax loss, the model can learn features with strong discriminability. Toy experiments on MNIST dataset are conducted to show the effectiveness of the proposed method. Additionally, experiments on re-identification tasks are also provided to demonstrate the superior performance of embedding learning. Specifically, 65.48% / 62.68% mAP on CUHK03 labeled / detected dataset (person re-id) and 74.36% mAP on VeRi-776 dataset (vehicle re-id) are achieved respectively.",2020,IEEE Access,,10.1109/ACCESS.2020.3036185,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09249024.pdf
6efa92deb42ff10c7d96d239b3e7c41637a3f06b,0,,,0,1,Rodlike nanoparticle parameter measurement method based on improved Mask R-CNN segmentation,,2020,,,10.1007/s11760-020-01779-0,
7060003d09260c0522fbab47b8757ff3ca1c1873,0,,,0,1,FeatherNets: Convolutional Neural Networks as Light as Feather for Face Anti-Spoofing,"Face Anti-spoofing gains increased attentions recently in both academic and industrial fields. With the emergence of various CNN based solutions, the multi-modal(RGB, depth and IR) methods based CNN showed better performance than single modal classifiers. However, there is a need for improving the performance and reducing the complexity. Therefore, an extreme light network architecture(FeatherNet A/B) is proposed with a streaming module which fixes the weakness of Global Average Pooling and uses less parameters. Our single FeatherNet trained by depth image only, provides a higher baseline with 0.00168 ACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure with ""ensemble + cascade"" structure is presented to satisfy the performance preferred use cases. Meanwhile, the MMFD dataset is collected to provide more attacks and diversity to gain better generalization. We use the fusion method in the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the result of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and 0.9814(TPR@FPR=10e-4).",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1904.0929,10.1109/CVPRW.2019.00199,https://arxiv.org/pdf/1904.09290.pdf
725c81c956b574df77fc0cbe0b9856e17ca88e2c,0,,,0,1,A Performance Comparison of Loss Functions,"Generally, the deep neural network learns by way of a loss function, which is an approach to evaluate how well given dataset is predicted on a particular network architecture (or network model). If the prediction deviates too far from real data, a loss function would generate a very large value. Progressively, with the help of some optimization function, the loss function lowers the prediction error by providing the network architecture with information that can control the weights of the network architecture. Thus, the loss functions plays an important role in training the network architecture.Recently, several researchers have studied various loss functions such as Softmax, Modified softmax, Angular softmax, Additive-Margin softmax, Arcface, Center, and Focal losses. In this manuscript, we propose a new and simple loss function that just adds the existing loss functions. In addition, we conduct experiments with the MNIST dataset in order to compare the performance between all loss functions including the proposed and the existing loss functions. Resultingly, the experiments show that the proposed loss function is visibly superior to the ability to classify digit images. The experimental results also indicate that Arcface loss and Additive-Margin loss functions satisfy predefined test accuracy most quickly under two and three dimensional embedding, respectively. The fast learning ability of the both loss functions has the advantage of providing relatively high accuracy even when the number of train data is small.",2019,2019 International Conference on Information and Communication Technology Convergence (ICTC),,10.1109/ictc46691.2019.8939902,
727d03100d4a8e12620acd7b1d1972bbee54f0e6,1,[D12],,1,0,von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification,"A number of pattern recognition tasks, \textit{e.g.}, face verification, can be boiled down to classification or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, \textit{i.e.}, compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face verification using 4 different challenging face datasets, \textit{i.e.}, LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves state-of-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.",2017,ArXiv,1706.04264,,https://arxiv.org/pdf/1706.04264.pdf
736968afc1b0312938d1298303cbadb04a58b700,1,[D9],,0,1,Occlusion-guided compact template learning for ensemble deep network-based pose-invariant face recognition,"Concatenation of the deep network representations extracted from different facial patches helps to improve face recognition performance. However, the concatenated facial template increases in size and contains redundant information. Previous solutions aim to reduce the dimensionality of the facial template without considering the occlusion pattern of the facial patches. In this paper, we propose an occlusion-guided compact template learning (OGCTL) approach that only uses the information from visible patches to construct the compact template. The compact face representation is not sensitive to the number of patches that are used to construct the facial template and is more suitable for incorporating the information from different view angles for image-set based face recognition. Instead of using occlusion masks in face matching (e.g., DPRFS [38]), the proposed method uses occlusion masks in template construction and achieves significantly better image-set based face verification performance on a challenging database with a template size that is an order-of-magnitude smaller than DPRFS.",2019,ArXiv,1903.04752,,https://arxiv.org/pdf/1903.04752.pdf
73e301ac549cce16e32553da96110e143d461142,0,,,0,1,Face representation by deep learning: a linear encoding in a parameter space?,"Recently, Convolutional Neural Networks (CNNs) have achieved tremendous performances on face recognition, and one popular perspective regarding CNNs' success is that CNNs could learn discriminative face representations from face images with complex image feature encoding. However, it is still unclear what is the intrinsic mechanism of face representation in CNNs. In this work, we investigate this problem by formulating face images as points in a shape-appearance parameter space, and our results demonstrate that: (i) The encoding and decoding of the neuron responses (representations) to face images in CNNs could be achieved under a linear model in the parameter space, in agreement with the recent discovery in primate IT face neurons, but different from the aforementioned perspective on CNNs' face representation with complex image feature encoding; (ii) The linear model for face encoding and decoding in the parameter space could achieve close or even better performances on face recognition and verification than state-of-the-art CNNs, which might provide new lights on the design strategies for face recognition systems; (iii) The neuron responses to face images in CNNs could not be adequately modelled by the axis model, a model recently proposed on face modelling in primate IT cortex. All these results might shed some lights on the often complained blackbox nature behind CNNs' tremendous performances on face recognition.",2019,ArXiv,1910.09768,,https://arxiv.org/pdf/1910.09768.pdf
73f38254a39b0dfa7447477100d580a766847d60,0,,,0,1,Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agenda,"Participation on social media platforms has many benefits but also poses substantial threats. Users often face an unintended loss of privacy, are bombarded with mis-/disinformation, or are trapped in filter bubbles due to over-personalized content. These threats are further exacerbated by the rise of hidden AI-driven algorithms working behind the scenes to shape users' thoughts, attitudes, and behaviour. We investigate how multimedia researchers can help tackle these problems to level the playing field for social media users. We perform a comprehensive survey of algorithmic threats on social media and use it as a lens to set a challenging but important research agenda for effective and real-time user nudging. We further implement a conceptual prototype and evaluate it with experts to supplement our research agenda. This paper calls for solutions that combat the algorithmic threats on social media by utilizing machine learning and multimedia content analysis techniques but in a transparent manner and for the benefit of the users.",2020,ACM Multimedia,2009.07632,10.1145/3394171.3414692,https://arxiv.org/pdf/2009.07632.pdf
750486b2278ba542987655eede1b4da8c10f7f2e,1,[D10],,1,1,Softmax Dissection: Towards Understanding Intra- and Inter-clas Objective for Embedding Learning,"The softmax loss and its variants are widely used as objectives for embedding learning, especially in applications like face recognition. However, the intra- and inter-class objectives in the softmax loss are entangled, therefore a well-optimized inter-class objective leads to relaxation on the intra-class objective, and vice versa. In this paper, we propose to dissect the softmax loss into independent intra- and inter-class objective (D-Softmax). With D-Softmax as objective, we can have a clear understanding of both the intra- and inter-class objective, therefore it is straightforward to tune each part to the best state. Furthermore, we find the computation of the inter-class objective is redundant and propose two sampling-based variants of D-Softmax to reduce the computation cost. Training with regular-scale data, experiments in face verification show D-Softmax is favorably comparable to existing losses such as SphereFace and ArcFace. Training with massive-scale data, experiments show the fast variants of D-Softmax significantly accelerates the training process (such as 64x) with only a minor sacrifice in performance, outperforming existing acceleration methods of softmax in terms of both performance and efficiency.",2020,AAAI,1908.01281,10.1609/AAAI.V34I07.6729,https://arxiv.org/pdf/1908.01281.pdf
7522679b432649785f1355a7013d448b837a08c2,0,,,0,1,Facial Emotion Recognition using Neighborhood Features,"We present a new method for human facial emotions recognition. For this purpose, initially, we detect faces in the images by using the famous cascade classifiers. Subsequently, we then extract a localized regional descriptor (LRD) which represents the features of a face based on regional appearance encoding. The LRD formulates and models various spatial regional patterns based on the relationships between local areas themselves instead of considering only raw and unprocessed intensity features of an image. To classify facial emotions into various classes of facial emotions, we train a multiclass support vector machine (M-SVM) classifier which recognizes these emotions during the testing stage. Our proposed method takes into account robust features and is independent of gender and facial skin color for emotion recognition. Moreover, our method is illumination and orientation invariant. We assessed our method on two benchmark datasets and compared it with four reference methods. Our proposed method outperformed them considering both the datasets.",2020,,,10.14569/ijacsa.2020.0110137,https://pdfs.semanticscholar.org/7522/679b432649785f1355a7013d448b837a08c2.pdf
769b03fca1aeceafdd2cbfb82216c6e616f69c3d,0,,,0,1,Long-Term Face Tracking for Crowded Video-Surveillance Scenarios,"Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that do not operate in real-time, often making them impractical for video-surveillance. In this paper, we present a long-term multi-face tracking architecture conceived for working in crowded contexts, particularly unconstrained in terms of movement and occlusions, and where the face is often the only visible part of the person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on face verification. Additionally, a correction module is included to correct past track assignments with no extra computational cost. We present a series of experiments introducing novel, specialized metrics for the evaluation of long-term tracking capabilities and a video dataset that we publicly release. Findings demonstrate that, in this context, our approach allows to obtain up to 50% longer tracks than state-of-the-art deep learning trackers.",2020,ArXiv,2010.08675,,https://arxiv.org/pdf/2010.08675.pdf
76cd5e43df44e389483f23cb578a9015d1483d70,1,[M5],,1,0,Face Verification from Depth using Privileged Information,"In this paper, a deep Siamese architecture for depth-based face verification is presented. The proposed approach efficiently verifies if two face images belong to the same person while handling a great variety of head poses and occlusions. The architecture, namely JanusNet, consists in a combination of a depth, a RGB and a hybrid Siamese network. During the training phase, the hybrid network learns to extract complementary mid-level convolutional features which mimic the features of the RGB network, simultaneously leveraging on the light invariance of depth images. At testing time, the model, relying only on depth data, achieves state-of-art results and real time performance, despite the lack of deep-oriented depth-based datasets.",2018,BMVC,,,http://bmvc2018.org/contents/papers/0410.pdf
77d2963441c35514fcc10465b46025cc94797a76,0,,,0,1,Asymmetric metric learning for knowledge transfer,"Knowledge transfer from large teacher models to smaller student models has recently been studied for metric learning, focusing on fine-grained classification. In this work, focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we introduce asymmetric metric learning, a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task.  We systematically evaluate different teacher and student models, metric learning and knowledge transfer loss functions on the new asymmetric testing as well as the standard symmetric testing task, where database and queries are represented by the same model. We find that plain regression is surprisingly effective compared to more complex knowledge transfer mechanisms, working best in asymmetric testing. Interestingly, our asymmetric metric learning approach works best in symmetric testing, allowing the student to even outperform the teacher.",2020,ArXiv,2006.16331,,https://arxiv.org/pdf/2006.16331.pdf
7b0ca99b4109f715aab8203cbed7283805e5851d,1,[M3],,1,1,Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition,"Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15% F1-score increase respect to non-adaptive state-of-theart methods.",2020,,2012.09571,,https://arxiv.org/pdf/2012.09571.pdf
7b0ed3d67375a4542133c992f4e55fd4ade0cd90,1,[D9],,0,1,Knowledge Distillation via Route Constrained Optimization,"Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a higher lower bound of congruence loss. In this work, we consider the knowledge distillation from the perspective of curriculum learning by teacher's routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace. RCO achieves 84.3% accuracy on one-to-million task with only 0.8 M parameters, which push the SOTA by a large margin.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.09149,10.1109/ICCV.2019.00143,https://arxiv.org/pdf/1904.09149.pdf
7c9ef04c66bfc5c0ae4a6ae175a3f13414db8ecb,0,,,1,1,SISTEMA PARA EL RECONOCIMIENTO DE PERSONAS POR UN ROBOT MÓVIL ASISTENTE PEOPLE RECOGNITION SYSTEM FOR AN ASSISTIVE MOBILE ROBOT,"Human-Robot interaction is increasingly relevant to the field of robotics, as mobile robots are used in education, health care, or assisting humans in everyday tasks. In these applications, providing a personalized service is paramount to reach a satisfactory end-user experience. A required feature to yield such a service is to be able to recognize which person the robot has to interact with. To address that issue, this Bachelor’s thesis explores face recognition: a nonintrusive, autonomous approach of identification using biometric identifiers from an user’s face. Face recognition has gained relevancy in the recent years and can be used reliably in certain applications, as the advances in machine learning and the creation of huge public datasets have improved state-of-the-art performance considerably. In this way, the purpose of this work is to adapt and integrate a modern face recognition pipeline in the Robot Operating System (ROS), the most popular robotic software architecture, for its use in home environments by service robots. To accomplish this task, a number of open-source implementations for face detection and deep feature extraction have been considered, two of the main components of a face recognition pipeline. Additionally, a pose estimation method named OpenPose has been employed for the task of face detection, given that it has other useful features that can be applied to human-robot interaction, for example, approaching humans safely, or recognizing non-verbal behavior. These methods have been compared in terms of accuracy and performance in common benchmark datasets to aid the choice of the final implementation, which has been integrated in the ROS ecosystem.",2019,,,,https://pdfs.semanticscholar.org/7c9e/f04c66bfc5c0ae4a6ae175a3f13414db8ecb.pdf
7fa4e972da46735971aad52413d17c4014c49e6e,1,[D9],,1,0,How to Train Triplet Networks with 100K Identities?,"Training triplet networks with large-scale data is challenging in face recognition. Due to the number of possible triplets explodes with the number of samples, previous studies adopt the online hard negative mining(OHNM) to handle it. However, as the number of identities becomes extremely large, the training will suffer from bad local minima because effective hard triplets are difficult to be found. To solve the problem, in this paper, we propose training triplet networks with subspace learning, which splits the space of all identities into subspaces consisting of only similar identities. Combined with the batch OHNM, hard triplets can be found much easier. Experiments on the large-scale MS-Celeb-1M challenge with 100 K identities demonstrate that the proposed method can largely improve the performance. In addition, to deal with heavy noise and large-scale retrieval, we also make some efforts on robust noise removing and efficient image retrieval, which are used jointly with the subspace learning to obtain the state-of-the-art performance on the MS-Celeb-1M competition (without external data in Challenge1).",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),1709.0294,10.1109/ICCVW.2017.225,https://arxiv.org/pdf/1709.02940.pdf
801639c024a46149bddb74398f1c3d57661a95b9,1,[D9],,1,0,Faithful Face Image Completion for HMD Occlusion Removal,"Head-mounted-displays (HMDs) provide immersive experiences of virtual content. While being flexible, HMDs could be a hindrance for Virtual Reality (VR) applications such as VR teleconference where facial components and expressions of the user are partially occluded thus cannot be seen by others. We present an automatic face image completion solution that treats the occluded region as a hole and completes the hole with the help of an occlusion-free reference image of the same person. Given the occluded input image and an occlusion-free reference image, our method first computes head pose features from estimated facial landmarks. The head pose features, as well as images, are then fed into a generative adversarial network (GAN) to synthesize the output image. Our method can generate faithful results from various input cases and outperforms other face completion methods. It provides a light-weighted solution to HMD occlusion removal and has the potential to benefit VR applications.",2019,2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),,10.1109/ISMAR-Adjunct.2019.00-36,
81e028b9ef5e6e6a1c6661fdd9120a2d955bd239,1,[D10],,1,1,DebFace: De-biasing Face Recognition,"We address the problem of bias in automated face recognition algorithms, where errors are consistently lower on certain cohorts belonging to specific demographic groups. We present a novel de-biasing adversarial network that learns to extract disentangled feature representations for both unbiased face recognition and demographics estimation. The proposed network consists of one identity classifier and three demographic classifiers (for gender, age, and race) that are trained to distinguish identity and demographic attributes, respectively. Adversarial learning is adopted to minimize correlation among feature factors so as to abate bias influence from other factors. We also design a new scheme to combine demographics with identity features to strengthen robustness of face representation in different demographic groups. The experimental results show that our approach is able to reduce bias in face recognition as well as demographics estimation while achieving state-of-the-art performance.",2019,ArXiv,1911.0808,,http://cvlab.cse.msu.edu/pdfs/Gong_Liu_Jain_arxiv2019.pdf
8239255ebe5a346c8fc7732f32c1fa7e88b62050,0,,,0,1,Outlier-Robust Neural Aggregation Network for Video Face Identification,"Current approaches for video face recognition rely on image sets containing faces of exclusively one identity. However, as image sets are created by unsupervised methods, it is necessary to consider outlier-afflicted sets for real-life applications. In this paper, we propose an Outlier-Robust Neural Aggregation Network (ORNAN). First, we embed each image into a feature space using a Convolutional Neural Network (CNN). With the help of two cascaded attention blocks, we predict outliers within the image set. By integrating this knowledge into our aggregation network, we adaptively aggregate all feature vectors to form a single feature, mitigating the influence of outliers and noisy features. We show that our network is robust against outliers using outlier-afflicted IJB-B and IJB-C benchmarks while maintaining similar performance without outliers.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803028,
838e7b46185e72c43dda1ceb2d5f1a0154542c16,0,,,1,0,An Ethical Highlighter for People-Centric Dataset Creation,"Important ethical concerns arising from computer vision datasets of people have been receiving significant attention, and a number of datasets have been withdrawn as a result. To meet the academic need for people-centric datasets, we propose an analytical framework to guide ethical evaluation of existing datasets and to serve future dataset creators in avoiding missteps. Our work is informed by a review and analysis of prior works and highlights where such ethical challenges arise.",2020,ArXiv,2011.13583,,https://arxiv.org/pdf/2011.13583.pdf
846cc70e137dd6e090f58d1fedac8677a6ec609b,1,[D10],,1,1,Discrimination-aware Network Pruning for Deep Model Compression,"We study network pruning which aims to remove redundant channels/kernels and hence speed up the inference of deep networks. Existing pruning methods either train from scratch with sparsity constraints or minimize the reconstruction error between the feature maps of the pre-trained models and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, while the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. In this paper, we propose a simple-yet-effective method called discrimination-aware channel pruning (DCP) to choose the channels that actually contribute to the discriminative power. Note that a channel often consists of a set of kernels. Besides the redundancy in channels, some kernels in a channel may also be redundant and fail to contribute to the discriminative power of the network, resulting in kernel level redundancy. To solve this, we propose a discrimination-aware kernel pruning (DKP) method to further compress deep networks by removing redundant kernels. To prevent DCP/DKP from selecting redundant channels/kernels, we propose a new adaptive stopping condition, which helps to automatically determine the number of selected channels/kernels and often results in more compact models with better performance. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of our methods. For example, on ILSVRC-12, the resultant ResNet-50 model with 30% reduction of channels even outperforms the baseline model by 0.36% in terms of Top-1 accuracy. The pruned MobileNetV1 and MobileNetV2 achieve 1.93x and 1.42x inference acceleration on a mobile device, respectively, with negligible performance degradation. The source code and the pre-trained models are available at this https URL.",2020,ArXiv,2001.0105,,https://arxiv.org/pdf/2001.01050.pdf
8550d18f7148d0aa1fec89b4871a40b2ff28be84,0,,,0,1,Expression-Aware Face Reconstruction Via A Dual-Stream Network,"Recently, 3D face reconstruction from a single image has achieved promising results by adopting the 3D Morphable Model (3DMM). However, as face images in-the-wild have various expressions, it is difficult for 3DMM to handle diverse facial expressions with a large range of variations, due to the limited expressive ability of its linear model, thereby resulting in distortion and ambiguity on facial local regions. To tackle this issue, we present a novel dual-stream network to deal with expression variations. Specifically, in the geometry stream, we propose novel Attribute Spatial Maps to record the spatial information of facial identity and expression attributes in the 2D image space separately. This avoids the interaction between the two attributes, thus keeping the identity information and further improving the ability to cope with expression changes. In the texture stream, we utilize the 3DMM albedo map to a style transfer based method for synthesizing facial appearance, which results in expression-irrelevant as well as realistic face textures. Both quantitative and qualitative evaluations on public datasets demonstrate the ability of our approach to achieve comparable results in face reconstructions under expression variations.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/icme46284.2020.9102811,
8598526318f685ea29dd151c09e72cf4929b7838,0,,,0,1,Triple ANet: Adaptive Abnormal-aware Attention Network for WCE Image Classification,"Accurate detection of abnormal regions in Wireless Capsule Endoscopy (WCE) images is crucial for early intestine cancer diagnosis and treatment, while it still remains challenging due to the relatively low contrasts and ambiguous boundaries between abnormalities and normal regions. Additionally, the huge intra-class variances, alone with the high degree of visual similarities shared by inter-class abnormalities prevent the network from robust classification. To tackle these dilemmas, we propose an Adaptive Abnormal-aware Attention Network (Triple ANet) with Adaptive Dense Block (ADB) and Abnormal-aware Attention Module (AAM) for automatic WCE image analysis. ADB is designed to assign one attention score for each dense connection in dense blocks and to enhance useful features, while AAM aims to adaptively adjust the respective field according to the abnormal regions and help pay attention to abnormalities. Moreover, we propose a novel Angular Contrastive loss (AC Loss) to reduce the intra-class variances and enlarge the inter-class differences effectively. Our methods achieved 89.41% overall accuracy and showed better performance compared with state-of-the-art WCE image classification methods. The source code is available at https://github.com/Guo-Xiaoqing/Triple-ANet.",2019,MICCAI,,10.1007/978-3-030-32239-7_33,
865cab74e0c9b32698a4972266a5261f7a144b1c,1,"[D9], [D13], [D14], [M3]",,1,1,Global-Local GCN: Large-Scale Label Noise Cleansing for Face Recognition,"In the field of face recognition, large-scale web-collected datasets are essential for learning discriminative representations, but they suffer from noisy identity labels, such as outliers and label flips. It is beneficial to automatically cleanse their label noise for improving recognition accuracy. Unfortunately, existing cleansing methods cannot accurately identify noise in the wild. To solve this problem, we propose an effective automatic label noise cleansing framework for face recognition datasets, FaceGraph. Using two cascaded graph convolutional networks, FaceGraph performs global-to-local discrimination to select useful data in a noisy environment. Extensive experiments show that cleansing widely used datasets, such as CASIA-WebFace, VGGFace2, MegaFace2, and MS-Celeb-1M, using the proposed method can improve the recognition performance of state-of-the-art representation learning methods like Arcface. Further, we cleanse massive self-collected celebrity data, namely MillionCelebs, to provide 18.8M images of 636K identities. Training with the new data, Arcface surpasses state-of-the-art performance by a notable margin to reach 95.62% TPR at 1e-5 FPR on the IJB-C benchmark.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.00775,https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.pdf
86a2301e7e4779e7ad3d1abda9d188f9ace4f1f3,1,,1,1,1,LinCos-Softmax: Learning Angle-Discriminative Face Representations With Linearity-Enhanced Cosine Logits,"In recent years, the angle-based softmax losses have significantly improved the performance of face recognition whereas these loss functions are all based on cosine logit. A potential weakness is that the nonlinearity of the cosine function may undesirably saturate the angular optimization between the features and the corresponding weight vectors, thereby preventing the network from fully learning to maximize the angular discriminability of features. As a result, the generalization of learned features may be compromised. To tackle this issue, we propose a Linear-Cosine Softmax Loss (LinCos-Softmax) to more effectively learn angle-discriminative facial features. The main characteristic of the loss function we propose is the use of an approximated linear logit. Compared with the conventional cosine logit, it has a stronger linear relationship with the angle on enhancing angular discrimination through Taylor expansion. We also propose an automatic scale parameter selection scheme, which can conveniently provide an appropriate scale for different logits without the need for exhaustive parameter search to improve performance. In addition, we propose a margin-enhanced Linear-Cosine Softmax Loss (m-LinCos-Softmax) to further enlarge inter-class distances and reduce intra-class variations. Experimental results on several face recognition benchmarks (LFW, AgeDB-30, CFP-FP, MegaFace Challenge 1) demonstrate the effectiveness of the proposed method and its superiority to existing angular softmax loss variants.",2020,IEEE Access,,10.1109/ACCESS.2020.3002270,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09116942.pdf
87e78bcc187eabc8bc1689602f510b18ada4ce9e,1,[D14],,1,0,Deep Residual Equivariant Mapping for Multi-angle Face Recognition,"Face recognition has caught a lot of attention and plenty of valuable methods have been proposed during the past decades. However, because it is hard to learn geometrically invariant representations, existing face recognition methods still perform relatively poorly in conducting multi-angle face recognition. In this paper, we hypothesize that there is an inherent mapping between the frontal and non-frontal faces, and the non-frontal face representations can be converted into the frontal face representations by an equivariant mapping. To carry out the mapping, we propose a Multi-Angle Deep Residual Equivariant Mapping (MADREM) block which adaptively maps the non-frontal face representation to the frontal face representation. It can be considered the MADREM block carry out face alignment and face normalization in the feature space. The residual equivariant mapping block can enhance the discriminative power of the face representations. Finally, we achieve an accuracy of 99.78% on the LFW dataset and 94.25% on CFP-FP dataset based on proposed multiscale-convolution and residual equivariant mapping block.",2019,CCBR,,10.1007/978-3-030-31456-9_16,
88e03dacc1934d638f39153bad479c4333145ab3,0,,,0,1,Subjective Versus Objective Face Image Quality Evaluation For Face Recognition,"The performance of any face recognition system gets affected by the quality of the probe and the reference images. Rejecting or recapturing images with low-quality can improve the overall performance of the biometric system. There are many statistical as well as learning-based methods that provide quality scores given an image for the task of face recognition. In this study, we take a different approach by asking 26 participants to provide subjective quality scores that represent the ease of recognizing the face on the images from a smartphone based face image dataset. These scores are then compared to measures implemented from ISO/IEC TR 29794-5. We observe that the subjective scores outperform the implemented objective scores while having a low correlation with them. Furthermore, we analyze the effect of pose, illumination, and distance on face recognition similarity scores as well as the generated mean opinion scores.",2019,ICBEA,,10.1145/3345336.3345338,
8accd7a0f4fbb86ed1ebd6eae20b344b42727410,1,[D9],,1,0,Audio-visual Speech Separation with Adversarially Disentangled Visual Representation,"Speech separation aims to separate individual voice from an audio mixture of multiple simultaneous talkers. Although audio-only approaches achieve satisfactory performance, they build on a strategy to handle the predefined conditions, limiting their application in the complex auditory scene. Towards the cocktail party problem, we propose a novel audio-visual speech separation model. In our model, we use the face detector to detect the number of speakers in the scene and use visual information to avoid the permutation problem. To improve our model’s generalization ability to unknown speakers, we extract speech-related visual features from visual inputs explicitly by the adversarially disentangled method, and use this feature to assist speech separation. Besides, the time-domain approach is adopted, which could avoid the phase reconstruction problem existing in the time-frequency domain models. To compare our model’s performance with other models, we create two benchmark datasets of 2-speaker mixture from GRID and TCDTIMIT audio-visual datasets. Through a series of experiments, our proposed model is shown to outperform the state-of-the-art audio-only model and three audio-visual models.",2020,ArXiv,2011.14334,,https://arxiv.org/pdf/2011.14334.pdf
8b2c695865ddcd63f0fe83cd0c5abca26bacb295,0,,,0,1,Harnessing GAN with Metric Learning for One-Shot Generation on a Fine-Grained Category,"We propose a GAN-based one-shot generation method on a fine-grained category, which represents a subclass of a category, typically with diverse examples. One-shot generation refers to a task of taking an image which belongs to a class not used in the training phase and then generating a set of new images belonging to the same class. Generative Adversarial Network (GAN), which represents a type of deep neural networks with competing generator and discriminator, has proven to be useful in generating realistic images. Especially DAGAN, which maps the input image to a low-dimensional space via an encoder and then back to the example space via a decoder, has been quite effective with datasets such as handwritten character datasets. However, when the class corresponds to a fine-grained category, DAGAN occasionally generates images which are regarded as belonging to other classes due to the rich variety of the examples in the class and the low dissimilarities of the examples among the classes. For example, it accidentally generates facial images of different persons when the class corresponds to a specific person. To circumvent this problem, we introduce a metric learning with a triplet loss to the bottleneck layer of DAGAN to penalize such a generation. We also extend the optimization algorithm of DAGAN to an alternating procedure for two types of loss functions. Our proposed method outperforms DAGAN in the GAN-test task for VGG-Face dataset and CompCars dataset by 5.6% and 4.8% in accuracy, respectively. We also conducted experiments for the data augmentation task and observed 4.5% higher accuracy for our proposed method over DAGAN for VGG-Face dataset.",2019,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),,10.1109/ICTAI.2019.00130,
8d5aa448fefeed5646143b56978c6f4337887f0e,1,[M3],,1,1,Deep Feature Augmentation for Occluded Image Classification,"Abstract Due to the difficulty in acquiring massive task-specific occluded images, the classification of occluded images with deep convolutional neural networks (CNNs) remains highly challenging. To alleviate the dependency on large-scale occluded image datasets, we propose a novel approach to improve the classification accuracy of occluded images by fine-tuning the pre-trained models with a set of augmented deep feature vectors (DFVs). The set of augmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs are generated by randomly adding difference vectors (DVs), extracted from a small set of clean and occluded image pairs, to the real DFVs. In the fine-tuning, the back-propagation is conducted on the DFV data flow to update the network parameters. The experiments on various datasets and network structures show that the deep feature augmentation significantly improves the classification accuracy of occluded images without a noticeable influence on the performance of clean images. Specifically, on the ILSVRC2012 dataset with synthetic occluded images, the proposed approach achieves 11.21% and 9.14% average increases in classification accuracy for the ResNet50 networks fine-tuned on the occlusion-exclusive and occlusion-inclusive training sets, respectively.",2020,ArXiv,2011.00768,10.1016/j.patcog.2020.107737,https://arxiv.org/pdf/2011.00768.pdf
8d965fbe99b5bdd71d764f502b239aae5dde89ec,0,,,0,1,Research on Inception Module Incorporated Siamese Convolutional Neural Networks to Realize Face Recognition,"Face recognition is an active research subject of biometrics due to its significant research and application prospects. The performance of face recognition can be affected by a series of uncontrollable factors, such as illumination, expression, posture and occlusion, which restricts its real-world applications. Therefore, improving the robustness of face recognition to environmental changes became an urgent problem. In this paper, a simplified deep convolutional neural network structure having high robustness under unlimited conditions is designed for face recognition. This structure can improve training speed and face recognition accuracy, and be suitable for small-scale data sets. Inception Module Incorporated Siamese Convolutional Neural Networks (IMISCNN) is developed based on effective reduction of external interference and better features extraction by adopting the Siamese network structure. A cyclical learning rate strategy is also introduced in IMISCNN for better model convergence. Compared to classical face recognition algorithms, such as PCA, PCA and SVM, CNN, PCANet, and the original SNN et al. The accuracy of IMISCNN in CASIA-webface and Extended Yale B standard face database is 99.36% and 99.21%, respectively. Its feasibility and effectiveness have been verified in our experiments.",2020,IEEE Access,,10.1109/ACCESS.2019.2963211,
8de1c724a42d204c0050fe4c4b4e81a675d7f57c,0,,,1,0,Deep Face Recognition: A Survey,"Face recognition made tremendous leaps in the last five years with a myriad of systems proposing novel techniques substantially backed by deep convolutional neural networks (DCNN). Although face recognition performance sky-rocketed using deep-learning in classic datasets like LFW, leading to the belief that this technique reached human performance, it still remains an open problem in unconstrained environments as demonstrated by the newly released IJB datasets. This survey aims to summarize the main advances in deep face recognition and, more in general, in learning face representations for verification and identification. The survey provides a clear, structured presentation of the principal, state-of-the-art (SOTA) face recognition techniques appearing within the past five years in top computer vision venues. The survey is broken down into multiple parts that follow a standard face recognition pipeline: (a) how SOTA systems are trained and which public data sets have they used; (b) face preprocessing part (detection, alignment, etc.); (c) architecture and loss functions used for transfer learning (d) face recognition for verification and identification. The survey concludes with an overview of the SOTA results at a glance along with some open issues currently overlooked by the community.",2018,"2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",,10.1109/SIBGRAPI.2018.00067,https://talhassner.github.io/home/projects/DeepFaceSurvey/Masietal2018deepfacesurvey.pdf
8e000216feb96cf335bcff744ccfc2bd428080d4,0,,,0,1,Data-Free Point Cloud Network for 3D Face Recognition,"Point clouds-based Networks have achieved great attention in 3D object classification, segmentation and indoor scene semantic parsing. In terms of face recognition, 3D face recognition method which directly consume point clouds as input is still under study. Two main factors account for this: One is how to get discriminative face representations from 3D point clouds using deep network; the other is the lack of large 3D training dataset. To address these problems, a data-free 3D face recognition method is proposed only using synthesized unreal data from statistical 3D Morphable Model to train a deep point cloud network. To ease the inconsistent distribution between model data and real faces, different point sampling methods are used in train and test phase. In this paper, we propose a curvature-aware point sampling(CPS) strategy replacing the original furthest point sampling(FPS) to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate features deeply. A PointNet++ like Network is used to extract face features directly from point clouds. The experimental results show that the network trained on generated data generalizes well for real 3D faces. Fine tuning on a small part of FRGCv2.0 and Bosphorus, which include real faces in different poses and expressions, further improves recognition accuracy.",2019,ArXiv,1911.04731,,https://arxiv.org/pdf/1911.04731.pdf
8fc6702796837ee227cc29e9f7e07d0c4fd4a026,0,,,0,1,Can Generative Colourisation Help Face Recognition?,"Generative colourisation methods can be applied to automatically convert greyscale images to realistically looking colour images. In a face recognition system, such techniques might be employed as a pre-processing step in scenarios where either one or both face images to be compared are only available in greyscale format. In an experimental setup which reflects said scenarios, we investigate if generative colourisation can improve face sample utility and overall biometric performance of face recognition. To this end, subsets of the FERET and FRGCv2 face image databases are converted to greyscale and colourised applying two versions of the DeOldify colourisation algorithm. Face sample quality assessment is done using the FaceQnet quality estimator. Biometric performance measurements are conducted for the widely used ArcFace system and reported according to standardised metrics. Obtained results indicate that, for the tested systems, the application of generative colourisation does neither improve face image quality nor recognition performance. However, generative colourisation was found to aid face detection and subsequent feature extraction of the used face recognition system which results in a decrease of the overall 0 reject rate.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
8fee9b8c44626c4ac6b96ef183394bc4f36dc95f,1,,1,1,0,Quantifying Facial Age by Posterior of Age Comparisons,"We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person's actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed `MegaAge', which consists of 41,941 images. Data can be downloaded from our project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and github.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-the-art results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.",2017,BMVC,1708.09687,10.5244/C.31.108,https://arxiv.org/pdf/1708.09687.pdf
90307ae0012d5388c6786a2a0ca4685f76aabfc5,0,,,0,1,Speaker Representation Learning using Global Context Guided Channel and Time-Frequency Transformations,"In this study, we propose the global context guided channel and time-frequency transformations to model the long-range, non-local time-frequency dependencies and channel variances in speaker representations. We use the global context information to enhance important channels and recalibrate salient time-frequency locations by computing the similarity between the global context and local features. The proposed modules, together with a popular ResNet based model, are evaluated on the VoxCeleb1 dataset, which is a large scale speaker verification corpus collected in the wild. This lightweight block can be easily incorporated into a CNN model with little additional computational costs and effectively improves the speaker verification performance compared to the baseline ResNet-LDE model and the Squeeze&Excitation block by a large margin. Detailed ablation studies are also performed to analyze various factors that may impact the performance of the proposed modules. We find that by employing the proposed L2-tf-GTFC transformation block, the Equal Error Rate decreases from 4.56% to 3.07%, a relative 32.68% reduction, and a relative 27.28% improvement in terms of the DCF score. The results indicate that our proposed global context guided transformation modules can efficiently improve the learned speaker representations by achieving time-frequency and channel-wise feature recalibration.",2020,,,,https://isca-speech.org/archive/Interspeech_2020/pdfs/1845.pdf
918996bac4632a1673e69e2907b145dabc20dda4,0,,,0,1,Stacked Multi-Target Network for Robust Facial Landmark Localisation,"We thoroughly analyse regression-based face alignment methods and introduce a novel stacked multi-target network for robust facial landmark localisation. The primary heatmap regression-based network concentrates on locating the coarse position of pre-defined landmarks while the secondary coordinate regression-based network is responsible for modelling fine sub-pixel features. Specifically, we elaborate the differences among widely-used Cross Entropy related loss functions and propose a new Bilateral Inhibition Cross Entropy loss function, which enlarges the margin between elements in the output heatmaps. Besides, in order to deal with the discrepancy between optimization and evaluation, we propose to dynamically adjust the radius of kernel function during the training process. We demonstrate that training with decreasing radius in temporal order performs much better than assigning it spatially, i.e. decreasing radius along the stages of stacked hourglass networks. Finally, we innovatively limit the output of the secondary coordinate regression network to a reasonable range by importing the hinge loss to refine the coarse coordinate locations for sub-pixel accuracy. Extensive experiments on public datasets such as 300-W, COFW, and AFLW demonstrate that our proposed method performs superiorly to the state-of-the-art approaches.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00028,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Yang_Stacked_Multi-Target_Network_for_Robust_Facial_Landmark_Localisation_CVPRW_2019_paper.pdf
926f5674a8143733241a544bcaa0dcfc3ab42646,0,,,1,0,A Comparative Study of Face Re-identification Systems under Real-World Conditions,"Face re-identification is largely thought of as a solved problem in the research community, with State-ofthe-art systems attaining human-level performance on unconstrained image datasets. However, these results do not seem to translate to the real world. In systems matching people in surveillance-like footage to highquality images, reported performance is much lower than what the literature would suggest. In this work, we contribute a multi-modal dataset for evaluating real-world performance of facial re-identification systems. We then perform a verification and re-identification evaluation for state-of-art systems on both this dataset and the popular benchmarking dataset Labelled Faces in the Wild (LFW).",2018,,,,https://pure.qub.ac.uk/portal/files/154793203/IMVIP_2018_paper_18.pdf
94a3b65b358d0fd8d161110004ecd137a6ec557f,1,[D9],,1,0,A Plug-in Method for Representation Factorization,"In this work, we focus on decomposing the latent representations in GANs or learned feature representations in deep auto-encoders into semantically controllable factors in a semi-supervised manner, without modifying the original trained models. Specifically, we propose a Factors Decomposer-Entangler Network (FDEN) that learns to decompose a latent representation into mutually independent factors. Given a latent representation, the proposed framework draws a set of interpretable factors, each aligned to independent factors of variations by maximizing their total correlation in an information-theoretic means. As a plug-in method, we have applied our proposed FDEN to the existing networks of Adversarially Learned Inference and Pioneer Network and conducted computer vision tasks of image-to-image translation in semantic ways, e.g., changing styles while keeping an identify of a subject, and object classification in a few-shot learning scheme. We have also validated the effectiveness of our method with various ablation studies in qualitative, quantitative, and statistical examination.",2019,,1905.11088,,https://arxiv.org/pdf/1905.11088.pdf
9507ea9cdeda30115e9bbbfa8090b24f9e1a26a3,0,,,0,1,Supporting large-scale image recognition with out-of-domain samples,"This article presents an efficient end-to-end method to perform instance-level recognition employed to the task of labeling and ranking landmark images. In a first step, we embed images in a high dimensional feature space using convolutional neural networks trained with an additive angular margin loss and classify images using visual similarity. We then efficiently re-rank predictions and filter noise utilizing similarity to out-of-domain images. Using this approach we achieved the 1st place in the 2020 edition of the Google Landmark Recognition challenge.",2020,ArXiv,2010.0165,,https://arxiv.org/pdf/2010.01650.pdf
96e318f8ff91ba0b10348d4de4cb7c2142eb8ba9,1,[D9],,1,0,State-of-the-art face recognition performance using publicly available software and datasets,"We are interested in the reproducibility of face recognition systems. By reproducibility we mean: is the scientific community, and are the researchers from different sides, capable of reproducing the last published results by a big company, that has at its disposal huge computational power and huge proprietary databases? With the constant advancements in GPU computation power and availability of open-source software, the reproducibility of published results should not be a problem. But, if architectures of the systems are private and databases are proprietary, the reproducibility of published results can not be easily attained. To tackle this problem, we focus on training and evaluation of face recognition systems on publicly available data and software. We are also interested in comparing the best Deep Neural Net (DNN) based results with a baseline “classical” system. This paper exploits the OpenFace open-source system to generate a deep convolutional neural network model using publicly available datasets. We study the impact of the size of the datasets, their quality and compare the performance to a classical face recognition approach. Our focus is to have a fully reproducible model. To this end, we used publicly available datasets (FRGC, MS-celeb-lM, MOBIO, LFW), as well publicly available software (OpenFace) to train our model in order to do face recognition. Our best trained model achieves 97.52% accuracy on the Labelled in the Wild dataset (LFW) dataset which is lower than Google's best reported results of 99.96% but slightly better than FaceBook's reported result of 97.35%. We also evaluated our best model on the challenging video dataset MOBIO and report competitive results with the best reported results on this database.",2018,2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP),,10.1109/ATSIP.2018.8364450,https://zenodo.org/record/1323670/files/hmani-atsip-2018.pdf
9921683cb4ce6d49bc85974b6897cdb6edcb614c,0,,,0,1,Attention-Based Query Expansion Learning,"Query expansion is a technique widely used in image search consisting in combining highly ranked images from an original query into an expanded query that is then reissued, generally leading to increased recall and precision. An important aspect of query expansion is choosing an appropriate way to combine the images into a new query. Interestingly, despite the undeniable empirical success of query expansion, ad-hoc methods with different caveats have dominated the landscape, and not a lot of research has been done on learning how to do query expansion. In this paper we propose a more principled framework to query expansion, where one trains, in a discriminative manner, a model that learns how images should be aggregated to form the expanded query. Within this framework, we propose a model that leverages a self-attention mechanism to effectively learn how to transfer information between the different images before aggregating them. Our approach obtains higher accuracy than existing approaches on standard benchmarks. More importantly, our approach is the only one that consistently shows high accuracy under different regimes, overcoming caveats of existing methods.",2020,ECCV,2007.08019,10.1007/978-3-030-58604-1_11,https://arxiv.org/pdf/2007.08019.pdf
9a2d83ed7d7cc647421e976d8669b023974fff67,0,,,0,1,MaskGAN: Towards Diverse and Interactive Facial Image Manipulation,"Facial image manipulation has achieved great progress in recent years. However, previous methods either operate on a predefined set of face attributes or leave users little freedom to interactively manipulate images. To overcome these drawbacks, we propose a novel framework termed MaskGAN, enabling diverse and interactive face manipulation. Our key insight is that semantic masks serve as a suitable intermediate representation for flexible face manipulation with fidelity preservation. MaskGAN has two main components: 1) Dense Mapping Network (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically, DMN learns style mapping between a free-form user modified mask and a target image, enabling diverse generation results. EBST models the user editing behavior on the source mask, making the overall framework more robust to various manipulated inputs. Specifically, it introduces dual-editing consistency as the auxiliary supervision signal. To facilitate extensive studies, we construct a large-scale high-resolution face dataset with fine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively evaluated on two challenging tasks: attribute transfer and style copy, demonstrating superior performance over other state-of-the-art methods. The code, models, and dataset are available at https://github.com/switchablenorms/CelebAMask-HQ.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1907.11922,10.1109/cvpr42600.2020.00559,https://arxiv.org/pdf/1907.11922.pdf
9af0c4bd5255974c2842179de4df93a85b150314,0,,,0,1,Redesigning the classification layer by randomizing the class representation vectors,"Neural image classification models typically consist of two components. The first is an image encoder, which is responsible for encoding a given raw image into a representative vector. The second is the classification component, which is often implemented by projecting the representative vector onto target class vectors. The target class vectors, along with the rest of the model parameters, are estimated so as to minimize the loss function. In this paper, we analyze how simple design choices for the classification layer affect the learning dynamics. We show that the standard cross-entropy training implicitly captures visual similarities between different classes, which might deteriorate accuracy or even prevents some models from converging. We propose to draw the class vectors randomly and set them as fixed during training, thus invalidating the visual similarities encoded in these vectors. We analyze the effects of keeping the class vectors fixed and show that it can increase the inter-class separability, intra-class compactness, and the overall model accuracy, while maintaining the robustness to image corruptions and the generalization of the learned concepts.",2020,ArXiv,2011.08704,,https://arxiv.org/pdf/2011.08704.pdf
9b73de079e63ca32b990e724704226a64e1d55a0,0,,,1,1,Refined CNNs for Face Recognition Applications on Embedded Devices,"Deployment of deep learning models to embedded devices like smart phones or other smart end-user devices has been the hotspot in computer vision and artificial intelligence. However, the performance of recent state-of-the-art CNNs is dissatisfactory in real-world face applications. To solve this issue, we refined an efficient CNN architecture for face verification with extreme efficiency for real-time face applications in embedded environment, namely R-MobileFaceNet. We also proposed Dynamically Fuzzy Image Dataset, namely DFID, to evaluate the capacity of the models to deploy on embedded platforms. Our experiments proved our refined model was capable of embedded deployment, achieving higher accuracy improvement on DFID and neglectable accuracy loss on LFW. This paper also serves as an effective and efficient solution for deploying deep learning models to real-world face applications.",2020,ICMLC,,10.1145/3383972.3384025,
9bcaa98e7d13dff011feb54cca2bd5ea2895bbfb,0,,,0,1,Caffe Barista: Brewing Caffe with FPGAs in the Training Loop,"As the complexity of deep learning (DL) modelsincreases, their compute requirements increase accordingly. De-ploying a Convolutional Neural Network (CNN) involves twophases: training and inference. With the inference task typicallytaking place on resource-constrained devices, a lot of research hasexplored the field of low-power inference on custom hardwareaccelerators. On the other hand, training is both more compute-and memory-intensive and is primarily performed on power-hungry GPUs in large-scale data centres. CNN training onFPGAs is a nascent field of research. This is primarily due tothe lack of tools to easily prototype and deploy various hardwareand/or algorithmic techniques for power-efficient CNN training. This work presentsBarista, an automated toolflow that providesseamless integration of FPGAs into the training of CNNs withinthe popular deep learning framework Caffe. To the best of ourknowledge, this is the only tool that allows for such versatile andrapid deployment of hardware and algorithms for the FPGA-based training of CNNs, providing the necessary infrastructurefor further research and development.",2020,2020 30th International Conference on Field-Programmable Logic and Applications (FPL),2006.13829,10.1109/FPL50879.2020.00059,https://arxiv.org/pdf/2006.13829.pdf
9d14cdbf0af4af7a7bc0a62084fe9c0ab43d6502,0,,,1,1,Towards a Fast and Accurate Face Recognition System from Deep Representations,"Title of Dissertation: TOWARDS A FAST AND ACCURATE FACE RECOGNITION SYSTEM FROM DEEP REPRESENTATIONS Rajeev Ranjan Doctor of Philosophy, 2019 Dissertation directed by: Professor Rama Chellappa Department of Electrical and Computer Engineering The key components of a machine perception algorithm are feature extraction followed by classification or regression. The features representing the input data should have the following desirable properties: 1) they should contain the discriminative information required for accurate classification, 2) they should be robust and adaptive to several variations in the input data due to illumination, translation/rotation, resolution, and input noise, 3) they should lie on a simple manifold for easy classification or regression. Over the years, researchers have come up with various hand crafted techniques to extract meaningful features. However, these features do not perform well for data collected in unconstrained settings due to large variations in appearance and other nuisance factors. Recent developments in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on various machine perception tasks such as object detection and recognition. DCNNs are highly non-linear regressors because of the presence of hierarchical convolutional layers with non-linear activation. Unlike the hand crafted features, DCNNs learn the feature extraction and feature classification/regression modules from the data itself in an end-to-end fashion. This enables the DCNNs to be robust to variations present in the data and at the same time improve their discriminative ability. Ever-increasing computation power and availability of large datasets have led to significant performance gains from DCNNs. However, these developments in deep learning are not directly applicable to the face analysis tasks due to large variations in illumination, resolution, viewpoint, and attributes of faces acquired in unconstrained settings. In this dissertation, we address this issue by developing efficient DCNN architectures and loss functions for multiple face analysis tasks such as face detection, pose estimation, landmarks localization, and face recognition from unconstrained images and videos. In the first part of this dissertation, we present two face detection algorithms based on deep pyramidal features. The first face detector, called DP2MFD, utilizes the concepts of deformable parts model (DPM) in the context of deep learning. It is able to detect faces of various sizes and poses in unconstrained conditions. It reduces the gap in training and testing of DPM on deep features by adding a normalization layer to the DCNN. The second face detector, called Deep Pyramid Single Shot Face Detector (DPSSD), is fast and capable of detecting faces with large scale variations (especially tiny faces). It makes use of the inbuilt pyramidal hierarchy present in a DCNN, instead of creating an image pyramid. Extensive experiments on publicly available unconstrained face detection datasets show that both these face detectors are able to capture the meaningful structure of faces and perform significantly better than many traditional face detection algorithms. In the second part of this dissertation, we present two algorithms for simultaneous face detection, landmarks localization, pose estimation and gender recognition using DCNNs. The first method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. The second approach extends HyperFace to incorporate additional tasks of face verification, age estimation and smile detection, in All-In-One Face. HyperFace and All-In-One Face exploit the synergy among the tasks which improves individual performances. In the third part of this dissertation, we focus on improving the task of face verification by designing a novel loss function that maximizes the inter-class distance and minimizes the intra-class distance in the feature space. We propose a new loss function, called Crystal Loss, that adds an L2-constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. We additionally describe a deep learning pipeline for unconstrained face identification and verification which achieves state-of-the-art performance on several benchmark datasets. We provide the design details of the various modules involved in automatic face recognition: face detection, landmark localization and alignment, and face identification/verification. We present experimental results for end-to-end face verification and identification on IARPA Janus Benchmarks A, B and C (IJB-A, IJB-B, IJB-C), and the Janus Challenge Set 5 (CS5). Though DCNNs have surpassed human-level performance on tasks such as object classification and face verification, they can easily be fooled by adversarial attacks. These attacks add a small perturbation to the input image that causes the network to mis-classify the sample. In the final part of this dissertation, we focus on safeguarding the DCNNs and neutralizing adversarial attacks by compact feature learning. In particular, we show that learning features in a closed and bounded space improves the robustness of the network. We explore the effect of Crystal Loss, that enforces compactness in the learned features, thus resulting in enhanced robustness to adversarial perturbations. Additionally, we propose compact convolution, a novel method of convolution that when incorporated in conventional CNNs improves their robustness. Compact convolution ensures feature compactness at every layer such that they are bounded and close to each other. Extensive experiments show that Compact Convolutional Networks (CCNs) neutralize multiple types of attacks, and perform better than existing methods in defending adversarial attacks, without incurring any additional training overhead compared to CNNs. Towards a Fast and Accurate Face Recognition System from Deep Representations",2019,,,10.13016/hdxh-ign6,
9d3ab64f84b267572035a0f32a818c773b1845d9,0,,,0,1,Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds,"Local and global patterns of an object are closely related. Although each part of an object is incomplete, the underlying attributes about the object are shared among all parts, which makes reasoning the whole object from a single part possible. We hypothesize that a powerful representation of a 3D object should model the attributes that are shared between parts and the whole object, and distinguishable from other objects. Based on this hypothesis, we propose to learn point cloud representation by bidirectional reasoning between the local structures at different abstraction hierarchies and the global shape without human supervision. Experimental results on various benchmark datasets demonstrate the unsupervisedly learned representation is even better than supervised representation in discriminative power, generalization ability, and robustness. We show that unsupervisedly trained point cloud models can outperform their supervised counterparts on downstream classification tasks. Most notably, by simply increasing the channel width of an SSG PointNet++, our unsupervised model surpasses the state-of-the-art supervised methods on both synthetic and real-world 3D object classification datasets. We expect our observations to offer a new perspective on learning better representation from data structures instead of human annotations for point cloud understanding.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.12971,10.1109/cvpr42600.2020.00542,https://arxiv.org/pdf/2003.12971.pdf
9e4fb830ac2b074275c9665b0f58675d30ebe415,0,,,1,0,A Network for Makeup Face Verification Based upon Deep Learning,"Makeup, derived from the human pursuit of beauty, it changes the image of people appearance, brings more beautiful enjoyment and spiritual pleasure. However, recent studies have shown that facial makeup have a negative effect on face verification. To solve this problem, we formulate an end-to-end deep learning network which is composed of a stem CNN and a novel mapping module. Specifically, we pre-train our framework on a comprehensive dataset and fine-tune our mapping module on makeup datasets. Then we experimentally validate the proposal on these datasets. Experimental results demonstrate that the proposal achieves promising performance compared to the existing state-of-the-art methods.",2020,"2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC)",,10.1109/ICIVC50857.2020.9177431,
9e7464ba9bdaa0e3de8e02547fbd8267abfa4191,0,,,0,1,Mesh Guided One-shot Face Reenactment Using Graph Convolutional Networks,"Face reenactment aims to animate a source face image to a different pose and expression provided by a driving image. Existing approaches are either designed for a specific identity, or suffer from the identity preservation problem in the one-shot or few-shot scenarios. In this paper, we introduce a method for one-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the source mesh and driving mesh) as guidance to learn the optical flow needed for the reenacted face synthesis. Technically, we explicitly exclude the driving face's identity information in the reconstructed driving mesh. In this way, our network can focus on the motion estimation for the source face without the interference of driving face shape. We propose a motion net to learn the face motion, which is an asymmetric autoencoder. The encoder is a graph convolutional network (GCN) that learns a latent motion vector from the meshes, and the decoder serves to produce an optical flow image from the latent vector with CNNs. Compared to previous methods using sparse keypoints to guide the optical flow learning, our motion net learns the optical flow directly from 3D dense meshes, which provide the detailed shape and pose information for the optical flow, so it can achieve more accurate expression and pose on the reenacted face. Extensive experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",2020,ACM Multimedia,2008.07783,10.1145/3394171.3413865,https://arxiv.org/pdf/2008.07783.pdf
9ea37d031a8f112292c0d0f8d731b837462714e9,0,,,1,1,Face Recognition: From Traditional to Deep Learning Methods,"Starting in the seventies, face recognition has become one of the most researched topics in computer vision and biometrics. Traditional methods based on hand-crafted features and traditional machine learning techniques have recently been superseded by deep neural networks trained with very large datasets. In this paper we provide a comprehensive and up-to-date literature review of popular face recognition methods including both traditional (geometry-based, holistic, feature-based and hybrid methods) and deep learning methods.",2018,ArXiv,1811.00116,,https://arxiv.org/pdf/1811.00116.pdf
9eedf157222527670b565fd025067ab2062c24ac,0,,,0,1,Delving into Inter-Image Invariance for Unsupervised Visual Representations,"Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remain much less explored. One major obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since there are no pair annotations available. In this work, we present a rigorous and comprehensive study on inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. Through carefully-designed comparisons and analysis, we propose a unified framework that supports the integration of unsupervised intra- and inter-image invariance learning. With all the obtained recipes, our final model, namely InterCLR, achieves state-of-the-art performance on standard benchmarks. Code and models will be available at this https URL.",2020,ArXiv,2008.11702,,https://arxiv.org/pdf/2008.11702.pdf
9f73735295bb0c9a569307023cf4f887084d9a4a,0,,,0,1,Multilevel Feature Fusion With 3D Convolutional Neural Network for EEG-Based Workload Estimation,"Mental workload is defined as the proportion of the information processing capability used to perform a task. High cognitive load requires additional resources to process information; this demand for additional resources may reduce the processing efficiency and performance. Therefore, the technique of workload estimation can ensure a proper working environment to promote the working efficiency of each person. In this paper, we propose a three-dimensional convolutional neural network (3D CNN) employing a multilevel feature fusion algorithm for mental workload estimation using electroencephalogram (EEG) signals. The 1D EEG signals are converted to 3D EEG images to enable the 3D CNN to learn the spectral and spatial information over the scalp. The multilevel feature fusion framework integrates local and global neuronal activities by workload tasks in the 3D CNN algorithm. Multilevel features are extracted in each layer of the 3D convolution operation and each multilevel feature is multiplied by a weighting factor, which determines the importance of the feature. The weighting factor is adaptively estimated for each EEG image by a backpropagation process. Furthermore, we generate subframes from each EEG image and propose a temporal attention technique based on the long short-term memory model (LSTM) to extract a significant subframe at each multilevel feature that is strongly correlated with task difficulty. To verify the performance of our network, we performed the Sternberg task to measure the mental workload of the participant, which was classified according to its difficulty as low or high workload condition. We showed that the difficulty of the workload was well designed, which was reflected in the behavior of the participant. Our network is trained on this dataset and the accuracy of our network is 90.8 %, which is better than that of conventional algorithms. We also evaluated our method using the public EEG dataset and achieved 93.9 % accuracy.",2020,IEEE Access,,10.1109/ACCESS.2020.2966834,https://ieeexplore.ieee.org/ielx7/6287639/8948470/08960298.pdf
9f99b7fcf947723248e435ef0a7d553752d30965,1,[M3],,1,1,A Face Recognition System for Assistive Robots,"Assistive robots collaborating with people demand strong Human-Robot interaction capabilities. In this way, recognizing the person the robot has to interact with is paramount to provide a personalized service and reach a satisfactory end-user experience. To this end, face recognition: a non-intrusive, automatic mechanism of identification using biometric identifiers from an user's face, has gained relevance in the recent years, as the advances in machine learning and the creation of huge public datasets have considerably improved the state-of-the-art performance. In this work we study different open-source implementations of the typical components of state-of-the-art face recognition pipelines, including face detection, feature extraction and classification, and propose a recognition system integrating the most suitable methods for their utilization in assistant robots. Concretely, for face detection we have considered MTCNN, OpenCV's DNN, and OpenPose, while for feature extraction we have analyzed InsightFace and Facenet. We have made public an implementation of the proposed recognition framework, ready to be used by any robot running the Robot Operating System (ROS). The methods in the spotlight have been compared in terms of accuracy and performance in common benchmark datasets, namely FDDB and LFW, to aid the choice of the final system implementation, which has been tested in a real robotic platform.",2020,APPIS,,10.1145/3378184.3378225,https://riuma.uma.es/xmlui/bitstream/10630/19402/3/APPIS_Face_Resumen.pdf
a1027f7c27273c8c0aac30bbd31da87a0fa342db,1,[D10],,1,1,Discriminability Distillation in Group Representation Learning,"Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin.",2020,ECCV,2008.1085,10.1007/978-3-030-58607-2_1,https://arxiv.org/pdf/2008.10850.pdf
a22441947b93b99b761a336d47278fa9a532b808,1,[D10],,1,1,Neighborhood-Aware Attention Network for Semi-supervised Face Recognition,"Although face recognition has achieved fairly remarkable results in recent years, it heavily relies on large- scale labeled data to train the high-capacity deep convolutional neural networks. It is unrealistic to collect larger labeled datasets to further boost the performance, which requires burdensome and expensive annotation efforts. Meanwhile, there exist numerous unlabeled face images. It is challenging but promising to jointly utilize limited labeled and abundant unlabeled data to obtain higher performance gain, which is the target of semi-supervised learning. In this paper, we propose a bottom- up method, Neighborhood-Aware Attention Network (NAAN), for semi-supervised face recognition. It clusters unlabeled face images by collaboratively predicting pairwise relations based on their neighborhood information, where the neighborhood is defined as a k-hop ego network centered in the given sample called ""ego"". Considering the different importance of neighbors, we employ the graph attention network to learn the ego's representation. We evaluate our model on two face recognition datasets MegaFace and IJB-A, and it yields favorably comparable performance to the fully-supervised results.",2020,2020 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN48605.2020.9207042,http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20121.pdf
a286e00f1927979e457eeeda4eabaef061a2a81b,0,,,0,1,Deep Position-Sensitive Tracking,"Classification-based tracking strategies often face more challenges from intra-class discrimination than from inter-class separability. Even for deep convolutional neural networks that have been widely proven to be effective in various vision tasks, their intra-class discriminative capability is still limited by the weakness of softmax loss, especially for targets not seen in the training dataset. By taking intrinsic attributes of training samples into account, in this paper, we propose a position-sensitive loss coupled with softmax loss to achieve intra-class compactness and inter-class explicitness. Particularly, two additive margins are introduced to encode the position attribute for decision boundary maximization, which is also utilized with the proposed loss to supervise the fine-tuned features on the pre-trained model. With the nearest neighbor ranking measurement in the feature embedding domain, the whole scheme is able to reach an optimized balance between the feature-level inter-class semantic separability and instance-level intra-class relative distance ranking. We evaluate the proposed work on different popular benchmarks, and experimental results demonstrate that our tracking strategy performs favorably against most of the state-of-the-art trackers in the comparison of accuracy and robustness.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2019.2922125,
a28e5985afd80abaa95b2ba1fe08667a6cfb1227,0,,,0,1,SAR-Net: A End-to-End Deep Speech Accent Recognition Network,"This paper proposes a end-to-end deep network to recognize kinds of accents under the same language, where we develop and transfer the deep architecture in speaker-recognition area to accent classification task for learning utterance-level accent representation. Compared with the individual-level feature in speaker-recognition, accent recognition throws a more challenging issue in acquiring compact group-level features for the speakers with the same accent, hence a good discriminative accent feature space is desired. Our deep framework adopts multitask-learning mechanism and mainly consists of three modules: a shared CNNs and RNNs based front-end encoder, a core accent recognition branch, and an auxiliary speech recognition branch, where we take speech spectrogram as input. More specifically, with the sequential descriptors learned from a shared encoder, the accent recognition branch first condenses all descriptors into an embedding vector, and then explores different discriminative loss functions which are popular in face recognition domain to enhance embedding discrimination. Additionally, due to the accent is a speaking-related timbre, adding speech recognition branch effectively curbs the over-fitting phenomenon in accent recognition during training. We show that our network without any data-augment preproccessings is significantly ahead of the baseline system on the accent classification track in the Accented English Speech Recognition Challenge 2020 (AESRC2020), where the state-of-the-art loss function Circle-Loss achieves the best discriminative optimization for accent representation.",2020,ArXiv,2011.12461,,https://arxiv.org/pdf/2011.12461.pdf
a2f7e16738dac272e6a4ff5f213395ef12c3a989,1,[D10],,0,1,Partial FC: Training 10 Million Identities on a Single Machine,"Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available this https URL.",2020,ArXiv,2010.05222,,https://arxiv.org/pdf/2010.05222.pdf
a32811ad17ff692e7f5826057dccd2c767b5b458,0,,,0,1,Full Face-and-Head 3D Model With Photorealistic Texture,"In the recent period, significant progress has been achieved towards reconstructing the 3D face model from face image. With the support of the render engines and sufficient data, the reconstruction results are fine in detail. Nevertheless, the research on the 3D face reconstruction with texture from a single unrestricted face image is imperfect. The rebuild process lacks essential structure and texture information in the profile and the craniofacial region. To address this problem, we present a method of creating a 3D full face-and-head model with photorealistic texture from a single “in-the-wild” face image in this paper. To this end, we introduce a pipeline to integrate the highly-detailed face model into the basic model. Specifically, the basic model was built by multilinear optimization, and the highly-detailed face model which represents the facial features generated by constrained illumination distribution. Additionally, to infer the invisible region texture information corresponding to the input face image, we design an effective architecture with the generative adversarial network (GAN) for panoramic UV texture generation. The final results after UV texture mapping were visualized in the experiment, which demonstrates that the model faithfully recovers the photorealistic details in arbitrary perspective. Furthermore, compared to the state-of-the-art facial modeling techniques and existing commercial solutions, our method takes less input and performs better in surface detail.",2020,IEEE Access,,10.1109/ACCESS.2020.3031886,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09241395.pdf
a35483c9becc95faa16bf70a8c6355566a205091,1,,1,1,0,FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis,"Face synthesis has achieved advanced development by using generative adversarial networks (GANs). Existing methods typically formulate GAN as a two-player game, where a discriminator distinguishes face images from the real and synthesized domains, while a generator reduces its discriminativeness by synthesizing a face of photorealistic quality. Their competition converges when the discriminator is unable to differentiate these two domains. Unlike two-player GANs, this work generates identity-preserving faces by proposing FaceID-GAN, which treats a classifier of face identity as the third player, competing with the generator by distinguishing the identities of the real and synthesized faces (see Fig.1). A stationary point is reached when the generator produces faces that have high quality as well as preserve identity. Instead of simply modeling the identity classifier as an additional discriminator, FaceID-GAN is formulated by satisfying information symmetry, which ensures that the real and synthesized images are projected into the same feature space. In other words, the identity classifier is used to extract identity features from both input (real) and output (synthesized) face images of the generator, substantially alleviating training difficulty of GAN. Extensive experiments show that FaceID-GAN is able to generate faces of arbitrary viewpoint while preserve identity, outperforming recent advanced approaches.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00092,http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2021.pdf
a3a5ba95591fe73e1dbcdaa51a4eba90280e14c9,1,[D9],,1,1,Geometry Guided Feature Aggregation in Video Face Recognition,"Video-based face recognition has attracted a significant amount of research interest in both academia and industry due to its wide applications such as surveillance and security. Different from image-based face recognition, abundant information, extracted from a series of frames in a video, would contribute a lot to successful recognition. In other words, the key to improving video face recognition capability is aggregating and integrating profuse information within a video. Existing methods of feature aggregation across frames narrowly focus on the importance of a single frame, while ignoring the geometric relationship among frames in feature space. In this work, we present a geometry-based feature aggregation method rather than a better recognition model. It considers not only the importance of each frame but also the geometric relationship among frames in feature space, which yields more distinguishing video-level representation. Extensive evaluations on IJB-A and YTF datasets indicate that the proposed aggregation method considerably outperforms other feature aggregation methods.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00326,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Peng_Geometry_Guided_Feature_Aggregation_in_Video_Face_Recognition_ICCVW_2019_paper.pdf
a56065201159c33ab5bda585fd2286764eab15a5,0,,,1,0,Led3D: A Lightweight and Efficient Deep Approach to Recognizing Low-Quality 3D Faces,"Due to the intrinsic invariance to pose and illumination changes, 3D Face Recognition (FR) has a promising potential in the real world. 3D FR using high-quality faces, which are of high resolutions and with smooth surfaces, have been widely studied. However, research on that with low-quality input is limited, although it involves more applications. In this paper, we focus on 3D FR using low-quality data, targeting an efficient and accurate deep learning solution. To achieve this, we work on two aspects: (1) designing a lightweight yet powerful CNN; (2) generating finer and bigger training data. For (1), we propose a Multi-Scale Feature Fusion (MSFF) module and a Spatial Attention Vectorization (SAV) module to build a compact and discriminative CNN. For (2), we propose a data processing system including point-cloud recovery, surface refinement, and data augmentation (with newly proposed shape jittering and shape scaling). We conduct extensive experiments on Lock3DFace and achieve state-of-the-art results, outperforming many heavy CNNs such as VGG-16 and ResNet-34. In addition, our model can operate at a very high speed (136 fps) on Jetson TX2, and the promising accuracy and efficiency reached show its great applicability on edge/mobile devices.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00592,
a5fcc57b60b154ce28044ffb6124a4021b4116be,0,,,1,0,Two-branch Recurrent Network for Isolating Deepfakes in Videos,,2020,,2008.03412,,https://arxiv.org/pdf/2008.03412.pdf
a5fea75ba9003a8da2d0dde2d14ed7bae78e3a12,0,,,0,1,The DKU-DukeECE Systems for VoxCeleb Speaker Recognition Challenge 2020.,"In this paper, we present the system submission for the VoxCeleb Speaker Recognition Challenge 2020 (VoxSRC-20) by the DKU-DukeECE team. For track 1, we explore various kinds of state-of-the-art front-end extractors with different pooling layers and objective loss functions. For track 3, we employ an iterative framework for self-supervised speaker representation learning based on a deep neural network (DNN). For track 4, we investigate the whole system pipeline for speaker diarization, including voice activity detection (VAD), uniform segmentation, speaker embedding extraction, and clustering.",2020,,2010.12731,,https://arxiv.org/pdf/2010.12731.pdf
a7cd674f36411f1b38a5b7e15f0b25e3420b873a,0,,,0,1,A Black-box Attack on Neural Networks Based on Swarm Evolutionary Algorithm,"Neural networks play an increasingly important role in the field of machine learning and are included in many applications in society. Unfortunately, neural networks suffer from adversarial samples generated to attack them. However, most of the generation approaches either assume that the attacker has full knowledge of the neural network model or are limited by the type of attacked model. In this paper, we propose a new approach that generates a black-box attack to neural networks based on the swarm evolutionary algorithm. Benefiting from the improvements in the technology and theoretical characteristics of evolutionary algorithms, our approach has the advantages of effectiveness, black-box attack, generality, and randomness. Our experimental results show that both the MNIST images and the CIFAR-10 images can be perturbed to successful generate a black-box attack with 100\% probability on average. In addition, the proposed attack, which is successful on distilled neural networks with almost 100\% probability, is resistant to defensive distillation. The experimental results also indicate that the robustness of the artificial intelligence algorithm is related to the complexity of the model and the data set. In addition, we find that the adversarial samples to some extent reproduce the characteristics of the sample data learned by the neural network model.",2020,ACISP,1901.09892,10.1007/978-3-030-55304-3_14,https://arxiv.org/pdf/1901.09892.pdf
a8d84a20321fcb11729370d44daec97bc4ad98ae,1,"[D9], [D12]",,1,0,A feature learning approach for face recognition with robustness to noisy label based on top-N prediction,"Abstract Collecting a vast amount of face data with identity labels to train a convolutional neural network is an effective mean to learn a discriminative feature representation for face recognition. However, the datasets with larger scale often contain more noisy labels, that directly affects the ultimate performance of the learned model. This paper proposes an end-to-end feature learning method with robustness to noisy label. First, a data filtering method is proposed to automatically online filter out the data with 0 label, by checking the consistency between the annotated label and the results of top-N prediction. Then the loss functions of softmax and center loss are simply revised to only supervise the reserved feature. Finally, we use MS-Celeb-1M dataset, which contains massive noisy labels, to train a 128-D feature representation without any pre-train or data pre-clean. A single learned model gets an accuracy of 99.43% on LFW test set, that is very close to the model trained using the clean data.",2019,Neurocomputing,,10.1016/j.neucom.2018.10.075,
a99193bfc31184f551a5f1b32cf7637417078b75,0,,,0,1,Towards a Reliable Face Recognition System,"Face Recognition (FR) is an important area in computer vision with many applications such as security and automated border controls. The recent advancements in this domain have pushed the performance of models to human-level accuracy. However, the varying conditions in the real-world expose more challenges for their adoption. In this paper, we investigate the performance of these models. We analyze the performance of a cross-section of face detection and recognition models. Experiments were carried out without any preprocessing on three state-of-the-art face detection methods namely HOG, YOLO and MTCNN, and three recognition models namely, VGGface2, FaceNet and Arcface. Our results indicated that there is a significant reliance by these methods on preprocessing for optimum performance.",2020,EANN,,10.1007/978-3-030-48791-1_23,
ac057602f513f5abae0ddffbd49acc21f8592559,0,,,0,1,Disentangling in Latent Space by Harnessing a Pretrained Generator,"Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality generative power, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through this extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.",2020,ArXiv,2005.07728,,https://arxiv.org/pdf/2005.07728.pdf
adbb663b140d3d14fc21d85f1b05403cad8e867a,1,[D10],,1,1,Neural Architecture Search for Deep Face Recognition,"By the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature that does not need too much cooperation and can significantly improve the user's experience at the same time. Face recognition is one of the main functions of electronic equipment propaganda. Hence it's virtually worth researching in computer vision. Previous work in this field has focused on two directions: converting loss function to improve recognition accuracy in traditional deep convolution neural networks (Resnet); combining the latest loss function with the lightweight system (MobileNet) to reduce network size at the minimal expense of accuracy. But none of these has changed the network structure. With the development of AutoML, neural architecture search (NAS) has shown excellent performance in the benchmark of image classification. In this paper, we integrate NAS technology into face recognition to customize a more suitable network. We quote the framework of neural architecture search which trains child and controller network alternately. At the same time, we mutate NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize policy gradient algorithm to search the architecture automatically with the most classical cross-entropy loss. The network architectures we searched out have got state-of-the-art accuracy in the large-scale face dataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with relatively small network size. To the best of our knowledge, this proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.",2019,ArXiv,1904.09523,,https://arxiv.org/pdf/1904.09523.pdf
b286bd248a25a21433ff1be716a9f26b86b19eed,0,,,1,0,Face Recognition Algorithm Bias: Performance Differences on Images of Children and Adults,"In this work, we examine if current state-of-the-art deep learning face recognition systems exhibit a negative bias (i.e., poorer performance) for children when compared to the performance obtained on adults. The systems selected for this work are five top performing commercial-off-the-shelf face recognition systems, two government-off-the-shelf face recognition systems and one open-source face recognition solution. The datasets used to evaluate the performance of the systems are both unconstrained in age, pose, illumination, and expression and are publicly available. These datasets are indicative of photo journalistic face datasets published and evaluated on over the last few years. Our findings show a negative bias for each algorithm on children. Genuine and imposter distributions highlight the performance bias between the datasets further supporting the need for a deeper investigation into algorithm bias as a function of age cohorts. To combat the performance decline on the child demographic, several score-level fusion strategies were evaluated. This work identifies the best score-level fusion technique for the child demographic.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00280,
b4e42daba36f1007fb792912424d0914a39eb9a5,0,,,0,1,FaRE: Open Source Face Recognition Performance Evaluation Package,"Biometrics-related research has been accelerated significantly by deep learning technology. However, there are limited open-source resources to help researchers evaluate their deep learning-based biometrics algorithms efficiently, especially for the face recognition tasks. In this work, we design, implement, and evaluate a computationally lightweight, maintainable, scalable, generalizable, and extendable face recognition evaluation toolbox named FaRE that supports both online and offline evaluation to provide feedback to algorithm development and accelerate biometricsrelated research. FaRE includes a set of evaluation metrics and provides various APIs for commonly-used face recognition datasets including LFW, CFP, UHDB31, and IJBseries datasets. FaRE can be easily extended to include other datasets. The package is publically available for research use at https://github.com/uh-cbl/FaRE.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803411,
b4f2deca8e85663134a729676edc2aca337e913e,0,,,0,1,The Mertens Unrolled Network (MU-Net): A High Dynamic Range Fusion Neural Network for Through the Windshield Driver Recognition,"Face recognition of vehicle occupants through windshields in unconstrained environments poses a number of unique challenges ranging from glare, poor illumination, driver pose and motion blur. In this paper, we further develop the hardware and software components of a custom vehicle imaging system to better overcome these challenges. After the build out of a physical prototype system that performs High Dynamic Range (HDR) imaging, we collect a small dataset of through-windshield image captures of known drivers. We then re-formulate the classical Mertens-Kautz-Van Reeth HDR fusion algorithm as a pre-initialized neural network, which we name the Mertens Unrolled Network (MU-Net), for the purpose of fine-tuning the HDR output of through-windshield images. Reconstructed faces from this novel HDR method are then evaluated and compared against other traditional and experimental HDR methods in a pre-trained state-of-the-art (SOTA) facial recognition pipeline, verifying the efficacy of our approach.",2020,ArXiv,2002.12257,10.1117/12.2566765,https://arxiv.org/pdf/2002.12257.pdf
b53054911382ec4c57ed3080feefcb3e85034f63,1,,1,0,1,Deep Convolutional Neural Network Using Triplet Loss to Distinguish the Identical Twins,"In face recognition, distinguishing identical twins faces is a challenging task because of the high level of correlation in facial appearance.Generally, facial recognition is easy to make mistakes when it comes to twins or similar faces. To deal with the high level of correlation in similar faces, we proposed a deep convolutional neural network (CNN) using triplet loss function to differentiate the identical twins. We applied a hybrid strategy by combining the deep CNN model, which learns an embedding from facial images to Euclidean space and triplet loss function to evaluate the L2 distance between facial images into Euclidean space, Obtained L2 distance shows the level of similarity between corresponding faces. We implemented two different CNN models on our raw pixel images; additionally, we used different techniques to reduce the overfitting problem such as dropout and batch normalization, additionally L2 regularization. Our method achieves the best mean validation accuracy above 87.2%.",2019,2019 IEEE Globecom Workshops (GC Wkshps),,10.1109/GCWkshps45667.2019.9024704,
b6b23958a6c9e71224d15beb3f84da63e122a2ca,0,,,0,1,The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems,"This paper presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning have focused mainly on synthetic datasets and use a very limited number of applications. OARF includes different data partitioning methods (horizontal, vertical and hybrid) as well as emerging applications in image, text and structured data, which represent different scenarios in federated learning. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution and learning task complexity. We have developed reference implementations, and evaluated the important aspects of federated learning, including model accuracy, communication cost, differential privacy, secure multiparty computation and vertical federated learning.",2020,ArXiv,2006.07856,,https://arxiv.org/pdf/2006.07856.pdf
b6f340798c0eac2b706ded8045a0e3a286479e51,1,[M4],,1,0,Pose Agnostic Cross-spectral Hallucination via Disentangling Independent Factors,"The cross-sensor gap is one of the challenges that arise much research interests in Heterogeneous Face Recognition (HFR). Although recent methods have attempted to fill the gap with deep generative networks, most of them suffered from the inevitable misalignment between different face modalities. Instead of imaging sensors, the misalignment primarily results from geometric variations (e.g., pose and expression) on faces that stay independent from spectrum. Rather than building a monolithic but complex structure, this paper proposes a Pose Agnostic Cross-spectral Hallucination (PACH) approach to disentangle the independent factors and deal with them in individual stages. In the first stage, an Unsupervised Face Alignment (UFA) network is designed to align the near-infrared (NIR) and visible (VIS) images in a generative way, where 3D information is effectively utilized as the pose guidance. Thus the task of the second stage becomes spectrum transform with paired data. We develop a Texture Prior Synthesis (TPS) network to accomplish complexion control and consequently generate more realistic VIS images than existing methods. Experiments on three challenging NIR-VIS datasets verify the effectiveness of our approach in producing visually appealing images and achieving state-of-the-art performance in cross-spectral HFR.",2019,ArXiv,1909.04365,,https://arxiv.org/pdf/1909.04365.pdf
b6f758be954d34817d4ebaa22b30c63a4b8ddb35,1,[D9],,1,0,A Proximity-Aware Hierarchical Clustering of Faces,"In this paper, we propose an unsupervised face clustering algorithm called “Proximity-Aware Hierarchical Clustering” (PAHC) that exploits the local structure of deep representations. In the proposed method, a similarity measure between deep features is computed by evaluating linear SVM margins. SVMs are trained using nearest neighbors of sample data, and thus do not require any external training data. Clus- ters are then formed by thresholding the similarity scores. We evaluate the clustering performance using three challenging un- constrained face datasets, including Celebrity in Frontal-Profile (CFP), IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3) datasets. Experimental results demonstrate that the proposed approach can achieve significant improvements over state-of-the-art methods. Moreover, we also show that the proposed clustering algorithm can be applied to curate a set of large-scale and noisy training dataset while maintaining sufficient amount of images and their variations due to nuisance factors. The face verification performance on JANUS CS3 improves significantly by finetuning a DCNN model with the curated MS-Celeb-1M dataset which contains over three million face images.",2017,2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),1703.04835,10.1109/FG.2017.134,https://arxiv.org/pdf/1703.04835.pdf
b88b44e82fd47257e05fe2adaec33b538cd82fe2,0,,,0,1,Deep Discriminative Embedding with Ranked Weight for Speaker Verification,,2020,ICONIP,,10.1007/978-3-030-63823-8_10,
b9044152085e29e41d3bd131702a821ac4832804,1,[D9],,1,0,Deep Learning with Maxout Activations for Visual Recognition and Verification,"Visual recognition is one of the most active research topics in computer vision due to its potential applications in self-driving cars, healthcare, social media, manufacturing, etc. For image classification tasks, deep convolutional neural networks have achieved state-of-the-art results, and many activation functions have been proposed to enhance the classification performance of these networks. We explore the performance of multiple maxout activation variants on image classification, facial recognition and verification tasks using convolutional neural networks. Our experiments compare rectified linear unit, leaky rectified linear unit, scaled exponential linear unit, and hyperbolic tangent to four maxout variants. Throughout the experiments, we find that maxout networks train relatively slower than networks comprised of traditional activation functions. We found that on average, across all datasets, rectified linear units perform better than any maxout activation when the number of convolutional filters is increased six times.",2019,2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),,10.1109/IRI.2019.00033,
bae0a603d88f47b0ebdb1e325031c36f63dba738,1,[M1],,1,0,Self-Supervised Learning of Face Representations for Video Face Clustering,"Analyzing the story behind TV series and movies often requires understanding who the characters are and what they are doing. With improving deep face models, this may seem like a solved problem. However, as face detectors get better, clustering/identification needs to be revisited to address increasing diversity in facial appearance. In this paper, we address video face clustering using unsupervised methods. Our emphasis is on distilling the essential information, identity, from the representations obtained using deep pre-trained face networks. We propose a self-supervised Siamese network that can be trained without the need for video/track based supervision, and thus can also be applied to image collections. We evaluate our proposed method on three video face clustering datasets. The experiments show that our methods outperform current state-of-the-art methods on all datasets. Video face clustering is lacking a common benchmark as current works are often evaluated with different metrics and/or different sets of face tracks. The datasets and code are available at https://github.com/vivoutlaw/SSIAM.",2019,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),1903.01,10.1109/FG.2019.8756609,https://arxiv.org/pdf/1903.01000.pdf
bbcec0e050148bff21c3d7a1f0879beb7fbeb0d9,0,,,0,1,"Deep learning achieves perfect anomaly detection on 108, 308 retinal images including unlearned diseases","Optical coherence tomography (OCT) scanning is useful in detecting various retinal diseases. However, there are not enough ophthalmologists who can diagnose retinal OCT images in much of the world. To provide OCT screening inexpensively and extensively, an automated diagnosis system is indispensable. Although many machine learning techniques have been presented for assisting ophthalmologists in diagnosing retinal OCT images, there is no technique that can diagnose independently without relying on an ophthalmologist, i.e., there is no technique that does not overlook any anomaly, including unlearned diseases. As long as there is a risk of overlooking a disease with a technique, ophthalmologists must double-check even those images that the technique classifies as normal. Here, we show that our deep-learning-based binary classifier (normal or abnormal) achieved a perfect classification on 108,308 two-dimensional retinal OCT images, i.e., 1 positive rate = 1.000000 and 1 negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000. Although the test set included three types of diseases, two of these were not used for training. However, all test images were correctly classified. Furthermore, we demonstrated that our scheme was able to cope with differences in patient race. No conventional approach has achieved the above performances. Our work has a sufficient possibility of raising automated diagnosis techniques for retinal OCT images from ""assistant for ophthalmologists"" to ""independent diagnosis system without ophthalmologists"".",2020,ArXiv,2001.05859,10.2139/ssrn.3581363,https://arxiv.org/pdf/2001.05859.pdf
bc45cd9964702333569f3136ce7eb9e8581fbb1a,0,,,0,1,Occluded Face Recognition in the Wild by Identity-Diversity Inpainting,"Face recognition has achieved advanced development by using convolutional neural network (CNN) based recognizers. Existing recognizers typically demonstrate powerful capacity in recognizing un-occluded faces, but often suffer from accuracy degradation when directly identifying occluded faces. This is mainly due to insufficient visual and identity cues caused by occlusions. On the other hand, generative adversarial network (GAN) is particularly suitable when it needs to reconstruct visually plausible occlusions by face inpainting. Motivated by these observations, this paper proposes identity-diversity inpainting to facilitate occluded face recognition. The core idea is integrating GAN with an optimized pre-trained CNN recognizer which serves as the third player to compete with the generator by distinguishing diversity within the same identity class. To this end, a collect of identity-centered features is applied in the recognizer as supervision to enable the inpainted faces clustering towards their identity centers. In this way, our approach can benefit from GAN for reconstruction and CNN for representation, and simultaneously addresses two challenging tasks, face inpainting and face recognition. Experimental results compared with 4 state-of-the-arts prove the efficacy of the proposed approach.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2020.2967754,
bc4b82d4ce44f99a127d4b745d5266a0d64192c0,0,,,0,1,Face Detection Based on Receptive Field Enhanced Multi-Task Cascaded Convolutional Neural Networks,"With the continuous development of deep learning, face detection methods have made the greatest progress. For real-time detection, cascade CNN based on the lightweight model is still the dominant structure that predicts face in a coarse-to-fine manner with strong generalization ability. Compared to other methods, it is not required for a fixed size of the input. However, MTCNN still has poor performance in detecting tiny targets. To improve model generalization ability, we propose a Receptive Field Enhanced Multi-Task Cascaded CNN. This network takes advantage of the Inception-V2 block and receptive field block to enhance the feature discriminability and robustness for small targets. The experimental results show that the performance of our network is improved by 1.08% on the AFW, 2.84% on the PASCAL FACE, 1.31% on the FDDB, and 2.3%, 2.1%, and 6.6% on the three sub-datasets of the WIDER FACE benchmark in comparison with MTCNN respectively. Furthermore, our structure uses 16% fewer parameters.",2020,IEEE Access,,10.1109/ACCESS.2020.3023782,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195457.pdf
bc91183f2c4193d7189d11e20f288811bc34da79,1,[D9],,1,0,Discriminant Deep Feature Learning based on joint supervision Loss and Multi-layer Feature Fusion for heterogeneous face recognition,"Abstract Heterogeneous face recognition (HFR) is still a challenging problem in computer vision community due to large appearance difference between near infrared (NIR) and visible light (VIS) modalities. Recently, breakthroughs have been made for traditional face recognition by applying deep learning on a huge amount of labeled VIS face samples. However, the same deep learning approach cannot be simply applied to HFR task due to large domain difference as well as insufficient pairwise images in different modalities during training. In general, the pooling layer of deep network can play the role of feature reduction, but also lead to the loss of useful face information, resulting in a decrease in the performance of HFR problem. It is important to eliminate modal-related information and retain more facial identity information. In this paper, we propose a novel method called Discriminant Deep Feature Learning Based on Joint Supervision Loss and Multi-layer Feature Fusion (DDFLJM) for HFR task. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new loss function called Scatter Loss (SL), which embeds both inter- and intra-class information for effectively training the deep model. To make full use of the various layers of the deep network, a Dimension Reduction Block (DRB) is designed to effectively extract the auxiliary features on multiple mid-level layers. An orthogonality constraint is introduced to the DRB block to reduce spectrum variations of two different modalities. The proposed SL is applied to multiple layers of network for joint supervision training, which enables multiple layers of the network to obtain discriminative identity features. Moreover, a Modified Gate Two-stream Neural Network (MGTNN) is adopted to fuse multiple-layer features. Extensive experiments are carried out on two challenging NIR-VIS HFR datasets CASIA NIR-VIS 2.0 and Oulu-CASIA NIR-VIS, demonstrating the superiority of the proposed method.",2019,Comput. Vis. Image Underst.,,10.1016/J.CVIU.2019.04.003,
bd09a441161fa9dcb9dc7cb12069b92dd29e7871,1,"[M5], [M3]",,1,1,Does Face Recognition Accuracy Get Better With Age? Deep Face Matchers Say No,"Previous studies generally agree that face recognition accuracy is higher for older persons than for younger persons. But most previous studies were before the wave of deep learning matchers, and most considered accuracy only in terms of the verification rate for genuine pairs. This paper investigates accuracy for age groups 16-29, 30-49 and 50-70, using three modern deep CNN matchers, and considers differences in the impostor and genuine distributions as well as verification rates and ROC curves. We find that accuracy is lower for older persons and higher for younger persons. In contrast, a pre deep learning matcher on the same dataset shows the traditional result ofhigher accuracy for older persons, although its overall accuracy is much lower than that of the deep learning matchers. Comparing the impostor and genuine distributions, we conclude that impostor scores have a larger effect than genuine scores in causing lower accuracy for the older age group. We also investigate the effects of training data across the age groups. Our results show that fine-tuning the deep CNN models on additional images ofolder persons actually lowers accuracy for the older age group. Also, we fine-tune and train from scratch two models using age-balanced training datasets, and these results also show lower accuracy for older age group. These results argue that the lower accuracy for the older age group is not due to imbalance in the original training data.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1911.06396,10.1109/WACV45572.2020.9093357,https://arxiv.org/pdf/1911.06396.pdf
bec253e82076dc363b8fd72d5c8fadf8f5b7e475,0,,,0,1,Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose,"Real-world talking faces often accompany with natural head movement. However, most existing talking face video generation methods only consider facial animation with fixed head pose. In this paper, we address this problem by proposing a deep neural network model that takes an audio signal A of a source person and a very short video V of a target person as input, and outputs a synthesized high-quality talking face video with personalized head pose (making use of the visual information in V), expression and lip synchronization (by considering both A and V). The most challenging issue in our work is that natural poses often cause in-plane and out-of-plane head rotations, which makes synthesized talking face video far from realistic. To address this challenge, we reconstruct 3D face animation and re-render it into synthesized frames. To fine tune these frames into realistic ones with smooth background transition, we propose a novel memory-augmented GAN module. By first training a general mapping based on a publicly available dataset and fine-tuning the mapping using the input short video of target person, we develop an effective strategy that only requires a small number of frames (about 300 frames) to learn personalized talking behavior including head pose. Extensive experiments and two user studies show that our method can generate high-quality (i.e., personalized head movements, expressions and good lip synchronization) talking face videos, which are naturally looking with more distinguishing head movement effects than the state-of-the-art methods.",2020,,2002.10137,,https://arxiv.org/pdf/2002.10137.pdf
bf064d345a232ac53706405b0e5810974b1a4e6f,1,[D13],,1,1,FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization,"This paper studies face recognition (FR) and normalization in surveillance imagery. Surveillance FR is a challenging problem that has great values in law enforcement. Despite recent progress in conventional FR, less effort has been devoted to surveillance FR. To bridge this gap, we propose a Feature Adaptation Network (FAN) to jointly perform surveillance FR and normalization. Our face normalization mainly acts on the aspect of image resolution, closely related to face super-resolution. However, previous face super-resolution methods require paired training data with pixel-to-pixel correspondence, which is typically unavailable between real low- and high-resolution faces. Our FAN can leverage both paired and unpaired data as we disentangle the features into identity and non-identity components and adapt the distribution of the identity features, which breaks the limit of current face super-resolution methods. We further propose a random scale augmentation scheme to learn resolution robust identity features, with advantages over previous fixed scale augmentation. Extensive experiments on LFW, WIDER FACE, QUML-SurvFace and SCface datasets have demonstrated the superiority of our proposed method compared to the state of the arts on surveillance face recognition and normalization.",2019,ArXiv,1911.1168,,https://arxiv.org/pdf/1911.11680.pdf
c00dc60a7fd011a9c013e14f2a1be413a552ce7e,1,[M5],,1,0,A Set of Distinct Facial Traits Learned by Machines Is Not Predictive of Appearance Bias in the Wild.,"We seek to determine whether state-of-the-art, black box face processing technology can learn to make biased trait judgments from human first impression biases. Using features extracted with FaceNet, a widely used face recognition framework, we train a transfer learning model on human subjects' first impressions of personality traits in other faces as measured by social psychologists. We measure the extent to which this appearance bias can be embedded in state-of-the-art face recognition models and benchmark learning performance for subjective perceptions of personality traits from faces. In particular, we find that features extracted with FaceNet can be used to predict human appearance biases for deliberately manipulated faces but not for randomly generated faces scored by humans. Additionally, in contrast to prior work in social psychology, the model does not find a significant signal correlating politicians' vote shares with perceived competence bias. With Local Interpretable Model-Agnostic Explanations (LIME), we provide several explanations for this discrepancy. Our results suggest that some signals of appearance bias documented in social psychology are not embedded by the machine learning techniques we investigate.",2020,,2002.05636,,https://arxiv.org/pdf/2002.05636.pdf
c0608626b2780a54667ba683679edcff13998cd2,1,,1,1,0,Data-Specific Adaptive Threshold for Face Recognition and Authentication,"Many face recognition systems boost the performance using deep learning models, but only a few researches go into the mechanisms for dealing with online registration. Although we can obtain discriminative facial features through the state-of-the-art deep model training, how to decide the best threshold for practical use remains a challenge. We develop a technique of adaptive threshold mechanism to improve the recognition accuracy. We also design a face recognition system along with the registering procedure to handle online registration. Furthermore, we introduce a new evaluation protocol to better evaluate the performance of an algorithm for real-world scenarios. Under our proposed protocol, our method can achieve a 22% accuracy improvement on the LFW dataset.",2019,2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),1810.1116,10.1109/MIPR.2019.00034,https://arxiv.org/pdf/1810.11160.pdf
c12016bf259a2820725ac782c7f836c16a433d67,1,,1,1,1,FAN-Face: a Simple Orthogonal Improvement to Deep Face Recognition,"It is known that facial landmarks provide pose, expression and shape information. In addition, when matching, for example, a profile and/or expressive face to a frontal one, knowledge of these landmarks is useful for establishing correspondence which can help improve recognition. However, in prior work on face recognition, facial landmarks are only used for face cropping in order to remove scale, rotation and translation variations. This paper proposes a simple approach to face recognition which gradually integrates features from different layers of a facial landmark localization network into different layers of the recognition network. To this end, we propose an appropriate feature integration layer which makes the features compatible before integration. We show that such a simple approach systematically improves recognition on the most difficult face recognition datasets, setting a new state-of-theart on IJB-B, IJB-C and MegaFace datasets.",2020,AAAI,,10.1609/AAAI.V34I07.6953,https://pdfs.semanticscholar.org/d847/93f84108d3d6f812c1caf64b27bed5474533.pdf
c2cb8ecbe281793401b605634805c47daba6b944,0,,,0,1,FastReID: A Pytorch Toolbox for Real-world Person Re-identification,"We present FastReID, as a widely used object reidentification (re-id) software system in JD AI Research. High modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art algorithms, including person re-id, partial re-id, crossdomain re-id and vehicle re-id, and plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most complete and high-performance toolbox supports single and multiple GPU servers, you can reproduce our project results very easily and are very welcome to use it, the code and models are available at https://github.com/JDAI-CV/fast-reid.",2020,,,,
c35f0f804caa5d75d8a2a65335ad7bbdbac97665,0,,,0,1,Sphere Margins Softmax for Face Recognition,"In conjunction with the cross-entropy loss, softmax function has gained great popularity in supervised deep convolutional neural networks (DCNNs). Although original softmax loss reduces training difficulty and makes multi classification problems easier to converge, the module does not explicitly encourage compactness within class and separability between classes, from this point of view it is not particularly suitable for face recognition tasks. In this paper, we reformulate the softmax loss with sphere margins (SM-Softmax) by normalizing both weights and extracted features of the last fully connected layer and have quantitatively adjustable angular margin by hyperparameter m1 and m2. Extensive experiments on CASIA-WebFace and Labeled Face in the Wild (LFW) validate that our SM-Softmax gives better results than the present state-of-the-art methods while adopting the same experimental configuration (benchmark datasets and network structure) and the convergence speed in the early stage is accelerated significantly.",2020,2020 39th Chinese Control Conference (CCC),,10.23919/CCC50068.2020.9188526,
c573b4510ca4b4e30d5084988ad6a9184c522696,0,,,0,1,Meta-Learning for Short Utterance Speaker Recognition with Imbalance Length Pairs,"In practical settings, a speaker recognition system needs to identify a speaker given a short utterance, while the enrollment utterance may be relatively long. However, existing speaker recognition models perform poorly with such short utterances. To solve this problem, we introduce a meta-learning framework for imbalance length pairs. Specifically, we use a Prototypical Networks and train it with a support set of long utterances and a query set of short utterances of varying lengths. Further, since optimizing only for the classes in the given episode may be insufficient for learning discriminative embeddings for unseen classes, we additionally enforce the model to classify both the support and the query set against the entire set of classes in the training set. By combining these two learning schemes, our model outperforms existing state-of-the-art speaker verification models learned with a standard supervised learning framework on short utterance (1-2 seconds) on the VoxCeleb datasets. We also validate our proposed model for unseen speaker identification, on which it also achieves significant performance gains over the existing approaches. The codes are available at this https URL.",2020,INTERSPEECH,2004.02863,10.21437/interspeech.2020-1283,https://arxiv.org/pdf/2004.02863.pdf
c6b28d6bc3b99a2a2b62585c0d5585ee61cfca1a,1,[D9],,1,1,Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective,"This paper considers learning deep features from long-tailed data. We observe that in the deep feature space, the head classes and the tail classes present different distribution patterns. The head classes have a relatively large spatial span, while the tail classes have a significantly small spatial span, due to the lack of intra-class diversity. This uneven distribution between head and tail classes distorts the overall feature space, which compromises the discriminative ability of the learned features. In response, we seek to expand the distribution of the tail classes during training, so as to alleviate the distortion of the feature space. To this end, we propose to augment each instance of the tail classes with certain disturbances in the deep feature space. With the augmentation, a specified feature vector becomes a set of probable features scattered around itself, which is analogical to an atomic nucleus surrounded by the electron cloud. Intuitively, we name it as ``feature cloud''. The intra-class distribution of the feature cloud is learned from the head classes, and thus provides higher intra-class variation to the tail classes. Consequentially, it alleviates the distortion of the learned feature space, and improves deep representation learning on long tailed data. Extensive experimental evaluations on person re-identification and face recognition tasks confirm the effectiveness of our method.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.10826,10.1109/CVPR42600.2020.00304,https://arxiv.org/pdf/2002.10826.pdf
c6b854413e89627a6d99e8832bd18bfbcd1d3346,0,,,0,1,On Role and Location of Normalization before Model-based Data Augmentation in Residual Blocks for Classification Tasks,"Regularization is crucial to the success of many practical deep learning models, in particular in frequent scenarios where there are only a few to a moderate number of accessible training samples. In addition to weight decay, noise injection and dropout, regularization based on multi-branch architectures, such as Shake-Shake regularization, has been proven successful in many applications and attracted more and more attention. However, beyond model-based representation augmentation, it is unclear how Shake-Shake regularization helps to provide further improvement on classification tasks, let alone the baffling interaction between batch normalization and shaking. In this work, we present our investigation on Shake-Shake regularization. One of our findings illustrates the phenomenon that batch normalization in residual blocks is indispensable when shaking is applied to model branches, along with which we also empirically demonstrate the most effective location to place a batch normalization layer in a shaking regularized residual block. Based on these findings, we believe our work is beneficial to future studies on the research topic of refining control for model-based representation augmentation.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683668,
c738defac63395e68d5c7a27eca28fcdd6b3b876,0,,,0,1,Deep Metric Learning with Spherical Embedding,"Deep metric learning has attracted much attention in recent years, due to seamlessly combining the distance metric learning and deep neural network. Many endeavors are devoted to design different pair-based angular loss functions, which decouple the magnitude and direction information for embedding vectors and ensure the training and testing measure consistency. However, these traditional angular losses cannot guarantee that all the sample embeddings are on the surface of the same hypersphere during the training stage, which would result in unstable gradient in batch optimization and may influence the quick convergence of the embedding learning. In this paper, we first investigate the effect of the embedding norm for deep metric learning with angular distance, and then propose a spherical embedding constraint (SEC) to regularize the distribution of the norms. SEC adaptively adjusts the embeddings to fall on the same hypersphere and performs more balanced direction update. Extensive experiments on deep metric learning, face recognition, and contrastive self-supervised learning show that the SEC-based angular space learning strategy significantly improves the performance of the state-of-the-art.",2020,NeurIPS,2011.02785,,https://arxiv.org/pdf/2011.02785.pdf
c7c8d150ece08b12e3abdb6224000c07a6ce7d47,1,[D9],,1,0,DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification,"MeshFace photos have been widely used in many Chinese business organizations to protect ID face photos from being misused. The occlusions incurred by random meshes severely degenerate the performance of face verification systems, which raises the MeshFace verification problem between MeshFace and daily photos. Previous methods cast this problem as a typical low-level vision problem, i.e., blind inpainting. They recover perceptually pleasing clear ID photos from MeshFaces by enforcing pixel level similarity between the recovered ID images and the ground-truth clear ID images and then perform face verification on them. Essentially, face verification is conducted on a compact feature space rather than the image pixel space. Therefore, this paper argues that pixel level similarity and feature level similarity jointly offer the key to improve the verification performance. Based on this insight, we offer a novel feature oriented blind face inpainting framework. Specifically, we implement this by establishing a novel DeMeshNet, which consists of three parts. The first part addresses blind inpainting of the MeshFaces by implicitly exploiting extra supervision from the occlusion position to enforce pixel level similarity. The second part explicitly enforces a feature level similarity in the compact feature space, which can explore informative supervision from the feature space to produce better inpainting results for verification. The last part copes with face alignment within the net via a customized spatial transformer module when extracting deep facial features. All three parts are implemented within an end-to-end network that facilitates efficient optimization. Extensive experiments on two MeshFace data sets demonstrate the effectiveness of the proposed DeMeshNet as well as the insight of this paper.",2018,IEEE Transactions on Information Forensics and Security,1611.05271,10.1109/TIFS.2017.2763119,https://arxiv.org/pdf/1611.05271.pdf
c7fc277c042e64aa667ac596b65fb4a03146df89,0,,,0,1,A Genetic Algorithm Enabled Similarity-Based Attack on Cancellable Biometrics,"Cancellable biometrics (CB) as a means for biometric template protection approach refers to an irreversible yet similarity preserving transformation on the original template. With similarity preserving property, the matching between template and query instance can be performed in the transform domain without jeopardizing accuracy performance. Unfortunately, this trait invites a class of attack, namely similarity-based attack (SA). SA produces a preimage, an inverse of transformed template, which can be exploited for impersonation and cross-matching. In this paper, we propose a Genetic Algorithm enabled similarity-based attack framework (GASAF) to demonstrate that CB schemes whose possess similarity preserving property are highly vulnerable to similarity-based attack. Besides that, a set of new metrics is designed to measure the effectiveness of the similarity-based attack. We conduct the experiment on two representative CB schemes, i.e. BioHashing and Bloom-filter. The experimental results attest the vulnerability under this type of attack.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",1905.03021,10.1109/BTAS46853.2019.9185997,https://arxiv.org/pdf/1905.03021.pdf
c80f98023476e0e904db1cdd9669c2d8e5e1f7a0,0,,,0,1,Google Landmarks Dataset v2 – A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,"While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real-world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.01804,10.1109/cvpr42600.2020.00265,https://arxiv.org/pdf/2004.01804.pdf
c85147d279ba2e3360b2a45a68a5c4f1a1f8f625,0,,,1,0,Selecting active frames for action recognition with 3D convolutional network,"Recent applications of Convolutional Neural Networks, especially 3-Dimensional Convoltutional Neural Networks (3DCNNs) for human action recognition  (HAR) in videos have widely used. In this paper, we  use a multi-stream framework which is a combination  from separated networks with different kind of input  generated from unique video dataset. To achieve the  high results, firstly, we proposed a method to extract  the active frames (called Selected Active Frames -  SAF) from a videos to build datasets for 3DCNNs in  video classifying problem. Second, we deploy a new  approach called Vote fusion which considered as an  effective fusion method for ensembling multi-stream  networks. From the various datasets generated from  videos, we extract frames by our method and feed  into 3DCNNs for feature extraction, then we carry out  training and then fuse the results of softmax layers  of these streams. We evaluate the proposed methods  on solving action recognition problem. These method  are carried on three well-known datasets (HMFB51,  UCF101, and KTH). The results are also compared to  the state-of-the-art results to illustrate the efficiency  and effectiveness in our approach",2018,,,,https://pdfs.semanticscholar.org/c851/47d279ba2e3360b2a45a68a5c4f1a1f8f625.pdf
cb2470aade8e5630dcad5e479ab220db94ecbf91,0,,,1,0,Exploring Facial Differences in European Countries Boundary by Fine-Tuned Neural Networks,"Travel Agents and retailers are always curious about where their customers come from, as this would help them increase their sale and optimize their marketing models. In this study, we build a system to predict where people come from in Europe by analyzing their faces. The countries that have been chosen for the study are Russia, Italy, Germany, Spain, and France. In the first stage of the study, we implement different neural network classifiers on the dataset of people's faces that we collected from Twitter. The highest accuracy achieved is 53.2%, while human accuracy is only 26.96%. In the second stage of the study, we analyze 11 different facial features that might differentiate people in those five countries. The study lays the groundwork for future work to find out differences/similarities of people's faces around the world regardless of their current geographic location.",2018,2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),,10.1109/MIPR.2018.00062,
cc989b88f3799835f16842b066a36e171b607e7f,1,,1,1,1,ShuffleFaceNet: A Lightweight Face Architecture for Efficient and Highly-Accurate Face Recognition,"The recent success of convolutional neural networks has led to the development of a variety of new effective and efficient architectures. However, few of them have been designed for the specific case of face recognition. Inspired on the state-of-the-art ShuffleNetV2 model, a lightweight face architecture is presented in this paper. The proposal, named ShuffleFaceNet, introduces significant modifications in order to improve face recognition accuracy. First, the Global Average Pooling layer is replaced by a Global Depth-wise Convolution layer, and Parametric Rectified Linear Unit is used as a non-linear activation function. Under the same experimental conditions, ShuffleFaceNet achieves significantly superior accuracy than the original ShuffleNetV2, maintaining the same speed and compact storage. In addition, extensive experiments conducted on three challenging benchmark face datasets, show that our proposal improves not only state-of-the-art lightweight models but also very deep face recognition models.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00333,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Martindez-Diaz_ShuffleFaceNet_A_Lightweight_Face_Architecture_for_Efficient_and_Highly-Accurate_Face_ICCVW_2019_paper.pdf
ccbb94f8a025467d6001b60dd10d2078d8d9ab0e,0,,,0,1,Scalable Logo Recognition Using Proxies,"Logo recognition is the task of identifying and classifying logos. Logo recognition is a challenging problem as there is no clear definition of a logo and there are huge variations of logos, brands and re-training to cover every variation is impractical. In this paper, we formulate logo recognition as a few-shot object detection problem. The two main components in our pipeline are universal logo detector and few-shot logo recognizer. The universal logo detector is a class-agnostic deep object detector network which tries to learn the characteristics of what makes a logo. It predicts bounding boxes on likely logo regions. These logo regions are then classified by logo recognizer using nearest neighbor search, trained by triplet loss using proxies. We also annotated a first of its kind product logo dataset containing 2000 logos from 295K images collected from Amazon called PL2K. Our pipeline achieves 97% recall with 0.6 mAP on PL2K test dataset and state-of-the-art 0.565 mAP on the publicly available FlickrLogos-32 test set without fine-tuning.",2019,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),1811.08009,10.1109/WACV.2019.00081,https://arxiv.org/pdf/1811.08009.pdf
cdd71d9377bb682e8b683c4700cae650ade96eba,0,,,0,1,On visual BMI analysis from facial images,"Abstract Automatically assessing body mass index (BMI) from facial images is an interesting and challenging problem in computer vision. Facial feature extraction is an important step for visual BMI estimation. This work studies the visual BMI estimation problem based on the characteristics and performance of different facial representations, which has not been well studied yet. Various facial representations, including geometry based representations and deep learning based, are comprehensively evaluated and analyzed from three perspectives: the overall performance on visual BMI prediction, the redundancy in facial representations and the sensitivity to head pose changes. The experiments are conducted on two databases: a new dataset we collected, called the FIW-BMI and an existing large dataset Morph II. Our studies provide some deep insights into the facial representations for visual BMI analysis: 1) The deep model based methods perform better than geometry based methods. Among them, the VGG-Face and Arcface show more robustness than others in most cases; 2) Removing the redundancy in VGG-Face representation can increase the accuracy and efficiency in BMI estimation; 3) Large head poses lead to low performance for BMI estimation. The Arcface, VGG-Face and PIGF are more robust than the others to head pose variations.",2019,Image Vis. Comput.,,10.1016/J.IMAVIS.2019.07.003,
cf29bf5080d8eff9394021025160afe02653789c,0,,,0,1,Privacy Prediction of Lightweight Convolutional Neural Network,,2020,,,10.1007/978-981-15-9739-8_39,
cfb7b3cd096e571337f6a5fd6a26b94a982ca6d1,0,,,0,1,"Learning from the Past: Meta-Continual Learning with Knowledge Embedding for Jointly Sketch, Cartoon, and Caricature Face Recognition","This paper deals with a challenging task of learning from different modalities by tackling the difficulty problem of jointly face recognition between abstract-like sketches, cartoons, caricatures and real-life photographs. Due to the significant variations in the abstract faces, building vision models for recognizing data from these modalities is an extremely challenging. We propose a novel framework termed as Meta-Continual Learning with Knowledge Embedding to address the task of jointly sketch, cartoon, and caricature face recognition. In particular, we firstly present a deep relational network to capture and memorize the relation among different samples. Secondly, we present the construction of our knowledge graph that relates image with the label as the guidance of our meta-learner. We then design a knowledge embedding mechanism to incorporate the knowledge representation into our network. Thirdly, to mitigate catastrophic forgetting, we use a meta-continual model that updates our ensemble model and improves its prediction accuracy. With this meta-continual model, our network can learn from its past. The final classification is derived from our network by learning to compare the features of samples. Experimental results demonstrate that our approach achieves significantly higher performance compared with other state-of-the-art approaches.",2020,ACM Multimedia,,10.1145/3394171.3413892,
d0d955edbc44067e7fb469d1884eb59236dc770b,1,,1,1,1,Data Uncertainty Learning in Face Recognition,"Modeling data uncertainty is important for noisy images, but seldom explored for face recognition. The pioneer work, PFE, considers uncertainty by modeling each face image embedding as a Gaussian distribution. It is quite effective. However, it uses fixed feature (mean of the Gaussian) from an existing model. It only estimates the variance and relies on an ad-hoc and costly metric. Thus, it is not easy to use. It is unclear how uncertainty affects feature learning. This work applies data uncertainty learning to face recognition, such that the feature (mean) and uncertainty (variance) are learnt simultaneously, for the first time. Two learning methods are proposed. They are easy to use and outperform existing deterministic methods as well as PFE on challenging unconstrained scenarios. We also provide insightful analysis on how incorporating uncertainty estimation helps reducing the adverse effects of noisy samples and affects the feature learning.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.11339,10.1109/cvpr42600.2020.00575,https://arxiv.org/pdf/2003.11339.pdf
d39729b02f54da845301c7999632a826bf4566e8,0,,,1,0,Face Identification for an in-vehicle Surveillance System Using Near Infrared Camera,"Face identification is an essential topic in surveillance system research. Surveillance systems have many unconstrained conditions, e.g., brightness, occlusion, and user state variations. In this paper, we propose a multi-SVM based face recognition method using a near-infrared camera. Our method has a face identification scenario optimized for an in-vehicle surveillance system, which comprises two steps: (i) registering a driver and (ii) recognizing whether the driver is a registered. We perform feature extraction and recognition for each facial landmark. In the case of extreme exposure to light, we convert normal face images into simulated light overexposed images for learning. Thus, face classifiers for normal and extreme illumination conditions are simultaneously generated. We also create a new face dataset and evaluate our method with both our new and PolyU NIR datasets. Experimental results show that we achieve significantly higher recognition accuracy than existing methods.",2018,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2018.8639472,
d56eb17f086da77c71398b3d6c45a44ef1a9a5d4,1,"[M5], [M1]",,1,0,Post-Comparison Mitigation of Demographic Bias in Face Recognition Using Fair Score Normalization,"Current face recognition systems achieved high progress on several benchmark tests. Despite this progress, recent works showed that these systems are strongly biased against demographic sub-groups. Consequently, an easily integrable solution is needed to reduce the discriminatory effect of these biased systems. Previous work introduced fairness-enhancing solutions that strongly degrades the overall system performance. In this work, we propose a novel fair score normalization approach that is specifically designed to reduce the effect of bias in face recognition and subsequently lead to a significant overall performance boost. Our hypothesis is built on the notation of individual fairness by designing a normalization approach that leads to treating ""similar"" individuals ""similarly"". Experiments were conducted on two publicly available datasets captured under controlled and in-the-wild circumstances. The results show that our fair normalization approach enhances the overall performance by up to 14.8% under intermediate 0 match rate settings and up to 30.7% under high security settings. Our proposed approach significantly reduces the errors of all demographic groups, and thus reduce bias. Especially under in-the-wild conditions, we demonstrated that our fair normalization method improves the recognition performance of the effected population sub-groups by 31.6%. Unlike previous work, our proposed fairness-enhancing solution does not require demographic information about the individuals, leads to an overall performance boost, and can be easily integrated in existing biometric systems.",2020,ArXiv,2002.03592,,https://arxiv.org/pdf/2002.03592.pdf
d7c6b3628725638188dee085b58752f754697d99,1,[D9],,1,0,Learning an Evolutionary Embedding via Massive Knowledge Distillation,"Knowledge distillation methods aim at transferring knowledge from a large powerful teacher network to a small compact student one. These methods often focus on close-set classification problems and matching features between teacher and student networks from a single sample. However, many real-world classification problems are open-set. This paper proposes an Evolutionary Embedding Learning (EEL) framework to learn a fast and accurate student network for open-set problems via massive knowledge distillation. First, we revisit the formulation of canonical knowledge distillation and make it suitable for the open-set problems with massive classes. Second, by introducing an angular constraint, a novel correlated embedding loss (CEL) is proposed to match embedding spaces between the teacher and student network from a global perspective. Lastly, we propose a simple yet effective paradigm towards a fast and accurate student network development for knowledge distillation. We show the possibility to implement an accelerated student network without sacrificing accuracy, compared with its teacher network. The experimental results are quite encouraging. EEL achieves better performance with other state-of-the-art methods for various large-scale open-set problems, including face recognition, vehicle re-identification and person re-identification.",2020,International Journal of Computer Vision,,10.1007/s11263-019-01286-x,
d83f8a61c11c5b1a92ef412c8f1cf2ec7bd4a026,0,,,0,1,The JD AI Speaker Verification System for the FFSVC 2020 Challenge,"This paper presents the development of our systems for the Interspeech 2020 Far-Field Speaker Verification Challenge (FFSVC). Our focus is the task 2 of the challenge, which is to perform far-field text-independent speaker verification using a single microphone array. The FFSVC training set provided by the challenge is augmented by pre-processing the far-field data with both beamforming, voice channel switching, and a combination of weighted prediction error (WPE) and beamforming. Two open-access corpora, CHData in Mandarin and VoxCeleb2 in English, are augmented using multiple methods and mixed with the augmented FFSVC data to form the final training data. Four different model structures are used to model speaker characteristics: ResNet, extended time-delay neural network (ETDNN), Transformer, and factorized TDNN (FTDNN), whose output values are pooled across time using the self-attentive structure, the statistic pooling structure, and the GVLAD structure. The final results are derived by fusing the adaptively normalized scores of the four systems with a two-stage fusion method, which achieves a minimum of the detection cost function (minDCF) of 0.3407 and an equal error rate (EER) of 2.67% on the development set of the challenge.",2020,INTERSPEECH,,10.21437/interspeech.2020-3062,https://isca-speech.org/archive/Interspeech_2020/pdfs/3062.pdf
dbcfb8263d4085e4b83fbbe998968eab5510953c,0,,,0,1,PerSeg : segmenting salient objects from bag of single image perturbations,"Salient object segmentation is an important computer vision problem having applications in numerous areas such as video surveillance, scene parsing, autonomous navigation etc. For images, this task is quite challenging due to clutter/texture present in the background, low resolution and/or low contrast of the object(s) of interest etc. In case of videos, additional issues such as object deformation, camera motion and presence of multiple moving objects make the foreground object segmentation a significantly difficult and open problem. However, motion pattern can also act as an important cue to identify the foreground objects against the background. This is exploited by the recent approaches via aggregation of temporally perturbed information from a series of consecutive frames. Unfortunately for images, this additional cue is not available. In this paper, we propose to emulate the effect of such perturbations by constructing a bag of multiple augmentations applied on a single input image. Saliency features are estimated independently from each perturbed image in this bag, which are further combined using a novel aggregation strategy based on a convolutional gated recurrent encoder-decoder unit. Through extensive experiments on the benchmark datasets, we show better or very competitive performance when compared with the state-of-the-art methods. We further observe that even with a bag constructed using simple affine transformations, we achieve impressive performances, proving the robustness of the proposed framework.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-08388-1,
ddb35477ddcf8d66a2d695265e7f26d4ce1946c2,0,,,0,1,"Information Retrieval Technology: 15th Asia Information Retrieval Societies Conference, AIRS 2019, Hong Kong, China, November 7–9, 2019, Proceedings","This book constitutes the refereed proceedings of the 15th Information Retrieval Technology Conference, AIRS 2019, held in Hong Kong, China, in November 2019.The 14 full papers presented together with 3 short papers were carefully reviewed and selected from 27 submissions. The scope of the conference covers applications, systems, technologies and theory aspects of information retrieval in text, audio, image, video and multimedia data.",2020,AIRS,,10.1007/978-3-030-42835-8,
de00fffe4b64aef3797e05e74b5d3d07065b20ee,1,[M3] ,,0,1,Advances in Speaker Recognition for Telephone and Audio-Visual Data: the JHU-MIT Submission for NIST SRE19,"We present a condensed description of the joint effort of JHUCLSP, JHU-HLTCOE and MIT-LL for NIST SRE19. NIST SRE19 consisted of a Tunisian Arabic Telephone Speech challenge (CTS) and an audio-visual (AV) evaluation based on Internet video content. The audio-visual evaluation included the regular audio condition but also novel visual (face recognition) and multi-modal conditions. For CTS and AV-audio conditions, successful systems were based on x-Vector embeddings with very deep encoder networks, i.e, 2D residual networks (ResNet34) and Factorized TDNN (F-TDNN). For CTS, PLDA back-end domain-adapted using SRE18 eval labeled data provided significant gains w.r.t. NIST SRE18 results. For AVaudio, cosine scoring with x-Vector fine-tuned to full-length recordings outperformed PLDA based systems. In CTS, the best fusion attained EER=2.19% and Cprimary=0.205, which are around 50% and 30% better than SRE18 CTS results respectively. The best single system was HLTCOE wide ResNet with EER=2.68% and Cprimary=0.258. In AV-audio, our primary fusion attained EER=1.48% and Cprimary=0.087, which was just slightly better than the best single system (EER=1.78%, Cprimary=0.101). For the AV-video condition, our systems were based on pre-trained face detectors–MT-CNN and RetinaFace– and face recognition embeddings–ResNets trained with additive angular margin softmax. We focused on selecting the best strategies to select the enrollment faces and how to cluster and combine the embeddings of the faces of the multiple subjects in the test recording. Our primary fusion attained EER=1.87% and Cprimary=0.052. For the multi-modal condition, we just added the calibrated scores of the individual audio and video systems. Thus, we assumed complete independence between audio and video modalities. The multi-modal fusion provided impressive improvement with EER=0.44% and Cprimary=0.018.",2020,,,10.21437/odyssey.2020-39,https://www.isca-speech.org/archive/Odyssey_2020/pdfs/88.pdf
decbcd2604042e3286c28631ba3218d568889ed9,0,,,0,1,Large Margin Few-Shot Learning,"The key issue of few-shot learning is learning to generalize. This paper proposes a large margin principle to improve the generalization capacity of metric based methods for few-shot learning. To realize it, we develop a unified framework to learn a more discriminative metric space by augmenting the classification loss function with a large margin distance loss function for training. Extensive experiments on two state-of-the-art few-shot learning methods, graph neural networks and prototypical networks, show that our method can improve the performance of existing models substantially with very little computational overhead, demonstrating the effectiveness of the large margin principle and the potential of our method.",2018,ArXiv,1807.02872,,https://arxiv.org/pdf/1807.02872.pdf
dfbeb3ca7a01fe80c49b76baa50bf092f71eef4a,0,,,0,1,A Survey of Deep Learning-Based Object Detection,"Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in people’s life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning algorithms for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline thoroughly and deeply, in this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at first. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.",2019,IEEE Access,1907.09408,10.1109/ACCESS.2019.2939201,https://arxiv.org/pdf/1907.09408.pdf
e0fa271bd71d9ddb339a917b2ece0767ae6ab538,1,[D10],,1,1,BWCFace: Open-set Face Recognition using Body-worn Camera,"With computer vision reaching an inflection point in the past decade, face recognition technology has become pervasive in policing, intelligence gathering, and consumer applications. Recently, face recognition technology has been deployed on bodyworn cameras to keep officers safe, enabling situational awareness and providing evidence for trial. However, limited academic research has been conducted on this topic using traditional techniques on datasets with small sample size. This paper aims to bridge the gap in the state-of-the-art face recognition using bodyworn cameras (BWC). To this aim, the contribution of this work is two-fold: (1) collection of a dataset called BWCFace consisting of a total of 178K facial images of 132 subjects captured using the body-worn camera in in-door and daylight conditions, and (2) open-set evaluation of the latest deep-learning-based Convolutional Neural Network (CNN) architectures combined with five different loss functions for face identification, on the collected dataset. Experimental results on our BWCFace dataset suggest a maximum of 33.89% Rank-1 accuracy obtained when facial features are extracted using SENet-50 trained on a large scale VGGFace2 facial image dataset. However, performance improved up to a maximum of 99.00% Rank-1 accuracy when pretrained CNN models are fine-tuned on a subset of identities in our BWCFace dataset. Equivalent performances were obtained across body-worn camera sensor models used in existing face datasets. The collected BWCFace dataset and the pretrained/ fine-tuned algorithms are publicly available to promote further research and development in this area. A downloadable link of this dataset and the algorithms is available by contacting the authors.",2020,ArXiv,2009.11458,,https://arxiv.org/pdf/2009.11458.pdf
e28c5a6c4ef336b7abd3b4cb15e53d431e96dc05,1,[M4],,1,0,Learning Flow-based Feature Warping for Face Frontalization with Illumination Inconsistent Supervision,"Despite recent advances in deep learning-based face frontalization methods, photo-realistic and illumination preserving frontal face synthesis is still challenging due to large pose and illumination discrepancy during training. We propose a novel Flow-based Feature Warping Model (FFWM) which can learn to synthesize photo-realistic and illumination preserving frontal images with illumination inconsistent supervision. Specifically, an Illumination Preserving Module (IPM) is proposed to learn illumination preserving image synthesis from illumination inconsistent image pairs. IPM includes two pathways which collaborate to ensure the synthesized frontal images are illumination preserving and with fine details. Moreover, a Warp Attention Module (WAM) is introduced to reduce the pose discrepancy in the feature level, and hence to synthesize frontal images more effectively and preserve more details of profile images. The attention mechanism in WAM helps reduce the artifacts caused by the displacements between the profile and the frontal images. Quantitative and qualitative experimental results show that our FFWM can synthesize photo-realistic and illumination preserving frontal images and performs favorably against the state-of-the-art results.",2020,ECCV,2008.06843,10.1007/978-3-030-58610-2_33,https://arxiv.org/pdf/2008.06843.pdf
e4c52a81246b2d4458d29ba231a6c211c19e370b,1,[D9],,1,1,Learn to Propagate Reliably on Noisy Affinity Graphs,"Recent works have shown that exploiting unlabeled data through label propagation can substantially reduce the labeling cost, which has been a critical issue in developing visual recognition models. Yet, how to propagate labels reliably, especially on a dataset with unknown outliers, remains an open question. Conventional methods such as linear diffusion lack the capability of handling complex graph structures and may perform poorly when the seeds are sparse. Latest methods based on graph neural networks would face difficulties on performance drop as they scale out to noisy graphs. To overcome these difficulties, we propose a new framework that allows labels to be propagated reliably on large-scale real-world data. This framework incorporates (1) a local graph neural network to predict accurately on varying local structures while maintaining high scalability, and (2) a confidence-based path scheduler that identifies outliers and moves forward the propagation frontier in a prudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our confidence guided framework can significantly improve the overall accuracies of the propagated labels, especially when the graph is very noisy.",2020,ArXiv,2007.08802,,https://arxiv.org/pdf/2007.08802.pdf
e4e847c78932c9594ee13455f67c360baf0228c2,1,[D9],,1,0,Evaluation of maxout activations in deep learning across several big data domains,"This study investigates the effectiveness of multiple maxout activation function variants on 18 datasets using Convolutional Neural Networks. A network with maxout activation has a higher number of trainable parameters compared to networks with traditional activation functions. However, it is not clear if the activation function itself or the increase in the number of trainable parameters is responsible in yielding the best performance for different entity recognition tasks. This paper investigates if an increase in the number of convolutional filters on traditional activation functions performs equal-to or better-than maxout networks. Our experiments compare the Rectified Linear Unit, Leaky Rectified Linear Unit, Scaled Exponential Linear Unit, and Hyperbolic Tangent activations to four maxout function variants. We observe that maxout networks train relatively slower than networks with traditional activation functions, e.g. Rectified Linear Unit. In addition, we found that on average, across all datasets, the Rectified Linear Unit activation function performs better than any maxout activation when the number of convolutional filters is increased. Furthermore, adding more filters enhances the classification accuracy of the Rectified Linear Unit networks, without adversely affecting their advantage over maxout activations with respect to network-training speed.",2019,Journal of Big Data,,10.1186/s40537-019-0233-0,
e58dd160a76349d46f881bd6ddbc2921f08d1050,1,[D9],,1,0,Merge or Not? Learning to Group Faces via Imitation Learning,"Given a large number of unlabeled face images, face grouping aims at clustering the images into individual identities present in the data. This task remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation. In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections. Often, a user needs to correct the erroneous grouping manually. In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. This is achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL). In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards. Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.",2018,AAAI,1707.03986,,https://arxiv.org/pdf/1707.03986.pdf
e6c6e7597585b3ac38b4b90f11a786dd1f0ad383,0,,,1,0,Investigating Bias in Deep Face Analysis: The KANFace Dataset and Empirical Study,"Deep learning-based methods have pushed the limits of the state-of-the-art in face analysis. However, despite their success, these models have raised concerns regarding their bias towards certain demographics. This bias is inflicted both by limited diversity across demographics in the training set, as well as the design of the algorithms. In this work, we investigate the demographic bias of deep learning models in face recognition, age estimation, gender recognition and kinship verification. To this end, we introduce the most comprehensive, large-scale dataset of facial images and videos to date. It consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The data are manually annotated in terms of identity, exact age, gender and kinship. The performance of state-of-the-art models is scrutinized and demographic bias is exposed by conducting a series of experiments. Lastly, a method to debias network embeddings is introduced and tested on the proposed benchmarks.",2020,Image Vis. Comput.,2005.07302,10.1016/j.imavis.2020.103954,https://arxiv.org/pdf/2005.07302.pdf
e76432d26daefdb8ab40e11521a9dbd0396e0d63,0,,,0,1,Towards Palmprint Verification On Smartphones,"With the rapid development of mobile devices, smartphones have gradually become an indispensable part of people's lives. Meanwhile, biometric authentication has been corroborated to be an effective method for establishing a person's identity with high confidence. Hence, recently, biometric technologies for smartphones have also become increasingly sophisticated and popular. But it is noteworthy that the application potential of palmprints for smartphones is seriously underestimated. Studies in the past two decades have shown that palmprints have outstanding merits in uniqueness and permanence, and have high user acceptance. However, currently, studies specializing in palmprint verification for smartphones are still quite sporadic, especially when compared to face- or fingerprint-oriented ones. In this paper, aiming to fill the aforementioned research gap, we conducted a thorough study of palmprint verification on smartphones and our contributions are twofold. First, to facilitate the study of palmprint verification on smartphones, we established an annotated palmprint dataset named MPD, which was collected by multi-brand smartphones in two separate sessions with various backgrounds and illumination conditions. As the largest dataset in this field, MPD contains 16,000 palm images collected from 200 subjects. Second, we built a DCNN-based palmprint verification system named DeepMPV+ for smartphones. In DeepMPV+, two key steps, ROI extraction and ROI matching, are both formulated as learning problems and then solved naturally by modern DCNN models. The efficiency and efficacy of DeepMPV+ have been corroborated by extensive experiments. To make our results fully reproducible, the labeled dataset and the relevant source codes have been made publicly available at this https URL.",2020,ArXiv,2003.13266,,https://arxiv.org/pdf/2003.13266.pdf
e7d2e0d31378ace9deeddbdc2b3623602e1c3e29,0,,,0,1,Deep in-situ learning for object recognition,"T his dissertation is about in-situ object recognition, meaning that specific objects (instances) can be learned from a few training examples that depict them within the place where such objects are commonly present or being used. Learning to recognize objects in-situ opposes to conventional approaches in deep learning of relying on largescale class-level datasets of grouped instances, utilizing complex image acquisition setups or utilizing synthetic data. We aim for a scalable, robust, and real-time system based on Convolutional Neural Networks (CNNs) that learn discriminative features from images depicting objects from an egocentric point of view. We are particularly interested in learning objects from a few examples taken directly by an agent or by a demonstrator, and where the CNN does not need a finetuning process for learning additional instances, motivated by the computational limitations in most autonomous platforms. We hope our approach will be helpful for robotic tasks such as object manipulation, human-robot interaction, semantic mapping, scene understanding, autonomous navigation, and contribute to FARSCOPE’s vision on advancing the state-of-the-art of autonomy in robots and intelligent systems.",2019,,,,https://research-information.bris.ac.uk/files/218781900/Final_Copy_2019_11_28_Lagunes_Fortiz_M_A_PhD.pdf
e8a90ec963168efb31470cd998400b29d6f74023,1,[D13],,1,1,Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network,"With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked. So, it is important to study how face recognition networks are subject to attacks. In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously. Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images. For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces. Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better. The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.",2018,ArXiv,1811.12026,10.1007/s11042-020-09604-z,https://arxiv.org/pdf/1811.12026.pdf
e9f8cdd089b926c5dcdeeeda78db7d52a298d598,1,[M3],,1,1,A2-LINK: Recognizing Disguised Faces via Active Learning and Adversarial Noise Based Inter-Domain Knowledge,"Face recognition in the unconstrained environment is an ongoing research challenge. Although several covariates of face recognition such as pose and low resolution have received significant attention, “disguise” is considered an onerous covariate of face recognition. One of the primary reasons for this is the scarcity of large and representative labeled databases, along with the lack of algorithms that work well for multiple covariates in such environments. In order to address the problem of face recognition in the presence of disguise, the paper proposes an active learning framework termed as A2-LINK. Starting with a face recognition machine-learning model, A2-LINK intelligently selects training samples from the target domain to be labeled and, using hybrid noises such as adversarial noise, fine-tunes a model that works well both in the presence and absence of disguise. Experimental results demonstrate the effectiveness and generalization of the proposed framework on the DFW and DFW2019 datasets with state-of-the-art deep learning featurization models such as LCSSE, ArcFace, and DenseNet.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2020.2998912,http://iab-rubric.org/papers/2020_TBIOM_A2LINK.pdf
ea4fd6b3527e913a038c8ad66e3a2ee1e0adf0c7,1,[M3],,0,1,The JHU-MIT System Description for NIST SRE19 AV,"This document represents the SRE19 AV submission by the team composed of JHU-CLSP, JHU-HLTCOE and MIT Lincoln Labs. All the developed systems for the audio and video conditions consisted of Neural network embeddings with some flavor of PLDA/cosine back-end. Primary fusions obtained Actual DCF of 0.250 on SRE18 VAST eval, 0.183 on SRE19 AV dev audio, 0.140 on SRE19 AV dev video and 0.054 on SRE19 AV multi-modal.",2019,,,,https://www.ll.mit.edu/sites/default/files/publication/doc/JHU-MIT-system-description-nist-villalba-124945.pdf
ea71c3c87da30d999cbf55dd3d0888751c7f61cf,1,[M4],,1,1,Learning Domain-Invariant Discriminative Features for Heterogeneous Face Recognition,"Heterogeneous face recognition (HFR), referring to matching face images across different domains, is a challenging problem due to the vast cross-domain discrepancy and insufficient pairwise cross-domain training data. This article proposes a quadruplet framework for learning domain-invariant discriminative features (DIDF) for HFR, which integrates domain-level and class-level alignment in one unified network. The domain-level alignment reduces the cross-domain distribution discrepancy. The class-level alignment based on a special quadruplet loss is developed to further diminish the intra-class variations and enlarge the inter-class separability among instances, thus handling the misalignment and adversarial equilibrium problems confronted by the domain-level alignment. With a bidirectional cross-domain data selection strategy, the quadruplet loss-based method prominently enriches the training set and further eliminates the cross-modality shift. Benefiting from the joint supervision and mutual reinforcement of these two components, the domain invariance and class discrimination of identity features are guaranteed. Extensive experiments on the challenging CASIA NIR-VIS 2.0 database, the Oulu-CASIA NIR&VIS database, the BUAA-VisNir database, and the IIIT-D viewed sketch database demonstrate the effectiveness and preferable generalization capability of the proposed method.",2020,IEEE Access,,10.1109/ACCESS.2020.3038906,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09262951.pdf
eaeea227a426a90e73bfe6527db6fab23632a492,0,,,1,0,Supplementary Material of Modeling the Uncertainty of Contextual-Connections between Tracklets for Unconstrained Video-based Face Recognition,"where Tgt and Ttt are corresponding temperature factors, k ∈ {0, 1}, αp is the positive penalty and αn is the negative penalty. Directly looking for the label assignment that minimizes E(x,y) is a combinatorial optimization problem which is intractable. Instead, similar to [9], we use mean field method to approximate the distribution P (X,Y) ∝ exp(−E(X,Y)) by the product of independent marginals",2019,,,,https://pdfs.semanticscholar.org/eaee/a227a426a90e73bfe6527db6fab23632a492.pdf
eb7d94d5143f6fd7bf11c266fd59a6d2139414cc,1,[M1],,1,1,How (Not) to Measure Bias in Face Recognition Networks,"Within the last years Face Recognition (FR) systems have achieved human-like (or better) performance, leading to extensive deployment in large-scale practical settings. Yet, especially for sensible domains such as FR we expect algorithms to work equally well for everyone, regardless of somebody’s age, gender, skin colour and/or origin. In this paper, we investigate a methodology to quantify the amount of bias in a trained Convolutional Neural Network (CNN) model for FR that is not only intuitively appealing, but also has already been used in the literature to argue for certain debiasing methods. It works by measuring the “blindness” of the model towards certain face characteristics in the embeddings of faces based on internal cluster validation measures. We conduct experiments on three openly available FR models to determine their bias regarding race, gender and age, and validate the computed scores by comparing their predictions against the actual drop in face recognition performance for minority cases. Interestingly, we could not link a crisp clustering in the embedding space to a strong bias in recognition rates—it is rather the opposite. We therefore offer arguments for the reasons behind this observation and argue for the need of a less näıve clustering approach to develop a working measure for bias in FR models.",2020,ANNPR,,10.1007/978-3-030-58309-5_10,https://digitalcollection.zhaw.ch/bitstream/11475/20277/3/2020_Gluege-etal_Bias-in-face-recognition-networks_ANNPR.pdf
eb9f4b80623341813c1af9e0f421641a40f341bc,0,,,0,1,Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network,"Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g. ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3\% to 82.78\%, mCE from 76.0\% to 48.9\% and mFR from 57.7\% to 32.3\% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at this https URL",2020,ArXiv,2001.06268,,https://arxiv.org/pdf/2001.06268.pdf
ebff807c5bafcc037e5fa8e2b8963f063f1fdc40,0,,,0,1,Adversarial Margin Maximization Networks,"The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability. Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts. In addition, research works also show that DNNs are vulnerable to adversarial examples---maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions. In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view. We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy. It encourages a large margin in the input space, just like the support vector machines. With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner. Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts. Code and models for reproducing our results will be made publicly available.",2019,IEEE transactions on pattern analysis and machine intelligence,1911.05916,10.1109/TPAMI.2019.2948348,https://arxiv.org/pdf/1911.05916.pdf
eccbf6cb023ca027524877ab9b2a21e7ed1dd3d1,0,,,0,1,ASFD: Automatic and Scalable Face Detector,"In this paper, we propose a novel Automatic and Scalable Face Detector (ASFD), which is based on a combination of neural architecture search techniques as well as a new loss design. First, we propose an automatic feature enhance module named Auto-FEM by improved differential architecture search, which allows efficient multi-scale feature fusion and context enhancement. Second, we use Distance-based Regression and Margin-based Classification (DRMC) multi-task loss to predict accurate bounding boxes and learn highly discriminative deep features. Third, we adopt compound scaling methods and uniformly scale the backbone, feature modules, and head networks to develop a family of ASFD, which are consistently more efficient than the state-of-the-art face detectors. Extensive experiments conducted on popular benchmarks, e.g. WIDER FACE and FDDB, demonstrate that our ASFD-D6 outperforms the prior strong competitors, and our lightweight ASFD-D0 runs at more than 120 FPS with Mobilenet for VGA-resolution images.",2020,ArXiv,2003.11228,,https://arxiv.org/pdf/2003.11228.pdf
ed7181956b28cae5de142c01e0e6884ce3491c2c,0,,,0,1,An Attention-Based Speaker Naming Method for Online Adaptation in Non-Fixed Scenarios,"A speaker naming task, which finds and identifies the active speaker in a certain movie or drama scene, is crucial for dealing with high-level video analysis applications such as automatic subtitle labeling and video summarization. Modern approaches have usually exploited biometric features with a gradient-based method instead of rule-based algorithms. In a certain situation, however, a naive gradient-based method does not work efficiently. For example, when new characters are added to the target identification list, the neural network needs to be frequently retrained to identify new people and it causes delays in model preparation. In this paper, we present an attention-based method which reduces the model setup time by updating the newly added data via online adaptation without a gradient update process. We comparatively analyzed with three evaluation metrics(accuracy, memory usage, setup time) of the attention-based method and existing gradient-based methods under various controlled settings of speaker naming. Also, we applied existing speaker naming models and the attention-based model to real video to prove that our approach shows comparable accuracy to the existing state-of-the-art models and even higher accuracy in some cases.",2019,ArXiv,1912.00649,,https://arxiv.org/pdf/1912.00649.pdf
ed9384cef417f69c070d7bca07958f6fd1c5e8d3,1,[D11],,1,0,SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses for Mobile Platforms,"Virtual applications through mobile platforms are one of the most critical and ever-growing fields in AI, where ubiquitous and real-time person authentication has become critical after the breakthrough of all services provided via mobile devices. In this context, face verification technologies can provide reliable and robust user authentication, given the availability of cameras in these devices, as well as their widespread use in everyday applications. The rapid development of deep Convolutional Neural Networks has resulted in many accurate face verification architectures. However, their typical size (hundreds of megabytes) makes them infeasible to be incorporated in downloadable mobile applications where the entire file typically may not exceed 100 Mb. Accordingly, we address the challenge of developing a lightweight face recognition network of just a few megabytes that can operate with sufficient accuracy in comparison to much larger models. The network also should be able to operate under different poses, given the variability naturally observed in uncontrolled environments where mobile devices are typically used. In this paper, we adapt the lightweight SqueezeNet model, of just 4.4MB, to effectively provide cross-pose face recognition. After trained on the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23% on the difficult frontal vs. profile comparison, and0.54% on profile vs. profile images. Under less extreme variations involving frontal images in any of the enrolment/query images pair, EER is pushed down to<0.3%, and the FRR at FAR=0.1%to less than 1%. This makes our light model suitable for face recognition where at least acquisition of the enrolment image can be controlled. At the cost of a slight degradation in performance, we also test an even lighter model (of just 2.5MB) where regular convolutions are replaced with depth-wise separable convolutions.",2020,ArXiv,2007.08566,,https://arxiv.org/pdf/2007.08566.pdf
ee02bed42e11e88907c69e899d192019900f1a14,0,,,0,1,Angular Visual Hardness,"Recent convolutional neural networks (CNNs) have led to impressive performance but often suffer from poor calibration. They tend to be overconfident, with the model confidence not always reflecting the underlying 1 ambiguity and hardness. In this paper, we propose angular visual hardness (AVH), a score given by the normalized angular distance between the sample feature embedding and the target classifier to measure sample hardness. We validate this score with an in-depth and extensive scientific study, and observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models improve on the classification of harder examples. We observe that the training dynamics of AVH is vastly different compared to the training loss. Specifically, AVH quickly reaches a plateau for all samples even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. We also find that AVH has a statistically significant correlation with human visual hardness. Finally, we demonstrate the benefit of AVH to a variety of applications such as self-training for domain adaptation and domain generalization.",2020,ICML,1912.02279,,https://arxiv.org/pdf/1912.02279.pdf
ee0c1cefb180f408ef3765657bff06d31d325832,1,[D11],,1,1,Deep Polynomial Neural Networks,"Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of DCNNs. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that $\Pi$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning.",2020,ArXiv,2006.13026,,https://arxiv.org/pdf/2006.13026.pdf
f22e2eb59c4cf3f124c14bbdd0c7c93fb1c74cd6,0,,,0,1,Primate Face Identification in the Wild,"Ecological imbalance owing to rapid urbanization and deforestation has adversely affected the population of several wild animals. This loss of habitat has skewed the population of several non-human primate species like chimpanzees and macaques and has constrained them to co-exist in close proximity of human settlements, often leading to human-wildlife conflicts while competing for resources. For effective wildlife conservation and conflict management, regular monitoring of population and of conflicted regions is necessary. However, existing approaches like field visits for data collection and manual analysis by experts is resource intensive, tedious and time consuming, thus necessitating an automated, non-invasive, more efficient alternative like image based facial recognition. The challenge in individual identification arises due to unrelated factors like pose, lighting variations and occlusions due to the uncontrolled environments, that is further exacerbated by limited training data. Inspired by human perception, we propose to learn representations that are robust to such nuisance factors and capture the notion of similarity over the individual identity sub-manifolds. The proposed approach, Primate Face Identification (PFID), achieves this by training the network to distinguish between positive and negative pairs of images. The PFID loss augments the standard cross entropy loss with a pairwise loss to learn more discriminative and generalizable features, thus making it appropriate for other related identification tasks like open-set, closed set and verification. We report state-of-the-art accuracy on facial recognition of two primate species, rhesus macaques and chimpanzees under the four protocols of classification, verification, closed-set identification and open-set recognition.",2019,PRICAI,1907.02642,10.1007/978-3-030-29894-4_32,https://arxiv.org/pdf/1907.02642.pdf
f26fd03feb0fa85737b5cc16fc29a72634ea2e0b,1,[M3],,0,1,Toward Driver Face Recognition in the Intelligent Traffic Monitoring Systems,"This paper models the driver face recognition problem under the intelligent traffic monitoring systems as severe illumination variation face recognition with single sample problem. Firstly, in the point of view of numerical value sign, the current illumination invariant unit is derived from the subtraction of two pixels in the face local region, which may be positive or negative, we propose a generalized illumination robust (GIR) model based on positive and negative illumination invariant units to tackle severe illumination variations. Then, the GIR model can be used to generate several GIR images based on the local edge-region or the local block-region, which results in the edge-region based GIR (EGIR) image or the block-region based GIR (BGIR) image. For single GIR image based classification, the GIR image utilizes the saturation function and the nearest neighbor classifier, which can develop EGIR-face and BGIR-face. For multi GIR images based classification, the GIR images employ the extended sparse representation classification (ESRC) as the classifier that can form the EGIR image based classification (GIRC) and the BGIR image based classification (BGIRC). Further, the GIR model is integrated with the pre-trained deep learning (PDL) model to construct the GIR-PDL model. Finally, the performances of the proposed methods are verified on the Extended Yale B, CMU PIE, AR, self-built Driver and VGGFace2 face databases. The experimental results indicate that the proposed methods are efficient to tackle severe illumination variations.",2020,IEEE Transactions on Intelligent Transportation Systems,,10.1109/TITS.2019.2945923,
f288b372a1a0c4c9a28a5ca9865385dacb48a6ad,1,[D17],,1,0,Single Unit Status in Deep Convolutional Neural Network Codes for Face Identification: Sparseness Redefined,"Deep convolutional neural networks (DCNNs) trained for face identification develop representations that generalize over variable images, while retaining subject (e.g., gender) and image (e.g., viewpoint) information. Identity, gender, and viewpoint codes were studied at the ""neural unit"" and ensemble levels of a face-identification network. At the unit level, identification, gender classification, and viewpoint estimation were measured by deleting units to create variably-sized, randomly-sampled subspaces at the top network layer. Identification of 3,531 identities remained high (area under the ROC approximately 1.0) as dimensionality decreased from 512 units to 16 (0.95), 4 (0.80), and 2 (0.72) units. Individual identities separated statistically on every top-layer unit. Cross-unit responses were minimally correlated, indicating that units code non-redundant identity cues. This ""distributed"" code requires only a sparse, random sample of units to identify faces accurately. Gender classification declined gradually and viewpoint estimation fell steeply as dimensionality decreased. Individual units were weakly predictive of gender and viewpoint, but ensembles proved effective predictors. Therefore, distributed and sparse codes co-exist in the network units to represent different face attributes. At the ensemble level, principal component analysis of face representations showed that identity, gender, and viewpoint information separated into high-dimensional subspaces, ordered by explained variance. Identity, gender, and viewpoint information contributed to all individual unit responses, undercutting a neural tuning analogy for face attributes. Interpretation of neural-like codes from DCNNs, and by analogy, high-level visual codes, cannot be inferred from single unit responses. Instead, ""meaning"" is encoded by directions in the high-dimensional space.",2020,ArXiv,2002.06274,,https://arxiv.org/pdf/2002.06274.pdf
f3291d682a4fb4e2521beee9c70398cd5ff38b3b,1,[D9],,1,1,Lightweight Face Recognition Challenge,"Face representation using Deep Convolutional Neural Network (DCNN) embedding is the method of choice for face recognition. Current state-of-the-art face recognition systems can achieve high accuracy on existing in-the-wild datasets. However, most of these datasets employ quite limited comparisons during the evaluation, which does not simulate a real-world scenario, where extensive comparisons are encountered by a face recognition system. To this end, we propose two large-scale datasets (DeepGlint-Image with 1.8M images and IQIYI-Video with 0.2M videos) and define an extensive comparison metric (trillion-level pairs on the DeepGlint-Image dataset and billion-level pairs on the IQIYI-Video dataset) for an unbiased evaluation of deep face recognition models. To ensure fair comparison during the competition, we define light-model track and large-model track, respectively. Each track has strict constraints on computational complexity and model size. To the best of our knowledge, this is the most comprehensive and unbiased benchmarks for deep face recognition. To facilitate future research, the proposed datasets are released and the online test server is accessible as part of the Lightweight Face Recognition Challenge at the International Conference on Computer Vision, 2019.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00322,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Deng_Lightweight_Face_Recognition_Challenge_ICCVW_2019_paper.pdf
f41f0395f42b8655a630e82bd6e40d73d4c420ef,1,[D16],,1,0,Plug-in Factorization for Latent Representation Disentanglement,"In this work, we propose a Factorized Disentangler-Entangler Network (FDEN) that learns to decompose a latent representation into two mutually independent factors, namely, identity and style. Given a latent representation, the proposed framework draws a set of interpretable factors aligned to identity of an observed data and learns to maximize the independency between these factors. Our work introduces an idea for a plug-in method to disentangle latent representations of already learned deep models with no affect to the model. In doing so, it brings the possibilities of extending state-of-the-art models to solve different tasks and also maintain the performance of its original task. Thus, FDEN is naturally applicable to jointly perform multiple tasks such as few-shot learning and image-to-image translation in a single framework. We show the effectiveness of our work in disentangling a latent representation in two parts. First, to evaluate the alignment of factor to an identity, we perform few-shot learning using only the aligned factor. Then, to evaluate the effectiveness of decomposition of latent representation and to show that plugin method does not affect the deep model in its performance, we perform image-to-image style transfer by mixing factors of different images. These evaluations show, qualitatively and quantitatively, that our proposed framework can indeed disentangle a latent representation.",2019,ArXiv,,,
f60070d3a4d333aa1436e4c372b1feb5b316a7ba,0,,,1,0,Face Recognition via Centralized Coordinate Learning,"Owe to the rapid development of deep neural network (DNN) techniques and the emergence of large scale face databases, face recognition has achieved a great success in recent years. During the training process of DNN, the face features and classification vectors to be learned will interact with each other, while the distribution of face features will largely affect the convergence status of network and the face similarity computing in test stage. In this work, we formulate jointly the learning of face features and classification vectors, and propose a simple yet effective centralized coordinate learning (CCL) method, which enforces the features to be dispersedly spanned in the coordinate space while ensuring the classification vectors to lie on a hypersphere. An adaptive angular margin is further proposed to enhance the discrimination capability of face features. Extensive experiments are conducted on six face benchmarks, including those have large age gap and hard negative samples. Trained only on the small-scale CASIA Webface dataset with 460K face images from about 10K subjects, our CCL model demonstrates high effectiveness and generality, showing consistently competitive performance across all the six benchmark databases.",2018,ArXiv,1801.05678,,https://arxiv.org/pdf/1801.05678.pdf
f7a7f27516a219122000b9b915289b7127dca2f2,1,[D9],,1,0,A Light CNN for Deep Face Representation With Noisy Labels,"The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit the large amount of training data. When training data are obtained from the Internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called max-feature-map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance, meanwhile, reducing the number of parameters and computational costs. Finally, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning.",2018,IEEE Transactions on Information Forensics and Security,1511.02683,10.1109/TIFS.2018.2833032,https://arxiv.org/pdf/1511.02683.pdf
fa03cac5aa5192822a85273852090ca20a6c47aa,1,,1,1,1,Towards Interpretable Face Recognition,"Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1805.00611,10.1109/ICCV.2019.00944,https://arxiv.org/pdf/1805.00611.pdf
fa46285eb5218c4c543514dd7a34370ff9d99e77,1,[D14],,1,1,Mis-classified Vector Guided Softmax Loss for Face Recognition,"Face recognition has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs), the central task of which is how to improve the feature discrimination. To this end, several margin-based (e.g., angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from three issues: 1) Obviously, they ignore the importance of informative features mining for discriminative learning; 2) They encourage the feature margin only from the ground truth class, without realizing the discriminability from other non-ground truth classes; 3) The feature margin between different classes is set to be same and fixed, which may not adapt the situations very well. To cope with these issues, this paper develops a novel loss function, which adaptively emphasizes the mis-classified feature vectors to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative face features. To the best of our knowledge, this is the first attempt to inherit the advantages of feature margin and feature mining into a unified loss function. Experimental results on several benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives. Our code is available at http://www.cbsr.ia.ac.cn/users/xiaobowang/.",2020,AAAI,1912.00833,10.1609/AAAI.V34I07.6906,https://arxiv.org/pdf/1912.00833.pdf
feea73095b1be0cbae1ad7af8ba2c4fb6f316d35,0,,,1,0,Deep Face Recognition with Center Invariant Loss,"Convolutional Neural Networks (CNNs) have been widely used for face recognition and got extraordinary performance with large number of available face images of different people. However, it is hard to get uniform distributed data for all people. In most face datasets, a large proportion of people have few face images. Only a small number of people appear frequently with more face images. These people with more face images have higher impact on the feature learning than others. The imbalanced distribution leads to the difficulty to train a CNN model for feature representation that is general for each person, instead of mainly for the people with large number of face images. To address this challenge, we proposed a center invariant loss which aligns the center of each person to enforce the learned features to have a general representation for all people. The center invariant loss penalizes the difference between each center of classes. With center invariant loss, we can train a robust CNN that treats each class equally regardless the number of class samples. Extensive experiments demonstrate the effectiveness of the proposed approach. We achieve state-of-the-art results on LFW and YTF datasets.",2017,ACM Multimedia,,10.1145/3126686.3126693,http://www1.ece.neu.edu/~yuewu/files/2017/twu024.pdf
fffbcbf1625f4da9a2e4f71a8cdd1689aa59c56c,0,,,0,1,Face Recognition: A Novel Multi-Level Taxonomy based Survey,"In a world where security issues have been gaining growing importance, face recognition systems have attracted increasing attention in multiple application areas, ranging from forensics and surveillance to commerce and entertainment. To help understanding the landscape and abstraction levels relevant for face recognition systems, face recognition taxonomies allow a deeper dissection and comparison of the existing solutions. This paper proposes a new, more encompassing and richer multi-level face recognition taxonomy, facilitating the organization and categorization of available and emerging face recognition solutions; this taxonomy may also guide researchers in the development of more efficient face recognition solutions. The proposed multi-level taxonomy considers levels related to the face structure, feature support and feature extraction approach. Following the proposed taxonomy, a comprehensive survey of representative face recognition solutions is presented. The paper concludes with a discussion on current algorithmic and application related challenges which may define future research directions for face recognition.",2020,IET Biom.,1901.00713,10.1049/iet-bmt.2019.0001,https://arxiv.org/pdf/1901.00713.pdf