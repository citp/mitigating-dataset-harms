paperId,cites D1,cites D2,cites D3,title,abstract,year,venue,arxivId,doi,pdfUrl
001973a77bf8fa82314de667af5b041d856b0069,1,0,0,Trajectory Factory: Tracklet Cleaving and Re-Connection by Deep Siamese Bi-GRU for Multiple Object Tracking,"Multi-Object Tracking (MOT) is a challenging task in the complex scene such as surveillance and autonomous driving. In this paper, we propose a novel tracklet processing method to cleave and re-connect tracklets on crowd or longterm occlusion by Siamese Bi-Gated Recurrent Unit (GRU). The tracklet generation utilizes object features extracted by CNN and RNN to create the high-confidence tracklet candidates in sparse scenario. Due to mis-tracking in the generation process, the tracklets from different objects are split into several sub-tracklets by a bidirectional GRU. After that, a Siamese GRU based tracklet re-connection method is applied to link the sub-tracklets which belong to the same object to form a whole trajectory. In addition, we extract the track-let images from existing MOT datasets and propose a novel dataset to train our networks. The proposed dataset contains more than 95160 pedestrian images. It has 793 different persons in it. On average, there are 120 images for each person with positions and sizes. Experimental results demonstrate the advantages of our model over the state-of-the-art methods on MOTI6.",2018,2018 IEEE International Conference on Multimedia and Expo (ICME),1804.04555,10.1109/ICME.2018.8486454,https://arxiv.org/pdf/1804.04555.pdf
004acfec16c36649408c561faa102dd9de76f085,1,1,0,Multi-level Factorisation Net for Person Re-identification,"Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.09132,10.1109/CVPR.2018.00225,https://arxiv.org/pdf/1803.09132.pdf
009019f67019320bb1da759f60ee24daa7f96f51,1,0,0,Simultaneous Identification and Tracking of Multiple People Using Video and IMUs,"Most modern approaches for multiple people tracking rely on human appearance to exploit similarity between person detections. In this work we propose an alternative tracking method that does not depend on visual appearance and is still capable to deal with very dynamic motions and long-term occlusions. We make this feasible by: (i) incorporating additional information from body-worn inertial sensors, (ii) designing a neural network to relate person detections to orientation measurements and (iii) formulating a graph labeling problem to obtain a tracking solution that is globally consistent with the video and inertial recordings. We evaluate our approach on several challenging tracking sequences and achieve a very high IDF1 score of 91.2%. We outperform appearance-based baselines in scenarios where appearance is less informative and are on-par in situations with discriminative people appearance.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00106,http://openaccess.thecvf.com/content_CVPRW_2019/papers/BMTT/Henschel_Simultaneous_Identification_and_Tracking_of_Multiple_People_Using_Video_and_CVPRW_2019_paper.pdf
009e625561119aa9affb936ad74e611fd1fa36d4,1,0,0,A Simple Baseline for Multi-Object Tracking,"There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problem. It remarkably outperforms the state-of-the-arts on the public datasets at $30$ fps. We hope this baseline could inspire and help evaluate new ideas in this field. The code and the pre-trained models are available at \url{this https URL}.",2020,ArXiv,,,https://www.chunyuwang.org/img/oneshottracker.pdf
00a3ff242919164646a9b941c370a4add2db0a2f,0,1,0,Joint Attention Mechanism for Person Re-Identification,"Although person re-identification (ReID) has drawn increasing research attention due to its potential to address the problem of analysis and processing of massive monitoring data, it is very challenging to learn discriminative information when the people in the images are occluded, in large pose variations or from different perspectives. To address this problem, we propose a novel joint attention person ReID (JA-ReID) architecture. The idea is to learn two complementary feature representations by combining a soft pixel-level attention mechanism and a hard region-level attention mechanism. The soft pixel-level attention mechanism learns a discriminative embedding for the fine-grained information by exploring the salient parts in the feature maps. The hard region-level attention mechanism conducts uniform partitions on the convolutional feature maps for learning local features. We have achieved competitive results in three popular benchmarks, including Market1501, DukeMTMC-reID, and CUHK03. The experimental results verify the adaptability of the joint attention mechanism to non-rigid deformation of the human body, which can effectively improve the accuracy of ReID.",2019,IEEE Access,,10.1109/ACCESS.2019.2927170,
00b504c1ff34d86356d893ba1f0159b1710c7bc9,0,1,0,Distilled Person Re-Identification: Towards a More Scalable System,"Person re-identification (Re-ID), for matching pedestrians across non-overlapping camera views, has made great progress in supervised learning with abundant labelled data. However, the scalability problem is the bottleneck for applications in large-scale systems. We consider the scalability problem of Re-ID from three aspects: (1) low labelling cost by reducing label amount, (2) low extension cost by reusing existing knowledge and (3) low testing computation cost by using lightweight models. The requirements render scalable Re-ID a challenging problem. To solve these problems in a unified system, we propose a Multi-teacher Adaptive Similarity Distillation Framework, which requires only a few labelled identities of target domain to transfer knowledge from multiple teacher models to a user-specified lightweight student model without accessing source domain data. We propose the Log-Euclidean Similarity Distillation Loss for Re-ID and further integrate the Adaptive Knowledge Aggregator to select effective teacher models to transfer target-adaptive knowledge. Extensive evaluations show that our method can extend with high scalability and the performance is comparable to the state-of-the-art unsupervised and semi-supervised Re-ID methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00128,http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf
00b9079c3320d93832a23441d3519463cf0e84e9,1,0,0,Long/Short-Term Appearance Modeling and Two-Step Association for Multi-Object Tracking,"Vision-based multi-object tracking has many potential applications in intelligent transportation systems and intelligent vehicles. Tracking by detection, as a popular approach to multi-object tracking, first obtains detection responses from video sequence and then associates them into tracks for every object. Existing tracking-by-detection methods can work well in constrained scenarios. However, in those complicated scenarios with occlusion and adverse illumination conditions, the detection stage is deteriorated and thus makes it difficult to track objects accurately. In this paper, we present a robust tracker that represents object appearance using stable temporal features and associates the detection responses through a two-step association process. We propose to use Bi-LSTM (Bidirectional Long Short-Term Memory) to model object appearance and obtain reliable temporal features. Then, we estimate the affinity between tracks and detections based on multiple cues including appearance, motion and shape, and integrate the affinity into a two-step association procedure. Our method is verified on MOT datasets and the experimental results are promising as compared to the state-of-the-art.",2019,2019 IEEE Intelligent Transportation Systems Conference (ITSC),,10.1109/ITSC.2019.8916882,
00e3957212517a252258baef833833921dd308d4,0,1,0,Adaptively Weighted Multi-task Deep Network for Person Attribute Classification,"Multi-task learning aims to boost the performance of multiple prediction tasks by appropriately sharing relevant information among them. However, it always suffers from the negative transfer problem. And due to the diverse learning difficulties and convergence rates of different tasks, jointly optimizing multiple tasks is very challenging. To solve these problems, we present a weighted multi-task deep convolutional neural network for person attribute analysis. A novel validation loss trend algorithm is, for the first time proposed to dynamically and adaptively update the weight for learning each task in the training process. Extensive experiments on CelebA, Market-1501 attribute and Duke attribute datasets clearly show that state-of-the-art performance is obtained; and this validates the effectiveness of our proposed framework.",2017,ACM Multimedia,,10.1145/3123266.3123424,http://homepage.fudan.edu.cn/fengrui/files/2020/03/Adaptively-Weighted-Multi-task-Deep-Network-for-Person-Attribute-Classification.pdf
0100b4bf12bb2a9405331cb9208da7fea285785f,1,0,0,CANU-ReID: A Conditional Adversarial Network for Unsupervised person Re-IDentification.,,2020,,1904.01308,,https://arxiv.org/pdf/1904.01308.pdf
01d309cb815a69ada1196ca3c97273ea70c06655,0,1,0,PERSON RE-IDENTIFICATION BY REFINED ATTRIBUTE PREDICTION AND WEIGHTED MULTI-PART CONSTRAINTS,"Person re-identification (re-id) aims to match person images captured in non-overlapping camera views. Convolutional Neural Network (CNN) has been verified to be powerful in pedestrian feature extraction. However, the CNN features focus more on global visual information, which are sensitive to environmental variations. In comparison, attribute features contain semantic information and prove to be more stable to cross-view appearance changes. In this paper, we present a novel network which leverages high-level semantic attributes to enhance pedestrian descriptors. By introducing hand-crafted multi-colorspaces and texture information to refine CNN features, we acquire a more invariant and reliable feature representation for attribute prediction. The attribute-based stream is further embedded into a part-based CNN branch for re-id. This part-based CNN is trained with a weighted integration of multi-part identification losses. Experiments on two public datasets demonstrate significant performance improvements of our method over state of the arts.",2018,2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP),,10.1109/GlobalSIP.2018.8646466,
01f3f0293c2a13bc122088e837b513a123f37c70,1,0,0,Multi-object Tracking with Neural Gating Using Bilinear LSTM,"In recent deep online and near-online multi-object tracking approaches, a difficulty has been to incorporate long-term appearance models to efficiently score object tracks under severe occlusion and multiple missing detections. In this paper, we propose a novel recurrent network model, the Bilinear LSTM, in order to improve the learning of long-term appearance models via a recurrent network. Based on intuitions drawn from recursive least squares, Bilinear LSTM stores building blocks of a linear predictor in its memory, which is then coupled with the input in a multiplicative manner, instead of the additive coupling in conventional LSTM approaches. Such coupling resembles an online learned classifier/regressor at each time step, which we have found to improve performances in using LSTM for appearance modeling. We also propose novel data augmentation approaches to efficiently train recurrent models that score object tracks on both appearance and motion. We train an LSTM that can score object tracks based on both appearance and motion and utilize it in a multiple hypothesis tracking framework. In experiments, we show that with our novel LSTM model, we achieved state-of-the-art performance on near-online multiple object tracking on the MOT 2016 and MOT 2017 benchmarks.",2018,ECCV,,10.1007/978-3-030-01237-3_13,http://web.engr.oregonstate.edu/~lif/1925.pdf
01fc14785c7efd1f528dc9def1b058f7ad3f42e9,0,1,0,Person Search by Separated Modeling and A Mask-Guided Two-Stream CNN Model,"In this work, we tackle the problem of person search, which is a challenging task consisted of pedestrian detection and person re-identification (re-ID). Instead of sharing representations in a single joint model, we find that separating detector and re-ID feature extraction yields better performance. In order to extract more representative features for each identity, we segment out the foreground person from the original image patch. We propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams. We also propose a Confidence Weighted Stream Attention method which further re-adjusts the relative importance of the two streams by incorporating the detection confidence. Furthermore, we simplify the whole pipeline by incorporating semantic segmentation into the re-ID network, which is trained by bounding boxes as weakly-annotated masks and identification labels simultaneously. From the experiments on two standard person search benchmarks i.e. CUHK-SYSU and PRW, we achieve mAP of 83.3% and 32.8% respectively, surpassing the state of the art by a large margin. The extensive ablation study and model inspection further justifies our motivation.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2973513,
020dc284dd062ef6fed4659df539d79eac3043c2,0,0,1,Joint graph regularized dictionary learning and sparse ranking for multi-modal multi-shot person re-identification,"Abstract The promising achievement of sparse ranking in image-based recognition gives rise to a number of development on person re-identification (Re-ID) which aims to reconstruct the probe as a linear combination of few atoms/images from an over-complete dictionary/gallery. However, most of the existing sparse ranking based Re-ID methods lack considering the geometric relationships between probe, gallery, and cross-modal images of the same person in multi-shot Re-ID. In this paper, we propose a novel joint graph regularized dictionary learning and sparse ranking method for multi-modal multi-shot person Re-ID. First, we explore the probe-based geometrical structure by enforcing the smoothness between the codings/coefficients, which refers to the multi-shot images from the same person in probe. Second, we explore the gallery-based geometrical structure among gallery images, which encourages the multi-shot images from the same person in the gallery making similar contributions while reconstructing a certain probe image. Third, we explore the cross-modal geometrical structure by enforcing the smoothness between the cross-modal images and thus extend our model for the multi-modal case. Finally, we design an APG based optimization to solve the problem. Comprehensive experiments on benchmark datasets demonstrate the superior performance of the proposed model. The code is available at https://github.com/ttaalle/Lhc .",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107352,
025b22d5daa5d115a039bd4003ea52100198d6f5,1,0,0,Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification,,2020,ArXiv,2007.1389,,https://arxiv.org/pdf/2007.13890.pdf
0290a9336dc1cbb589e1c6b70f2c55eedfcdaa12,1,0,0,Attention Network Robustification for Person ReID,"The task of person re-identification (ReID) has attracted growing attention in recent years with improving performance but lack of focus on real-world applications. Most state of the art methods use large pre-trained models, e.g., ResNet50 (~25M parameters), as their backbone, which makes it tedious to explore different architecture modifications. In this study, we focus on small-sized randomly initialized models which enable us to easily introduce network and training modifications suitable for person ReID public datasets and real-world setups. We show the robustness of our network and training improvements by outperforming state of the art results in terms of rank-1 accuracy and mAP on Market1501 (96.2, 89.7) and DukeMTMC (89.8, 80.3) with only 6.4M parameters and without using re-ranking. Finally, we show the applicability of the proposed ReID network for multi-object tracking.",2019,ArXiv,,,
02cf480fa940e833be5322132c1a19fe82bcdc3d,1,1,0,Attribute-Aware Pedestrian Image Editing,"Pedestrian image generation is a very challenging task. Existing generation methods have drawbacks including body distortion, inadequate visual details and large vague areas. In this paper, we propose Attribute-aware Pedestrian Image Editing (APIE) to address these problems based on given visual attributes. Our model denominated as APIE-Net, has three mechanisms including an attribute-aware segmentation network, a multi-scale discriminator and a latent-variable discriminator. Experiments on Market-1501 and DukeMTMC-reID datasets show that APIE-Net can generate satisfying pedestrian images with given attributes. Moreover, the generated images can augment the original datasets thus improve the performance in pedestrian-related tasks such as person re-identification (re-ID) and attribute prediction. Especially in person re-ID tasks our method outperforms state-of-the-art methods by a large margin.",2019,ICIG,,10.1007/978-3-030-34120-6_4,
02e97e65fd0ec9a6d98a255d0396eb796a5e444a,1,0,0,Online Multiple View Tracking: Targets Association Across Cameras,"Most multiple object tracking algorithms relying on a single view have failed to follow the trajectories of targets when they have been completely hidden by obstacles. In this paper, we introduce a novel method of collaborative tracking in a synchronized overlapping cameras network. We propose an efficient target association method between cameras based on the tracking results of each target on each view. Our framework naturally handles obstacle occlusions and mutual target occlusions. We implemented our multiple object tracking algorithm by Decision Making algorithm [30] on each view. The tracking outcomes on each camera are collected and associated into targets. The feedback from the central association helps the individual cameras in tracking hidden targets, even in the case of complete occlusion. We use the standard MOT metric to validate our method. The experimental results on each view show that the multiple view tracking system outperforms the single view ones. The source code will be available publicly.",2018,BMVC,,,http://bmvc2018.org/contents/workshops/vibe2018/VIBE004.pdf
0353fe24ecd237f4d9ae4dbc277a6a67a69ce8ed,1,1,0,Discriminative Feature Representation for Person Re-identification by Batch-contrastive Loss,"In the past few years, person re-identification (reID) has developed rapidly due to the success of deep convolutional neural networks. The softmax loss function is an important component for learning discriminative features. However, the classifier trained by the softmax loss is difficult to distinguish the hard samples. In this work, we introduce a new auxiliary loss function, called batch-contrastive loss, for person reID to further separate the features of different identities and pulls the features of same identity closer. Furthermore, the proposed loss function does not rely on the pairwise or triplet sampling which is commonly used in the Siamese model. We test our loss function on two large-scale person reID benchmarks, Market-1501 and DukeMTMC datasets. Under the combination of the batch-contrastive loss and the softmax loss, even only employing the generic L2-distance metric, we can achieve competitive results among the state-of-the-arts.",2018,ACML,,,https://pdfs.semanticscholar.org/0353/fe24ecd237f4d9ae4dbc277a6a67a69ce8ed.pdf
0397ea12f39e2e7939b0a22a732aff80fe83bd4e,1,0,0,Improving Performance of DeepCC Tracker by Background Comparison and Trajectory Refinement,"DukeMTMCT is the largest and most completely labeled dataset in Multi-Target Multi-Camera Tracking (MTMCT). We investigate a state-of-the-art work on DukeMTMCT named DeepCC, and dig out two main problems. The first problem is that the openpose is prone to false detection, which seriously affects performance. The second problem is that two different persons may be assigned with the same ID. According to the corresponding problems, we not only propose a method to measure the similarity between detected bounding box and its original background avoiding false detection caused by OpenPose, but also design a strategy to correct the tracking trajectories which are affected by the unreliability of the correlation matrix clustering method proposed by DeepCC. Our method outperforms the state-of-the-art on DukeMTMCT.",2019,2019 Twelfth International Conference on Ubi-Media Computing (Ubi-Media),,10.1109/Ubi-Media.2019.00042,
03b435223a85da8d8e6730a9dd39ab04fbd5feef,1,1,0,Self-Supervised Agent Learning for Unsupervised Cross-Domain Person Re-Identification,"Unsupervised person re-identification (Re-ID) has better scalability and practicability than supervised Re-ID in the actual deployment. However, it is difficult to learn a discriminative Re-ID model without annotations. To address the above issue, we propose an end-to-end Self-supervised Agent Learning (SAL) algorithm by exploiting a set of agents as a bridge to reduce domain gaps for unsupervised cross-domain person Re-ID. The proposed SAL model enjoys several merits. First, to the best of our knowledge, this is the first work to exploit self-supervised learning for unsupervised person Re-ID. Second, our model has designed three effective learning mechanisms including supervised label learning in source domain, similarity consistency learning in target domain, and self-supervised learning in cross domain, which can learn domain-invariant yet discriminative representations through the principled lens of agent learning by reducing domain discrepancy adaptively. Extensive experimental results on three standard benchmarks demonstrate that the proposed SAL performs favorably against state-of-the-art unsupervised person Re-ID methods.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3016869,
03bcae84f89428978b78e363a4d295703bdd2982,1,1,1,Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification,"Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrieval-based search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-the-art performance while reducing 50% parameters and 53% FLOPs compared to others.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1903.09776,10.1109/ICCV.2019.00385,https://arxiv.org/pdf/1903.09776.pdf
03df42c643872aa664a7d6a8f5dbb12cbc3d09f3,1,1,0,An End-to-End Noise-Weakened Person Re-Identification and Tracking With Adaptive Partial Information,"Aiming to recognize persons of interest cross cameras in different locations, the technique of person re-identification (re-ID) has attracted unprecedented attention in the field of public security. However, most of the existing work ignores the influence of background noise and pedestrian’s partial information on recognition accuracy. Moreover, the tracking procedure which has a great importance on the real world is often stripped out of the re-ID framework. Therefore, this paper proposes an end-to-end noise-weakened person re-ID and tracking model with adaptive partial information. First, to suppress the background noise and improve the feature discriminability, Mask R-CNN is applied to extract the foreground “pedestrians” out of the complicated background for feature supplement. Second, an adaptive pose estimation model is proposed to make an in-depth analysis of every human body part, thus boosting the robustness against the posture change and individual difference. Finally, to fuse the tracking procedure, a scope prediction scheme based on the pedestrian’s moving speed is presented to replace the traditional full domain estimation approach, thus greatly reducing the computational complexity. The extensive experiments have been conducted and the results demonstrate that our method achieves 89.78% and 81.87% rank 1 accuracy on Market-1501 and DukeMTMC-reID with real-time tracking capability, which exhibits great superiority than the state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2899032,
03e3a13f03f44de0cb507d334b5ef22db6886abb,1,1,0,Isosceles Constraints for Person Re-Identification,"In the existing works of person re-identification (ReID), batch hard triplet loss has achieved great success. However, it only cares about the hardest samples within the batch. For any probe, there are massive mismatched samples (crucial samples) outside the batch which are closer than the matched samples. To reduce the disruptive influence of crucial samples, we propose a novel isosceles contraint for triplet. Theoretically, we show that if a matched pair has equal distance to any one of mismatched sample, the matched pair should be infinitely close. Motivated by this, the isosceles constraint is designed for the two mismatched pairs of each triplet, to restrict some matched pairs with equal distance to different mismatched samples. Meanwhile, to ensure that the distance of mismatched pairs are larger than the matched pairs, margin constraints are necessary. Minimizing the isosceles and margin constraints with respect to the feature extraction network makes the matched pairs closer and the mismatched pairs farther away than the matched ones. By this way, crucial samples are effectively reduced and the performance on ReID is improved greatly. Likewise, our isosceles contraint can be applied to quadruplet as well. Comprehensive experimental evaluations on Market-1501, DukeMTMC-reID and CUHK03 datasets demonstrate the advantages of our isosceles constraint over the related state-of-the-art approaches.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3020648,
03f8b0e39b579ab4f2d15f3c64329bacd8280ecc,0,1,1,Temporal Complementary Learning for Video Person Re-Identification,"This paper proposes a Temporal Complementary Learning Network that extracts complementary features of consecutive video frames for video person re-identification. Firstly, we introduce a Temporal Saliency Erasing (TSE) module including a saliency erasing operation and a series of ordered learners. Specifically, for a specific frame of a video, the saliency erasing operation drives the specific learner to mine new and complementary parts by erasing the parts activated by previous frames. Such that the diverse visual features can be discovered for consecutive frames and finally form an integral characteristic of the target identity. Furthermore, a Temporal Saliency Boosting (TSB) module is designed to propagate the salient information among video frames to enhance the salient feature. It is complementary to TSE by effectively alleviating the information loss caused by the erasing operation of TSE. Extensive experiments show our method performs favorably against state-of-the-arts. The source code is available at this https URL.",2020,ECCV,2007.09357,10.1007/978-3-030-58595-2_24,https://arxiv.org/pdf/2007.09357.pdf
040c0612e0f006fa93f140ccb97b9738efcf74a5,1,1,1,One Shot Domain Adaptation for Person Re-Identification,"How to effectively address the domain adaptation problem is a challenging task for person re-identification (reID). In this work, we make the first endeavour to tackle this issue according to one shot learning. Given an annotated source training set and a target training set that only one instance for each category is annotated, we aim to achieve competitive re-ID performance on the testing set of the target domain. To this end, we introduce a similarity-guided strategy to progressively assign pseudo labels to unlabeled instances with different confidence scores, which are in turn leveraged as weights to guide the optimization as training goes on. Collaborating with a simple self-mining operation, we make significant improvement in the domain adaptation tasks of re-ID. In particular, we achieve the mAP of 71.5% in the adaptation task of DukeMTMC-reID to Market1501 with one shot setting, which outperforms the state-of-arts of unsupervised domain adaptation more than 17.8%. Under the five shots setting, we achieve competitive accuracy of the fully supervised setting on Market-1501. Code will be made available.",2018,,1811.10144,,https://arxiv.org/pdf/1811.10144.pdf
041b96260d58b4604d6509e9e70d69e205062136,1,0,0,Progressive Multi-stage Feature Mix for Person Re-Identification.,"Image features from a small local region often give strong evidence in person re-identification task. However, CNN suffers from paying too much attention on the most salient local areas, thus ignoring other discriminative clues, e.g., hair, shoes or logos on clothes. %BDB proposes to randomly drop one block in a batch to enlarge the high response areas. Although BDB has achieved remarkable results, there still room for improvement. In this work, we propose a Progressive Multi-stage feature Mix network (PMM), which enables the model to find out the more precise and diverse features in a progressive manner. Specifically, 1. to enforce the model to look for different clues in the image, we adopt a multi-stage classifier and expect that the model is able to focus on a complementary region in each stage. 2. we propose an Attentive feature Hard-Mix (A-Hard-Mix) to replace the salient feature blocks by the negative example in the current batch, whose label is different from the current sample. 3. extensive experiments have been carried out on reID datasets such as the Market-1501, DukeMTMC-reID and CUHK03, showing that the proposed method can boost the re-identification performance significantly.",2020,,2007.08779,,https://arxiv.org/pdf/2007.08779.pdf
04d1dd61f10d74fdda3410afceb7eef07a67dcf0,1,1,0,PH-GCN: Person Re-identification with Part-based Hierarchical Graph Convolutional Network,"The person re-identification (Re-ID) task requires to robustly extract feature representations for person images. Recently, part-based representation models have been widely studied for extracting the more compact and robust feature representations for person images to improve person Re-ID results. However, existing part-based representation models mostly extract the features of different parts independently which ignore the relationship information between different parts. To overcome this limitation, in this paper we propose a novel deep learning framework, named Part-based Hierarchical Graph Convolutional Network (PH-GCN) for person Re-ID problem. Given a person image, PH-GCN first constructs a hierarchical graph to represent the pairwise relationships among different parts. Then, both local and global feature learning are performed by the messages passing in PH-GCN, which takes other nodes information into account for part feature representation. Finally, a perceptron layer is adopted for the final person part label prediction and re-identification. The proposed framework provides a general solution that integrates local, global and structural feature learning simultaneously in a unified end-to-end network. Extensive experiments on several benchmark datasets demonstrate the effectiveness of the proposed PH-GCN based Re-ID approach.",2019,ArXiv,1907.08822,,https://arxiv.org/pdf/1907.08822.pdf
05301ac3ff2d1c0af7bcf3d2f1abcae044a75948,1,0,0,An Event-Driven Multiple Objects Surveillance System 35,"Traditional surveillance systems are constrained because of a fixed and preset pattern of monitoring. It can reduce the reliability of the system and cause an increased generation of false alarms. It results in an increased processing activity of the system, which causes an augmented consumption of system resources and power. Within this framework, a human surveillance system is proposed based on the event-driven awakening and self-organization principle. The proposed system overcomes these downsides up to a certain level. It is achieved by intelligently merging an assembly of sensors with two cameras, actuators, a lighting module and cost-effective embedded processors. With the exception of low-power event detectors, all other system modules remain in the sleep mode. These modules are activated only upon detection of an event and as a function of the sensing environment condition. It reduces power consumption and processing activity of the proposed system. An effective combination of a sensor assembly and a robust classifier suppresses generation of false alarms and improves system reliability. An experimental setup is realized in order to verify the functionality of the proposed system. Results confirm proper functionality of the implemented system. A 62.3-fold system memory utilization and bandwidth consumption reduction compared to traditional counterparts is achieved, i.e. a result of the proposed system self-organization and event-driven awakening features. It confirms that the proposed system outperforms its classical counterparts in terms of processing activity, power consumption and usage of resources.",2018,,,,https://pdfs.semanticscholar.org/0530/1ac3ff2d1c0af7bcf3d2f1abcae044a75948.pdf
0545c16ac10981cf2d36e96f145815b8d81344a1,0,1,0,Crossing Generative Adversarial Networks for Cross-View Person Re-identification,"Person re-identification (\textit{re-id}) refers to matching pedestrians across disjoint yet non-overlapping camera views. The most effective way to match these pedestrians undertaking significant visual variations is to seek reliably invariant features that can describe the person of interest faithfully. Most of existing methods are presented in a supervised manner to produce discriminative features by relying on labeled paired images in correspondence. However, annotating pair-wise images is prohibitively expensive in labors, and thus not practical in large-scale networked cameras. Moreover, seeking comparable representations across camera views demands a flexible model to address the complex distributions of images. In this work, we study the co-occurrence statistic patterns between pairs of images, and propose to crossing Generative Adversarial Network (Cross-GAN) for learning a joint distribution for cross-image representations in a unsupervised manner. Given a pair of person images, the proposed model consists of the variational auto-encoder to encode the pair into respective latent variables, a proposed cross-view alignment to reduce the view disparity, and an adversarial layer to seek the joint distribution of latent representations. The learned latent representations are well-aligned to reflect the co-occurrence patterns of paired images. We empirically evaluate the proposed model against challenging datasets, and our results show the importance of joint invariant features in improving matching rates of person re-id with comparison to semi/unsupervised state-of-the-arts.",2019,Neurocomputing,1801.0176,10.1016/j.neucom.2019.01.093,https://arxiv.org/pdf/1801.01760.pdf
056adbf0b00ff13dae94c0a5b02bb7cb180794c0,1,0,0,Learning Non-Uniform Hypergraph for Multi-Object Tracking,"The majority of Multi-Object Tracking (MOT) algorithms based on the tracking-by-detection scheme do not use higher order dependencies among objects or tracklets, which makes them less effective in handling complex scenarios. In this work, we present a new near-online MOT algorithm based on non-uniform hypergraph, which can model different degrees of dependencies among tracklets in a unified objective. The nodes in the hypergraph correspond to the tracklets and the hyperedges with different degrees encode various kinds of dependencies among them. Specifically, instead of setting the weights of hyperedges with different degrees empirically, they are learned automatically using the structural support vector machine algorithm (SSVM). Several experiments are carried out on various challenging datasets (i.e., PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark), to demonstrate that our method achieves favorable performance against the state-of-the-art MOT methods.",2019,AAAI,1812.03621,10.1609/aaai.v33i01.33018981,https://arxiv.org/pdf/1812.03621.pdf
057dfb33380f8cadea486bbc8c26b8331eac2e2b,1,0,0,Learning a Neural Solver for Multiple Object Tracking,"Graphs offer a natural way to formulate Multiple Object Tracking (MOT) within the tracking-by-detection paradigm. However, they also introduce a major challenge for learning methods, as defining a model that can operate on such structured domain is not trivial. As a consequence, most learning-based work has been devoted to learning better features for MOT and then using these with well-established optimization frameworks. In this work, we exploit the classical network flow formulation of MOT to define a fully differentiable framework based on Message Passing Networks (MPNs). By operating directly on the graph domain, our method can reason globally over an entire set of detections and predict final solutions. Hence, we show that learning in MOT does not need to be restricted to feature extraction, but it can also be applied to the data association step. We show a significant improvement in both MOTA and IDF1 on three publicly available benchmarks. Our code is available at https://bit.ly/motsolv.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1912.07515,10.1109/cvpr42600.2020.00628,https://arxiv.org/pdf/1912.07515.pdf
0599dda12859459117589e738b78eb54ef64b21f,1,0,0,"Towards Real-Time Systems for Vehicle Re-Identification, Multi-Camera Tracking, and Anomaly Detection","Vehicle re-identification, multi-camera vehicle tracking, and anomaly detection are essential for city-scale intelligent transportation systems. Both vehicle re-id and multi-camera tracking are challenging due to variations in aspect-ratio, occlusion, and orientation. Robust re-id and tracking systems must consider small scale variations in a vehicle’s appearance to accurately distinguish among vehicles of the same make, model, and color. Scalability is critical for multi-camera systems, as the number of objects in a scene is not known a-priori. Anomaly detection presents a unique challenge due to a dearth of annotations and varied video quality. In this paper, we address the task of vehicle re-id by introducing an unsupervised excitation layer to enhance representation learning. We propose a multi-camera tracking pipeline leveraging this re-id feature extractor to compute a distance matrix and perform clustering to obtain multi-camera vehicle trajectories. Lastly, we leverage background modeling techniques to localize anomalies such as stalled vehicles and collisions. We show the effectiveness of our proposed method on the NVIDIA AI City Challenge, where we obtain 7th place out of 41 teams for the task of vehicle re-id, with an mAP score of 66.68% and achieve state-of-the-art results on the Vehicle-ID dataset. We also obtain an IDF1 score of 12.45% on multi-camera vehicle tracking, and an S4 score of 29.52% for task of anomaly detection, ranking in the top 5 for both tracks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00319,
05a1269f7d8b0e739edf9b84ca0cc3d1110f923e,1,0,0,SAFER-LC Report on risk evaluation system and use cases for pilot test,"No part of this document may be copied, reproduced, disclosed or distributed by any means whatsoever, including electronic without the express permission of the International Union of Railways (UIC), Coordinator of the EU SAFER-LC Project. The same applies for translation, adaptation or transformation, arrangement or reproduction by any method or procedure whatsoever. The document reflects only the author’s views and neither INEA nor the Commission is liable of any use that may be made of the information contained therein. The use of the content provided is at the sole risk of the user. Deliverable D3.4",2019,,,,https://pdfs.semanticscholar.org/05a1/269f7d8b0e739edf9b84ca0cc3d1110f923e.pdf
05db52162e6859084fa5f02428ef183b652b7f92,0,1,0,Batch Coherence-Driven Network for Part-aware Person Re-Identification,"Existing part-aware person re-identification methods typically employ two separate steps: namely, body part detection and part-level feature extraction. However, part detection introduces an additional computational cost and is inherently challenging for low-quality images. Accordingly, in this work, we propose a simple framework named Batch Coherence-Driven Network (BCD-Net) that bypasses body part detection during both the training and testing phases while still learning semantically aligned part features. Our key observation is that the statistics in a batch of images are stable, and therefore that batch-level constraints are robust. First, we introduce a batch coherence-guided channel attention (BCCA) module that highlights the relevant channels for each respective part from the output of a deep backbone model. We investigate channelpart correspondence using a batch of training images, then impose a novel batch-level supervision signal that helps BCCA to identify part-relevant channels. Second, the mean position of a body part is robust and consequently coherent between batches throughout the training process. Accordingly, we introduce a pair of regularization terms based on the semantic consistency between batches. The first term regularizes the high responses of BCD-Net for each part on one batch in order to constrain it within a predefined area, while the second encourages the aggregate of BCD-Nets responses for all parts covering the entire human body. The above constraints guide BCD-Net to learn diverse, complementary, and semantically aligned part-level features. Extensive experimental results demonstrate that BCDNet consistently achieves state-of-the-art performance on four large-scale ReID benchmarks.",2020,ArXiv,2009.09692,,https://arxiv.org/pdf/2009.09692.pdf
05e6751fbdabaab4b97af7904982fd8f68b3680e,0,1,0,Augmenting data with GANs to segment melanoma skin lesions,"This paper presents a novel strategy that employs Generative Adversarial Networks (GANs) to augment data in the skin lesion segmentation task, which is a fundamental first step in the automated melanoma detection process. The proposed framework generates both skin lesion images and their segmentation masks, making the data augmentation process extremely straightforward. In order to thoroughly analyze how the quality and diversity of synthetic images impact the efficiency of the method, we remodel two different well known GANs: a Deep Convolutional GAN (DCGAN) and a Laplacian GAN (LAPGAN). Experimental results reveal that, by introducing such kind of synthetic data into the training process, the overall accuracy of a state-of-the-art Convolutional/Deconvolutional Neural Network for melanoma skin lesion segmentation is increased.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-7717-y,https://iris.unimore.it/bitstream/11380/1176919/1/2018_MTAP_REVISION_Augmenting_data_with_GANs_to_segment_melanoma_skin_lesions.pdf
061f205009b39438e39448164064233162884252,1,1,0,Mixed High-Order Attention Network for Person Re-Identification,"Attention has become more attractive in person re-identification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at http://www.bhchen.cn.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.05819,10.1109/ICCV.2019.00046,https://arxiv.org/pdf/1908.05819.pdf
06219e285b8fd8f5a1c493693ed0c6dca7869815,1,1,0,Single-Label Multi-Class Image Classification by Deep Logistic Regression,"The objective learning formulation is essential for the success of convolutional neural networks. In this work, we analyse thoroughly the standard learning objective functions for multi-class classification CNNs: softmax regression (SR) for single-label scenario and logistic regression (LR) for multi-label scenario. Our analyses lead to an inspiration of exploiting LR for single-label classification learning, and then the disclosing of the negative class distraction problem in LR. To address this problem, we develop two novel LR based objective functions that not only generalise the conventional LR but importantly turn out to be competitive alternatives to SR in single label classification. Extensive comparative evaluations demonstrate the model learning advantages of the proposed LR functions over the commonly adopted SR in single-label coarse-grained object categorisation and cross-class fine-grained person instance identification tasks. We also show the performance superiority of our method on clothing attribute classification in comparison to the vanilla LR function.",2019,AAAI,1811.084,10.1609/aaai.v33i01.33013486,https://arxiv.org/pdf/1811.08400.pdf
064043d3a28175b7e6697b49bae255e4a45ed797,1,1,0,Re-ranking with ranking-reflected similarity for person re-identification,"Abstract A recent concern for person re-identification (Re-ID) is re-ranking after initial results to improve Re-ID accuracy. In this paper, we propose a novel re-ranking method using a ranking-reflected metric to measure the similarity between the ordered set of K-nearest neighbors (OKNN) of a probe and that of a gallery. The proposed metric for ranking-reflected similarity (RSS) reflects the ranking of the shared elements between the two OKNNs. Using RSS, a re-ranking procedure is proposed that prioritizes galleries having neighbors similar to a probe’s neighbor in the perspective of ranking order. In the experiment, we show that the proposed method improves the Re-ID accuracy by add-on to the state-of-the-art methods.",2019,Pattern Recognit. Lett.,,10.1016/j.patrec.2019.09.020,
066de5ba4e66942ecffa1a0753458ea2f6ff560e,0,1,0,Attentional Part-based Network for Person Re-identification,"Part-based network is an effective method to improve performance in person re-identification (re-ID). Most existing methods assume the availability of well-aligned person bounding box images as model input. However, automatic detection in some datasets causes misalignment which negatively affects the performance. In this work, we propose an Attentional Part-based CNN (AP-CNN) model which combines learning partial features and attention selection. First, we partition feature map into several horizontal stripes. Second, we use attention selection in each stripe to align the pedestrian images. Inside, we introduce a free-parameter attention model with skip-layer connection which maximizes the complementary information of different levels without increasing the complexity of network. Results on four datasets validate the competitiveness of AP-CNN over the state-of-the-art achieving Rank-1 accuracy of 94.4% on Market-1501, 87.3% on DukeMTMC-ReID, 73.7% on CUHK03-labeled and 72.6% on CUHK03-detected.",2019,2019 IEEE Visual Communications and Image Processing (VCIP),,10.1109/VCIP47243.2019.8965993,
07335131c3f8b032ad2ddca317ccbac0997ec6f7,0,0,1,Ordered or Orderless: A Revisit for Video based Person Re-Identification,"Is recurrent network really necessary for learning a good visual representation for video based person re-identification (VPRe-id)? In this paper, we first show that the common practice of employing recurrent neural networks (RNNs) to aggregate temporal- spatial features may not be optimal. Specifically, with a diagnostic analysis, we show that the recurrent structure may not be effective learn temporal dependencies as we expected and implicitly yields an orderless representation. Based on this observation, we then present a simple yet surprisingly powerful approach for VPRe-id, where we treat VPRe-id as an efficient orderless ensemble of image based person re-identification problem. More specifically, we divide videos into individual images and re-identify person with ensemble of image based rankers. Under the i.i.d. assumption, we provide an error bound that sheds light upon how could we improve VPRe-id. Our work also presents a promising way to bridge the gap between video and image based person re-identification. Comprehensive experimental evaluations demonstrate that the proposed solution achieves state-of-the-art performances on multiple widely used datasets (iLIDS-VID, PRID 2011, and MARS).",2020,IEEE transactions on pattern analysis and machine intelligence,1912.11236,10.1109/tpami.2020.2976969,https://arxiv.org/pdf/1912.11236.pdf
07a4feb747b0c874c79ab3d37b3d6e84cbabce6d,1,0,0,Online Multi-Target Tracking with Tensor-Based High-Order Graph Matching,"In this paper we formulate multi-target tracking (MTT) as a high-order graph matching problem and propose a $\ell_{1}$-norm tensor power iteration solution. Concretely, the search for trajectory-observation correspondences in MTT task is cast as a hypergraph matching problem to maximize a multi-linear objective function over all permutations of the associations. This function is defined by a tensor representing the affinity between association tuples where pair-wise similarities, motion consistency and spatial structural information can be embedded expediently. To solve the matching problem, a dual-direction unit $\ell_{1}$-norm constrained tensor power iteration algorithm is proposed. Additionally, as measuring the appearance affinity with features extracted from the rectangle patch, which is adopted in most methods, has a weak discrimination when bounding boxes overlap each other heavily, we present a deep pair-wise appearance similarity metric based on object mask in this paper where just the features from true target region are utilized. Experimental evaluation shows that our approach achieves an accuracy comparable to state-of-the-art online trackers. The source code of the proposed approach will be released to facilitate further studies on the MTT problem.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545450,
07ce316d3ea10ae7e0487e0f77c79eeb5b56ed73,1,0,0,A Bayesian Filter for Multi-view 3D Multi-object Tracking with Occlusion Handling.,"This paper proposes an online multi-camera multi-object tracker that only requires monocular detector training, independent of the multi-camera configurations, allowing seamless extension/deletion of cameras without retraining effort. The proposed algorithm has a linear complexity in the total number of detections across the cameras, and hence scales gracefully with the number of cameras. It operates in the 3D world frame, and provides 3D trajectory estimates of the objects. The key innovation is a high fidelity yet tractable 3D occlusion model, amenable to optimal Bayesian multi-view multi-object filtering, which seamlessly integrates, into a single Bayesian recursion, the sub-tasks of track management, state estimation, clutter rejection, and occlusion/misdetection handling. The proposed algorithm is evaluated on the latest WILDTRACKS dataset, and demonstrated to work in very crowded scenes on a new dataset.",2020,IEEE transactions on pattern analysis and machine intelligence,2001.04118,10.1109/TPAMI.2020.3034435,https://ieeexplore.ieee.org/ielx7/34/4359286/09242263.pdf
07cf931577c44bd63de6f087cfe1dfdb62a58071,0,1,0,An Overview of Image-Based Person Re-identification,"Person re-identification (person re-ID) is refers to pedestrians matching under a multi camera network in a non-overlapping field of view, that is, whether the pedestrian targets taken by different cameras are the same. In this paper, the image based traditional person re-ID is introduced from feature description and metric learning. In addition, with the extensive application of the deep learning algorithm in recent years, it also brings about the change of person re-ID algorithm. From the three aspects of loss function design, local feature and data augmentation, this paper introduces some work of person re-ID algorithm based on deep learning. Finally, it summarizes the development of pedestrian datasets in recent years, and looks forward to the future development trend of person re-ID.",2018,,,10.1007/978-3-319-98776-7_39,
07f4ba45b771ed123b08261d88acda19406a7987,1,0,0,Real-Time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification,"Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on large-scale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.",2018,2018 IEEE International Conference on Multimedia and Expo (ICME),1809.04427,10.1109/ICME.2018.8486597,https://arxiv.org/pdf/1809.04427.pdf
0827008c7e1a5f549647a658a3f1fbc5cb3743f0,0,1,0,Person Re-identification on Mobile Devices Based on Deep Learning,"Person Re-identification as an important supplement to face recognition, refers to a network of cameras with non-overlapping vision domains, through the use of computer vision technology to solve the cross-camera and cross-scene pedestrian recognition and retrieval. That is, determine whether there is a specific pedestrian to be detected in the different images or different video sequences. At present, many person re-identification research works mainly carried out through experimental verification and evaluation on large ReID datasets such as Market-1501, DukeMTMC-reID, MSMT17, and CUHK03. In this paper, based on the existing deep-learning person re-identification research, combines with the actual application scenarios, under the premise of analyzing the technical feasibility, we propose a complete process for Person Re-identification based on mobile devices, aims to combine pedestrian detection and person re-identification to perform real-time pedestrian detection and query. In this process, not only the features extracted by pedestrians can be reused, but also the research on person re-identification can be better and effectively applied, such as tracking criminals and searching for missing children.",2020,,,10.12792/ICIAE2020.044,https://pdfs.semanticscholar.org/3a99/8822dd385d0ee163bf91617ad82512e170f5.pdf
083426edfe47f6d5de84da6637e5c72f53022e04,1,1,0,Person Re-identification with Neural Architecture Search,"Most of the existing person re-identification (ReID) methods use a classification network pre-trained on external data as the backbone and then fine-tune it, which results in a network architecture that is fixed and dependent on pre-training of external data. There are also some methods that are specifically designed by human experts for ReID, but manual network design becomes more difficult as network requirements increase and often fails to achieve optimal settings. In this paper, we consider using emerging neural architecture search (NAS) technology as a tool to solve above problems. However, most of NAS methods deal with classification tasks, which causes NAS to not be directly extended to ReID. In order to coordinate the inconsistency between the two optimization goals, we propose to establish an objective function with the assistant of the triplet loss to guide the direction of architecture search. Finally, it is no longer dependent on external data to automatically generate a ReID network with excellent performance using NAS directly on the target dataset. The experimental results on three public datasets validate that our method can automatically and efficiently find the network architecture suitable for ReID.",2019,PRCV,,10.1007/978-3-030-31654-9_46,
08650c1ce3a3012a2ca03378373d6256f36da941,1,0,0,Learning Camera-Invariant Representation for Person Re-identification,"Person re-identification (re-ID) problem aims to retrieve a person from an image gallery captured across multiple cameras. However, images of the same identity have variations due to the change in camera views. So learning a camera-invariant representation is one objective of re-identification. In this paper, we propose a camera-style transfer model for generating images, and a fake triplet loss for training the person feature embedding model. We train a StarGAN, a kind of generative adversarial networks, as our transfer model, which can transfer the style of an image from one camera to multiple different camera-styles by a generator network. So the image set is expanded with style-transferred images. However, style transferring yields image distortion, which misleads the training of feature embedding model. To overcome the influence of image distortion, we consider the gap between fake and real images, then we propose a fake triplet loss to capture the camera-invariant information of fake images. We do a series of experiments on the Market-1501, DukeMTMC-reID, and CUHK03 datasets, and show the effectiveness of our methods.",2019,ICANN,,10.1007/978-3-030-30484-3_11,https://e-nns.org/icann2019/online_posters/175.pdf
087f44d1dbcad7715ae862d139996ef28fea1634,0,1,0,A Network Framework for Small-Sample Learning,"Small-sample learning involves training a neural network on a small-sample data set. An expansion of the training set is a common way to improve the performance of neural networks in small-sample learning tasks. However, improper constraints in expanding training data will reduce the performance of the neural networks. In this article, we present certain conditions for incorporation of additional training data. According to these conditions, we propose a neural network framework for self-training using self-generated data called small-sample learning network (SSLN). The SSLN consists of two parts: the expression learning network and the sample recall generative network, both of which are constructed based on restricted Boltzmann machine (RBM). We show that this SSLN can converge as well as the RBM. Moreover, the experiment results on MNIST Digit, SVHN, CIFAR10, and STL-10 data sets reveal the superiority of the SSLN over other models.",2020,IEEE Transactions on Neural Networks and Learning Systems,,10.1109/TNNLS.2019.2951803,
088e7b24bd1cf6e5922ae6c80d37439e05fadce9,1,1,0,Let Features Decide for Themselves: Feature Mask Network for Person Re-identification,"Person re-identification aims at establishing the identity of a pedestrian from a gallery that contains images of multiple people obtained from a multi-camera system. Many challenges such as occlusions, drastic lighting and pose variations across the camera views, indiscriminate visual appearances, cluttered backgrounds, imperfect detections, motion blur, and noise make this task highly challenging. While most approaches focus on learning features and metrics to derive better representations, we hypothesize that both local and global contextual cues are crucial for an accurate identity matching. To this end, we propose a Feature Mask Network (FMN) that takes advantage of ResNet high-level features to predict a feature map mask and then imposes it on the low-level features to dynamically reweight different object parts for a locally aware feature representation. This serves as an effective attention mechanism by allowing the network to focus on local details selectively. Given the resemblance of person re-identification with classification and retrieval tasks, we frame the network training as a multi-task objective optimization, which further improves the learned feature descriptions. We conduct experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach respectively achieves significant improvements of 5.3%, 9.1% and 10.7% in mAP measure relative to the state-of-the-art.",2017,ArXiv,1711.07155,,https://arxiv.org/pdf/1711.07155.pdf
08b28a8f2699501d46d87956cbaa37255000daa3,1,1,0,MaskReID: A Mask Based Deep Ranking Neural Network for Person Re-identification,"Person retrieval faces many challenges including cluttered background, appearance variations (e.g., illumination, pose, occlusion) among different camera views and the similarity among different person's images. To address these issues, we put forward a novel mask based deep ranking neural network with a skipped fusing layer. Firstly, to alleviate the problem of cluttered background, masked images with only the foreground regions are incorporated as input in the proposed neural network. Secondly, to reduce the impact of the appearance variations, the multi-layer fusion scheme is developed to obtain more discriminative fine-grained information. Lastly, considering person retrieval is a special image retrieval task, we propose a novel ranking loss to optimize the whole network. The proposed ranking loss can further mitigate the interference problem of similar negative samples when producing ranking results. The extensive experiments validate the superiority of the proposed method compared with the state-of-the-art methods on many benchmark datasets.",2018,ArXiv,1804.03864,,https://arxiv.org/pdf/1804.03864.pdf
08d2a558ea2deb117dd8066e864612bf2899905b,1,1,0,Person Re-identification with Deep Similarity-Guided Graph Neural Network,"The person re-identification task requires to robustly estimate visual similarities between person images. However, existing person re-identification models mostly estimate the similarities of different image pairs of probe and gallery images independently while ignores the relationship information between different probe-gallery pairs. As a result, the similarity estimation of some hard samples might not be accurate. In this paper, we propose a novel deep learning framework, named Similarity-Guided Graph Neural Network (SGGNN) to overcome such limitations. Given a probe image and several gallery images, SGGNN creates a graph to represent the pairwise relationships between probe-gallery pairs (nodes) and utilizes such relationships to update the probe-gallery relation features in an end-to-end manner. Accurate similarity estimation can be achieved by using such updated probe-gallery relation features for prediction. The input features for nodes on the graph are the relation features of different probe-gallery image pairs. The probe-gallery relation feature updating is then performed by the messages passing in SGGNN, which takes other nodes’ information into account for similarity estimation. Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information. The effectiveness of our proposed method is validated on three public person re-identification datasets.",2018,ECCV,1807.09975,10.1007/978-3-030-01267-0_30,https://arxiv.org/pdf/1807.09975.pdf
08fef8498ceabb241092ba95606eb3f7a349fa1c,0,1,0,Multimodal GAN for Energy Efficiency and Cloud Classification in Internet of Things,"Efficient processing of large-scale multimodal sensor data is a key issue for applying the Internet of Things (IoT). Accurate cloud classification is critical for weather and climate monitoring, which are parts of IoT applications. In this paper, we propose a novel generative deep model named multimodal generative adversarial network (Multimodal GAN) to improve both the energy efficiency and the cloud classification accuracy in IoT. The proposed Multimodal GAN is composed of a discriminator and a generator, each of which is devised to a two-stream network. The branches of two-stream structure correspond to the cloud visual information and the cloud scalar information, respectively. Therefore, the Multimodal GAN is capable of generating the cloud visual information and cloud scalar information simultaneously. Afterward, the training set is extended by the generated multimodal cloud samples, and the deep multimodal cloud classification model is trained by the extended training set. As a result, the classification model possesses high generalization ability and is less prone to be over-fitting. Moreover, the feature representations extracted from the classification model reflect the salient information of raw multimodal cloud data, and therefore they can be stored and transmitted in IoT. The effectiveness of the proposed method in energy efficiency and cloud classification is validated on the multimodal cloud dataset.",2019,IEEE Internet of Things Journal,,10.1109/JIOT.2018.2866328,
09577ab4dd5497dc6de1c38e882f508cf2b7e511,0,1,0,LRDNN: Local-refining based Deep Neural Network for Person Re-Identification with Attribute Discerning,"Recently, pose or attribute information has been widely used to solve person re-identification (reID) problem. However, the inaccurate output from pose or attribute modules will impair the final person re-ID performance. Since re-ID, pose estimation and attribute recognition are all based on the person appearance information, we propose a Local-refining based Deep Neural Network (LRDNN) to aggregate pose estimation and attribute recognition to improve the re-ID performance. To this end, we add a pose branch to extract the local spatial information and optimize the whole network on both person identity and attribute objectives. To diminish the negative affect from unstable pose estimation, a novel structure called channel parse block (CPB) is introduced to learn weights on different feature channels in pose branch. Then two branches are combined with compact bilinear pooling. Experimental results on Market1501 and DukeMTMC-reid datasets illustrate the effectiveness of the proposed method.",2019,IJCAI,,10.24963/ijcai.2019/146,https://pdfs.semanticscholar.org/e6e0/7adffb8a021ce9beb0525540afd7c7e8612b.pdf
096f3c94431fed2d285dfda67bc572c8f3a2f9ab,0,1,0,GAN-based person search via deep complementary classifier with center-constrained Triplet loss,"Abstract This paper addresses the person search task, which is a computer vision technology that finds the location of a pedestrian and retrieves it in a video taken by a single camera or multiple cameras. This task is much more challenging than the conventional settings for person re-identification or pedestrian detection since the search is susceptible to factors such as different resolutions, similar pedestrians, lighting, viewing angles and occlusion. Moreover, the person search task is a typical big data-small sample problem because each pedestrian only has a few images. It is difficult for the model to learn the discriminant features of pedestrians with a small quantity of pedestrian data. This paper proposes a framework for person search that uses the original training set without collecting extra data by implementing a generative adversarial network (GAN) to generate unlabeled samples. We propose a deep complementary classifier for pedestrian detection to leverage complementary object regions for pedestrian/non-pedestrian classification. In the re-identification section, we propose a center-constrained triplet loss that avoids the complicated triplet selection of the triplet loss and simultaneously pushes away all the distances of rather similar negative centers and the positive center. Experiments show that the GAN-generated data can effectively help to improve the discriminating ability of the CNN model. On the two large-scale datasets, CUHK-SYSU and PRW, we achieve a performance improvement over the baseline CNN. We additionally apply the proposed center-constrained triplet loss and complementary classifiers in the training model, and we achieve mAP improvements over the original method of +1.9% on CUHK-SYSU and +2.5% on PRW.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107350,
09c218a27a03c6ed6bb6fe7871648260b5d58902,1,0,0,Deep Visual Re-Identification with Confidence,"Transportation systems often rely on understanding the flow of vehicles or pedestrian. From traffic monitoring at the city scale, to commuters in train terminals, recent progress in sensing technology make it possible to use cameras to better understand the demand, i.e., better track moving agents (e.g., vehicles and pedestrians). Whether the cameras are mounted on drones, vehicles, or fixed in the built environments, they inevitably remain scatter. We need to develop the technology to re-identify the same agents across images captured from non-overlapping field-of-views, referred to as the visual re-identification task. State-of-the-art methods learn a neural network based representation trained with the cross-entropy loss function. We argue that such loss function is not suited for the visual re-identification task hence propose to model confidence in the representation learning framework. We show the impact of our confidence-based learning framework with three methods: label smoothing, confidence penalty, and deep variational information bottleneck. They all show a boost in performance validating our claim. Our contribution is generic to any agent of interest, i.e., vehicles or pedestrians, and outperform highly specialized state-of-the-art methods across 5 datasets. The source code and models are shared towards an open science mission.",2019,,1906.04692,,https://arxiv.org/pdf/1906.04692.pdf
09dd6be53387959af47d55db139b188aade67022,0,1,0,Effective multi-shot person re-identification through representative frames selection and temporal feature pooling,"Multi-shot person re-identification (ReID) is a popular case of person ReID in which a set of images are processed for each person. However, using entire image set for person ReID as most experimented proposals is not always effective because of time and memory consuming. The main contribution of this work is the proposed strategies for (1) choosing representative image frames for each individual instead of entire set of frames, and (2) temporal feature pooling in multi-shot person ReID. These strategies are efficiently integrated in a person ReID framework which uses GoG (Gaussian of Gaussian) and XQDA (metric learning Cross-view Quadratic Discriminant Analysis) for person representation and matching. The effectiveness of the proposed framework on two benchmark datasets (PRID 2011 and iLIDS-VID) in terms of re-identification accuracy, computational time, and storage requirements are deeply investigated and analyzed. The experimental results allow to provide several recommendations on the use of these schemes based on the characteristics of the working dataset and the requirement of the applications. Furthermore, the study also offers a desktop-based application for person search and ReID. The implementation of the proposed framework will be made publicly available.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-08183-y,
09e8d359b8ea09c40cf1620c200bfa24b02ddac7,1,0,0,Detecting Small Objects Using a Channel-Aware Deconvolutional Network,"Detecting small objects is a challenging task due to their low resolution and noisy representation even using deep learning methods. In this paper, we propose a novel object detection method based on the channel-aware deconvolutional network (CADNet) for accurate small object detection. Specifically, we develop the channel-aware deconvolution (ChaDeConv) layer to exploit the correlations of feature maps in different channels across deeper layers, improving the recall rate of small objects at low additional computational costs. Following the ChaDeConv layer, the multiple region proposal sub-network (Multi-RPN) is employed to supervise and optimize multiple detection layers simultaneously to achieve better accuracy. The Multi-RPN module is only used in the training phase and does not increase the computation cost of the inference. In addition, we design a new anchor matching strategy based on the center point translation (CPTMatching) of anchors to select more extending anchors as positive samples in the training phase. The extensive experiments on the PASCAL VOC 2007/2012, MS COCO, and UAVDT datasets show that the proposed CADNet achieves state-of-the-art performance compared to the existing methods.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2906246,
0a2673c93813831b865c96aef74d07779b78ba7d,1,0,0,CycAs: Self-supervised Cycle Association for Learning Re-identifiable Descriptions,"This paper proposes a self-supervised learning method for the person re-identification (re-ID) problem, where existing unsupervised methods usually rely on pseudo labels, such as those from video tracklets or clustering. A potential drawback of using pseudo labels is that errors may accumulate and it is challenging to estimate the number of pseudo IDs. We introduce a different unsupervised method that allows us to learn pedestrian embeddings from raw videos, without resorting to pseudo labels. The goal is to construct a self-supervised pretext task that matches the person re-ID objective. Inspired by the \emph{data association} concept in multi-object tracking, we propose the \textbf{Cyc}le \textbf{As}sociation (\textbf{CycAs}) task: after performing data association between a pair of video frames forward and then backward, a pedestrian instance is supposed to be associated to itself. To fulfill this goal, the model must learn a meaningful representation that can well describe correspondences between instances in frame pairs. We adapt the discrete association process to a differentiable form, such that end-to-end training becomes feasible. Experiments are conducted in two aspects: We first compare our method with existing unsupervised re-ID methods on seven benchmarks and demonstrate CycAs' superiority. Then, to further validate the practical value of CycAs in real-world applications, we perform training on self-collected videos and report promising performance on standard test sets.",2020,ArXiv,2007.07577,,https://arxiv.org/pdf/2007.07577.pdf
0a269cfe1ea94d3a93463dba96a6f7abe69380fa,0,1,0,Learning part-alignment feature for person re-identification with spatial-temporal-based re-ranking method,"Person re-identification is to identify a target person in different cameras with non-overlapping views. It is a challenging task due to various viewpoints of persons, diversified illuminations, and complicated environments. In addition, body parts are usually misaligned because of the less precise bounding boxes, which play a significant role in person re-identification, so it is crucial to make them aligned for better performance. In this paper, we propose a network to learn powerful features combining global features and local-alignment features for person re-identification. For each body part, instead of fixed horizontal partition, a key points detection network is adopted to locate body parts that contain more precise and distinctive information. Besides, a novel re-ranking approach is proposed to refine the rough initial rank list by exploiting the spatial-temporal information. Unlike most existing re-ranking based methods fine-tuning the rough initial rank list only by k-nearest neighbors and their k-reverse-nearest neighbors, our method exploits spatial-temporal information which can be easily stored in the name of images, so it can be implemented in any baseline to improve the performance. Experiments on the GRID, Market-1501, and DukeMTMC-reID are conducted to prove the effectiveness of our method.",2019,World Wide Web,,10.1007/s11280-019-00734-5,
0a808a17f5c86413bd552a324ee6ba180a12f46d,0,1,0,Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association,"Person re-identification is an important task that requires learning discriminative visual features for distinguishing different person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for effective visual features. Compared with other auxiliary information, language can describe a specific person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the effectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.",2018,ECCV,1808.01571,10.1007/978-3-030-01270-0_4,https://arxiv.org/pdf/1808.01571.pdf
0a949f20592bdf83cc030c0f38c9e665fbba948c,1,0,0,A Novel Pedestrian Reidentification Method Based on a Multiview Generative Adversarial Network,"Emerging deep learning (DL) techniques have greatly improved pedestrian reidentification (PRI) performance. However, the existing DL-based PRI methods cannot learn robust feature representations owing to the single view of query images and the limited number of extractable features. Inspired by generative adversarial networks (GANs), this paper proposes a novel PRI method based on a pedestrian multiview GAN (PmGAN) and a classification recognition network (CRN). The PmGAN consists of three generators and one multiclass discriminator. The three generators produce pedestrian images from the front, side and back, while the multiclass discriminator determines whether the input image is a real image or a generated image. In addition to expanding the existing pedestrian datasets, the PmGAN can generate pedestrian images from front, side and back views based on a given query image and thereby increase the feature semantic space of the query image. To verify the performance of our method, the PmGAN was compared with mainstream pedestrian image generation models, and then the proposed method was contrasted with mainstream PRI methods. The results show that the proposed PmGAN greatly improved the performance of mainstream PRI methods. For example, the combination of the PmGAN and Pyramidal increased the mean average precision (mAP) on three common datasets by 1.2% on average. The research findings provide new insights into the application of multiview generation in PRI tasks.",2020,IEEE Access,,10.1109/ACCESS.2020.3029180,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09214812.pdf
0aa8acd766c6084b6128f9769b59632c42564df2,1,0,0,Dense Scene Multiple Object Tracking with Box-Plane Matching,"Multiple Object Tracking (MOT) is an important task in computer vision. MOT is still challenging due to the occlusion problem, especially in dense scenes. Following the tracking-by-detection framework, we propose the Box-Plane Matching (BPM) method to improve the MOT performacne in dense scenes. First, we design the Layer-wise Aggregation Discriminative Model (LADM) to filter the noisy detections. Then, to associate remaining detections correctly, we introduce the Global Attention Feature Model (GAFM) to extract appearance feature and use it to calculate the appearance similarity between history tracklets and current detections. Finally, we propose the Box-Plane Matching strategy to achieve data association according to the motion similarity and appearance similarity between tracklets and detections. With the effectiveness of the three modules, our team achieves the 1st place on the Track-1 leaderboard in the ACM MM Grand Challenge HiEve 2020.",2020,ACM Multimedia,2007.15576,10.1145/3394171.3416283,https://arxiv.org/pdf/2007.15576.pdf
0b0688205678bdc3a228adf62f9fc86dafaf5794,0,0,1,Urgent image-to-video person reidentification by cross-media transfer cycle generative adversarial networks,"Abstract. Recently, image-to-video person reidentification (IVPR) has attracted enormous research interest, and various models are proposed. IVPR is often applied to urgent situations, such as suspect tracking and lost-human locating. Existing IVPR models are under supervised frameworks, which require a large number of labeled image-to-video pairs. This severely limits their real-time efficiency in urgent situations, because annotation is much more time-consuming. To solve the urgent image-to-video person reidentification (UIVPR) problem, we propose a cross-media transfer cycle generative adversarial networks (CTC-GAN) network. Our model aims to alleviate the “media-gap” between image-to-video pairs without newly labeled pairs. We make an existing completely labeled dataset as guidance for CTC-GAN to achieve domain adaptation and make urgent image-to-video matching easier for person reidentification. We introduce cycle GANs for image(video)-to-video(image) translation and extract cross-media features using a triplet constraint in the source domain for different media features. Furthermore, we train the model in the labeled source domain by reconstructing the image (video) as its related video (image). Then, train the model in the unlabeled target domain by reconstructing itself along with source data, so as to ensure that the discriminative model can be used in target domain. Through CTC-GAN, our network can retain pedestrian discriminative information as much as possible, to ensure the matching rate in the target domain. To validate the effectiveness of our approach, we implement substantial experiments on two large-scale person reidentification datasets compared with six existing state-of-the-art unsupervised revised person reidentification models, and experimental results demonstrate that our method can solve UIVPR effectively.",2019,J. Electronic Imaging,,10.1117/1.JEI.28.1.013052,
0b198c9aececb8f4172ace5c25d468141c5df6bf,0,1,0,Person Re-Identification with Hybrid Loss and Hard Triplets Mining,"Person re-identification is the process of recognizing a person through a network of cameras. Recently, many models of person re-identification based on deep learning have been proposed. In these models, the choice of loss function is vital, since different loss function has different characteristics. Cross-entropy and triplet losses are two commonly used loss functions. Unfortunately, triplet loss cannot measure the overall spatial distribution of features, while the cross-entropy loss does not have enough discriminant between features. In this paper, we propose a new hybrid loss function to learn a better spatial distribution of features and distance between features. Furthermore, we design a strategy to mine hard triplets to accelerate the learning. Experimental results demonstrate that the proposed method is effective and improves the accuracy of person re-identification when compared with the state-of-the-art.",2018,2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM),,10.1109/BigMM.2018.8499463,http://static.tongtianta.site/paper_pdf/c457532c-594d-11e9-afeb-00163e08bb86.pdf
0b7336da882274fe69ae651b8a2604ea671c924e,1,1,0,GreyReID: A Two-stream Deep Framework with RGB-grey Information for Person Re-identification,"In this paper, we observe that most false positive images (i.e., different identities with query images) in the top ranking list usually have the similar color information with the query image in person re-identification (Re-ID). Meanwhile, when we use the greyscale images generated from RGB images to conduct the person Re-ID task, some hard query images can obtain better performance compared with using RGB images. Therefore, RGB and greyscale images seem to be complementary to each other for person Re-ID. In this paper, we aim to utilize both RGB and greyscale images to improve the person Re-ID performance. To this end, we propose a novel two-stream deep neural network with RGB-grey information, which can effectively fuse RGB and greyscale feature representations to enhance the generalization ability of Re-ID. Firstly, we convert RGB images to greyscale images in each training batch. Based on these RGB and greyscale images, we train the RGB and greyscale branches, respectively. Secondly, to build up connections between RGB and greyscale branches, we merge the RGB and greyscale branches into a new joint branch. Finally, we concatenate the features of all three branches as the final feature representation for Re-ID. Moreover, in the training process, we adopt the joint learning scheme to simultaneously train each branch by the independent loss function, which can enhance the generalization ability of each branch. Besides, a global loss function is utilized to further fine-tune the final concatenated feature. The extensive experiments on multiple benchmark datasets fully show that the proposed method can outperform the state-of-the-art person Re-ID methods. Furthermore, using greyscale images can indeed improve the person Re-ID performance.",2019,ArXiv,1908.05142,,https://arxiv.org/pdf/1908.05142.pdf
0b98a1efa7bef2acb2091d5b1659430ef4df1364,0,1,0,A Data Augmentation Method Based on Generative Adversarial Networks for Grape Leaf Disease Identification,"The identification of grape leaf diseases based on deep learning is critical to controlling the spread of diseases and ensuring the healthy development of the grape industry. Focusing on the lack of training images of grape leaf diseases, this paper proposes a novel model named Leaf GAN, which is based on generative adversarial networks (GANs), to generate images of four different grape leaf diseases for training identification models. A generator model with degressive channels is first designed to generate grape leaf disease images; then, the dense connectivity strategy and instance normalization are fused into an efficient discriminator to identify real and fake disease images by utilizing their excellent feature extraction capability on grape leaf lesions. Finally, the deep regret gradient penalty method is applied to stabilize the training process of the model. Using a total of 4,062 grape leaf disease images, the Leaf GAN model ultimately generates 8,124 grape leaf disease images. The generated grape leaf disease images based on Leaf GAN model can obtain better performance than DCGAN and WGAN in terms of the Fréchet inception distance. The experimental results show that the proposed Leaf GAN model generates sufficient grape leaf disease images with prominent lesions, providing a feasible solution for the data augmentation of grape leaf disease images. For the eight prevailing classification models with the expanded dataset, the identification performance based on CNNs indicated higher accuracies, whereby all the accuracies were better than those of the initial dataset with other data augmentation methods. Among them, Xception achieves a recognition accuracy of 98.70% on the testing set. The results demonstrate that the proposed data augmentation method represents a new approach to overcoming the overfitting problem in disease identification and can effectively improve the identification accuracy.",2020,IEEE Access,,10.1109/ACCESS.2020.2998839,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09104723.pdf
0bf6ec6a25b15408adc8d22b5e735f044a7ce2e4,0,1,0,Supplementary Material: Style Normalization and Restitution for Generalizable Person Re-identification,"Network Details. We use ResNet-50 [9, 1, 43, 25] as our base network for both baselines and our schemes. We build a strong baseline Baseline with some commonly used tricks integrated. Similar to [1, 43, 25], the last spatial downsample operation in the last Conv block is removed. The proposed SNR module is added after the last layer of each convolutional block/stage of the first four stages. The input image resolution is 256×128.",2020,,,,http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Jin_Style_Normalization_and_CVPR_2020_supplemental.pdf
0bf90fd8239bbaed46df6c525160e999c7e6c5e7,1,0,0,Occlusion Handling in Tracking Multiple People Using RNN,"In tracking-by-detection of multiple targets in video sequences, ID-switch is an undesirable error due to long (short) occlusion among targets. In this paper, we propose an occlusion handling method based on Recurrent Neural Network (RNN) to remedy this issue. The method reconstructs missed detection boxes in order to preserve the ID number of targets after occlusion by predicting the detections in next frames. The prediction is accomplished by learning the motion of targets using a novel RNN. Applying this technique on tracking results of several state-of-the-arts shows that their ID-switch error is reduced.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451140,
0c0e26737fbc27d2dc7aab58783b155b009a06cf,1,1,0,Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification,"In this paper we introduce an ensemble method for convolutional neural network (CNN), called ""virtual branching,"" which can be implemented with nearly no additional parameters and computation on top of standard CNNs. We propose our method in the context of person re-identification (re-ID). Our CNN model consists of shared bottom layers, followed by ""virtual"" branches, where neurons from a block of regular convolutional and fully-connected layers are partitioned into multiple sets. Each virtual branch is trained with different data to specialize in different aspects, e.g., a specific body region or pose orientation. In this way, robust ensemble representations are obtained against human body misalignment, deformations, or variations in viewing angles, at nearly no any additional cost. The proposed method achieves competitive performance on multiple person re-ID benchmark datasets, including Market-1501, CUHK03, and DukeMTMC-reID.",2018,ArXiv,1803.05872,,https://arxiv.org/pdf/1803.05872.pdf
0c5d9f4bf8b11a92a4d42b796052aca20a2a43fd,1,1,0,Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-Identification,"This paper considers the domain adaptive person re-identification (re-ID) problem: learning a re-ID model from a labeled source domain and an unlabeled target domain. Conventional methods are mainly to reduce feature distribution gap between the source and target domains. However, these studies largely neglect the intra-domain variations in the target domain, which contain critical factors influencing the testing performance on the target domain. In this work, we comprehensively investigate into the intra-domain variations of the target domain and propose to generalize the re-ID model w.r.t three types of the underlying invariance, i.e., exemplar-invariance, camera-invariance and neighborhood-invariance. To achieve this goal, an exemplar memory is introduced to store features of the target domain and accommodate the three invariance properties. The memory allows us to enforce the invariance constraints over global training batch without significantly increasing computation cost. Experiment demonstrates that the three invariance properties and the proposed memory are indispensable towards an effective domain adaptation system. Results on three re-ID domains show that our domain adaptation accuracy outperforms the state of the art by a large margin. Code is available at: https://github.com/zhunzhong07/ECN",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.0199,10.1109/CVPR.2019.00069,https://arxiv.org/pdf/1904.01990.pdf
0c67b4859fc9443ed11228bb4b56e3578d174f40,0,1,0,Improving Person Re-identification by Mask Guiding and Part Pooling,"Person re-identification (re-ID) is a promising computer vision task. State-of-the-art methods mainly utilize deep learning based approaches to learn visual features for describing person appearances. Due to occlusion, complex background, different postures and light intensity, the technology faces many challenges. In this paper, a person re-identification method is proposed combining mask guiding and part pooling. First, person mask is generated by a segmentation sub-net, combining Macro-Micro Adversarial Network (MMAN) with Fully Convolution Networks (FCN). To alleviate the influence of background, a mask guiding strategy is designed integrating the mask with person feature map. Then a part pooling strategy is employed to extract local features of the person. The final loss function of the network is defined as the combination of global loss and local loss, which can describe the person in a comprehensive manner. Four public datasets are employed to test the performance of the proposed method. Our method achieves rank-1/mAP of 89.05%/72.83% on the Market-1501, 79.03%/62.06% on the DukeMTMC-reID, 46.79%/29.61% on the MSMT-17, 49.50%/45.03% on the CUHK03-NP. Experimental results show that the designed mask guiding and part pooling strategies can improve person re-ID performance.",2020,ICMLC,,10.1145/3383972.3383993,
0c88506b5e6091906e656dfeb9ee1060946d4aab,0,1,0,Large Margin In Softmax Cross-Entropy Loss,"Deep convolutional neural networks (CNNs) are trained mostly based on the softmax cross-entropy loss to produce promising performance on various image classification tasks. While much research effort has been made to improve the building blocks of CNNs, the classifier margin in the loss attracts less attention for optimizing CNNs in contrast to the kernel-based methods, such as SVM. In this paper, we propose a novel method to induce a large-margin CNN for improving the classification performance. By analyzing the formulation of the softmax loss, we clarify the margin embedded in the loss as well as its connection to the distribution of softmax logits. Based on this analysis, the proposed method is formulated as regularization imposed on the logits to induce a largemargin classifier in a compatible form with the softmax loss. The experimental results on image classification using various CNNs demonstrate that the proposed method favorably improves performance compared to the other large-margin losses.",2019,BMVC,,,https://bmvc2019.org/wp-content/uploads/papers/0636-paper.pdf
0cf0ad8235929417d904acd1c672713ca4fdb105,1,0,0,Fusion of Head and Full-Body Detectors for Multi-object Tracking,"In order to track all persons in a scene, the tracking-by-detection paradigm has proven to be a very effective approach. Yet, relying solely on a single detector is also a major limitation, as useful image information might be ignored. Consequently, this work demonstrates how to fuse two detectors into a tracking system. To obtain the trajectories, we propose to formulate tracking as a weighted graph labeling problem, resulting in a binary quadratic program. As such problems are NP-hard, the solution can only be approximated. Based on the Frank-Wolfe algorithm, we present a new solver that is crucial to handle such difficult problems. Evaluation on pedestrian tracking is provided for multiple scenarios, showing superior results over single detector tracking and standard QP-solvers. Finally, our tracker ranks 2nd on the MOT16 benchmark and 1st on the new MOT17 benchmark, outperforming over 90 trackers.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00192,
0d7e73c938e57e1c09dc548366b56a4a9010ca81,1,1,1,IAUnet: Global Context-Aware Feature Learning for Person Re-Identification,"Person reidentification (reID) by convolutional neural network (CNN)-based networks has achieved favorable performance in recent years. However, most of existing CNN-based methods do not take full advantage of spatial-temporal context modeling. In fact, the global spatial-temporal context can greatly clarify local distractions to enhance the target feature representation. To comprehensively leverage the spatial-temporal context information, in this work, we present a novel block, interaction-aggregation-update (IAU), for high-performance person reID. First, the spatial-temporal IAU (STIAU) module is introduced. STIAU jointly incorporates two types of contextual interactions into a CNN framework for target feature learning. Here, the spatial interactions learn to compute the contextual dependencies between different body parts of a single frame, while the temporal interactions are used to capture the contextual dependencies between the same body parts across all frames. Furthermore, a channel IAU (CIAU) module is designed to model the semantic contextual interactions between channel features to enhance the feature representation, especially for small-scale visual cues and body parts. Therefore, the IAU block enables the feature to incorporate the globally spatial, temporal, and channel context. It is lightweight, end-to-end trainable, and can be easily plugged into existing CNNs to form IAUnet. The experiments show that IAUnet performs favorably against state of the art on both image and video reID tasks and achieves compelling results on a general object categorization task. The source code is available at https://github.com/blue-blue272/ImgReID-IAnet.",2020,IEEE transactions on neural networks and learning systems,2009.01035,10.1109/tnnls.2020.3017939,https://arxiv.org/pdf/2009.01035.pdf
0d96b7aa5a41dddc3e0118918584031f4708c72c,1,1,0,Pedestrian Models for Autonomous Driving Part II: high level models of human behaviour,"Abstract—Autonomous vehicles (AVs) must share space with pedestrians, both in carriageway cases such as cars at pedestrian crossings and off-carriageway cases such as delivery vehicles navigating through crowds on pedestrianized high-streets. Unlike static obstacles, pedestrians are active agents with complex, inter- active motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behaviour as well as detecting and tracking them. This narrative review article is Part II of a pair, together surveying the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low-level image detection to high-level psychological models, from the perspective of an AV designer. This self-contained Part II covers the higher levels of this stack, consisting of models of pedestrian behaviour, from prediction of individual pedestrians’ likely destinations and paths, to game-theoretic models of interactions between pedestrians and autonomous vehicles. This survey clearly shows that, although there are good models for optimal walking behaviour, high-level psychological and social modelling of pedestrian behaviour still remains an open research question that requires many conceptual issues to be clarified. Early work has been done on descriptive and qualitative models of behaviour, but much work is still needed to translate them into quantitative algorithms for practical AV control.",2020,ArXiv,2003.11959,10.1109/TITS.2020.3006767,https://arxiv.org/pdf/2003.11959.pdf
0da7897660d78b1f75e433eb26cb6f16f7de1295,0,1,0,VR-PROUD: Vehicle Re-identification using PROgressive Unsupervised Deep architecture,"Abstract Vehicle re-identification (Re-ID) is one of the primary components of an automated visual surveillance system. It aims to automatically identify/search vehicles in a multi-camera network usually having non-overlapping field-of-views. Majority of the approaches dealing with the re-ID problem tackle it in a supervised manner which have certain limitations that pose challenges of generalization e.g., large amount of annotated data is required for training and is often limited to the dynamic growth of the data. Unsupervised learning techniques can potentially cope with such issues by drawing inference directly from the unlabeled input data and have been effectively employed in the context of person re-ID. To this end, this paper presents an approach that essentially formulates the whole vehicle re-ID problem into an unsupervised learning paradigm using a progressive two step cascaded framework. It combines a CNN architecture for feature extraction and an unsupervised technique to enable self-paced progressive learning. It also incorporates the contextual information into the proposed progressive framework that significantly improves the convergence of the learned algorithm. Moreover, the approach is generic and has been the first attempt to tackle the vehicle re-ID problem in an unsupervised manner. The performance of the proposed algorithm has been thoroughly analyzed over two large publically available benchmark datasets VeRi and VehicleID for vehicle re-ID using image-to-image and cross-camera search strategies and achieved better performance in comparison to current state-of-the-art approaches using standard evaluation metrics.",2019,Pattern Recognit.,,10.1016/J.PATCOG.2019.01.008,http://static.tongtianta.site/paper_pdf/f37ec9c4-90d3-11e9-8572-00163e08bb86.pdf
0db41739f514c4c911c54a4c90ab5f07db3862dc,1,0,0,NCA-Net for Tracking Multiple Objects across Multiple Cameras,"Tracking multiple pedestrians across multi-camera scenarios is an important part of intelligent video surveillance and has great potential application for public security, which has been an attractive topic in the literature in recent years. In most previous methods, artificial features such as color histograms, HOG descriptors and Haar-like feature were adopted to associate objects among different cameras. But there are still many challenges caused by low resolution, variation of illumination, complex background and posture change. In this paper, a feature extraction network named NCA-Net is designed to improve the performance of multiple objects tracking across multiple cameras by avoiding the problem of insufficient robustness caused by hand-crafted features. The network combines features learning and metric learning via a Convolutional Neural Network (CNN) model and the loss function similar to neighborhood components analysis (NCA). The loss function is adapted from the probability loss of NCA aiming at object tracking. The experiments conducted on the NLPR_MCT dataset show that we obtain satisfactory results even with a simple matching operation. In addition, we embed the proposed NCA-Net with two existing tracking systems. The experimental results on the corresponding datasets demonstrate that the extracted features using NCA-net can effectively make improvement on the tracking performance.",2018,Sensors,,10.3390/s18103400,https://pdfs.semanticscholar.org/0db4/1739f514c4c911c54a4c90ab5f07db3862dc.pdf
0dca825e231559d9c0ee14ba7449ecd86a2362ba,0,1,0,MGD: Mask Guided De-occlusion Framework for Occluded Person Re-identification,"Person re-identification (ReID) is a challenging task in computer vision area due to the dramatic changes across different non-overlapping camera views, e.g., lighting, view angle, and pose, among which occlusion is one of the hardest challenges. Recently, occluded person re-identification (Occluded-ReID) is proposed to address this problem. Nevertheless, current occluded-ReID methods focus on how to learn a matching function between partial-body images and full-body images while ignore the structural information of the full body. To handle this problem, we propose a novel framework called Mask Guided De-occlusion (MGD) for occluded Person Re-identification. The MGD mainly consists of three components, i.e., a Coarse-to-Fine Mask Generation (CFMG) module, a Person De-Occlusion (PDO) module and a Person Feature Extractor (PFE). The key module CFMG aims to locate the occlusion areas by manipulating the instance segmentation masks through a two-stage process. The proposed PDO module is to reconstruct the occluded pedestrian. After that, all the images are fed into the PFE module to obtain their feature vectors. With PDO and CFMG modules, the proposed method MGD reduces the impact of occlusions and thus improves the performance of Occluded-ReID. The extensive experiments conducted on several public occluded ReID datasets show that our method is effective and outperforms the state-of-the-art methods.",2019,IScIDE,,10.1007/978-3-030-36189-1_34,
0e36bf238d2db6c970ade0b5f68811ed6debc4e8,1,1,0,Recognizing Partial Biometric Patterns,"Biometric recognition on partial captured targets is challenging, where only several partial observations of objects are available for matching. In this area, deep learning based methods are widely applied to match these partial captured objects caused by occlusions, variations of postures or just partial out of view in person re-identification and partial face recognition. However, most current methods are not able to identify an individual in case that some parts of the object are not obtainable, while the rest are specialized to certain constrained scenarios. To this end, we propose a robust general framework for arbitrary biometric matching scenarios without the limitations of alignment as well as the size of inputs. We introduce a feature post-processing step to handle the feature maps from FCN and a dictionary learning based Spatial Feature Reconstruction (SFR) to match different sized feature maps in this work. Moreover, the batch hard triplet loss function is applied to optimize the model. The applicability and effectiveness of the proposed method are demonstrated by the results from experiments on three person re-identification datasets (Market1501, CUHK03, DukeMTMC-reID), two partial person datasets (Partial REID and Partial iLIDS) and two partial face datasets (CASIA-NIR-Distance and Partial LFW), on which state-of-the-art performance is ensured in comparison with several state-of-the-art approaches. The code is released online and can be found on the website: this https URL.",2018,ArXiv,1810.07399,,https://arxiv.org/pdf/1810.07399.pdf
0e3a7035672d19abd71f72c612be0d1caeecb63c,1,0,0,GMOT-40: A Benchmark for Generic Multiple Object Tracking,"Multiple Object Tracking (MOT) has witnessed remarkable advances in recent years. However, existing studies dominantly request prior knowledge of the tracking target (e.g., pedestrians), and hence may not generalize well to unseen categories. In contrast, Generic Multiple Object Tracking (GMOT), which requires little prior information about the target, is largely under-explored. In this paper, we make contributions to boost the study of GMOT in three aspects. First, we construct the first public GMOT dataset, dubbed GMOT-40, which contains 40 carefully annotated sequences evenly distributed among 10 object categories. In addition, two tracking protocols are adopted to evaluate different characteristics of tracking algorithms. Second, by noting the lack of devoted tracking algorithms, we have designed a series of baseline GMOT algorithms. Third, we perform a thorough evaluations on GMOT-40, involving popular MOT algorithms (with necessary modifications) and the proposed baselines. We will release the GMOT-40 benchmark, the evaluation results, as well as the baseline algorithm to the public upon the publication of the paper.",2020,ArXiv,2011.11858,,https://arxiv.org/pdf/2011.11858.pdf
0e6beea0b90b7072c96fd6acd6c7186bd8aeadd1,1,1,1,Pose-Guided Feature Alignment for Occluded Person Re-Identification,"Persons are often occluded by various obstacles in person retrieval scenarios. Previous person re-identification (re-id) methods, either overlook this issue or resolve it based on an extreme assumption. To alleviate the occlusion problem, we propose to detect the occluded regions, and explicitly exclude those regions during feature generation and matching. In this paper, we introduce a novel method named Pose-Guided Feature Alignment (PGFA), exploiting pose landmarks to disentangle the useful information from the occlusion noise. During the feature constructing stage, our method utilizes human landmarks to generate attention maps. The generated attention maps indicate if a specific body part is occluded and guide our model to attend to the non-occluded regions. During matching, we explicitly partition the global feature into parts and use the pose landmarks to indicate which partial features belonging to the target person. Only the visible regions are utilized for the retrieval. Besides, we construct a large-scale dataset for the Occluded Person Re-ID problem, namely Occluded-DukeMTMC, which is by far the largest dataset for the Occlusion Person Re-ID. Extensive experiments are conducted on our constructed occluded re-id dataset, two partial re-id datasets, and two commonly used holistic re-id datasets. Our method largely outperforms existing person re-id methods on three occlusion datasets, while remains top performance on two holistic datasets.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00063,http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf
0e9f5a7330d1a7a873197a51046b6f8b7eccb634,0,1,0,Data augmentation of random grid-hiding for video object segmentation,"Video object segmentation is an important field in computer vision. However, the challenges in video object segmentation such as background clutter, occlusion and edge ambiguity cannot be avoided. In addition, existing labeled video object segmentation datasets are limited in size, which prevents CNN models from reaching their full generalization capabilities. In this paper, we propose a novel approach, called random grid-hiding (RGH), to perform data augmentation. We divide the training image into several rectangular regions and hide some regions randomly during model training. Thus, the convolutional neural network automatically focuses on the discriminative parts of the image. When the most discriminative part of the image is hidden, it compels the network focus on the other related parts of the image. Further, occlusion images are randomly generated in various levels. More features can be obtained by random grid-hiding, which can effectively reduce the risk of overfitting. Our approach is an effective extension of the data augmentation (such as random cropping and random flipping), and leads to improved accuracy in the task of the video object segmentation method on DAVIS dataset. Our experimental results show that the proposed method is a stable and effective method for data augmentation.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-7569-5,
0f0a9468f7fa6bfa85e20a4ae1495acdb90e841b,1,1,0,Deep Regression Neural Network for End-to-End Person Re-Identification,"Person re-identification can be seen as a process of open set recognition. Usually, the deep learning models consider the person re-identification model as a classification model with a softmax layer. However, the softmax layer cannot be extended to unknown classes because of its closed nature, so the classification model is just regarded as the feature extractor. To overcome the problem mentioned above and make the person re-identification process end-to-end, this paper cast the person re-identification into a regression process and calculates the probability that persons in the images belong to the same identity. First, this paper proposes a deep regression model, named deep regression neural network integrating adaptive multi-attribute fusion method (DRNN-AMAF), which can make the person re-identification as regression analysis. Second, attributes are taken as the basis of this model for calculating the probability of persons belonging to the same identity, and each attribute corresponds to each branch of the deep regression neural network. Finally, hard labels of multiple attributes are adaptively fused into a soft label by the proposed multi-label fusion method based on the idea of Bayesian inference, which makes the attribute labels suitable for regression tasks. The comprehensive experiments on available public databases are conducted, and the experimental results show that our model produces competitive performance compared with the state-of-the-art approaches.",2019,IEEE Access,,10.1109/ACCESS.2019.2927626,
0f3eb3719b6f6f544b766e0bfeb8f962c9bd59f4,1,0,0,Eliminating Exposure Bias and Loss-Evaluation Mismatch in Multiple Object Tracking,"Identity Switching remains one of the main difficulties Multiple Object Tracking (MOT) algorithms have to deal with. Many state-of-the-art approaches now use sequence models to solve this problem but their training can be affected by biases that decrease their efficiency. In this paper, we introduce a new training procedure that confronts the algorithm to its own mistakes while explicitly attempting to minimize the number of switches, which results in better training. We propose an iterative scheme of building a rich training set and using it to learn a scoring function that is an explicit proxy for the target tracking metric. Whether using only simple geometric features or more sophisticated ones that also take appearance into account, our approach outperforms the state-of-the-art on several MOT benchmarks.",2018,ArXiv,1811.10984,,https://arxiv.org/pdf/1811.10984.pdf
0f4167cb91f53c3c9fb50f9f56767443d6776f42,0,1,0,Towards Practical Implementations of Person Re-Identification from Full Video Frames,"Abstract With the major adoption of automation for cities security, person re-identification (Re-ID) has been extensively studied recently. In this paper, we argue that the current way of studying person re-identification, i.e. by trying to re-identify a person within already detected and pre-cropped images of people, is not sufficient to implement practical security applications, where the inputs to the system are the full frames of the video streams. To support this claim, we introduce the Full Frame Person Re-ID setting (FF-PRID) and define specific metrics to evaluate FF-PRID implementations. To improve robustness, we also formalize the hybrid human-machine collaboration framework, which is inherent to any Re-ID security applications. To demonstrate the importance of considering the FF-PRID setting, we build an experiment showing that combining a good people detection network with a good Re-ID model does not necessarily produce good results for the final application. This underlines a failure of the current formulation in assessing the quality of a Re-ID model and justifies the use of different metrics. We hope that this work will motivate the research community to consider the full problem in order to develop algorithms that are better suited to real-world scenarios.",2020,Pattern Recognit. Lett.,2009.01377,10.1016/j.patrec.2020.08.023,https://arxiv.org/pdf/2009.01377.pdf
0f9d773ad4af1c010f8ff84693a67e8d2ab09dac,1,0,0,Learning to Learn in a Semi-Supervised Fashion,"To address semi-supervised learning from both labeled and unlabeled data, we present a novel meta-learning scheme. We particularly consider that labeled and unlabeled data share disjoint ground truth label sets, which can be seen tasks like in person re-identification or image retrieval. Our learning scheme exploits the idea of leveraging information from labeled to unlabeled data. Instead of fitting the associated class-wise similarity scores as most meta-learning algorithms do, we propose to derive semantics-oriented similarity representations from labeled data, and transfer such representation to unlabeled ones. Thus, our strategy can be viewed as a self-supervised learning scheme, which can be applied to fully supervised learning tasks for improved performance. Our experiments on various tasks and settings confirm the effectiveness of our proposed approach and its superiority over the state-of-the-art methods.",2020,ArXiv,2008.11203,,https://arxiv.org/pdf/2008.11203.pdf
0fa15f54f537f41fdfb80a3eccfd710bc4346dc4,0,1,0,Attribute Adaptive Margin Softmax Loss using Privileged Information,"We present a novel framework to exploit privileged information for recognition which is provided only during the training phase. Here, we focus on recognition task where images are provided as the main view and soft biometric traits (attributes) are provided as the privileged data (only available during training phase). We demonstrate that more discriminative feature space can be learned by enforcing a deep network to adjust adaptive margins between classes utilizing attributes. This tight constraint also effectively reduces the class imbalance inherent in the local data neighborhood, thus carving more balanced class boundaries locally and using feature space more efficiently. Extensive experiments are performed on five different datasets and the results show the superiority of our method compared to the state-of-the-art models in both tasks of face recognition and person re-identification.",2020,ArXiv,2009.01972,,https://arxiv.org/pdf/2009.01972.pdf
0fbdaad9fe987dfecb187597675276073bc679c0,1,1,0,Bag of Negatives for Siamese Architectures,"Training a Siamese architecture for re-identification with a large number of identities is a challenging task due to the difficulty of finding relevant negative samples efficiently. In this work we present Bag of Negatives (BoN), a method for accelerated and improved training of Siamese networks that scales well on datasets with a very large number of identities. BoN is an efficient and loss-independent method, able to select a bag of high quality negatives, based on a novel online hashing strategy.",2019,BMVC,1908.02391,,https://arxiv.org/pdf/1908.02391.pdf
0ffb4e7e88af8160efe8fe1e0c06fec45236785a,1,1,0,Introducing Scene Understanding to Person Re-Identification using a Spatio-Temporal Multi-Camera Model,"In this paper, we investigate person re-identification (re-ID) in a multi-camera network for surveillance applications. To this end, we create a Spatio-Temporal Multi-Camera model (ST-MC model), which exploits statistical data on a person’s entry/exit points in the multi-camera network, to predict in which camera view a person will re-appear. The created ST-MC model is used as a novel extension to the Multiple Granularity Network (MGN) [1], which is the current state of the art in person re-ID. Compared to existing approaches that are solely based on Convolutional Neural Networks (CNNs), our approach helps to improve the re-ID performance by considering not only appearance-based features of a person from a CNN, but also contextual information. The latter serves as scene understanding information complimentary to person re-ID. Experimental results show that for the DukeMTMC-reID dataset [2][3], introduction of our ST-MC model substantially increases the mean Average Precision (mAP) and Rank-1 score from 77.2% to 84.1%, and from 88.6% to 96.2%, respectively.",2020,,,10.2352/ISSN.2470-1173.2020.10.IPAS-095,
1056e3d84f6324911b403acf5fa973c37278c8d8,1,1,0,Guided Saliency Feature Learning for Person Re-identification in Crowded Scenes,"Person Re-identification (Re-ID) in crowed scenes is a challenging problem, where people are frequently partially occluded by objects and other people. However, few studies have provided flexible solutions to re-identifying people in an image containing a partial occlusion body part. In this paper, we propose a simple occlusion-aware approach to address the problem. The proposed method first leverages a fully convolutional network to generate spatial features. And then we design a combination of a pose-guided and mask-guided layer to generate saliency heatmap to further guide discriminative feature learning. More importantly, we propose a new matching approach, called Guided Adaptive Spatial Matching (GASM), which expects that each spatial feature in the query can find the most similar spatial features of a person in a gallery to match. Especially, We use the saliency heatmap to guide the adaptive spatial matching by assigning the foreground human parts with larger weights adaptively. The effectiveness of the proposed GASM is demonstrated on two occluded person datasets: Crowd REID (51.52%) and Occluded REID (80.25%) and three benchmark person datasets: Market1501 (95.31%), DukeMTMC-reID (88.12%) and MSMT17 (79.52%). Additionally, GASM achieves good performance on cross-domain person Re-ID. The code and models are available at: https://github.com/ JDAI-CV/fast-reid/blob/master/projects/CrowdReID.",2020,ECCV,,10.1007/978-3-030-58604-1_22,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730358.pdf
1060e223710a9472ffa5bb68bbb4d629014f7dbf,0,1,0,Real-Time Pose Based Human Detection and Re-Identification with a Single Camera for Robot Person Following,"Title of thesis: REAL-TIME POSE BASED HUMAN DETECTION AND RE-IDENTIFICATION WITH A SINGLE CAMERA FOR ROBOT PERSON FOLLOWING John Bradford Welsh, Master of Science, 2017 Thesis directed by: Professor Gilmer Blankenship Department of Electrical Engineering In this work we address the challenge of following a person with a mobile robot, with a focus on the image processing aspect. We overview different historical approaches for person following and outline the advantages and disadvantages of each. We then show that recent convolutional neural networks trained for human pose detection are suitable for person detection as it relates to the robot following problem. We extend one such pose detection network to spatially embed the identity of individuals in the image, utilizing the pose features already computed. The proposed identity embedding allows the system to robustly track individuals in consecutive frames even in long term occlusion or absence. The final system provides a robust person tracking scheme which is suitable for person following. REAL-TIME POSE BASED HUMAN DETECTION AND RE-IDENTIFICATION WITH A SINGLE CAMERA FOR ROBOT PERSON FOLLOWING",2017,,,10.13016/M2DW1Q,
10a409ba424289e16be316375c55321056dbd26a,1,1,0,Multi-branch Body Region Alignment Network for Person Re-identification,"Person re-identification (Re-ID) aims to identify the same person images from a gallery set across different cameras. Human pose variations, background clutter and misalignment of detected human images pose challenges for Re-ID tasks. To deal with these issues, we propose a Multi-branch Body Region Alignment Network (MBRAN), to learn discriminative representations for person Re-ID. It consists of two modules, i.e., body region extraction and feature learning. Body region extraction module utilizes a single-person pose estimation method to estimate human keypoints and obtain three body regions. In the feature learning module, four global or local branch-networks share base layers and are designed to learn feature representation on three overlapping body regions and the global image. Extensive experiments have indicated that our method outperforms several state-of-the-art methods on two mainstream person Re-ID datasets.",2020,MMM,,10.1007/978-3-030-37731-1_28,
10b36c003542545f1e2d73e8897e022c0c260c32,1,0,0,Towards a Principled Integration of Multi-camera Re-identification and Tracking Through Optimal Bayes Filters,"With the rise of end-to-end learning through deep learning, person detectors and re-identification (ReID) models have recently become very strong. Multi-target multicamera (MTMC) tracking has not fully gone through this transformation yet. We intend to take another step in this direction by presenting a theoretically principled way of integrating ReID with tracking formulated as an optimal Bayes filter. This conveniently side-steps the need for dataassociation and opens up a direct path from full images to the core of the tracker. While the results are still sub-par, we believe that this new, tight integration opens many interesting research opportunities and leads the way towards full end-to-end tracking from raw pixels. Code and models for all experiments are publicly available.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1705.04608,10.1109/CVPRW.2017.187,https://arxiv.org/pdf/1705.04608.pdf
10c109e86c94a241e14842b8b810b0a527295edf,0,1,0,Multi-scale multi-patch person re-identification with exclusivity regularized softmax,"Abstract Discriminative feature learning is critical for person re-identification. To obtain abundant visual information from the input person image, we first propose a novel network that extracts multi-scale patch-level deep features. Then, we propose an improved softmax loss function for learning more compact and more discriminative feature vectors. Specifically, we integrate feature pyramid blocks and region-level global average pooling functions into the feature extraction network, introduce the well-established normalization techniques in face recognition algorithms into person re-ID, and penalize the redundancy in feature vectors by minimizing the l1,2 norm of the weight matrix in the softmax layer. Experiments on three large-scale datasets under the standard settings show the effectiveness of the proposed method. Moreover, we report our cross-domain re-ID results by training re-ID models on source datasets and testing them on other target datasets.",2020,Neurocomputing,,10.1016/j.neucom.2019.11.062,https://xinggangw.info/pubs/msmpreid.pdf
111ed2a3db118ef4fffef2de6e8481f548702f3e,1,1,0,Learning to Adapt Invariance in Memory for Person Re-identification,"This work considers the problem of unsupervised domain adaptation in person re-identification (re-ID), which aims to transfer knowledge from the source domain to the target domain. Existing methods are primary to reduce the inter-domain shift between the domains, which however usually overlook the relations among target samples. This paper investigates into the intra-domain variations of the target domain and proposes a novel adaptation framework w.r.t three types of underlying invariance, i.e., Exemplar-Invariance, Camera-Invariance, and Neighborhood-Invariance. Specifically, an exemplar memory is introduced to store features of samples, which can effectively and efficiently enforce the invariance constraints over the global dataset. We further present the Graph-based Positive Prediction (GPP) method to explore reliable neighbors for the target domain, which is built upon the memory and is trained on the source samples. Experiments demonstrate that 1) the three invariance properties are complementary and indispensable for effective domain adaptation, 2) the memory plays a key role in implementing invariance learning and improves the performance with limited extra computation cost, 3) GPP can facilitate the invariance learning and thus significantly improves the results, and 4) our approach produces new state-of-the-art adaptation accuracy on three re-ID large-scale benchmarks.",2020,IEEE transactions on pattern analysis and machine intelligence,1908.00485,10.1109/tpami.2020.2976933,https://arxiv.org/pdf/1908.00485.pdf
1121cf8e1e6c37f40cc8cb2a92fb26934ba0fff4,1,1,0,FSRM-STS: Cross-dataset pedestrian retrieval based on a four-stage retrieval model with Selection-Translation-Selection,"Abstract Pedestrian retrieval is widely used in intelligent video surveillance and is closely related to people’s lives. Although pedestrian retrieval from a single dataset has improved in recent years, obstacles such as a lack of sample data, domain gaps within and between datasets (arising from factors such as variation in lighting conditions, resolution, season and background etc.), reduce the generalizability of existing models. Factors such as these can act as barriers to the practical use of this technology. Cross-dataset learning is a way to obtain high-quality images from source datasets and can assist the learning of target datasets, thus helping to address the above problem. Existing studies of cross-dataset learning directly apply translated images from source datasets to target datasets, and seldom consider systematic strategies for further improving the quality of the translated images. There is therefore room for improvement in cross-dataset learning. This paper proposes a four-stage retrieval model based on Selection–Translation–Selection (FSRM-STS), to help address this problem. In the first stage of the model, images in pedestrian retrieval datasets are semantically segmented to provide information for image-translation. In the second stage, STS is proposed, based on four strategies to obtain high quality translation results from all source datasets and to generate auxiliary datasets. In the third stage, a pedestrian feature extraction model is proposed, based on both the auxiliary and target datasets. This converts each image in target datasets into an n-dimensional vector. In the final stage, the extracted image vectors are used for cross-dataset pedestrian retrieval. As the translation quality is improved, FSRM-STS achieves promising results for the cross-dataset pedestrian retrieval. Experimental results on four benchmark datasets Market-1501, DukeMTMC-reID, CUHK03 and VIPeR show the effectiveness of the proposed model. Finally, the use of parallel computing for accelerating the training speed and for realizing online applications is also discussed. A primary demo based on cloud computing is designed to verify the engineering solution in the future.",2020,Future Gener. Comput. Syst.,,10.1016/j.future.2020.02.028,
11442030f5ff6cf1fa740918403c1c8ff2c68081,0,1,0,A Global-Local Architecture Constrained by Multiple Attributes for Person Re-identification,"Person re-identification (person re-ID) is often considered as a sub-problem of image retrieval, which aims to match pedestrians under non-overlapping cameras. In this work, we present a novel global and local network structure integrating pedestrian identities with multiple attributes to improve the performance of person re-ID. The proposed framework consists of three modules: shared one, global one and local one. The shared module based on pre-trained residual network extracts low-level and mid-level features. And the global module guided by identification loss learns high-level semantic feature representations. To achieve accurate localization of local attribute features, we propose a multi-attributes partitioning learning method and consider pedestrian attributes as supervised information of the local module. Meanwhile, we employ whole-to-part spatial transformer networks (STNs) to achieve coarse-to-fine meaningful feature locations. By applying a multi-task learning strategy, we design various objective functions including identification and multiple attributes classification losses for training our model. The experimental results on several challenging datasets show our method significantly improves person re-ID performance and surpasses most of the state-of-the-art methods. Specifically, our model achieves 87.49% of the attribute recognition accuracy on Market1501 dataset.",2019,ICANN,,10.1007/978-3-030-30508-6_23,
11b7677da829c6c4053a19a49c40956f265b8d87,1,1,0,Weakly Supervised Discriminative Feature Learning With State Information for Person Identification,"Unsupervised learning of identity-discriminative visual feature is appealing in real-world tasks where manual labelling is costly. However, the images of an identity can be visually discrepant when images are taken under different \emph{states}, e.g. different camera views and poses. This visual discrepancy leads to great difficulty in unsupervised discriminative learning. Fortunately, in real-world tasks we could often know the states without human annotation, e.g. we can easily have the camera view labels in person re-identification and facial pose labels in face recognition. In this work we propose utilizing the state information as weak supervision to address the visual discrepancy caused by different states. We formulate a simple pseudo label model and utilize the state information in an attempt to refine the assigned pseudo labels by the weakly supervised decision boundary rectification and weakly supervised feature drift regularization. We evaluate our model on unsupervised person re-identification and pose-invariant face recognition. Despite the simplicity of our method, it could outperform the state-of-the-art results on Duke-reID, MultiPIE and CFP datasets with a standard ResNet-50 backbone. We also find our model could perform comparably with the standard supervised fine-tuning results on the three datasets. Code is available at \url{https://github.com/KovenYu/state-information}.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.11939,10.1109/cvpr42600.2020.00557,https://arxiv.org/pdf/2002.11939.pdf
11cb49d8f19f0491e1930d9471988a3c07b70bb4,1,1,0,Person Re-Identification With Triplet Focal Loss,"Person re-identification (ReID), which aims at matching individuals across non-overlapping cameras, has attracted much attention in the field of computer vision due to its research significance and potential applications. Triplet loss-based CNN models have been very successful for person ReID, which aims to optimize the feature embedding space such that the distances between samples with the same identity are much shorter than those of samples with different identities. Researchers have found that hard triplets’ mining is crucial for the success of the triplet loss. In this paper, motivated by focal loss designed for the classification model, we propose the triplet focal loss for person ReID. Triplet focal loss can up-weight the hard triplets’ training samples and relatively down-weight the easy triplets adaptively via simply projecting the original distance in the Euclidean space to an exponential kernel space. We conduct experiments on three largest benchmark datasets currently available for person ReID, namely, Market-1501, DukeMTMC-ReID, and CUHK03, and the experimental results verify that the proposed triplet focal loss can greatly outperform the traditional triplet loss and achieve competitive performances with the representative state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2884743,
11deaaffc236be736c4ffc241cf249acf889bed7,1,1,0,Beyond Triplet Loss: Person Re-identification with Fine-grained Difference-aware Pairwise Loss,"Person Re-IDentification (ReID) aims at re-identifying persons from different viewpoints across multiple cameras. Capturing the fine-grained appearance differences is often the key to accurate person ReID, because many identities can be differentiated only when looking into these fine-grained differences. However, most state-of-the-art person ReID approaches, typically driven by a triplet loss, fail to effectively learn the fine-grained features as they are focused more on differentiating large appearance differences. To address this issue, we introduce a novel pairwise loss function that enables ReID models to learn the fine-grained features by adaptively enforcing an exponential penalization on the images of small differences and a bounded penalization on the images of large differences. The proposed loss is generic and can be used as a plugin to replace the triplet loss to significantly enhance different types of state-of-the-art approaches. Experimental results on four benchmark datasets show that the proposed loss substantially outperforms a number of popular loss functions by large margins; and it also enables significantly improved data efficiency.",2020,ArXiv,2009.10295,,https://arxiv.org/pdf/2009.10295.pdf
11fcbc38b3fc4cf9f17dcd4b97e15b84c283c9f5,0,1,0,Meta Batch-Instance Normalization for Generalizable Person Re-Identification,"Although supervised person re-identification (Re-ID) methods have shown impressive performance, they suffer from a poor generalization capability on unseen domains. Therefore, generalizable Re-ID has recently attracted growing attention. Many existing methods have employed an instance normalization technique to reduce style variations, but the loss of discriminative information could not be avoided. In this paper, we propose a novel generalizable Re-ID framework, named Meta Batch-Instance Normalization (MetaBIN). Our main idea is to generalize normalization layers by simulating unsuccessful generalization scenarios beforehand in the meta-learning pipeline. To this end, we combine learnable batch-instance normalization layers with meta-learning and investigate the challenging cases caused by both batch and instance normalization layers. Moreover, we diversify the virtual simulations via our meta-train loss accompanied by a cyclic inner-updating manner to boost generalization capability. After all, the MetaBIN framework prevents our model from overfitting to the given source styles and improves the generalization capability to unseen domains without additional data augmentation or complicated network design. Extensive experimental results show that our model outperforms the stateof-the-art methods on the large-scale domain generalization Re-ID benchmark.",2020,ArXiv,2011.1467,,https://arxiv.org/pdf/2011.14670.pdf
123478b496a3fa39a9043ccaa660e81c473a14e9,1,1,1,A Bottom-Up Clustering Approach to Unsupervised Person Re-Identification,"Most person re-identification (re-ID) approaches are based on supervised learning, which requires intensive manual annotation for training data. However, it is not only resourceintensive to acquire identity annotation but also impractical to label the large-scale real-world data. To relieve this problem, we propose a bottom-up clustering (BUC) approach to jointly optimize a convolutional neural network (CNN) and the relationship among the individual samples. Our algorithm considers two fundamental facts in the re-ID task, i.e., diversity across different identities and similarity within the same identity. Specifically, our algorithm starts with regarding individual sample as a different identity, which maximizes the diversity over each identity. Then it gradually groups similar samples into one identity, which increases the similarity within each identity. We utilizes a diversity regularization term in the bottom-up clustering procedure to balance the data volume of each cluster. Finally, the model achieves an effective trade-off between the diversity and similarity. We conduct extensive experiments on the large-scale image and video re-ID datasets, including Market-1501, DukeMTMCreID, MARS and DukeMTMC-VideoReID. The experimental results demonstrate that our algorithm is not only superior to state-of-the-art unsupervised re-ID approaches, but also performs favorably than competing transfer learning and semi-supervised learning methods.",2019,AAAI,,10.1609/AAAI.V33I01.33018738,https://vana77.github.io/vana77.github.io/images/AAAI19.pdf
125e6a2bf7d2452fb734bf1b5fe2ec736e240820,1,1,0,Equidistance constrained metric learning for person re-identification,"Abstract Person re-identification (re-id), aiming to search a specific person among a non-overlapping camera network, has attracted plenty of interest in recent years. This task is highly challenging, especially when there exists only single image per person in the database. In this paper, we present an algorithm for learning a Mahalanobis distance for person re-identification. Our method has two distinctive features: (1) to obtain the best separability of the training data, we first minimize the intra-class distances to the most extent by forcing intra-class distances to be zero, and (2) to promote the generalization ability of the learned metric, we then maximize the minimum margin between different classes. Inspired by the simple geometric intuition that a regular simplex maximizes its minimum side length, provided the sum of all side length is fixed, our method, called EquiDistance constrained Metric Learning (EquiDML), applies least-square regression technique to map images of the same person to the same vertex of a regular simplex, and images of different persons to different vertices of a regular simplex. Consequently, under the learned metric, images of the same class are collapsed to a single point, while images of different classes are transformed to be equidistant. This simple motivation is further formulated as a convex optimization problem, solved by the projected gradient descent method and proved to be very effective in person re-identification task. Although it is fairly simple, our method outperforms the state-of-the-art methods on CUHK01, CUHK03, Market1501 and DukeMTMC-reID datasets, and achieves very competitive performance on the widely used VIPeR dataset.",2018,Pattern Recognit.,,10.1016/j.patcog.2017.09.014,
126af5c7fef24216ad9f02886cd3ef570902e500,1,1,1,Fuzzy Multilayer Clustering and Fuzzy Label Regularization for Unsupervised Person Reidentification,"Unsupervised person reidentification has received more attention due to its wide real-world applications. In this paper, we propose a novel method named fuzzy multilayer clustering (FMC) for unsupervised person reidentification. The proposed FMC learns a new feature space using a multilayer perceptron for clustering in order to overcome the influence of complex pedestrian images. Meanwhile, the proposed FMC generates fuzzy labels for unlabeled pedestrian images, which simultaneously considers the membership degree and the similarity between the sample and each cluster. We further propose the fuzzy label regularization (FLR) to train the convolutional neural network (CNN) using pedestrian images with fuzzy labels in a supervised manner. The proposed FLR could regularize the CNN training process and reduce the risk of overfitting. The effectiveness of our method is validated on three large-scale person reidentification databases, i.e., Market-1501, DukeMTMC-reID, and CUHK03.",2020,IEEE Transactions on Fuzzy Systems,,10.1109/TFUZZ.2019.2914626,
12c30b7f982b9277bbd4a56e9c90493de2e67694,1,1,0,Learning Sparse and Identity-Preserved Hidden Attributes for Person Re-Identification,"Person re-identification (Re-ID) aims at matching person images captured in non-overlapping camera views. To represent person appearance, low-level visual features are sensitive to environmental changes, while high-level semantic attributes, such as “short-hair” or “long-hair”, are relatively stable. Hence, researches have started to design semantic attributes to reduce the visual ambiguity. However, to train a prediction model for semantic attributes, it requires plenty of annotations, which are hard to obtain in practical large-scale applications. To alleviate the reliance on annotation efforts, we propose to incrementally generate Deep Hidden Attribute (DHA) based on baseline deep network for newly uncovered annotations. In particular, we propose an auto-encoder model that can be plugged into any deep network to mine latent information in an unsupervised manner. To optimize the effectiveness of DHA, we reform the auto-encoder model with additional orthogonal generation module, along with identity-preserving and sparsity constraints. 1) Orthogonally generating: In order to make DHAs different from each other, Singular Vector Decomposition (SVD) is introduced to generate DHAs orthogonally. 2) Identity-preserving constraint: The generated DHAs should be distinct for telling different persons, so we associate DHAs with person identities. 3) Sparsity constraint: To enhance the discriminability of DHAs, we also introduce the sparsity constraint to restrict the number of effective DHAs for each person. Experiments conducted on public datasets have validated the effectiveness of the proposed network. On two large-scale datasets, i.e., Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2946975,
12ca62fd314c1c1f51caa235a08abc612d3e5bde,0,1,0,Pedestrian Retrieval Using Generated Samples and Multistream Layer in Sensor Networks,"In this paper, we propose a novel loss function named the hybrid quadruplet loss (HQL) to utilize the generated samples for pedestrian retrieval in sensor networks. The proposed HQL employs a set of quadruplets in order to maintain an appropriate margin between the real sample and the generated sample, reduce the intra-class variations and enlarge the inter-class variations. By this way, the generalization of the deep model could be improved. Furthermore, to identify the extremely similar pedestrians, we propose a novel multistream layer to mine imperceptible information from different aspects. The proposed multistream layer utilizes various filters with different morphologies to capture discriminative features in multiple scales, and it is flexible to follow any convolutional layer. Experiments on the three large-scale pedestrian retrieval databases (Market1501, CUHK03, and DukeMTMC-reID) demonstrate that the proposed method outperforms other state-of-the-art methods.",2020,IEEE Transactions on Emerging Topics in Computational Intelligence,,10.1109/TETCI.2018.2876556,
12d62f1360587fdecee728e6c509acc378f38dc9,1,1,0,Feature Affinity-Based Pseudo Labeling for Semi-Supervised Person Re-Identification,"Vision-based person re-identification aims to match a person's identity across multiple images, which is a fundamental task in multimedia content analysis and retrieval. Deep neural networks have recently manifested great potential in this task. However, a major bottleneck of existing supervised deep networks is their reliance on a large amount of annotated training data. Manual labeling for person identities in large-scale surveillance camera systems is quite challenging and incurs significant costs. Some recent studies adopt generative model outputs as training data augmentation. To more effectively use these synthetic data for an improved feature learning and re-identification performance, this paper proposes a novel feature affinity-based pseudo labeling method with two possible label encodings. To the best of our knowledge, this is the first study that employs pseudo-labeling by measuring the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. We propose training the network with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. We show that both label encodings can be learned in a unified manner and help improve the overall performance. Our extensive experiments on three person re-identification datasets: Market-1501, DukeMTMC-reID, and CUHK03, demonstrate significant performance boost over the state-of-the-art person re-identification approaches.",2019,IEEE Transactions on Multimedia,1805.06118,10.1109/TMM.2019.2916456,https://arxiv.org/pdf/1805.06118.pdf
137b00bc484784fe2656e79cb95ea132446e75e8,0,1,0,Deep periocular representation aiming video surveillance,"Abstract Usually, in the deep learning community, it is claimed that generalized representations that yielding outstanding performance / effectiveness require a huge amount of data for learning, which directly affect biometric applications. However, recent works combining transfer learning from other domains have surmounted such data application constraints designing interesting and promising deep learning approaches in diverse scenarios where data is not so abundant. In this direction, a biometric system for the periocular region based on deep learning approach is designed and applied on two non-cooperative ocular databases. Impressive representation discrimination is achieved with transfer learning from the facial domain (a deep convolutional network, called VGG) and fine tuning in the specific periocular region domain. With this design, our proposal surmounts previous state-of-the-art results on NICE (mean decidability of 3.47 against 2.57) and MobBio (equal error rate of 5.42% against 8.73%) competition databases.",2018,Pattern Recognit. Lett.,,10.1016/j.patrec.2017.12.009,
139ebfc447777fad5ab2a551ca1aaab9d2578f73,0,1,0,Person re-identification in images with deep learning,"Video surveillance systems are of a great value for public safety. As one of the most import surveillance applications, person re-identification is defined as the problem of identifying people across images that have been captured by different surveillance cameras without overlapping fields of view. With the increasing need for automated video analysis, this task is increasingly receiving attention. However, this problem is challenging due to the large variations of lighting, pose, viewpoint and background. To tackle these different difficulties, in this thesis, we propose several deep learning based approaches to obtain a better person re-identification performance in different ways. In the first proposed approach, we use pedestrian attributes to enhance the person re-identification. The attributes are defined as semantic mid-level descriptions of persons, such as gender, accessories, clothing etc. They could be helpful to extract characteristics that are invariant to the pose and viewpoint variations thanks to the descriptor being on a higher semantic level. In order to make use of the attributes, we propose a CNN-based person re-identification framework composed of an identity classification branch and of an attribute recognition branch. At a later stage, these two cues are combined to perform person re-identification. Secondly, among the challenges, one of the most difficult is the variation under different viewpoint. The same person shows very different appearances from different points of view. To deal with this issue, we consider that the images under various orientations are from different domains. We propose an orientation-specific CNN. This framework performs body orientation regression in a gating branch, and in another branch learns separate orientation-specific layers as local experts. The combined orientation-specific CNN feature representations are used for the person re-identification task. Thirdly, learning a similarity metric for person images is a crucial aspect of person re-identification. As the third contribution, we propose a novel listwise loss function taking into account the order in the ranking of gallery images with respect to different probe images. Further, an evaluation gain-based weighting is introduced in the loss function to optimize directly the evaluation measures of person re-identification. At the end, in a large gallery set, many people could have similar clothing. In this case, using only the appearance of single person leads to strong ambiguities. In realistic settings, people often walk in groups rather than alone. As the last contribution, we propose to learn a deep feature representation with displacement invariance for group context and introduce a method to combine the group context and single-person appearance. For all the four contributions of this thesis, we carry out extensive experiments on popular benchmarks and datasets to demonstrate the effectiveness of the proposed systems.",2018,,,,https://tel.archives-ouvertes.fr/tel-02090746/file/these.pdf
13b0348342a5c2e7c87b5eb5156c0bcf45c3c951,1,0,1,Co-Segmentation Inspired Attention Networks for Video-Based Person Re-Identification,"Person re-identification (Re-ID) is an important real-world surveillance problem that entails associating a person’s identity over a network of cameras. Video-based Re-ID approaches have gained significant attention recently since a video, and not just an image, is often available. In this work, we propose a novel Co-segmentation inspired video Re-ID deep architecture and formulate a Co-segmentation based Attention Module (COSAM) that activates a common set of salient features across multiple frames of a video via mutual consensus in an unsupervised manner. As opposed to most of the prior work, our approach is able to attend to person accessories along with the person. Our plug-and-play and interpretable COSAM module applied on two deep architectures (ResNet50, SE-ResNet50) outperform the state-of-the-art methods on three benchmark datasets.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00065,http://innovarul.github.io/docs/iccv2019_vidreid_cosegmentation.pdf
13cab2d794750cb0e04b2a5e1e9da8234ae4cf1b,1,0,1,A New Data Selection Strategy for One-Shot Video-Based Person Re-Identification,"Person re-identification under one-shot setting is an under-researched semi-supervised task in essence, in which label estimation and data selection are two key modules. In this paper, we propose an effective strategy named Collaboration between the Labeled and Unlabeled(CLU) to estimate pseudo labels and select reliable data progressively. In CLU, label estimation is an iterative process where class center representations are estimated by high-scored unlabeled data and corrected by corresponding labeled data. Then reliable data are selected based on confidence scores decided by both labeled and unlabeled data actually. Moreover, a stepwise learning framework for one-shot video-based person re-identification named SCLU is implemented to validate the effectiveness of CLU. Extensive experiments conducted on MARS and DukeMTMC-VideoReID illustrate that CLU generates pseudo labels of high quality and facilitates the subsequent model training. Meanwhile, the developed scheme SCLU yields the state-of-the-art results on the two datasets.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803723,
1401ba028e5cb029c6ed1845e95bee6fa0683a3e,0,1,0,Identity Retaining and Redundancy Reducing Gan for Person Re-Identification,"Person re-identification (ReID) models trained on one domain suffer performance degradation when tested on other domains. The existing works address this problem by domain translation with identity information preserving. However, these methods focused on adding pixel constraints to preserve identity, which also preserves a lot of redundant information. Therefore, this paper propose an identity retaining and redundancy reducing generative adversarial network (IRGAN), a domain translation method for person ReID. IRGAN is implemented by an unequal-cycle strategy, which imposes both foreground and feature constraints to domain translation. By imposing part-level feature constraints, the redundant information generated by pixel constraints can be reduced. Thus the performance of the domain translation is significantly improved. Experimental results indicate that our method is effective.",2019,2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP),,10.1109/GlobalSIP45357.2019.8969471,
14110e0aa9b7dbe2d55414e0a79d64482c3f3908,1,1,0,FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification,"Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.",2018,NeurIPS,1810.02936,,https://arxiv.org/pdf/1810.02936.pdf
141133811391cf0f4ba324a056f09f72ef362ad6,0,1,0,Weakly Supervised Segmentation Framework with Uncertainty: A Study on Pneumothorax Segmentation in Chest X-ray,"Pneumothorax is a critical abnormality that shall be treated with higher priority, and hence a computerized triage scheme is needed. A deep-learning-based framework to automatically segment the pneumothorax in chest X-rays is developed to support the realization of a triage system. Since a large number of pixel-level annotations is commonly needed but difficult to obtain for deep learning model, we propose a weakly supervised framework that allows partial training data to be weakly annotated with only image-level labels. We employ the attention masks derived from an image-level classification model as the pixel-level masks for those weakly-annotated data. Because the attention masks are rough and may have errors, we further develop a spatial label smoothing regularization technique to explore the uncertainty for the incorrectness of the attention masks in the training of segmentation model. Experimental results show that the proposed weakly supervised segmentation algorithm relieves the need of well-annotated data and yield satisfactory performance on the pneumothorax segmentation.",2019,MICCAI,,10.1007/978-3-030-32226-7_68,
143a18d0db13644f5b9ec7af15279918f7ab6d89,0,1,0,Learning What and Where from Attributes to Improve Person Re-Identification,"Due to high-level semantic cues (what) and spatial properties (where) of person attribute, some recent works try to introduce it into person re-identification. However, jointly learning attributes and identity by directly combining their loss function does not work, because of the significant difference between these two tasks. To address this problem, we propose an Attribute-identity Feature Fusion Network (AFFNet) for person re-ID, which fuses attribute and identity recognition tasks not only on loss level, but also on feature level. Specifically, to learn different features for attribute and identity, we split them into two branches to avoid the interference effects between each other. These two types of features are then concatenated to form the final representation. In the attribute branch, we propose to combine hierarchical features and use a Feature Attention Block (FAB), to mining high-level semantic and spatial information, respectively. The experimental results on two public datasets show that the proposed method performs favorably against state-of-the-art methods.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8802961,
1454b646866052ef61a838737bd74ae191210cf0,1,0,0,Tracking the Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies,"The majority of existing solutions to the Multi-Target Tracking (MTT) problem do not combine cues over a long period of time in a coherent fashion. In this paper, we present an online method that encodes long-term temporal dependencies across multiple cues. One key challenge of tracking methods is to accurately track occluded targets or those which share similar appearance properties with surrounding objects. To address this challenge, we present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window. Our method allows to correct data association errors and recover observations from occluded states. We demonstrate the robustness of our data-driven approach by tracking multiple targets using their appearance, motion, and even interactions. Our method outperforms previous works on multiple publicly available datasets including the challenging MOT benchmark.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1701.01909,10.1109/ICCV.2017.41,https://arxiv.org/pdf/1701.01909.pdf
14b3a7aa61c15fd9cab0a4d8bc2a205a89fb572e,1,1,0,Hard-Aware Point-to-Set Deep Metric for Person Re-identification,"Person re-identification (re-ID) is a highly challenging task due to large variations of pose, viewpoint, illumination, and occlusion. Deep metric learning provides a satisfactory solution to person re-ID by training a deep network under supervision of metric loss, e.g., triplet loss. However, the performance of deep metric learning is greatly limited by traditional sampling methods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S) loss with a soft hard-mining scheme. Based on the point-to-set triplet loss framework, the HAP2S loss adaptively assigns greater weights to harder samples. Several advantageous properties are observed when compared with other state-of-the-art loss functions: (1) Accuracy: HAP2S loss consistently achieves higher re-ID accuracies than other alternatives on three large-scale benchmark datasets; (2) Robustness: HAP2S loss is more robust to outliers than other losses; (3) Flexibility: HAP2S loss does not rely on a specific weight function, i.e., different instantiations of HAP2S loss are equally effective. (4) Generality: In addition to person re-ID, we apply the proposed method to generic deep metric learning benchmarks including CUB-200-2011 and Cars196, and also achieve state-of-the-art results.",2018,ECCV,1807.11206,10.1007/978-3-030-01270-0_12,https://arxiv.org/pdf/1807.11206.pdf
14d485c24094c2721756ff138d463d3bd2d34681,0,1,0,Clustering and Dynamic Sampling Based Unsupervised Domain Adaptation for Person Re-Identification,"Person Re-Identification (Re-ID) has witnessed great improvements due to the advances of the deep convolutional neural networks (CNN). Despite this, existing methods mainly suffer from the poor generalization ability to unseen scenes because of the different characteristics between different domains. To address this issue, a Clustering and Dynamic Sampling (CDS) method is proposed in this paper, which tries to transfer the useful knowledge of existing labeled source domain to the unlabeled target one. Specifically, to improve the discriminability of CNN model on source domain, we use the commonly shared pedestrian attributes (e.g., gender, hat and clothing color etc.) to enrich the information and resort to the margin-based softmax (e.g., A-Softmax) loss to train the model. For the unlabeled target domain, we iteratively cluster the samples into several centers and dynamically select informative ones from each center to fine-tune the source-domain model. Extensive experiments on DukeMTMC-reID and Market-1501 datasets show that the proposed method greatly improves the state of the arts in unsupervised domain adaptation.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME.2019.00157,http://www.cbsr.ia.ac.cn/users/zlei/papers/JLWU-ICME-2019.pdf
14d9d5fd85641f91b1c6364fd1206e88d9252ae0,1,1,0,Deep Domain-Adversarial Image Generation for Domain Generalisation,"Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on Deep Domain-Adversarial Image Generation (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach.",2020,AAAI,2003.06054,10.1609/AAAI.V34I07.7003,https://arxiv.org/pdf/2003.06054.pdf
14ea09da023d015263e239c1876f547adcd2974c,1,1,0,Centralized embedding hypersphere feature learning for person re-identification,"ABSTRACT Deep metric learning has become a general method for person re-identification (ReID) recently. Existing methods train ReID model with various loss functions to learn feature representation and identify pedestrian. However, the interaction between person features and classification vectors in the training process is rarely concerned. Distribution of pedestrian features will greatly affect convergence of the model and the pedestrian similarity computing in the test phase. In this paper, we formulate improved softmax function to learn pedestrian features and classification vectors. Our method applies pedestrian feature representation to be scattered across the coordinate space and embedding hypersphere to solve the classification problem. Then, we propose an end-to-end convolutional neural network (CNN) framework with improved softmax function to improve the performance of pedestrian features. Finally, experiments are performed on four challenging datasets. The results demonstrate that our work is competitive compared to the state-of-the-art.",2019,,,10.1080/13682199.2019.1647947,
14fc9832887a9da04d8669ef59d0315c5794d6d3,0,0,1,Pedestrian Age and Gender Identification from Far View Images Using Convolutional Neural Network,"Identification of several attributes of pedestrians such as age and gender from far view images collected from the surveillance camera has been an emerging research topic nowadays due to its growing importance and applications in several fields like identifying people, access control, security issues, etc. It is quite a difficult task to perform inference in pedestrian attributes on images which have poor resolutions since those are collected from the surveillance system. However, recognizing pedestrian’s age and gender have been used in so many important applications. In this paper, we addressed the recognition of pedestrian’s age and gender from far view images as a multi-label single class classification problem and proposed a customized convolutional neural network (CNN) model that achieves a good accuracy on seven pedestrian attributes regarding age and gender inference. We worked on PETA dataset which is the largest dataset of its kind and consists of far view images. As per comparison, our proposed model outperforms than other models with an accuracy of 85.13% on PETA dataset and 67.82% on the real dataset in terms of attribute recognition accuracy.",2019,IJCCI,,10.1007/978-981-15-3607-6_13,
1515a32936c3bb751c47a1c71d4bf7bee77ca779,1,1,0,Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation,"Person re-identification (re-ID) aims at recognizing the same person from images taken across different cameras. To address this challenging task, existing re-ID models typically rely on a large amount of labeled training data, which is not practical for real-world applications. To alleviate this limitation, researchers now targets at cross-dataset re-ID which focuses on generalizing the discriminative ability to the unlabeled target domain when given a labeled source domain dataset. To achieve this goal, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) aims at learning deep image representation with pose and domain information properly disentangled. With the learned cross-domain pose invariant feature space, our proposed PDA-Net is able to perform pose disentanglement across domains without supervision in identities, and the resulting features can be applied to cross-dataset re-ID. Both of our qualitative and quantitative results on two benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art cross-dataset Re-ID approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1909.09675,10.1109/ICCV.2019.00801,https://arxiv.org/pdf/1909.09675.pdf
1542ecb808979bfc1151f4c5b1c86067d08f084a,0,1,0,Constrained Dominant sets and Its applications in computer vision,"In this thesis, we present new schemes which leverage a constrained clustering method to solve several computer vision tasks ranging from image retrieval, image segmentation and co-segmentation, to person re-identification. In the last decades clustering methods have played a vital role in computer vision applications; herein, we focus on the extension, reformulation, and integration of a well-known graph and game theoretic clustering method known as Dominant Sets. Thus, we have demonstrated the validity of the proposed methods with extensive experiments which are conducted on several benchmark datasets.",2020,,2002.06028,,https://arxiv.org/pdf/2002.06028.pdf
154a0de6cc6dc1da7f027651848d97874be2f638,0,1,0,Deep Square Similarity Learning for Person Re-Identification in the Edge Computing System,"The increasing number of mobile phones and we-bcams has led to exponential growth in video data. Because of data transmission delay and privacy, cloud computing has no advantage in processing video data. Therefore, the edge computing is the first choice for video analysis and processing. The main computing task is placed on the edge nodes, and the central node mainly performs task scheduling. One of the key technologies of video surveillance systems is Person Re-IDentification (Re-ID). The Re-ID is used to identify whether the target pedestrian is the same person that was collected by cameras that are not spatially overlapped. In this paper, we propose a deep square similarity learning (DSSL), which is used to measure the similarity of image pairs. The algorithm takes into account the difference correlation, first-order correlation and two-order correlation of image pairs. Finally, the training data automatically adjusts the network parameters and the weights of the three correlations to minimize the loss of the training set. Consequently, the proposed DSSL can improve the performance of Re-ID. We conducted experiments on the challenging Re-ID databases CuHK03 and Male1501. The results show that the proposed DSSL method is superior to many of the most advanced Re-ID methods, and the primary recognition rate is increased by 5% and 5.47% respectively.",2018,"2018 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",,10.1109/Cybermatics_2018.2018.00117,
156382f40f9396a7a3d1fa444fa0769fc467f958,1,1,0,Scalable Person Re-Identification by Harmonious Attention,"Existing person re-identification (re-id) deep learning methods rely heavily on the utilisation of large and computationally expensive convolutional neural networks. They are therefore not scalable to large scale re-id deployment scenarios with the need of processing a large amount of surveillance video data, due to the lengthy inference process with high computing costs. In this work, we address this limitation via jointly learning re-id attention selection. Specifically, we formulate a novel harmonious attention network (HAN) framework to jointly learn soft pixel attention and hard region attention alongside simultaneous deep feature representation learning, particularly enabling more discriminative re-id matching by efficient networks with more scalable model inference and feature matching. Extensive evaluations validate the cost-effectiveness superiority of the proposed HAN approach for person re-id against a wide variety of state-of-the-art methods on four large benchmark datasets: CUHK03, Market-1501, DukeMTMC, and MSMT17.",2019,International Journal of Computer Vision,,10.1007/s11263-019-01274-1,https://link.springer.com/content/pdf/10.1007/s11263-019-01274-1.pdf
15bcccdfa9a3dc62e2c68635fa0054928fa1079d,0,1,0,Bi-directional Re-ranking for Person Re-identification,"For person re-identification, previous re-ranking methods focus on the unidirectional query-find-gallery ranking list and target to improve the performance of person re-identification. However, the matched images with the same identity may get lower ranks in the query-find-gallery ranking list, which limits the improvement of these re-ranking methods. To solve this problem, we propose the Bi-directional re-ranking method. Different from existing methods, we consider the bi-directional matching including the query-find-gallery ranking list and the gallery-find-query ranking list. In addition, we construct the graph of image relationship based on feature distances and expand the qualified images other than the initial top-k nearest images. By combining the bi-directional re-ranking performance and the k-neighbor similarity score, we re-rank the initial ranking list and get higher improvements. Extensive experiments show that the Bi-directional re-ranking method can facilitate the state-of-the-art person re-identification methods.",2019,2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),,10.1109/MIPR.2019.00017,
1635be0f2b8d2043c4f17973f8d12cd6c936a821,0,1,0,Self-Paced Video Data Augmentation by Generative Adversarial Networks with Insufficient Samples,"An effective video classification method by means of a small number of samples is urgently needed. The deficiency of samples could be alleviated by generating samples through generative adversarial networks (GANs). However, the generation of videos in a typical category remains underexplored because the complex actions and the changeable viewpoints are difficult to simulate. Thus, applying GANs to perform video augmentation is difficult. In this study, we propose a generative data augmentation method for video classification using dynamic images. The dynamic image compresses the motion information of a video into a still image, removing the interference factors such as the background. Thus, utilizing the GANs to augment dynamic images can keep the categorical motion information and save memory compared with generating videos. To deal with the uneven quality of generated images, we propose a self-paced selection method to automatically select high-quality generated samples for training. These selected dynamic images are used to enhance the features, attain regularization, and finally achieve video augmentation. Our method is verified on two benchmark datasets, namely, HMDB51 and UCF101. Experimental results show that the method remarkably improves the accuracy of video classification under the circumstance of sample insufficiency and sample imbalance.",2020,ACM Multimedia,,10.1145/3394171.3414003,
163fcdae1d216586d52b680a54e75a75d330a425,1,1,0,A Novel Hard Mining Center-Triplet Loss for Person Re-identification,"Recently, center loss and triplet loss have proved their effectiveness for person re-identification. However, they have difficulties in making optimizations of the intra/inter-class distance and the cost of computing and mining hard training samples simultaneously. To solve these problems, in this paper, we propose a hard mining center-triplet loss, a novel improved strategy of triplet loss. For one thing, it combines the advantages of center loss and triplet loss aiming at minimizing the intra-class distance and maximizing the inter-class distance. For another thing, it employs hard sample mining strategy on the level of center of class instead of individual sample to mine hard triplets with the purpose to reducing the number of hard triplets for training and further reducing the cost of computing. Finally, the results on two large-scale datasets Market1501 and DukeMTMC-reID show the robustness and efficiency of our method in making optimizations of these problems simultaneously and learning robust feature representation, which also demonstrate that our method outperforms most of existing loss function and achieves better performance for person re-identification.",2019,PRCV,,10.1007/978-3-030-31726-3_17,
164bbdfcb1b5a00e6ec5e41b69704c765993595a,1,0,0,End-to-End Domain Adaptive Attention Network for Cross-Domain Person Re-Identification,"Person re-identification (re-ID) remains challenging in a real-world scenario, as it requires a trained network to generalise to totally unseen target data in the presence of variations across domains. Recently, generative adversarial models have been widely adopted to enhance the diversity of training data. These approaches, however, often fail to generalise to other domains, as existing generative person re-identification models have a disconnect between the generative component and the discriminative feature learning stage. To address the on-going challenges regarding model generalisation, we propose an end-to-end domain adaptive attention network to jointly translate images between domains and learn discriminative re-id features in a single framework. To address the domain gap challenge, we introduce an attention module for image translation from source to target domains without affecting the identity of a person. More specifically, attention is directed to the background instead of the entire image of the person, ensuring identifying characteristics of the subject are preserved. The proposed joint learning network results in a significant performance improvement over state-of-the-art methods on several benchmark datasets.",2020,ArXiv,2005.03222,,https://arxiv.org/pdf/2005.03222.pdf
164e426a1b7bb165058e18305fe69093fe4395ae,1,1,0,Orthogonal Center Learning with Subspace Masking for Person Re-Identification,"Person re-identification aims to identify whether pairs of images belong to the same person or not. This problem is challenging due to large differences in camera views, lighting and background. One of the mainstream in learning CNN features is to design loss functions which reinforce both the class separation and intra-class compactness. In this paper, we propose a novel Orthogonal Center Learning method with Subspace Masking for person re-identification. We make the following contributions: (i) we develop a center learning module to learn the class centers by simultaneously reducing the intra-class differences and inter-class correlations by orthogonalization; (ii) we introduce a subspace masking mechanism to enhance the generalization of the learned class centers; and (iii) we devise to integrate the average pooling and max pooling in a regularizing manner that fully exploits their *Wenjie Pei and Yu-Wing Tai are joint corresponding authors. W. Wang ·W. Pei · Q. Cao · S. Liu · X. Shen · Y. Tai Youtu X-lab, Tencent E-mail: weinong.wang@hotmail.com, wenjiecoder@gmail.com, freyaqcao@tencent.com, iushuhust@gmail.com, goodshenxy@gmail.com, yuwingtai@tencent.com powers. Extensive experiments show that our proposed method consistently outperforms the state-of-the-art methods on the large-scale ReID datasets including Market-1501, DukeMTMCReID, CUHK03 and MSMT17.",2019,ArXiv,,,
16681e571a927c30c607b781a608796cc1f15ca9,0,1,0,Adversarially Erased Learning for Person Re-identification by Fully Convolutional Networks,"The generalization ability of deep person re-identification networks is subject to inadequate person data and occlusions. To relieve this dilemma, we propose a feature-level augmentation strategy, Adversarially Erased Learning Module (AELM), using two adversarial classifiers. Specifically, we utilize a classifier to identify discriminative regions and erase them to increase the variant of features. Meanwhile, we input the erased feature maps to another classifier to discover new body regions, which effectively resist occlusion of key parts. To easily perform end-to-end training for AELM, we propose a novel Identity model based on Fully Convolutional Networks (IFCN) to directly obtain body response heatmap during the forward pass by selecting corresponding class-specific feature map. Thus, the discriminative regions can be identified and erased in a convenient way. Moreover, to capture discriminative region for AELM, we present a Complementary Attention Module (CoAM) combined with channel and spatial attention to automatically focus on which feature types and positions are meaningful in the feature maps. In this paper, CoAM and AELM are cascaded into one module which is applied to the outputs of different convolutional layers to integrate mid- and high-level semantic features. Experimental results on three challenging benchmarks demonstrate the effectiveness of the proposed method.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852283,
168f08358c2740b3ffec396f037939f9a0edda9d,1,0,0,Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events,"Along with the development of the modern smart city, human-centric video analysis is encountering the challenge of diverse and complex events in real scenes. A complex event relates to dense crowds, anomalous individual, or collective behavior. However, limited by the scale of available surveillance video datasets, few existing human analysis approaches report their performances on such complex events. To this end, we present a new large-scale dataset, named Human-in-Events or HiEve (human-centric video analysis in complex events), for understanding human motions, poses, and actions in a variety of realistic events, especially crowd & complex events. It contains a record number of poses (>1M), the largest number of action labels (>56k) for complex events, and one of the largest number of trajectories lasting for long terms (with average trajectory length >480). Besides, an online evaluation server is built for researchers to evaluate their approaches. Furthermore, we conduct extensive experiments on recent video analysis approaches, demonstrating that the HiEve is a challenging dataset for human-centric video analysis. We expect that the dataset will advance the development of cutting-edge techniques in human-centric analysis and the understanding of complex events. The dataset is available at this http URL",2020,ArXiv,2005.0449,,https://arxiv.org/pdf/2005.04490.pdf
16b4074ac113a41461584ec536105dde9bbcdac9,1,0,0,Safe-Net: Solid and Abstract Feature Extraction Network for Pedestrian Attribute Recognition,"Pedestrian attribute analysis, which is a vital component in the intelligent video surveillance area, can facilitate person retrieval, searching and indexing. However, the resolution of the surveillance video is relatively low. The ability of neural networks to learn abstract features (age, gender) and solid features (hat, backpack, clothes) on low-resolution images is limited due to the inability to accurately locate the human body and the disturbance of background noise. In this study, we adopt a Semantic Parsing Technique (SPT) as a pedestrian extractor to localize informative regions of human effectively. Experiments are conducted on the RAP, PA-100k and other datasets, we show that our proposed SAFE-Net is capable of capturing abstract features and solid features, and produces competitive performance with the state-of-the-art methods.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803069,
16ba690713626a07fe68ab3300cbbe8e80160bdb,1,1,0,Learning refined attribute-aligned network with attribute selection for person re-identification,"Abstract Effective person re-identification (Re-ID) is often required in real applications. While most exiting approaches either assume the detected pedestrian bounding box well-aligned or utilize limited human structural information (pose, attention, segmentation) to calibrate the misalignment. However, the value of utilizing attributes for pedestrian alignment is still under explored. Furthermore, the hierarchy of attributes in previous works has been largely ignored, appearance feature and attribute feature are often fused in a rigid way. This directly limits the discriminatory and robustness of feature representation. In this paper, we propose a Refined Attribute-aligned Network (RAN), which consists of a coarse-alignment and a fine-alignment module. First, the pre-trained part and attribute predictor are used to generate body parts and candidate attributes. Then the body parts are used for coarse alignment and the attributes are selected by an agent. The agent is optimized with policy gradient algorithm, which can maximize the accumulative reward to increase the probability of proper attribute selection. Finally, for the fine-alignment, the attribute maps and body part features are aggregated by a bilinear-pooling layer to support accurate Re-ID. Extensive experimental results based on multiple datasets including CUHK03, DukeMTMC and Market-1501 demonstrate the superiority of our method over state-of-the-art methods.",2020,Neurocomputing,,10.1016/j.neucom.2020.03.057,
16d2d202aef2149080343687a5c9b27904165f08,1,0,0,"A Review of Tracking, Prediction and Decision Making Methods for Autonomous Driving","This literature review focuses on three important aspects of an autonomous car system: tracking (assessing the identity of the actors such as cars, pedestrians or obstacles in a sequence of observations), prediction (predicting the future motion of surrounding vehicles in order to navigate through various traffic scenarios) and decision making (analyzing the available actions of the ego car and their consequences to the entire driving context). For tracking and prediction, approaches based on (deep) neural networks and other, especially stochastic techniques, are reported. For decision making, deep reinforcement learning algorithms are presented, together with methods used to explore different alternative actions, such as Monte Carlo Tree Search.",2019,ArXiv,1909.07707,,https://arxiv.org/pdf/1909.07707.pdf
17092ce59ed56eb5fcbf467a889c7ec3e930f4c2,0,1,0,Mask-Guided Contrastive Attention Model for Person Re-identification,"Person Re-identification (ReID) is an important yet challenging task in computer vision. Due to the diverse background clutters, variations on viewpoints and body poses, it is far from solved. How to extract discriminative and robust features invariant to background clutters is the core problem. In this paper, we first introduce the binary segmentation masks to construct synthetic RGB-Mask pairs as inputs, then we design a mask-guided contrastive attention model (MGCAM) to learn features separately from the body and background regions. Moreover, we propose a novel region-level triplet loss to restrain the features learnt from different regions, i.e., pulling the features from the full image and body region close, whereas pushing the features from backgrounds away. We may be the first one to successfully introduce the binary mask into person ReID task and the first one to propose region-level contrastive learning. We evaluate the proposed method on three public datasets, including MARS, Market-1501 and CUHK03. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. Mask and code will be released upon request.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00129,http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1085.pdf
175a8162adae369bcdc76821e78f33e4b809ff37,1,0,0,Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking,"Identity Switching remains one of the main difficulties Multiple Object Tracking (MOT) algorithms have to deal with. Many state-of-the-art approaches now use sequence models to solve this problem but their training can be affected by biases that decrease their efficiency. In this paper, we introduce a new training procedure that confronts the algorithm to its own mistakes while explicitly attempting to minimize the number of switches, which results in better training. We propose an iterative scheme of building a rich training set and using it to learn a scoring function that is an explicit proxy for the target tracking metric. Whether using only simple geometric features or more sophisticated ones that also take appearance into account, our approach outperforms the state-of-the-art on several MOT benchmarks.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00477,http://infoscience.epfl.ch/record/265748/files/6191.pdf
1783edb82e4154fc1afa79bf7cdd2491c958fc6c,1,0,0,Enhancing Diversity in Teacher-Student Networks via Asymmetric branches for Unsupervised Person Re-identification,"The objective of unsupervised person re-identification (Re-ID) is to learn discriminative features without laborintensive identity annotations. State-of-the-art unsupervised Re-ID methods assign pseudo labels to unlabeled images in the target domain and learn from these noisy pseudo labels. Recently introduced Mean Teacher Model is a promising way to mitigate the label noise. However, during the training, self-ensembled teacher-student networks quickly converge to a consensus which leads to a local minimum. We explore the possibility of using an asymmetric structure inside neural network to address this problem. First, asymmetric branches are proposed to extract features in different manners, which enhances the feature diversity in appearance signatures. Then, our proposed cross-branch supervision allows one branch to get supervision from the other branch, which transfers distinct knowledge and enhances the weight diversity between teacher and student networks. Extensive experiments show that our proposed method can significantly surpass the performance of previous work on both unsupervised domain adaptation and fully unsupervised Re-ID tasks. 1",2020,ArXiv,2011.13776,,https://arxiv.org/pdf/2011.13776.pdf
179f005b94c9ddf8e21358e33a07688258cd0463,1,0,0,Spatial-temporal Fusion Network with Residual Learning and Attention Mechanism: A Benchmark for Video-Based Group Re-ID,"Video-based group re-identification (Re-ID) remains to be a meaningful task under rare study. Group Re-ID contains the information of the relationship between pedestrians, while the video sequences provide more frames to identify the person. In this paper, we propose a spatial-temporal fusion network for the group Re-ID. The network composes of the residual learning played between the CNN and the RNN in a unified network, and the attention mechanism which makes the system focus on the discriminative features. We also propose a new group Re-ID dataset DukeGroupVid to evaluate the performance of our spatial-temporal fusion network. Comprehensive experimental results on the proposed dataset and other video-based datasets, PRID-2011, i-LIDS-VID and MARS, demonstrate the effectiveness of our model.",2019,PRCV,,10.1007/978-3-030-31654-9_42,
17adb315d1fdc0757dae5d72db4a66143102d1c3,1,0,0,ABD-Net: Attentive but Diverse Person Re-Identification,"Attention mechanisms have been found effective for person re-identification (Re-ID). However, the learned ``attentive'' features are often not naturally uncorrelated or ``diverse'', which compromises the retrieval performance based on the Euclidean distance. We advocate the complementary powers of attention and diversity for Re-ID, by proposing an Attentive but Diverse Network (ABD-Net). ABD-Net seamlessly integrates attention modules and diversity regularizations throughout the entire network to learn features that are representative, robust, and more discriminative. Specifically, we introduce a pair of complementary attention modules, focusing on channel aggregation and position awareness, respectively. Then, we plug in a novel orthogonality constraint that efficiently enforces diversity on both hidden activations and weights. Through an extensive set of ablation study, we verify that the attentive and diverse terms each contributes to the performance boosts of ABD-Net. It consistently outperforms existing state-of-the-art methods on there popular person Re-ID benchmarks.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.01114,10.1109/ICCV.2019.00844,https://arxiv.org/pdf/1908.01114.pdf
1822ca8db58b0382b0c64f310840f0f875ea02c0,1,1,1,Camera Style Adaptation for Person Re-identification,"Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art. Code is available at: https://github.com/zhunzhong07/CamStyle",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.10295,10.1109/CVPR.2018.00541,https://arxiv.org/pdf/1711.10295.pdf
1852ce546b40bb4a72495f490fc5311617826acb,1,1,0,Attributes-aided Part Detection and Refinement for Person Re-identification,"Person attributes are often exploited as mid-level human semantic information to help promote the performance of person re-identification task. In this paper, unlike most existing methods simply taking attribute learning as a classification problem, we perform it in a different way with the motivation that attributes are related to specific local regions, which refers to the perceptual ability of attributes. We utilize the process of attribute detection to generate corresponding attribute-part detectors, whose invariance to many influences like poses and camera views can be guaranteed. With detected local part regions, our model extracts local features to handle the body part misalignment problem, which is another major challenge for person re-identification. The local descriptors are further refined by fused attribute information to eliminate interferences caused by detection deviation. Extensive experiments on two popular benchmarks with attribute annotations demonstrate the effectiveness of our model and competitive performance compared with state-of-the-art algorithms.",2020,Pattern Recognit.,1902.10528,10.1016/J.PATCOG.2019.107016,https://arxiv.org/pdf/1902.10528.pdf
1866c0b9ea12ba23797dc3afd6f2d01f7dc997ac,0,1,0,Learning Bias-Free Representation for Large-Scale Person Re-Identification,"Person re-identification (re-ID) aims to match the same person across disjointed cameras. In practice, misalignment is one of the key problems that limits the re-ID accuracy. There are many causes, such as detection errors from automatic detectors, human pose changes and relative movement between people and cameras. However, most current automatic re-ID solutions are still limited in mitigating such misalignments. In this paper, we propose that the misalignment scenarios in re-ID can be divided into three basic types. To further study their effects, we visualize their negative impacts in the learned embedding space and compare the changes in the re-ID accuracy. To address these specific misalignment problems, we design three subnetworks to correct the corresponding misalignments and discuss the performance gains. Moreover, by integrating all subnetworks into one unified structure, we propose a bias-free representation learning method. Unlike previous approaches that mainly focus on one specific kind of misalignment, our proposed method can eliminate the feature bias that is introduced by multiple misalignment problems. Systematic visualizations and comparisons are conducted to demonstrate that our method can efficiently correct the feature bias during the representation learning step. Evaluations on three large-scale datasets show that our proposed bias-free representation learning method can outperform the state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2937509,
187fa98a8099f4f3e618453fa09b3c57c77c51a6,0,1,0,A Deep and Structured Metric Learning Method for Robust Person Re-Identification,"Abstract Person re-identification (re-ID) is to match different images of the same pedestrian. It has attracted increasing research interest in pattern recognition and machine learning. Traditionally, person re-ID is formulated as a metric learning problem with binary classification output. However, higher order relationship, such as triplet closeness among the instances, is ignored by such pair-wise based metric learning methods. Thus, the discriminative information hidden in these data is insufficiently explored. This paper proposes a new structured loss function to push the frontier of the person re-ID performance in realistic scenarios. The new loss function introduces two margin parameters. They operate as bounds to remove positive pairs of very small distances and negative pairs of large distances. A trade-off coefficient is assigned to the loss term of negative pairs to alleviate class-imbalance problem. By using a linear function with the margin-based objectives, the gradients w.r.t. weight matrices are no longer dependent on the iterative loss values in a multiplicative manner. This makes the weights update process robust to large iterative loss values. The new loss function is compatible with many deep learning architectures, thus, it induces new deep network with pair-pruning regularization for metric learning. To evaluate the performance of the proposed model, extensive experiments are conducted on benchmark datasets. The results indicate that the new loss together with the ResNet-50 backbone has excellent feature representation ability for person re-ID.",2019,Pattern Recognit.,,10.1016/J.PATCOG.2019.106995,
18fc7885794115ffde15de3811a669110648cad8,1,1,0,A Siamese Pedestrian Alignment Network for Person Re-identification,"Deep learning methods show strong ability in extracting high-level features for images in the field of person re-identification. The produced features help inherently distinguish pedestrian identities in images. However, on deep learning models over-fitting and discriminative ability of the learnt features are still challenges for person re-identification. To alleviate model over-fitting and further enhance the discriminative ability of the learnt features, we propose siamese pedestrian alignment networks (SPAN) for person re-identification. SPAN employs two streams of PAN (pedestrian alignment networks) to increase the size of network inputs over limited training samples and effectively alleviate network over-fitting in learning. In addition, a verification loss is constructed between the two PANs to adjust the relative distance of two input pedestrians of the same or different identities in the learned feature space. Experimental verification is conducted on six large person re-identification datasets and the experimental results demonstrate the effectiveness of the proposed SPAN for person re-identification.",2019,PRCV,,10.1007/978-3-030-31654-9_35,
1909d5129fdcb2cad5c65e405889bf346419cd06,1,1,0,Generated Data With Sparse Regularized Multi-Pseudo Label for Person Re-Identification,"Recently, Generative Adversarial Network (GAN) has been adopted to improve person re-identification (person re-ID) performance through data augmentation. However, directly leveraging generated data to train a re-ID model may easily lead to over-fitting issue on these extra data and decrease the generalisability of model to learn true ID-related features from real data. Inspired by the previous approach which assigns multi-pseudo labels on the generated data to reduce the risk of over-fitting, we propose to take sparse regularization into consideration. We attempt to further improve the performance of current re-ID models by using the unlabeled generated data. The proposed Sparse Regularized Multi-Pseudo Label (SRMpL) can effectively prevent the over-fitting issue when some larger weights are assigned to the generated data. Our experiments are carried out on two publicly available person re-ID datasets (e.g., Market-1501 and DukeMTMC-reID). Compared with existing unlabeled generated data re-ID solutions, our approach achieves competitive performance. Two classical re-ID models are used to verify our sparse regularization label on generated data, i.e., an ID-embedding network and a two-stream network.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.2972768,
193089d56758ab88391d846edd08d359b1f9a863,0,1,0,A Discriminatively Learned CNN Embedding for Person Reidentification,"In this article, we revisit two popular convolutional neural networks in person re-identification (re-ID): verification and identification models. The two models have their respective advantages and limitations due to different loss functions. Here, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a Siamese network that simultaneously computes the identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two input images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus taking full usage of the re-ID annotations. Our method can be easily applied on different pretrained networks. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID benchmarks. Further, we show that our architecture can also be applied to image retrieval. The code is available at https://github.com/layumi/2016_person_re-ID.",2018,ACM Trans. Multim. Comput. Commun. Appl.,1611.05666,10.1145/3159171,https://arxiv.org/pdf/1611.05666.pdf
193d3db21fb67d234ec1255f2da64c092b16beb1,0,1,0,Semantic Consistency and Identity Mapping Multi-Component Generative Adversarial Network for Person Re-Identification,"In a real world environment, person re-identification (Re-ID) is a challenging task due to variations in lighting conditions, viewing angles, pose and occlusions. Despite recent performance gains, current person Re-ID algorithms still suffer heavily when encountering these variations. To address this problem, we propose a semantic consistency and identity mapping multi-component generative adversarial network (SC-IMGAN) which provides style adaptation from one to many domains. To ensure that transformed images are as realistic as possible, we propose novel identity mapping and semantic consistency losses to maintain identity across the diverse domains. For the Re-ID task, we propose a joint verification-identification quartet network which is trained with generated and real images, followed by an effective quartet loss for verification. Our proposed method outperforms state-of-the-art techniques on six challenging person Re-ID datasets: CUHK01, CUHK03, VIPeR, PRID2011, iLIDS and Market-1501.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093323,http://openaccess.thecvf.com/content_WACV_2020/papers/Khatun_Semantic_Consistency_and_Identity_Mapping_Multi-Component_Generative_Adversarial_Network_for_WACV_2020_paper.pdf
19a0f34440c25323544b90d9d822a212bfed0eb5,0,1,0,Discovering Underlying Person Structure Pattern with Relative Local Distance for Person Re-identification,"Modeling the underlying person structure for person re-identification (re-ID) is difficult due to diverse deformable poses, changeable camera views and imperfect person detectors. How to exploit underlying person structure information without extra annotations to improve the performance of person re-ID remains largely unexplored. To address this problem, we propose a novel Relative Local Distance (RLD) method that integrates a relative local distance constraint into convolutional neural networks (CNNs) in an end-to-end way. It is the first time that the relative local constraint is proposed to guide the global feature representation learning. Specially, a relative local distance matrix is computed by using feature maps and then regarded as a regularizer to guide CNNs to learn a structure-aware feature representation. With the discovered underlying person structure, the RLD method builds a bridge between the global and local feature representation and thus improves the capacity of feature representation for person re-ID. Furthermore, RLD also significantly accelerates deep network training compared with conventional methods. The experimental results show the effectiveness of RLD on the CUHK03, Market-1501, and DukeMTMC-reID datasets. Code is available at \url{this https URL}.",2019,ArXiv,1901.101,,https://arxiv.org/pdf/1901.10100.pdf
19a40ce3f2272ad5cd19ef031f5531d8da3cfe76,1,1,0,Deep Soft Multilabel Reference Learning 3 . 1 . Problem formulation and Overview,"Although unsupervised person re-identification (RE-ID) has drawn increasing research attentions due to its potential to address the scalability problem of supervised RE-ID models, it is very challenging to learn discriminative information in the absence of pairwise labels across disjoint camera views. To overcome this problem, we propose a deep model for the soft multilabel learning for unsupervised RE-ID. The idea is to learn a soft multilabel (real-valued label likelihood vector) for each unlabeled person by comparing the unlabeled person with a set of known reference persons from an auxiliary domain. We propose the soft multilabel-guided hard negative mining to learn a discriminative embedding for the unlabeled target domain by exploring the similarity consistency of the visual features and the soft multilabels of unlabeled target pairs. Since most target pairs are cross-view pairs, we develop the cross-view consistent soft multilabel learning to achieve the learning goal that the soft multilabels are consistently good across different camera views. To enable effecient soft multilabel learning, we introduce the reference agent learning to represent each reference person by a reference agent in a joint embedding. We evaluate our unified deep model on Market-1501 and DukeMTMC-reID. Our model outperforms the state-of-theart unsupervised RE-ID methods by clear margins. Code is available at https://github.com/KovenYu/MAR.",,,,,https://pdfs.semanticscholar.org/19a4/0ce3f2272ad5cd19ef031f5531d8da3cfe76.pdf
19ae769aa067b5bd781882cd18551f7abe01bcf1,0,1,0,Data Generation for Improving Person Re-identification,"In this paper, we explore ways to address the challenges such as data bias caused by the lack of data on person re-identification problem. We propose a data generation framework from both intra- and inter-view aspects for data augmentation to advance the performance of the existing person re-identification algorithms. Specifically, for intra-view data generation, the proposed method generates useful predicted sequences within a camera view for certain person data expansion. The generated sequences well preserve the movement information of the camera and objects, which expands the original data with longer sequence length to tackle the problem caused by insufficient data from the root. For more challenging datasets which suffer from background clutters, we propose an inter-view image generation with automatic end-to-end background substitution to eliminate the influence by the background and increase the diversity of the training data as well, which makes the recognition system learn to focus on the regions of objects and image features related to identity. We then propose a flexible data augmentation method based on our data generation approaches to improve the performance of the person re-identification and analyze the advantages and applicability of these approaches respectively. Evaluated on the challenging re-id datasets, our method outperforms existing state-of-the-art approaches without any network structure modification on the baseline neural network. Cross-datasets evaluation results show that our method has favorable generalization ability and is potentially helpful for solving similar recognition tasks due to the common issue of insufficient data.",2017,ACM Multimedia,,10.1145/3123266.3123302,
19ccdbe94c4da8f5b289311a8915085eec921f3a,1,0,0,AlignedReID++: Dynamically matching local information for person re-identification,"Abstract Person re-identification (ReID) is a challenging problem, where global features of person images are not enough to solve unaligned image pairs. Many previous works used human pose information to acquire aligned local features to boost the performance. However, those methods need extra labeled data to train an available human pose estimation model. In this paper, we propose a novel method named Dynamically Matching Local Information (DMLI) that could dynamically align local information without requiring extra supervision. DMLI could achieve better performance, especially when encountering the human pose misalignment caused by inaccurate person detection boxes. Then, we propose a deep model name AlignedReID++ which is jointly learned with global features and local feature based on DMLI. AlignedReID++ improves the performance of global features, and could use DMLI to further increase accuracy in the inference phase. Experiments show effectiveness of our proposed method in comparison with several state-of-the-art person ReID approaches. Additionally, it achieves rank-1 accuracy of 92.8% on Market1501 and 86.2% on DukeMTMCReID with ResNet50. The code and models have been released 2 .",2019,Pattern Recognit.,,10.1016/J.PATCOG.2019.05.028,
19e9cc45b70731ec8ffcb5e3c994e3b8a8707219,0,1,0,Metadata Extraction Using DeepLab V3 and Probabilistic Latent Semantic Analysis for Intelligent Visual Surveillance Systems,"Recently, surveillance cameras are ubiquitous for both real-time monitoring and recording important moments. Temporarily seamless surveillance using multiple cameras requires increasing amount of human efforts and enormous size of storage. The use of dynamic cameras further requires advanced computer vision algorithms, and is another challenge for intelligent visual surveillance. To solve those problems, we present an enhanced metadata extraction method for robust object search and a person re-identification. More specifically, the proposed method accurately extracts an object region using a modified DeepLab version 3, and then extracts metadata including representative color, size, aspect ratio, and moving trajectory of the object. The proposed metadata extraction method can be applied to a wide range of surveillance systems such as search for missing children in a large public space and crowd monitoring system.",2020,2020 IEEE International Conference on Consumer Electronics (ICCE),,10.1109/ICCE46568.2020.9043166,
19eabf03074557ca266fa818e99ac88bd58178e5,0,1,0,DeepPFCN: Deep Parallel Feature Consensus Network For Person Re-Identification,"Person re-identification aims to associate images of the same person over multiple non-overlapping camera views at different times. Depending on the human operator, manual re-identification in large camera networks is highly time consuming and erroneous. Automated person re-identification is required due to the extensive quantity of visual data produced by rapid inflation of large scale distributed multi-camera systems. The state-of-the-art works focus on learning and factorize person appearance features into latent discriminative factors at multiple semantic levels. We propose Deep Parallel Feature Consensus Network (DeepPFCN), a novel network architecture that learns multi-scale person appearance features using convolutional neural networks. This model factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels. Finally consensus is built. The feature representations learned by DeepPFCN are more robust for the person re-identification task, as we learn discriminative scale-specific features and maximize multi-scale feature fusion selections in multi-scale image inputs. We further exploit average and max pooling in separate scale for person-specific task to discriminate features globally and locally. We demonstrate the re-identification advantages of the proposed DeepPFCN model over the state-of-the-art re-identification methods on three benchmark datasets: Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%, 64.3%, and 52.6% respectively on these benchmark datasets.",2019,ArXiv,1911.07776,10.1007/978-981-15-8697-2_37,https://arxiv.org/pdf/1911.07776.pdf
1a137ab2547cb4eb3728ed6570489dec16da8ede,1,0,1,Fine-Grained Person Re-identification,"Person re-identification (re-id) plays a critical role in tracking people via surveillance systems by matching people across non-overlapping camera views at different locations. Although most re-id methods largely depend on the appearance features of a person, such methods always assume that the appearance information (particularly color) is distinguishable. However, distinguishing people who dress in very similar clothes (especially the same type of clothes, e.g. uniform) is ineffective if relying only on appearance cues. We call this problem the fine-grained person re-identification (FG re-id) problem. To solve this problem, rather than relying on clothing color, we propose to exploit two types of local dynamic pose features: motion-attentive local dynamic pose feature and joint-specific local dynamic pose feature . They are complementary to each other and describe identity-specific pose characteristics, which are found to be more unique and discriminative against similar appearance between people. A deep neural network is formed to learn these local dynamic pose features and to jointly quantify motion and global visual cues. Due to the lack of a suitable benchmark dataset for evaluating the FG re-id problem, we also contribute a fine-grained person re-identification (FGPR) dataset, which contains 358 identities. Extensive evaluations on the FGPR dataset show that our proposed model achieves the best performance compared with related person re-id and fine-grained recognition methods for FG re-id. In addition, we verify that our method is still effective for conventional video-based person re-id.",2020,International Journal of Computer Vision,,10.1007/s11263-019-01259-0,
1aaf1c043e734321ddbdceb0392fd89c062fb33a,0,1,0,Pose-Invariant Embedding for Deep Person Re-Identification,"Pedestrian misalignment, which mainly arises from detector errors and pose variations, is a critical problem for a robust person re-identification (re-ID) system. With poor alignment, the feature learning and matching process might be largely compromised. To address this problem, this paper introduces pose-invariant embedding (PIE) as a pedestrian descriptor. First, in order to align pedestrians to a standard pose, the PoseBox structure is introduced, which is generated through pose estimation followed by affine transformations. Second, to reduce the impact of pose estimation errors and information loss during the PoseBox construction, we design a PoseBox fusion (PBF) CNN architecture that takes the original image, the PoseBox, and the pose estimation confidence as input. The proposed PIE descriptor is thus defined as the fully connected layer of the PBF network for the retrieval task. Experiments are conducted on the Market-1501, CUHK03-NP, and DukeMTMC-reID datasets. We show that PoseBox alone yields decent re-ID accuracy and that when integrated in the PBF network, the learned PIE descriptor produces competitive performance compared with state-of-the-art approaches.",2019,IEEE Transactions on Image Processing,1701.07732,10.1109/TIP.2019.2910414,https://arxiv.org/pdf/1701.07732.pdf
1ae4daf7441e5566e685981801abfc2f519c5372,1,1,0,Person re-identification based on Res2Net network,"Person re-identification (re-ID) has been gaining in popularity in the research community owing to its numerous applications and growing importance in the surveillance industry. Person re-ID remains challenging due to significant intra-class variations across different cameras. In this paper, we propose a multi-task network that simultaneously computes the identification loss and verification loss. Given a pair of input images, the network predicts the identities of the two input images and whether they belong to the same identity. In order to obtain deeper feature information of pedestrians, we propose to use the latest Res2Net network for feature extraction. Experiments on several large-scale person re-ID benchmark datasets demonstrate the accuracy of our approach. For example, rank-1 accuracies are 82.67% (+0.51) and 92.93% (+0.21) for the DukeMTMC and Market-1501 datasets, respectively. The proposed method shows encouraging improvements compared with state-of-the-art methods.",2019,ArXiv,,,https://arxiv.org/pdf/1910.04061.pdf
1b256be445d42deb174a91f3937d5a488b009d3a,0,1,0,Multi-Instance Convolutional Neural Network for multi-shot person re-identification,"Abstract This paper tackles the challenging problem of multi-shot person re-identification with Convolutional Neural Network (CNN). As no prior information about how importance each instance plays, it is non-trivial to exploit the interaction information shared by the multi-shot images to help identification. Traditional CNN is in single-shot architecture, then how to utilize the interaction information provided by multi-shot images becomes an important problem to solve. Furthermore, as data augmentation methods are not strictly label-preserving, it increases the difficulty to select discriminative instance for CNN training. In this paper, we propose a weakly supervised CNN framework named Multi-Instance Convolutional Neural Network (MICNN) to solve the aforementioned problem. We develop two paradigms, i.e., Embedding-Space paradigm and Instance-Space paradigm, which re-formulate the person re-identification problem as a multi-instance verification problem with part-based features extracted by neural network. We respectively devise a specific bag-level loss function which incorporates the characteristics of the multi-instance problem for each paradigm. Experiments show that the proposed IS method outperforms many related state-of-the-art techniques on four benchmark datasets: CUHK03, SYSUm, RAiD and Market-1501.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.01.076,
1ba61a4fedc217f7bd052d1b2904567c9985dc44,1,0,0,Person Re-identification for Improved Multi-person Multi-camera Tracking by Continuous Entity Association,"We present a novel approach to person tracking within the context of entity association. In large-scale distributed multi-camera systems, person re-identification is a challenging computer vision task as the problem is two-fold: detecting entities through identification and recognition techniques; and connecting entities temporally by associating them in often crowded environments. Since tracking essentially involves linking detections, we can reformulate it purely as a re-identification task. The inherent advantage of such a reformulation lies in the ability of the tracking algorithm to effectively handle temporal discontinuities in multi-camera environments. To accomplish this, we model human appearance, face biometric and location constraints across cameras. We do not make restrictive assumptions such as number of people in a scene. Our approach is validated by using a simple and efficient inference algorithm. Results on two publicly available datasets, CamNeT and DukeMTMC, are significantly better compared to other existing methods.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.84,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w6/papers/Narayan_Person_Re-Identification_for_CVPR_2017_paper.pdf
1bc9e07a049f2ef6f0ee1ab093e3cf1e90f5b05f,1,0,0,An online learned hough forest model based on improved multi-feature fusion matching for multi-object tracking,"Object tracking has been one of the most important and active research areas in the field of computer vision. In order to solve low accuracy in object occlusion and deformation for multi-object tracking, an online learned Hough forest model based on improved multi-feature fusion matching for multi-object tracking is proposed in this paper. Firstly, positive and negative samples are selected online according to low-level association among detection responses and construct the feature model of the object with color histogram, histogram of oriented gradient (HOG) and optical flow information. Secondly, longer trajectory associations are generated based on the online learned Hough forest framework. Finally, a trajectory matching algorithm based on multi-feature fusion is proposed, and we introduce two methods of similarity measure in color histogram and feature points matching based on the Gabor filter to generate the probability matrix with the weighted factor. Therefore, it can further form the complete trajectories of the objects by associating them gradually. We evaluate our approach on three public data sets, and show significant improvements compared with state-of-art methods.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-6519-y,
1bd06b436f82e913924eae5ec61e4063c26f12f8,1,0,0,Detection-Based Online Multi-target Tracking via Adaptive Subspace Learning,"Multi-target tracking is a challenging task and becomes more so when both camera and targets are in motion and the targets have similar appearances with frequent occlusions. To maintain a proper track in such scenarios, individual target representation and accurate data association methods are prime requirements for a robust multi-target tracker. We observe that a target can be modeled as a subspace by using its feature vectors over several consecutive frames. We propose an adaptive subspace model to handle the large range of target variations throughout the track. We also develop a novel two-step parallel scheme for data association which exploits scale and location information along with appearance information to distinguish the targets. The track results for challenging videos (containing occlusions and variations in pose and illumination) indicate that the proposed method achieves better/comparable tracking accuracy in comparison to several recent trackers.",2018,ICSM,,10.1007/978-3-030-04375-9_24,
1be42f20ff086a04092b4be73e105a318ffa4322,1,1,0,Harmonious Attention Network for Person Re-identification,"Existing person re-identification (re-id) methods either assume the availability of well-aligned person bounding box images as model input or rely on constrained attention selection mechanisms to calibrate misaligned images. They are therefore sub-optimal for re-id matching in arbitrarily aligned person images potentially with large human pose variations and unconstrained auto-detection errors. In this work, we show the advantages of jointly learning attention selection and feature representation in a Convolutional Neural Network (CNN) by maximising the complementary information of different levels of visual attention subject to re-id discriminative learning constraints. Specifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images. Extensive comparative evaluations validate the superiority of this new HA-CNN model for person re-id over a wide variety of state-of-the-art methods on three large-scale benchmarks including CUHK03, Market-1501, and DukeMTMC-ReID.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1802.08122,10.1109/CVPR.2018.00243,https://arxiv.org/pdf/1802.08122.pdf
1bfe59be5b42d6b7257da4b35a408239c01ab79d,1,1,0,Adversarially Occluded Samples for Person Re-identification,"Person re-identification (ReID) is the task of retrieving particular persons across different cameras. Despite its great progress in recent years, it is still confronted with challenges like pose variation, occlusion, and similar appearance among different persons. The large gap between training and testing performance with existing models implies the insufficiency of generalization. Considering this fact, we propose to augment the variation of training data by introducing Adversarially Occluded Samples. These special samples are both a) meaningful in that they resemble real-scene occlusions, and b) effective in that they are tough for the original model and thus provide the momentum to jump out of local optimum. We mine these samples based on a trained ReID model and with the help of network visualization techniques. Extensive experiments show that the proposed samples help the model discover new discriminative clues on the body and generalize much better at test time. Our strategy makes significant improvement over strong baselines on three large-scale ReID datasets, Market1501, CUHK03 and DukeMTMC-reID.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00535,http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2864.pdf
1c074df9e26645a5e220c9bb361cbabfa80951df,1,1,0,Adversarial Binary Coding for Efficient Person Re-Identification,"Person re-identification (ReID) aims at associating persons with the same identity across different views/scenes. Most existing methods improve matching accuracy by proposing high-dimensional real-valued features to represent person images comprehensively. However, considering the increasing data scale in real-world applications, the storage and matching efficiencies should be paid attention to as well. In this paper, we propose a binary coding approach for efficient ReID, inspired by the recent advances in adversarial learning. Specifically, the proposed Adversarial Binary Coding (ABC) implicitly fits the feature distribution to the expected binary one by optimizing the Wasserstein distance. To further enhance the semantic discriminability of binary codes, we seamlessly embed the ABC into a similarity measuring deep neural network. By end-to-end learning the framework, compact and discriminative binary features are generated for efficient and accurate ReID. Extensive experiments on large-scale benchmarks demonstrate the superiority of our approach over the state-of-the-art methods in both efficiency and accuracy.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),1803.10914,10.1109/ICME.2019.00126,https://arxiv.org/pdf/1803.10914.pdf
1c086b62f113524a03bbb7d8825433e7c355c099,1,0,0,Convolutional Recurrent Predictor: Implicit Representation for Multi-Target Filtering and Tracking,"Defining a multi-target motion model, an important step of tracking algorithms, is a challenging task due to various factors, from its theoretical formulation to its computational complexity. Using fixed models (as in several generative Bayesian algorithms, such as Kalman filters) can fail to accurately predict sophisticated target motions. On the other hand, sequential learning of the motion model (for example, using recurrent neural networks) can be computationally complex and difficult due to the variable unknown number of targets. In this paper, we propose a multi-target filtering and tracking algorithm which learns the motion model, simultaneously for all targets. It does so from an implicitly represented state map and performing spatio-temporal data prediction. To this end, the multi-target state is modeled over a continuous hypothetical target space, using random finite sets and Gaussian mixture probability hypothesis density formulations. The prediction step is recursively performed using a deep convolutional recurrent neural network with a long short-term memory architecture, which is trained as a regression block, on the fly, over probability density difference maps. Our approach is evaluated over widely used pedestrian tracking benchmarks, remarkably outperforming state-of-the-art multi-target filtering algorithms, while giving competitive results when compared with other tracking approaches: The proposed approach generates an average 40.40 and 62.29 optimal sub-pattern assignment errors on MOT15 and MOT16/17 datasets, respectively, while producing 62.0%, 70.0%, and 66.9% multi-object tracking accuracy on MOT16/17, PNNL Parking Lot, and PETS09 pedestrian tracking datasets, respectively, when publicly available detectors are used.",2019,IEEE Transactions on Signal Processing,1811.00313,10.1109/TSP.2019.2931170,https://arxiv.org/pdf/1811.00313.pdf
1c1bd2ab99a33c70c41333203ed9aa94eab7da35,1,0,0,Learning From Demonstration in the Wild,"Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose video to behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.",2019,2019 International Conference on Robotics and Automation (ICRA),1811.03516,10.1109/ICRA.2019.8794412,https://arxiv.org/pdf/1811.03516.pdf
1c636230c5afdc556616f235e54ce8bdf92f8a02,0,0,1,Progressive Ensemble Networks for Zero-Shot Recognition,"Despite the advancement of supervised image recognition algorithms, their dependence on the availability of labeled data and the rapid expansion of image categories raise the significant challenge of zero-shot learning. Zero-shot learning (ZSL) aims to transfer knowledge from labeled classes into unlabeled classes to reduce human labeling effort. In this paper, we propose a novel progressive ensemble network model with multiple projected label embeddings to address zero-shot image recognition. The ensemble network is built by learning multiple image classification functions with a shared feature extraction network but different label embedding representations, which enhance the diversity of the classifiers and facilitate information transfer to unlabeled classes. A progressive training framework is then deployed to gradually label the most confident images in each unlabeled class with predicted pseudo-labels and update the ensemble network with the training data augmented by the pseudo-labels. The proposed model performs training on both labeled and unlabeled data. It can naturally bridge the domain shift problem in visual appearances and be extended to the generalized zero-shot learning scenario. We conduct experiments on multiple ZSL datasets and the empirical results demonstrate the efficacy of the proposed model.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1805.07473,10.1109/CVPR.2019.01200,https://arxiv.org/pdf/1805.07473.pdf
1c838b9f37d1b4633e2addfbca2e51df0e24b0e3,1,1,0,Person Re-identification in Videos Acquired by Mobile Devices with First-Person Point-of-View,"In recent years, we have seen the performance of video-based person Re-Identification (ReID) methods have improved considerably. However, with the influx of varying video domains, such as egocentric videos, it has become apparent that there are still many open challenges to be faced. These challenges are due to factors such as poor video quality due to ego-motion, blurriness, severe changes in lighting conditions and perspective distortions. To facilitate the research towards conquering these challenges, this paper contributes a new, first-of-its-kind dataset called EgoReID. The dataset is captured using 3 mobile cellphones with non-overlapping field-of-view. It contains 900 IDs and around 10,200 tracks with a total of 176,000 detections. Moreover, for each video we also provide 12-sensor meta data. Directly applying current approaches to our dataset results in poor performance. Considering the unique nature of our dataset, we propose a new framework which takes advantage of both visual and sensor meta data to successfully perform Person ReID. In this paper, we propose to employ human body parsing and extract weighted local video features from different body regions. In addition, we also employ sensor meta data to determine target's next camera and their estimated time of arrival, such that the search is only performed among tracks present in the predicted next camera around the estimated time. This considerably improves our ReID performance as it significantly reduces our search space.",2018,,,,
1cdd3a935ac4adaadeac2b58857e6ec3bb1b56ad,1,1,0,Real-Time Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration,"Autonomous underwater robots working with teams of human divers may need to distinguish between different divers, e.g. to recognize a lead diver or to follow a specific team member. This paper describes a technique that enables autonomous underwater robots to track divers in real time as well as to reidentify them. The approach is an extension of Simple Online Realtime Tracking (SORT) with an appearance metric (deep SORT). Initial diver detection is performed with a custom CNN designed for realtime diver detection, and appearance features are subsequently extracted for each detected diver. Next, realtime tracking-by-detection is performed with an extension of the deep SORT algorithm. We evaluate this technique on a series of videos of divers performing human-robot collaborative tasks and show that our methods result in more divers being accurately identified during tracking. We also discuss the practical considerations of applying multi-person tracking to on-board autonomous robot operations, and we consider how failure cases can be addressed during on-board tracking.",2019,ArXiv,1910.09636,,https://arxiv.org/pdf/1910.09636.pdf
1d3130279981af0515ae30abd8d19a0892c3b8c4,0,1,0,Multi-Path and Multi-Loss Network for Person Re-Identification,"In person re-identification (re-ID), most state-of-the-art models extract features by convolutional neural networks to do similarity comparison. Feature representation becomes the key task for person re-ID. However, the learned features are not good enough based on a single-path and single-loss network because the learned objective only achieves one of the multiple minima. To improve feature representation, we propose a multi-path and multi-loss network (MPMLN) and concatenate multi-path features to represent pedestrian. Subsequently, we design MPMLN based on ResNet-50 and construct an end-to-end architecture. The backbone of our proposed network shares the local parameters for multiple paths and multiple losses. It has fewer parameters than multiple independent networks. Experimental results show that our MPMLN achieves the state-of-the-art performance on the public Market1501, DukeMTMC-reID and CUHK03 person re-ID benchmarks.",2019,ICMLC '19,,10.1145/3318299.3318331,
1d44189607962453df66ecff3721e7428c2ca3d5,1,0,0,Two Layer Network Flow for Fast Data Association on Multi Object Tracking,"Multi object tracking is one interesting topics of computer science that has many applications, such as survemance system, navigation robot, sports analysis, autonomous driving car, and others. One of the main task of multi-object tracking is data association. This study discusses data association on multi-object tracking and its completion with a two-layer network flow approach. Notice that each object on a frame as a node, then there is an edge connecting each node from one frame to other frame and then finding for the set of edges that provide the greatest probability of transition from one frame to the next, or in the optimization problem better known as max-cost network flow. The probability calculation is based on position distance and similarity feature between objects. The data used in this research is 2DMOT2015. The proposed method obtains highly competitive MOTA of 20.1% compared to existing method with fast computation speed by 215.8 fps",2018,2018 International Conference on Advanced Computer Science and Information Systems (ICACSIS),,10.1109/ICACSIS.2018.8618161,
1d9f6b30a57e55af2ca6792c6ca27e32297a7d89,1,1,0,Learning Generalisable Omni-Scale Representations for Person Re-Identification,"An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efficient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without requiring any target data for model adaptation. Our code and models are released at \texttt{this https URL}.",2019,ArXiv,1910.06827,,https://arxiv.org/pdf/1910.06827.pdf
1e2f07f7231eef629c78cba4ada0c9be29d77254,1,0,0,Group Re-Identification: Leveraging and Integrating Multi-Grain Information,"This paper addresses an important yet less-studied problem: re-identifying groups of people in different camera views. Group re-identification (Re-ID) is very challenging since it is not only interfered by view-point and human pose variations in the traditional single-object Re-ID tasks, but also suffers from group layout and group member variations. To handle these issues, we propose to leverage the information of multi-grain objects: individual person and subgroups of two and three people inside a group image. We compute multi-grain representations to characterize the appearance and spatial features of multi-grain objects and evaluate the importance weight of each object for group Re-ID, so as to handle the interferences from group dynamics. We compute the optimal group-wise matching by using a multi-order matching process based on the multi-grain representation and importance weights. Furthermore, we dynamically update the importance weights according to the current matching results and then compute a new optimal group-wise matching. The two steps are iteratively conducted, yielding the final matching results.Experimental results on various datasets demonstrate the effectiveness of our approach.",2018,MM '18,,10.1145/3240508.3240539,https://weiyaolin.github.io/pdf/GroupReid_acm.pdf
1e3043eac630533f1195e56ef88002f3433409ca,0,1,0,Backbone Can Not be Trained at Once: Rolling Back to Pre-trained Network for Person Re-Identification,"In person re-identification (ReID) task, because of its shortage of trainable dataset, it is common to utilize fine-tuning method using a classification network pre-trained on a large dataset. However, it is relatively difficult to sufficiently fine-tune the low-level layers of the network due to the gradient vanishing problem. In this work, we propose a novel fine-tuning strategy that allows low-level layers to be sufficiently trained by rolling back the weights of high-level layers to their initial pre-trained weights. Our strategy alleviates the problem of gradient vanishing in low-level layers and robustly trains the low-level layers to fit the ReID dataset, thereby increasing the performance of ReID tasks. The improved performance of the proposed strategy is validated via several experiments. Furthermore, without any add-ons such as pose estimation or segmentation, our strategy exhibits state-of-the-art performance using only vanilla deep convolutional neural network architecture.",2019,AAAI,1901.0614,10.1609/aaai.v33i01.33018859,https://arxiv.org/pdf/1901.06140.pdf
1e555128519d774e42428d4791e2faa417c62854,0,1,0,Light-Weight Visual Feature Based Labeling (LVFL) for Unsupervised Person Re-identification,"Person re-identification is a vital problem in smart video surveillance environment. Performing person reidentification with an unlabeled dataset is challenging. Even though certain labeling mechanisms are available in the literature, the computation overhead prevents the system to perform re-identification task dynamically. To overcome this issue, we propose a Light-weight Visual Feature based Labeling (LVFL) method to label the person re-identification images and reduce the computation overhead than the state-of-themethods. The computation overhead is reduced at three stages namely model initialization, neural network utilization and algorithmic complexity through evaluation of the cluster quality and Cumulative Match Curve (CMC) scores. The proposed method reports a reduced computation complexity than the traditional unsupervised person re-identification methods by determining a tight bound fine-tuning with a very less CMC score trade-off. Experimental results tested on three major benchmark datasets namely DukeMTMC re-id, Market1501 and CUHK03 show that the proposed LVFL produces a decent matching performance with a computation overhead reduction of about 29 % to 41 %.",2019,2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),,10.1109/SITIS.2019.00025,
1e7b5039af9d4bf3d7bd70cbd9abbe6430dba8e1,1,0,0,Object Tracking for Autonomous Driving Systems,Object Tracking for Autonomous Driving Systems,2020,,,,https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-81.pdf
1e7ff66f484e9c7b235b4335103619c8796af419,1,0,0,Bag of Tricks and a Strong Baseline for Deep Person Re-Identification,"This paper explores a simple and efficient baseline for person re-identification (ReID). Person re-identification (ReID) with deep neural networks has made progress and achieved high performance in recent years. However, many state-of-the-arts methods design complex network structure and concatenate multi-branch features. In the literature, some effective training tricks are briefly appeared in several papers or source codes. This paper will collect and evaluate these effective training tricks in person ReID. By combining these tricks together, the model achieves 94.5% rank-1 and 85.9% mAP on Market1501 with only using global features. Our codes and models are available at https://github.com/michuanhaohao/reid-strong-baseline.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1903.07071,10.1109/CVPRW.2019.00190,https://arxiv.org/pdf/1903.07071.pdf
1ea8591721da4c3a3f8b7f1a95c1acd5066aad3e,0,1,0,Improving Person Re-Identification by Combining Siamese Convolutional Neural Network and Re-Ranking Process,"Person re-identification (re-ID) is an active task with several challenges such as variations of poses, view points, lighting and occlusion. When considering person re-ID as an image retrieval process, measuring the appearance similarity of a pairwise person images is the essential phase. Re-ranking process can improve its accuracy especially when it is based on an other similarity metric. In this paper, we propose a pipeline composed of two methods: A Siamese Convolutional Neural Network (S-CNN) and a k-reciprocal nearest neighbors (k-RNN) re-ranking algorithm. While most existing re-ranking methods ignore the importance of original distance in re-ranking, we jointly combine the S-CNN similarity measure and Jaccard distance to revise the initial ranked list. An experimental study is conducted on two benchmark person re-ID datasets (Market-1501 and Duke-MTMC-reID). The obtained results confirm the effectiveness of our method. A mAP improvement of 11.6% and 15.68% is obtained respectively for the two testing datasets.",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909902,
1eb45acc4f27fc47ed13249273cf5cb86476b154,1,0,1,A two-stream network with joint spatial-temporal distance for video-based person re-identification,,2020,J. Intell. Fuzzy Syst.,,10.3233/jifs-192067,
1ef781ecf27c6c6ad6186aa517ced87109d49d8a,1,0,0,Online Multi-object Visual Tracking using a GM-PHD Filter with Deep Appearance Learning,"We propose a new online multi-object visual tracker based on a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter in combination with a similarity Convolutional Neural Network (CNN). The GM-PHD filter estimates the states and cardinality of an unknown and time varying number of targets in the scene handling target birth, death, clutter (false alarms) and missing detections in a unified framework, and has a linear complexity with the number of targets. However, it lacks the identity of targets. We combine spatio-temporal and visual similarities obtained from object bounding boxes and deep CNN appearance features, respectively, to alleviate its shortcoming of labelling targets across frames. We apply this developed method for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. Finally, we carry out extensive experiments on Multiple Object Tracking 2016 (MOTI6) and 2017 (MOTI7) benchmark datasets and find out that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and precision.",2019,2019 22th International Conference on Information Fusion (FUSION),,,
2011d4da646f794456bebb617d1500ddf71989ed,0,1,0,Transductive Centroid Projection for Semi-supervised Large-Scale Recognition,"Conventional deep semi-supervised learning methods, such as recursive clustering and training process, suffer from cumulative error and high computational complexity when collaborating with Convolutional Neural Networks. To this end, we design a simple but effective learning mechanism that merely substitutes the last fully-connected layer with the proposed Transductive Centroid Projection (TCP) module. It is inspired by the observation of the weights in the final classification layer (called anchors) converge to the central direction of each class in hyperspace. Specifically, we design the TCP module by dynamically adding an ad hoc anchor for each cluster in one mini-batch. It essentially reduces the probability of the inter-class conflict and enables the unlabelled data functioning as labelled data. We inspect its effectiveness with elaborate ablation study on seven public face/person classification benchmarks. Without any bells and whistles, TCP can achieve significant performance gains over most state-of-the-art methods in both fully-supervised and semi-supervised manners.",2018,ECCV,,10.1007/978-3-030-01228-1_5,http://openaccess.thecvf.com/content_ECCV_2018/papers/Yu_Liu_Transductive_Centroid_Projection_ECCV_2018_paper.pdf
205c2889a998f8f1b42c45a2b3a79d78873a4ba7,1,1,1,Multiple Expert Brainstorming for Domain Adaptive Person Re-identification,"Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEB-Net adopts a mutual learning strategy, where multiple networks with different architectures are pre-trained within a source domain as expert models equipped with specific features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with different architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme about authority of experts. Extensive experiments on large-scale datasets (Market-1501 and DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the state-of-the-arts.",2020,ECCV,2007.01546,10.1007/978-3-030-58571-6_35,https://arxiv.org/pdf/2007.01546.pdf
2077108111f40649c4661be229d892467b45bdef,0,1,0,A Shape Transformation-based Dataset Augmentation Framework for Pedestrian Detection,"Deep learning-based computer vision is usually data-hungry. Many researchers attempt to augment datasets with synthesized data to improve model robustness. However, the augmentation of popular pedestrian datasets, such as Caltech and Citypersons, can be extremely challenging because real pedestrians are commonly in low quality. Due to the factors like occlusions, blurs, and low-resolution, it is significantly difficult for existing augmentation approaches, which generally synthesize data using 3D engines or generative adversarial networks (GANs), to generate realistic-looking pedestrians. Alternatively, to access much more natural-looking pedestrians, we propose to augment pedestrian detection datasets by transforming real pedestrians from the same dataset into different shapes. Accordingly, we propose the Shape Transformation-based Dataset Augmentation (STDA) framework. The proposed framework is composed of two subsequent modules, i.e. the shape-guided deformation and the environment adaptation. In the first module, we introduce a shape-guided warping field to help deform the shape of a real pedestrian into a different shape. Then, in the second stage, we propose an environment-aware blending map to better adapt the deformed pedestrians into surrounding environments, obtaining more realistic-looking pedestrians and more beneficial augmentation results for pedestrian detection. Extensive empirical studies on different pedestrian detection benchmarks show that the proposed STDA framework consistently produces much better augmentation results than other pedestrian synthesis approaches using low-quality pedestrians. By augmenting the original datasets, our proposed framework also improves the baseline pedestrian detector by up to 38% on the evaluated benchmarks, achieving state-of-the-art performance.",2019,ArXiv,1912.0701,,https://arxiv.org/pdf/1912.07010.pdf
211e452f92c8934449a3520663d927ea6ec6c9ab,0,1,0,Resetting-Label Network Based on Fast Group Loss for Person Re-Identification,"In this paper, we propose a Resetting-Label Network based on Fast Group Loss for Person Re-Identification (RLFGL-ReID). The major challenge of Re-ID lies in how to preserve the similarity of the same person against large variations caused by complex background, different illuminations, and various view angles while discriminating different individuals. To address the above-mentioned problems, we propose the RLFGL-ReID that includes resetting-label (RL) and fast group loss (FGL). Two main contributions of our network are as follows. First, a new method, the resetting-label method, which resets the ID labels, is proposed for the Re-ID network. Resetting the ID labels of each pedestrian is beneficial in maximizing the inter-group distances between each people, achieving better performance on classifying different individuals. Second, a fast group loss, i.e., an advanced version of variance group loss (VGL), is proposed to simplify the training process and accelerate the loss computation. By doing so, the network can eliminate the restriction of inputting the whole group of data when training the network. To confirm the effectiveness of our method, we extensively conduct our method on several widely used person Re-ID benchmark datasets. As the result shows, our method achieves rank@1 accuracy of 98.38% on CUHK03, 95.46% on Market1501, and 91.2% on DukeMTMC-reID, outperforming the state-of-the-art methods and confirming the advantage of our method.",2019,IEEE Access,,10.1109/ACCESS.2019.2932073,
213c7d10e143e21d3e2df90f88eeef10778ca82d,1,0,0,Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives,"While the depth of modern Convolutional Neural Networks (CNNs) surpasses that of the pioneering networks with a significant margin, the traditional way of appending supervision only over the final classifier and progressively propagating gradient flow upstream remains the training mainstay. Seminal Deeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of optimization arising from gradient flow through a long chain. However, it is still vulnerable to issues including interference to the hierarchical representation generation process and inconsistent optimization objectives, as illustrated theoretically and empirically in this paper. Complementary to previous training strategies, we propose Dynamic Hierarchical Mimicking, a generic feature learning mechanism, to advance CNN training with enhanced generalization ability. Partially inspired by DSN, we fork delicately designed side branches from the intermediate layers of a given neural network. Each branch can emerge from certain locations of the main branch dynamically, which not only retains representation rooted in the backbone network but also generates more diverse representations along its own pathway. We go one step further to promote multi-level interactions among different branches through an optimization formula with probabilistic prediction matching losses, thus guaranteeing a more robust optimization process and better representation ability. Experiments on both category and instance recognition tasks demonstrate the substantial improvements of our proposed method over its corresponding counterparts using diverse state-of-the-art CNN architectures. Code and models are publicly available at https://github.com/d-li14/DHM.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.10739,10.1109/cvpr42600.2020.00766,https://arxiv.org/pdf/2003.10739.pdf
2161f6b7ee3c0acc81603b01dc0df689683577b9,1,1,0,End-to-End Deep Learning for Person Search,"Existing person re-identification (re-id) benchmarks and algorithms mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be found from whole images. To close the gap, we investigate how to localize and match query persons from the scene images without relying on the annotations of candidate boxes. Instead of breaking it down into two separate tasks—pedestrian detection and person re-id, we propose an end-to-end deep learning framework to jointly handle both tasks. A random sampling softmax loss is proposed to effectively train the model under the supervision of sparse and unbalanced labels. On the other hand, existing benchmarks are small in scale and the samples are collected from a few fixed camera views with low scene diversities. To address this issue, we collect a largescale and scene-diversified person search dataset, which contains 18,184 images, 8,432 persons, and 99,809 annotated bounding boxes1. We evaluate our approach and other baselines on the proposed dataset, and study the influence of various factors. Experiments show that our method achieves the best result.",2016,ArXiv,,,http://arxiv.org/pdf/1604.01850v1.pdf
21836e0823c6eb8447821b3c9bb631ea16e4bdbe,1,0,0,AsNet: Asymmetrical Network for Learning Rich Features in Person Re-Identification,"Learning part-based features with multiple branches has been proven as an effective way to deliver high performance person re-identification. Existing works mostly exploit extra constraints on different branches to ensure the diversity of extracted features, which may lead to the increased complexity in network architecture and the difficulty for training. In this letter, we propose a quite simple multi-branch structure consisting of a global branch as well as a part branch in an asymmetrical way. We empirically demonstrate that such simple architecture can provide surprisingly high performance without imposing any extra constraint. On top of this, we further prompt the performance with a lightweight implementation of attention module. Extensive experimental results prove that the proposed method, termed Asymmetrical Network (AsNet), outperforms state-of-the-art methods with obvious margin on standard benchmark datasets such as Market1501, DukeMTMC, CUHK03. We believe that AsNet can serve as a strong baseline for related research and the source code is publicly available at https://github.com/www0wwwjs1/asnet.git.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.2994815,
21926651e49cfd5b7c00c62777a59e057345137e,1,0,0,"FAMNet: Joint Learning of Feature, Affinity and Multi-Dimensional Assignment for Online Multiple Object Tracking","Data association-based multiple object tracking (MOT) involves multiple separated modules processed or optimized differently, which results in complex method design and requires non-trivial tuning of parameters. In this paper, we present an end-to-end model, named FAMNet, where Feature extraction, Affinity estimation and Multi-dimensional assignment are refined in a single network. All layers in FAMNet are designed differentiable thus can be optimized jointly to learn the discriminative features and higher-order affinity model for robust MOT, which is supervised by the loss directly from the assignment ground truth. In addition, we integrate single object tracking technique and a dedicated target management scheme into the FAMNet-based tracking system to further recover false negatives and inhibit noisy target candidates generated by the external detector. The proposed method is evaluated on a diverse set of benchmarks including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising performance on all of them in comparison with state-of-the-arts.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.04989,10.1109/ICCV.2019.00627,
219f3870fc460fc88d34992d5db623cf68a3e2f0,1,0,0,Video Person Re-Identification for Wide Area Tracking Based on Recurrent Neural Networks,"In this paper, we propose a video-based person re-identification system for wide area tracking based on a recurrent neural network architecture. Given short video sequences of a person, generated by a tracking algorithm, our video re-identification algorithm links these tracklets in full trajectories across a network of non-overlapping cameras in an open-world scenario. In our system, features are first extracted from each frame using a convolutional neural network. Then, a recurrent layer combines information across time-steps. The features from all time-steps are finally combined using temporal pooling to give an overall appearance feature for the complete sequence. Our system is trained to perform re-identification using a Siamese network architecture. Experiments are conducted on the iLIDS-VID and PRID-2011 video re-identification data sets as well as in the DukeMTMC multi-camera tracking data set.",2019,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2017.2736599,https://pureadmin.qub.ac.uk/ws/files/132850325/egpaper_CSVT_Niall_v52.pdf
21c534f9e2c46f2a2d5dbc1d9a1e899462792134,1,1,0,Focus on the Visible Regions: Semantic-Guided Alignment Model for Occluded Person Re-Identification,"The occlusion problem is very common in pedestrian retrieval scenarios. When persons are occluded by various obstacles, the noise caused by the occluded area greatly affects the retrieval results. However, many previous pedestrian re-identification (Re-ID) methods ignore this problem. To solve it, we propose a semantic-guided alignment model that uses image semantic information to separate useful information from occlusion noise. In the image preprocessing phase, we use a human semantic parsing network to generate probability maps. These maps show which regions of images are occluded, and the model automatically crops images to preserve the visible parts. In the construction phase, we fuse the probability maps with the global features of the image, and semantic information guides the model to focus on visible human regions and extract local features. During the matching process, we propose a measurement strategy that only calculates the distance of public areas (visible human areas on both images) between images, thereby suppressing the spatial misalignment caused by non-public areas. Experimental results on a series of public datasets confirm that our method outperforms previous occluded Re-ID methods, and it achieves top performance in the holistic Re-ID problem.",2020,Sensors,,10.3390/s20164431,https://pdfs.semanticscholar.org/ab4a/acdd41518af1afc337bbcb53e7ede4bb8da1.pdf
21f05c1239300c64134930fda190ea6d2fc11370,1,0,0,Rethinking Temporal Fusion for Video-based Person Re-identification on Semantic and Time Aspect,,2020,AAAI,1911.12512,10.1609/aaai.v34i07.6770,https://arxiv.org/pdf/1911.12512.pdf
21f1b1048a10c2fcfb8636b1f6b64863759dfc73,1,0,0,Accurate Long-Term Multiple People Tracking Using Video and Body-Worn IMUs,"Most modern approaches for video-based multiple people tracking rely on human appearance to exploit similarities between person detections. Consequently, tracking accuracy degrades if this kind of information is not discriminative or if people change apparel. In contrast, we present a method to fuse video information with additional motion signals from body-worn inertial measurement units (IMUs). In particular, we propose a neural network to relate person detections with IMU orientations, and formulate a graph labeling problem to obtain a tracking solution that is globally consistent with the video and inertial recordings. The fusion of visual and inertial cues provides several advantages. The association of detection boxes in the video and IMU devices is based on motion, which is independent of a person’s outward appearance. Furthermore, inertial sensors provide motion information irrespective of visual occlusions. Hence, once detections in the video are associated with an IMU device, intermediate positions can be reconstructed from corresponding inertial sensor data, which would be unstable using video only. Since no dataset exists for this new setting, we release a dataset of challenging tracking sequences, containing video and IMU recordings together with ground-truth annotations. We evaluate our approach on our new dataset, achieving an average IDF1 score of 91.2%. The proposed method is applicable to any situation that allows one to equip people with inertial sensors.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3013801,https://ieeexplore.ieee.org/ielx7/83/4358840/09166762.pdf
2201de5eb360d7511b7c8f8dbee8702b1faa1bf4,0,1,0,Weakly Supervised and On-line Machine Learning for Object Tracking and Recognition in Images and Videos,"This manuscript summarises the work that I have been involved in for my post-doctoral research and in the context of my PhD supervision activities during the past 11 years. I have conducted this work partly as a post-doctoral researcher at the Idiap Research Institute in Switzerland, and partly as an associate professor at the LIRIS laboratory and INSA Lyon in France.  The technical section of the manuscript comprises two main parts: the first part being on on-line learning approaches for visual object tracking in dynamic environments, and the second part on similarity metric learning algorithms and Siamese Neural Networks (SNN).  I first present our work on on-line multiple face tracking in a dynamic indoor environment, where we focused on the aspects of track creation and removal for long-term tracking. The automatic detection of the faces to track is challenging in this setting because they may not be detected for long periods of time, and false detections may occur frequently. Our proposed algorithm consisted in a recursive Bayesian framework with a separate track creation and removal step based on Hidden Markov Models including observation likelihood functions that are learnt off-line on a set of static and dynamic features related to the tracking behaviour and the objects’ appearance. This approach is very efficient and showed superior performance to the state of the art in on-line multiple object tracking. In the same context, we further developed a new on-line algorithm to estimate the Visual Focus of Attention from videos of persons sitting in a room. This unsupervised on-line learning approach is based on an incremental k-means algorithm and is able to automatically extract, from a video stream, the targets that the persons are looking at in a room.  I further present our research on on-line learnt robust appearance models for single-object tracking. In particular we focused on the problem of model-free, on-line tracking of arbitrary objects, where the state and model of the object to track is initialised in the first frame and updated throughout the rest of the video. Our first approach, called PixelTrack, consists in a combined detection and segmentation framework that robustly learns the appearance of the object to track and avoids drift by an effective on-line co-training algorithm. This method showed excellent tracking performance on public benchmarks, both in terms of robustness and speed, and is particularly suitable for tracking deformable objects. The second tracking approach, called MCT, employs an on-line learnt discriminative classifier that stochastically samples the training instances from a dynamic probability density function that is computed from moving  and possibly distracting image background regions. The use of this motion context showed to be very effective and lead to a significant gain in the overall tracking robustness and performance. We extended this idea by designing a set of features that concisely describe the visual context of the overall scene shown in a video at a given point in time. Then, we applied several complementary tracking algorithms on a set of training videos and computed the corresponding context features for each frame. Finally, we trained a discriminative classifier off-line that estimates the  most suitable tracker for a given context, and applied it on-line in an effective tracker-selection framework. Evaluated on several different “pools” of individual trackers, the combined model lead to an increased performance in terms of accuracy and robustness on challenging public benchmarks.  In the second part of the manuscript, I present several contributions related to SNNs for similarity metric learning. First, we proposed a new objective function and training algorithm called Triangular Similarity Metric Learning that enhances the convergence behaviour and achieved state-of-the-art results on pairwise verification tasks, like face, speaker or kinship verification. Then, I present our work on SNNs for gesture classification from inertial sensor data, where we proposed a new class-balanced learning strategy operating on tuples of training samples and an objective function based on a polar sine formulation. Finally, I present several contributions on SNN with deeper and more complex Convolutional Neural Network models applied to the problem of person re-identification in images. In this context, we proposed different neural architectures and triplet learning methods that include semantic prior knowledge, e.g. on pedestrian attributes, body orientation and surrounding group context, using a combination of  supervised and weakly supervised algorithms. Also, a new learning-to-rank algorithm for SNN, called Rank-Triplet, has been introduced and successfully applied to person re-identification. These recent works achieved state-of-the-art re-identification results on challenging pedestrian image datasets and opened new perspectives for future similarity metric approaches.",2019,,,,https://pdfs.semanticscholar.org/7a79/697510e9a49a5ee63bffd1184fe92b2f4e70.pdf
224047e49f6c2e9d4f8dbcab6b6aab55eba32c49,1,0,0,Multi-camera multi-player tracking with deep player identification in sports video,"Abstract Identity switches caused by inter-object interactions remain a critical problem for multi-player tracking in real-world sports video analysis. Existing approaches utilizing the appearance model is difficult to associate detections and preserve identities due to the similar appearance of players in the same team. Instead of the appearance model, we propose a distinguishable deep representation for player identity in this paper. A robust multi-player tracker incorporating with deep player identification is further developed to produce identity-coherent trajectories. The framework consists of three parts: (1) the core component, a Deep Player Identification (DeepPlayer) model that provides an adequate discriminative feature through the coarse-to-fine jersey number recognition and the pose-guided partial feature embedding; (2) an Individual Probability Occupancy Map (IPOM) model for players 3D localization with ID; and (3) a K-Shortest Path with ID (KSP-ID) model that links nodes in the flow graph by a proposed player ID correlation coefficient. With the distinguishable identity, the performance of tracking is improved. Experiment results illustrate that our framework handles the identity switches effectively, and outperforms state-of-the-art trackers on the sports video benchmarks.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107260,
226dd72a1c9b396c1f91401bfae400228486be7b,1,0,0,Online Multi-object Tracking Using Single Object Tracker and Markov Clustering,,2019,ICIG,,10.1007/978-3-030-34113-8_46,
2279a0cbd3b9d110b45b96c3263eac7884c2c1c5,1,1,0,Improving the Style Adaptation for Unsupervised Cross-Domain Person Re-identification,"Most existing person re-identification (Re-ID) methods are based on supervised learning, in which a large amount of labeled data are required for training. However, it remains a challenge task for adapting a model trained in a labeled source domain to an unlabeled target domain, due to the domain gap. To alleviate this problem, we design an unsupervised person style transfer adaptation pipeline for the task of unsupervised domain adaptation (UDA) Re-ID. Following the pipeline, we first apply an image translator to generate style-transferred images. To preserve the ID-related information after translation, we introduce the intra-class similarity and inter-domain diversity, which are crucial properties for Re-ID. In this way, a Cross-domain Similarity Generative Adversarial Network (CSGAN) is proposed to bridge the domain gap. CSGAN is learned by jointly optimizing an image translator and a domain-invariant feature representation network (DIFRN), which constrains the CSGAN to maintain the intra-class similarity and inter-domain diversity during image-image translation. Comparison with current competitive methods demonstrates that the effectiveness of the proposed method under the setting of unsupervised domain adaptation.",2020,2020 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN48605.2020.9207712,http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20258.pdf
227ae9128ffad7f614b360f5a5de007d85699445,0,1,0,Sampling Strategies for GAN Synthetic Data,"Generative Adversarial Networks (GANs) have been used widely to generate large volumes of synthetic data. This data is being utilised for augmenting with real examples in order to train deep Convolutional Neural Networks (CNNs). Studies have shown that the generated examples lack sufficient realism to train deep CNNs and are poor in diversity. Unlike previous studies of randomly augmenting the synthetic data with real data, we present our simple, effective and easy to implement synthetic data sampling methods to train deep CNNs more efficiently and accurately. To this end, we propose to maximally utilise the parameters learned during training of the GAN itself. These include discriminator’s realism confidence score and the confidence on the target label of the synthetic data. In addition to this, we explore reinforcement learning (RL) to automatically search a subset of meaningful synthetic examples from a large pool of GAN synthetic data. We evaluate our method on two challenging face attribute classification data sets viz. AffectNet and CelebA. Our extensive experiments clearly demonstrate the need of sampling synthetic data before augmentation, which also improves the performance of one of the state-of-the-art deep CNNs in vitro.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1909.04689,10.1109/ICASSP40776.2020.9054677,https://arxiv.org/pdf/1909.04689.pdf
22895b58f1256cbad680056f785f4eda03d32bc9,1,1,0,Improving person re-identification by multi-task learning,"We propose a novel Multi-Task Learning Network (MTNET) with four different subtasks for person re-identification mission. At the same time, the attribute recognition mission can be implemented by the same network. We achieve multi-mission by integrating four subtasks, such as identity identification, identity verification, attribute identification, attribute verification. Identity loss and attribute loss can provide complementary information on a different perspective by integrating multi-context information. Identity focuses on the overall contour and appearance, while attribute focuses on local aspects and dresses of one person. Identification loss and verification loss are used to optimize the distance of samples. Identification loss used to construct a robust category space, while verification loss used to optimize the space by minimizing the distance between similar images, and maximizing the distance between dissimilar images. Moreover, an effective verification loss named constraint contrast verification (CCV) is proposed to restrict the distance between feature pair to a foreseeable range that ensures the network has better convergence. The MTNet is an end-to-end deep learning framework, all the parameters and losses can be jointly optimized. We evaluate our approach with the state-of-the-art methods on two famous dataset Market1501 and DukeMTMC-reID. Experiments demonstrate that our MTNet achieves the very competitive results.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.01.027,
22b52d3f18eaf43993a3a91053f5efe6267144e7,1,0,0,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification,"Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.",2020,ICLR,2001.01526,,https://arxiv.org/pdf/2001.01526.pdf
22c7a944a73ea9614ed4dd7693fa76af5b7fe68b,0,1,0,GDMN: Group Decision-Making Network for Person Re-Identification,"Person re-identification (re-ID) is a widely studied yet still challenging problem in computer vision. It aims to match images of the same pedestrian captured from different cameras. Recently, deep learning has been widely used for feature extraction and distance metric learning in re-ID. However, most of them only consider a certain aspect of the input data and thus will make certain mistakes during the testing process. In this paper, group decision-making (GDM) theory is introduced for comprehensive decision. Furthermore, a novel GDM network (GDMN) is proposed which consists of two sub-networks. First, proposal generation network can generate proposals based on baseline networks for the following decision-making process. Then, decision evaluation network evaluates all the proposals and makes the comprehensive decision. The proposed GDMN can analyze the merits and drawbacks of existing methods and make a better decision. The experimental results on public re-ID benchmarks show that our approach significantly improves the performance of the baseline methods and achieves competitive results compared with other state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2877841,
23034bb4e9835cae7068a0456ec6c32f6b836433,1,0,0,Blind Image Quality Prediction for Object Detection,"Automatic video data analysis tools have become indispensable components in today's imaging applications. The accuracy of automatic analysis methods relies on the quality of images or videos that are processed. It is therefore essential to introduce objective metrics for predicting the quality of images as evaluated by automatic analysis algorithms. Object detection is the first and the most important step in the process of automatic video analysis. This paper proposes a new image quality model for predicting the performance of object detection. A video data set is constructed that considers different factors related to quality degradation in the imaging process, such as reduced image resolution, noise, and blur. The performances of commonly used low-complexity object detection algorithms are obtained for the data set. A no-reference regression model based on a bagging ensemble of regression trees is built to predict the accuracy of object detection using observable features in an image. Experimental results show that the proposed model provides more accurate predictions of image quality for object detection than commonly known image quality measures such as PSNR and SSIM.",2019,2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),,10.1109/MIPR.2019.00046,
231a12de5dedddf1184ae9caafbc4a954ce584c3,1,0,0,Closed and Open World Multi-shot Person Re-identification. (Ré-identification de personnes à partir de multiples images dans le cadre de bases d'identités fermées et ouvertes),"More than ever, in today’s context of insecurity and terrorism, person re-identification based on video surveillance images has become a hot research topic. Indeed, tracking an individual not only requires to track him within a camera, but also to re-identify him when he re-appears in other cameras. In recent years, remarkable progress has been achieved in person re-identification, notably thanks to the availability of larger datasets composed of thousands of identities captured by several cameras where each camera captures multiple images per identity. Yet, we are still far from being able to automatically re-identify people accurately in real life. Considering the evolution of the available research data and the real applications needs, this thesis has followed one major research axis. How can we tackle the challenging question of open world re-identification in which the person we want to re-identify might not appear in the database of known identities? A secondary research axis consisted in relevantly making use of the multiple images that are available for each identity. The open world re-identification task we consider in this thesis consists in two subtasks: a detection task and a re-identification task. We are given a set of known identities, the gallery identities, but since we are in an open world situation, this set of known identities is supposed not to be overcomplete. Therefore, when presented a query person also referred to as probe person, the detection task aims at determining whether or not the query person is a probable known gallery person. Since the probe person might look similar to several gallery identities, the goal of the re-identification task is to the gallery identities from the most probable match to the least likely one. Our first contribution, COPReV for Closed and Open world Person RE-identification and Verification, is mainly designed for tackling the decision aspect of the problem. We formulate the re-identification task solely as a verification task and aim at determining whether two sets of images represent the same person or two distinct people. With this information, we can find out whether the query person has been identified previously or not and if so, who he is. This is achieved by learning a linear transformation of the features so that the distance between features of the same person are below a threshold and that of distinct people are above that same threshold. The",2017,,,,https://pdfs.semanticscholar.org/d579/8305ee7eee5940478d8e5bcbccd28dc119c3.pdf
237bac2345a0eaee031268204b363b24a394885f,1,1,0,Instance-Guided Context Rendering for Cross-Domain Person Re-Identification,"Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00032,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/64520/2/Gong%20Instance-guided%20context%202020%20Accepted.pdf
23d17fec839d1306c168b97fe2bd4f6569001f3d,0,1,0,Quantum-Based Creative Generation Method for a Dancing Robot,"In this paper, we propose a creative generation process model based on the quantum modeling simulation method. This model is mainly aimed at generating the running trajectory of a dancing robot and the execution plan of the dancing action. First, we used digital twin technology to establish data mapping between the robot and the computer simulation environment to realize intelligent controllability of the robot's trajectory and the dance movements described in this paper. Second, we conducted many experiments and carried out a lot of research into information retrieval, information fidelity, and result evaluation. We constructed a multilevel three-dimensional spatial quantum knowledge map (M-3DQKG) based on the coherence and entangled states of quantum modeling and simulation. Combined with dance videos, we used regions with convolutional neural networks (R-CNNs) to extract character bones and movement features to form a movement library. We used M-3DQKG to quickly retrieve information from the knowledge base, action library, and database, and then the system generated action models through a holistically nested edge detection (HED) network. The system then rendered scenes that matched the actions through generative adversarial networks (GANs). Finally, the scene and dance movements were integrated, and the creative generation process was completed. This paper also proposes the creativity generation coefficient as a means of evaluating the results of the creative process, combined with artificial brain electroenchalographic data to assist in evaluating the degree of agreement between creativity and needs. This paper aims to realize the automation and intelligence of the creative generation process and improve the creative generation effect and usability of dance movements. Experiments show that this paper has significantly improved the efficiency of knowledge retrieval and the accuracy of knowledge acquisition, and can generate unique and practical dance moves. The robot's trajectory is novel and changeable, and can meet the needs of dance performances in different scenes. The creative generation process of dancing robots combined with deep learning and quantum technology is a required field for future development, and could provide a considerable boost to the progress of human society.",2020,Frontiers in Neurorobotics,,10.3389/fnbot.2020.559366,https://pdfs.semanticscholar.org/23d1/7fec839d1306c168b97fe2bd4f6569001f3d.pdf
23d23e7dc31628b8657d9fed92afeed5c0da4f82,0,0,1,Variation Generalized Feature Learning via Intra-view Variation Adaptation,"This paper addresses the variation generalized feature learning problem in unsupervised video-based person re-identification (re-ID). With advanced tracking and detection algorithms, large-scale intraview positive samples can be easily collected by assuming that the image frames within the tracking sequence belong to the same person. Existing methods either directly use the intra-view positives to model cross-view variations or simply minimize the intra-view variations to capture the invariant component with some discriminative information loss. In this paper, we propose a Variation Generalized Feature Learning (VGFL) method to learn adaptable feature representation with intra-view positives. The proposed method can learn a discriminative re-ID model without any manually annotated cross-view positive sample pairs. It could address the unseen testing variations with a novel variation generalized feature learning algorithm. In addition, an Adaptability-Discriminability (AD) fusion method is introduced to learn adaptable video-level features. Extensive experiments on different datasets demonstrate the effectiveness of the proposed method.",2019,IJCAI,,10.24963/ijcai.2019/116,https://pdfs.semanticscholar.org/b219/e716fe66472c5027dd4dc5a34e0d3035511e.pdf
23d306f42afbc99e17229f4152d7e9076229717d,0,1,0,Incremental Re-Identification by Cross-Direction and Cross-Ranking Adaption,"Person re-identification is widely applied in video surveillance and criminal investigation applications. To achieve better performance, an additional re-ranking step is often exploited. Related methods attempt to optimize the result according to every single query independently. However, in a practical scene, as the investigation process goes on, the other queries, in particular, the gradually accumulated logs, can be used to guide or regularize the current query. In this paper, we propose to optimize the result according to not only the current query itself but also the other queries and historical logs. We respectively investigate the cross-direction and the cross-ranking constraints among different queries. Based on the investigations, we propose a reciprocal optimization method to refine multiple ranking lists reciprocally. Experiments on the VIPeR, new-protocol CUHK03, and Market-1501 datasets confirm the effectiveness of our method. In particular, on the Market-1501 dataset, with full utilization of the other queries, the method achieves an accuracy rate of 94.66% at rank-1 and a very high mAP of 75.12%, and significantly outperforms the state-of-the-art methods.",2019,IEEE Transactions on Multimedia,,10.1109/TMM.2019.2898753,
242e19b95f1cf77a74f1a99a899a8dc7d34f7c10,0,0,1,Multi-scale 3D Convolution Network for Video Based Person Re-Identification,"This paper proposes a two-stream convolution network to extract spatial and temporal cues for video based person Re-Identification (ReID). A temporal stream in this network is constructed by inserting several Multi-scale 3D (M3D) convolution layers into a 2D CNN network. The resulting M3D convolution network introduces a fraction of parameters into the 2D CNN, but gains the ability of multi-scale temporal feature learning. With this compact architecture, M3D convolution network is also more efficient and easier to optimize than existing 3D convolution networks. The temporal stream further involves Residual Attention Layers (RAL) to refine the temporal features. By jointly learning spatial-temporal attention masks in a residual manner, RAL identifies the discriminative spatial regions and temporal cues. The other stream in our network is implemented with a 2D CNN for spatial feature extraction. The spatial and temporal features from two streams are finally fused for the video based person ReID. Evaluations on three widely used benchmarks datasets, i.e., MARS, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over existing 3D convolution networks and state-of-art methods.",2019,AAAI,1811.07468,10.1609/aaai.v33i01.33018618,https://arxiv.org/pdf/1811.07468.pdf
246128382419e9b00be75d7c1a362393abb4c3ce,1,0,0,All-speed Long-term Tracker Exploiting Blur,"Objects moving at high speeds along complex trajectories often appear in videos, especially videos of sports. Such objects move over non-negligible distances during exposure time of a single frame and therefore their position in the frame is not well defined. We propose a novel approach Tracking by Deblatting (TbD) which is based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. Non-causal TbD method estimates continuous, complete and accurate object trajectories. Full trajectory is estimated by fitting piecewise polynomials, which model physically justifiable trajectories. The output is a continuous trajectory function which assigns location for every real-valued time stamp from zero to the number of frames. As a result, tracked objects are precisely localised with higher temporal resolution than by conventional trackers. The proposed TbD tracker was evaluated on a newly created dataset of videos with ground truth obtained by a high-speed camera using a novel TIoU metric that generalises the traditional Intersection over Union and measures accuracy of intra-frame trajectories. Template learning in combination with a standard long-term tracker allows for long-term object tracking in all speeds. We show that from the trajectory function precise physical calculations are possible, such as radius, gravity or sub-frame object velocity. Results show high performance of TbD in terms of TIoU, recall and speed estimation.",2019,,,,https://dspace.cvut.cz/bitstream/handle/10467/82560/F3-DP-2019-Rozumnyi-Denys-thesis.pdf
2471f190cdce98bc3bf506a3d62cce1c9bbcb09d,0,1,0,Person ReID: Optimization of Domain Adaption Though Clothing Style Transfer Between Datasets,"It is manifested that when training and testing models on different datasets, the performance of trained models will severely dropped due to the differences in style of the datasets. In person ReID task, the clothing style is a crucial factor existing in different datasets, which has not been considered in the current research. We proposed a novel approach of Optimization of Domain Adaption Though Clothing Style Transfer (ODA-CST), which includes clothing mask extraction and clothing style transfer. Firstly, we generate the clothing mask by jointly locally extracting clothing and globally detecting the person. Meanwhile, we also organize a clothing mask dataset to improve the model. Our ODA-CST can effectively generate photos with the clothing style transferred, which is the first method that tries to solve the clothing style gap in person ReID task to the best knowledge. The importance of clothing style transfer and the effectiveness of our method are verified by the experiment.",2019,PRCV,,10.1007/978-3-030-31726-3_43,
2487d098c87f400b8222004b306367bb1910adf3,0,1,0,Multiview Generative Adversarial Network and Its Application in Pearl Classification,"This paper focuses on automatic pearl classification by adopting deep learning method, using multiview pearl images. Traditionally, in order to get a satisfying classification result, we need to collect a huge number of labeled pearl images, which however is expensive in industry. Fortunately, generative adversarial network (GAN) was proposed recently to effectively expand training set, so as to improve the performance of deep learning models. We thus propose a multiview GAN (MV-GAN) to automatically expand our labeled multiview pearl images, and the expanded data set is then used to train the multistream convolutional neural network (MS-CNN). The experiments show that the utilization of images generated by the MV-GAN can indeed significantly reduce the classification error of the basic MS-CNN (up to 26.71%, relatively), obtaining the state-of-the-art results. More interestingly, it can also help the MS-CNN resist the brightness disturbance, leading to more robust classification.",2019,IEEE Transactions on Industrial Electronics,,10.1109/TIE.2018.2885684,
24913ef9f3e7bc246d9b4da65848f1ab377f78fb,0,1,0,Deeply-Learned Spatial Alignment for Person Re-Identification,"A large class of Person Re-identification (ReID) approaches identify pedestrians with the TriHard loss. Though the TriHard loss is a robust ReID method, pose variance and viewpoint in pedestrians constrain the performance. To address this problem, we introduce a spatial transformer network (STN) to align pedestrians. Then, we illustrate the generality of the STN module in pose variance problem through the evaluations on feature representation network (FRN) like VGG, ResNet and DenseNet architectures respectively. Furthermore, based on the evaluation results, we propose a robust and high-performance ReID model which consists of the STN module, DenseNet backbone and TriHard loss. And finally, we prove that our ReID model is whole differentiable by formula derivation, therefore achieving an end-to-end high-performance ReID system. The experiments show that our ReID system outperforms the state-of-art methods on Market-1501, DukeMTMC-reID and CUHK03 datasets.",2019,IEEE Access,,10.1109/ACCESS.2019.2945353,
255485196a869c98aacce60a86074fccf07c01eb,0,0,1,A Survey of Deep Learning Solutions for Multimedia Visual Content Analysis,"The increasing use of social media networks on handheld devices, especially smartphones with powerful built-in cameras, and the widespread availability of fast and high bandwidth broadband connections, added to the popularity of cloud storage, is enabling the generation and distribution of massive volumes of digital media, including images and videos. Such media is full of visual information and holds immense value in today’s world. The volume of data involved calls for automated visual content analysis systems able to meet the demands of practice in terms of efficiency and effectiveness. Deep learning (DL) has recently emerged as a prominent technique for visual content analysis. It is data-driven in nature and provides automatic end-to-end learning solutions without the need to rely explicitly on predefined handcrafted feature extractors. Another appealing characteristic of DL solutions is the performance they can achieve, once the network is trained, under practical constraints. This paper identifies eight problem domains which require analysis of visual artifacts in multimedia. It surveys the recent, authoritative, and the best performing DL solutions and lists the datasets used in the development of these deep methods for the identified types of visual analysis problems. This paper also discusses the challenges that the DL solutions face which can compromise their reliability, robustness, and accuracy for visual content analysis.",2019,IEEE Access,,10.1109/ACCESS.2019.2924733,
25a1aef6f5db096fee277f9461ab2022bfe47b31,0,1,0,Identity-Aware Attribute Recognition via Real-Time Distributed Inference in Mobile Edge Clouds,"With the development of deep learning technologies, attribute recognition and person re-identification (re-ID) have attracted extensive attention and achieved continuous improvement via executing computing-intensive deep neural networks in cloud datacenters. However, the datacenter deployment cannot meet the real-time requirement of attribute recognition and person re-ID, due to the prohibitive delay of backhaul networks and large data transmissions from cameras to datacenters. A feasible solution thus is to employ mobile edge clouds (MEC) within the proximity of cameras and enable distributed inference. In this paper, we design novel models for pedestrian attribute recognition with re-ID in an MEC-enabled camera monitoring system. We also investigate the problem of distributed inference in the MEC-enabled camera network. To this end, we first propose a novel inference framework with a set of distributed modules, by jointly considering the attribute recognition and person re-ID. We then devise a learning-based algorithm for the distributions of the modules of the proposed distributed inference framework, considering the dynamic MEC-enabled camera network with uncertainties. We finally evaluate the performance of the proposed algorithm by both simulations with real datasets and system implementation in a real testbed. Evaluation results show that the performance of the proposed algorithm with distributed inference framework is promising, by reaching the accuracies of attribute recognition and person identification up to 92.9% and 96.6% respectively, and significantly reducing the inference delay by at least 40.6% compared with existing methods.",2020,ACM Multimedia,2008.05255,10.1145/3394171.3414048,https://arxiv.org/pdf/2008.05255.pdf
25fc32f3b6b4d5f94be61427625444486a709ee2,0,1,0,Random linear interpolation data augmentation for person re-identification,"Person Re-Identification (person re-ID) is an image retrieval task which identifies the same person in different camera views. Generally, a good person re-ID model requires a large dataset containing over 100000 images to reduce the risk of over-fitting. Most current handcrafted person re-ID datasets, however, are insufficient for training a learning model with high generalization ability. In addition, the lacking of images with various levels of occlusion is still remaining in most existing datasets. Motivated by these two problems, this paper proposes a new data augmentation method called Random Linear Interpolation that can enlarge the sizes of person re-ID datasets and improve the generalization ability of the learning model. The key enabler of our approach is generating fused images by interpolating pairs of original images. In other words, the innovation of the proposed approach is considering data augmentation between two random samples. Plenty of experimental results demonstrates that the proposed method is effective to improve baseline models. On Market1501 and DukeMTMC-reID datasets, our approach can achieve 92.71 % and 82.19 % rank-1 accuracy, respectively.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-7071-5,
261641979088d2aab86fafe867a1af5be52592dc,0,1,0,Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification,"Most of the existing person re-identification (re-ID) methods achieve promising accuracy in a supervised manner, but they assume the identity labels of the target domain is available. This greatly limits the scalability of person re-ID in real-world scenarios. Therefore, the current person re-ID community focuses on the cross-domain person re-ID that aims to transfer the knowledge from a labeled source domain to an unlabeled target domain and exploits the specific knowledge from the data distribution of the target domain to further improve the performance. To reduce the gap between the source and target domains, we propose a Smoothing Adversarial Domain Attack (SADA) approach that guides the source domain images to align the target domain images by using a trained camera classifier. To stabilize a memory trace of cross-domain knowledge transfer after its initial acquisition from the source domain, we propose a p-Memory Reconsolidation (pMR) method that reconsolidates the source knowledge with a small probability p during the self-training of the target domain. With both SADA and pMR, the proposed method significantly improves the cross-domain person re-ID. Extensive experiments on Market-1501 and DukeMTMC-reID benchmarks show that our pMR-SADA outperforms all of the state-of-the-arts by a large margin.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.01058,http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf
2627036c0aefe7fb19138079a43ef4973802da9f,0,1,0,Adversarial Camera Alignment Network for Unsupervised Cross-camera Person Re-identification,"In person re-identification (Re-ID), supervised methods usually need a large amount of expensive label information, while unsupervised ones are still unable to deliver satisfactory identification performance. In this paper, we introduce a novel person Re-ID task called unsupervised cross-camera person Re-ID, which only needs the within-camera (intra-camera) label information but not cross-camera (inter-camera) labels which are more expensive to obtain. In real-world applications, the intra-camera label information can be easily captured by tracking algorithms or few manual annotations. In this situation, the main challenge becomes the distribution discrepancy across different camera views, caused by the various body pose, occlusion, image resolution, illumination conditions, and background noises in different cameras. To address this situation, we propose a novel Adversarial Camera Alignment Network (ACAN) for unsupervised cross-camera person Re-ID. It consists of the camera-alignment task and the supervised within-camera learning task. To achieve the camera alignment, we develop a Multi-Camera Adversarial Learning (MCAL) to map images of different cameras into a shared subspace. Particularly, we investigate two different schemes, including the existing GRL (i.e., gradient reversal layer) scheme and the proposed scheme called ""other camera equiprobability"" (OCE), to conduct the multi-camera adversarial task. Based on this shared subspace, we then leverage the within-camera labels to train the network. Extensive experiments on five large-scale datasets demonstrate the superiority of ACAN over multiple state-of-the-art unsupervised methods that take advantage of labeled source domains and generated images by GAN-based models. In particular, we verify that the proposed multi-camera adversarial task does contribute to the significant improvement.",2019,ArXiv,1908.00862,,https://arxiv.org/pdf/1908.00862.pdf
2646bb54019fdafcc13482d8e5a8858ee451dc6b,1,0,0,Automated sewer pipe defect tracking in CCTV videos based on defect detection and metric learning,"Abstract Computer vision techniques are widely studied for automating the interpretation of sewer pipe inspection videos, yet previous studies mainly focus on defect detection and segmentation of individual images, which cannot identify if the defect is the same one across consecutive video frames (i.e. track the defect). Nevertheless, the number of unique defects in the video is required for evaluating the pipe condition. This paper proposes a framework for tracking multiple sewer defects in CCTV videos based on defect detection and metric learning. First, a deep learning -based defect detection model and a metric learning model is developed and trained respectively using with our sewer datasets. Then, using the detections and their features from the trained models as inputs, the tracking module predicts tracks by Kalman filter and associates tracks based on defect motion, appearance features, and defect types. Our experiments demonstrate the framework is able to track sewer defects in CCTV videos with a decent IDF1 score of 57.4%. We notice that tracking performance can be influenced by the detection accuracy and configurations of the metric learning module. By analyzing the tracking results based on different weights of the distance metrics, we find that assigning larger weights to appearance and defect class distance metrics tends to increase IDF1 score, while larger motion distance weight may degrade tracking accuracy. The proposed framework contributes by tracking multiple sewer defects, which can assist with counting unique defects in inspection videos.",2021,,,10.1016/j.autcon.2020.103438,
264b0ad7094a4a2bd4e781e7272eb85dbe234623,1,0,0,Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification,"Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.",2020,AIES,2001.03071,10.1145/3375627.3375875,https://arxiv.org/pdf/2001.03071.pdf
267ebbabf19e7f67aae6359be42ee2e12479ba17,0,1,0,Identity-Guided Human Semantic Parsing for Person Re-Identification,"Existing alignment-based methods have to employ the pretrained human parsing models to achieve the pixel-level alignment, and cannot identify the personal belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In this paper, we propose the identity-guided human semantic parsing approach (ISP) to locate both the human body parts and personal belongings at pixel-level for aligned person re-ID only with person identity labels. We design the cascaded clustering on feature maps to generate the pseudo-labels of human parts. Specifically, for the pixels of all images of a person, we first group them to foreground or background and then group the foreground pixels to human parts. The cluster assignments are subsequently used as pseudo-labels of human parts to supervise the part estimation and ISP iteratively learns the feature maps and groups them. Finally, local features of both human body parts and personal belongings are obtained according to the selflearned part estimation, and only features of visible parts are utilized for the retrieval. Extensive experiments on three widely used datasets validate the superiority of ISP over lots of state-of-the-art methods. Our code is available at this https URL",2020,ECCV,2007.13467,10.1007/978-3-030-58580-8_21,https://arxiv.org/pdf/2007.13467.pdf
26ac3ee756d4a24ec31de918f54098012e17fd25,1,1,0,Deep-Person: Learning Discriminative Deep Features for Person Re-Identification,"Recently, many methods of person re-identification (Re-ID) rely on part-based feature representation to learn a discriminative pedestrian descriptor. However, the spatial context between these parts is ignored for the independent extractor to each separate part. In this paper, we propose to apply Long Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as a sequence of body parts from head to foot. Integrating the contextual information strengthens the discriminative ability of local representation. We also leverage the complementary information between local and global feature. Furthermore, we integrate both identification task and ranking task in one network, where a discriminative embedding and a similarity measurement are learned concurrently. This results in a novel three-branch framework named Deep-Person, which learns highly discriminative features for person Re-ID. Experimental results demonstrate that Deep-Person outperforms the state-of-the-art methods by a large margin on three challenging datasets including Market-1501, CUHK03, and DukeMTMC-reID. Specifically, combining with a re-ranking approach, we achieve a 90.84% mAP on Market-1501 under single query setting.",2020,Pattern Recognit.,1711.10658,10.1016/j.patcog.2019.107036,https://arxiv.org/pdf/1711.10658.pdf
26d0bffc682e0faed8f70196b793431afacf065a,1,0,0,CNN-based Density Estimation and Crowd Counting: A Survey,"Accurately estimating the number of objects in a single image is a challenging yet meaningful task and has been applied in many applications such as urban planning and public safety. In the various object counting tasks, crowd counting is particularly prominent due to its specific significance to social security and development. Fortunately, the development of the techniques for crowd counting can be generalized to other related fields such as vehicle counting and environment survey, if without taking their characteristics into account. Therefore, many researchers are devoting to crowd counting, and many excellent works of literature and works have spurted out. In these works, they are must be helpful for the development of crowd counting. However, the question we should consider is why they are effective for this task. Limited by the cost of time and energy, we cannot analyze all the algorithms. In this paper, we have surveyed over 220 works to comprehensively and systematically study the crowd counting models, mainly CNN-based density map estimation methods. Finally, according to the evaluation metrics, we select the top three performers on their crowd counting datasets and analyze their merits and drawbacks. Through our analysis, we expect to make reasonable inference and prediction for the future development of crowd counting, and meanwhile, it can also provide feasible solutions for the problem of object counting in other fields. We provide the density maps and prediction results of some mainstream algorithm in the validation set of NWPU dataset for comparison and testing. Meanwhile, density map generation and evaluation tools are also provided. All the codes and evaluation results are made publicly available at this https URL.",2020,ArXiv,2003.12783,,https://arxiv.org/pdf/2003.12783.pdf
2788a2461ed0067e2f7aaa63c449a24a237ec341,1,1,0,Random Erasing Data Augmentation,"In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: this https URL.",2020,AAAI,1708.04896,10.1609/AAAI.V34I07.7000,https://arxiv.org/pdf/1708.04896.pdf
27a4b2a5575358c57a521eb6546f4951aeee514b,0,1,0,Visualization of spatial matching features during deep person re-identification,"Person re-identification (Re-ID) based on deep learning has made great progress and achieved state-of-the-art performance in recent years. However, the end-to-end properties of deep neural networks allow us to directly feedback the output results based on its input, making the inner working mechanism of the deep person Re-ID model and its decision reasons lack of transparency and explainability. This further impedes improvements to pedestrian recognition performance. As feature visualization has been proven to be an effective method for characterizing the middle layer of a neural network, we propose a novel gradient-based visualization method to interpret the internal features learned by deep person Re-ID. Based on the idea of transfer learning, this model regards the pretrained ResNet-50 on the ImageNet dataset as a basic network for deep person Re-ID. First, the network is fine-tuned on the person Re-ID dataset to achieve pedestrian classification, and then, the gradient-based visualization of the trained network is performed to highlight important regions contributing to image similarity. Experiments conducted on the Market-1501 dataset verify that our model can not only enable the network to identify key features of an individual across different images, but also provide visual interpretation for the pedestrian classification results to improve the reliability of person Re-ID and foster trust from users regarding its decisions.",2020,,,10.1007/s12652-020-01754-0,
27b2f28fca10afaa046ae3821685d05a267287cf,1,0,0,Recurrent Metric Networks and Batch Multiple Hypothesis for Multi-Object Tracking,"Multi-object tracking aims to recover the object trajectories, given multiple detections in video frames. Object feature extraction and similarity metric are the two keys to reliably associate trajectories. In this paper, we propose the recurrent metric network (RMNet), a convolutional neural network-recurrent neural network-based similarity metric framework for the multi-object tracking. Given a reference object, the RMNet takes as input random positive and negative detections and outputs similarity scores over time. The RMNet handles the long-term temporal object variations and false object detections by its long–short memory units. With the scores from RMNet, we introduce a batch multiple hypothesis (BMH) strategy, a simple yet efficient data association method for the batch multi-object tracking. BMH generates a hypothesis tree for each object with a dual-threshold hypothesis generation approach and, then, selects the best branch (or hypothesis) for each object as the batch tracking result. Specially, we model the hypothesis selection as a 0–1 programming problem and introduce a reward function to re-find the objects in case of missing detection. We evaluate our RMNet and BMH strategy on several popular datasets: 2DMOT2015, PETS2009, TUD, and KITTI. We achieve a performance comparable or superior to those of the state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2018.2889187,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08584998.pdf
281093c4d50f8990d2086c671272a85efe90cc9f,1,1,0,Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person Re-Identification,"RGB-Infrared person re-identification (RGB-IR ReID) is a cross-modality matching problem with promising applications in the dark environment. Most existing works use Euclidean metric based constraints to resolve the discrepancy between features of different modalities. However, these methods are incapable of learning angularly discriminative feature embedding because Euclidean distance cannot measure the included angle between embedding vectors effectively. As an angularly discriminative feature space is important for classifying the human images based on their embedding vectors, in this paper, we propose a novel ranking loss function, named Bi-directional Exponential Angular Triplet Loss, to help learn an angularly separable common feature space by explicitly constraining the included angles between embedding vectors. Moreover, to help stabilize and learn the magnitudes of embedding vectors, we adopt a common space batch normalization layer. Quantitative experiments on the SYSU-MM01 and RegDB dataset support our analysis. On SYSU-MM01 dataset, the performance is improved from 7.40% / 11.46% to 38.57% / 38.61% for rank1 accuracy / mAP compared with the baseline. The proposed method can be generalized to the task of single-modality Re-ID and improves the rank-1 accuracy / mAP from 92.0% / 81.7% to 94.7% / 86.6% on the Market-1501 dataset, from 82.6% / 70.6% to 87.6% / 77.1% on the DukeMTMC-reID dataset.",2020,ArXiv,2006.00878,,https://arxiv.org/pdf/2006.00878.pdf
281c48f743500f6802477e7fdc4b19db59915f87,1,0,0,Single Image Based Metric Learning via Overlapping Blocks Model for Person Re-Identification,"Considering the pedestrian structure characteristics, the first step of many person re-identification algorithms is to divide the pedestrian images or feature map into several blocks, and then the blocks in the same location are used to calculate the special loss functions that metric the differences between different images, to reduce the distance between intra-samples and to increase the distance between inter-samples. However, most of those blocks based deep metric learning methods only measure the difference between different images, but ignored the metrics between different blocks in a single image. In this paper, we propose a novel blocks based method for person re-identification called Overlapping Blocks Model (OBM), in which an innovative strategy of overlapping partition on convolutional features is used to construct multiple overlapping blocks structure and a novel overlapping blocks loss function is utilized to measure the difference between different blocks in a single image, to ensure more blocks can bring more discriminate information and higher performance. We conduct thorough validation experiments on the Market-1501, CUHK03, and DukeMTMC-reID datasets, which demonstrate that our proposed Overlapping Blocks Model can effectively improve the recognition performance of networks by adding the multiple overlapping blocks structure and the overlapping blocks loss.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00091,http://openaccess.thecvf.com/content_CVPRW_2019/papers/CEFRL/Chen_Single_Image_Based_Metric_Learning_via_Overlapping_Blocks_Model_for_CVPRW_2019_paper.pdf
28455323f84a10000e6321f229221fb37b858c9d,0,1,0,Improved Hard Example Mining by Discovering Attribute-based Hard Person Identity,"In this paper, we propose Hard Person Identity Mining (HPIM) that attempts to refine the hard example mining to improve the exploration efficacy in person re-identification. It is motivated by following observation: the more attributes some people share, the more difficult to separate their identities. Based on this observation, we develop HPIM via a transferred attribute describer, a deep multi-attribute classifier trained from the source noisy person attribute datasets. We encode each image into the attribute probabilistic description in the target person re-ID dataset. Afterwards in the attribute code space, we consider each person as a distribution to generate his view-specific attribute codes in different practical scenarios. Hence we estimate the person-specific statistical moments from zeroth to higher order, which are further used to calculate the central moment discrepancies between persons. Such discrepancy is a ground to choose hard identity to organize proper mini-batches, without concerning the person representation changing in metric learning. It presents as a complementary tool of hard example mining, which helps to explore the global instead of the local hard example constraint in the mini-batch built by randomly sampled identities. Extensive experiments on two person re-identification benchmarks validated the effectiveness of our proposed algorithm.",2019,ArXiv,1905.02102,,https://arxiv.org/pdf/1905.02102.pdf
2878d5398a4d45f7c48d97762c2d9df643ebbf68,1,0,1,Flow-Guided Attention Networks for Video-Based Person Re-Identification,"Person Re-Identification (ReID) is an important problem in many video analytics and surveillance applications,where a person's identity must be associated across a distributed network of cameras. Video-based person ReID has recently gained much interest because it can capture discriminant spatio-temporal information that is unavailable for image-based ReID. Despite recent advances, deep learning models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, two different flow-guided attention networks are proposed for fusion with any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information.Our first proposed network called Gated Attention relies on optical flow to generate gated attention with video-based feature that embed spatially. Hence the proposed framework allows to activate a common set of salient features across multiple frames. In contrast, our second network called Mutual Attention relies on the joint attention between image and optical flow features. This enables spatial attention between both sources of features, across motion and appearance cues. Both methods introduce a feature aggregation method that produce video features by identifying salient spatio-temporal information.Extensive experiments on two challenging video datasets indicate that using the proposed flow-guided spatio-temporal attention networks allows to improve recognition accuracy considerably, outperforming state-of-the-art methods for video-based person ReID. Additionally, our Mutual Attention network is able to process longer frame sequences with a wider range of appearance variations for highly accurate recognition.",2020,ArXiv,2008.03788,,https://arxiv.org/pdf/2008.03788.pdf
290b4beb3234d1a1828939a6950d0581165f5f01,0,1,0,Pose-Guided Visible Part Matching for Occluded Person ReID,"Occluded person re-identification is a challenging task as the appearance varies substantially with various obstacles, especially in the crowd scenario. To address this issue, we propose a Pose-guided Visible Part Matching (PVPM) method that jointly learns the discriminative features with pose-guided attention and self-mines the part visibility in an end-to-end framework. Specifically, the proposed PVPM includes two key components: 1) pose-guided attention (PGA) method for part feature pooling that exploits more discriminative local features; 2) pose-guided visibility predictor (PVP) that estimates whether a part suffers the occlusion or not. As there are no ground truth training annotations for the occluded part, we turn to utilize the characteristic of part correspondence in positive pairs and self-mining the correspondence scores via graph matching. The generated correspondence scores are then utilized as pseudo-labels for visibility predictor (PVP). Experimental results on three reported occluded benchmarks show that the proposed method achieves competitive performance to state-of-the-art methods. The source codes are available at https://github.com/hh23333/PVPM",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.0023,10.1109/cvpr42600.2020.01176,https://arxiv.org/pdf/2004.00230.pdf
299806f9cc752f980c08e8aa8f13962e9ee2308b,0,1,0,Semantic-Guided Shared Feature Alignment for Occluded Person Re-IDentification,,2020,ACML,,,http://proceedings.mlr.press/v129/ren20a/ren20a.pdf
29aba6e5204f8b25a4d0a4a121624d32e60b0927,1,0,0,A Novel Teacher-Student Learning Framework For Occluded Person Re-Identification,"Person re-identification (re-id) has made great progress in recent years, but occlusion is still a challenging problem which significantly degenerates the identification performance. In this paper, we design a teacher-student learning framework to learn an occlusion-robust model from the full-body person domain to the occluded person domain. Notably, the teacher network only uses large-scale full-body person data to simulate the learning process of occluded person re-id. Based on the teacher network, the student network then trains a better model by using inadequate real-world occluded person data. In order to transfer more knowledge from the teacher network to the student network, we equip the proposed framework with a co-saliency network and a cross-domain simulator. The co-saliency network extracts the backbone features, and two separated collaborative branches are followed by the backbone. One branch is a classification branch for identity recognition and the other is a co-saliency branch for guiding the network to highlight meaningful parts without any manual annotation. The cross-domain simulator generates artificial occlusions on full-body person data under a growing probability so that the teacher network could train a cross-domain model by observing more and more occluded cases. Experiments on four occluded person re-id benchmarks show that our method outperforms other state-of-the-art methods.",2019,ArXiv,1907.03253,,https://arxiv.org/pdf/1907.03253.pdf
29cfc448bf717a760da32128d157a2a510b52ed8,0,1,0,Cross-level reinforced attention network for person re-identification,"Abstract Attention mechanism is a simple and effective method to enhance discriminative performance of person re-identification (Re-ID). Most of previous attention-based works have difficulty in eliminating the negative effects of meaningless information. In this paper, a universal module, named Cross-level Reinforced Attention (CLRA), is proposed to alleviate this issue. Firstly, we fuse features of different semantic levels using adaptive weights. The fused features, containing richer spatial and semantic information, can better guide the generation of subsequent attention module. Then, we combine hard and soft attention to improve the ability to extract important information in spatial and channel domains. Through the CLRA, the network can aggregate and propagate more discriminative semantic information. Finally, we integrate the CLRA with Harmonious Attention CNN (HA-CNN) and form a novel Cross-level Reinforced Attention CNN (CLRA-CNN) to optimize person Re-ID. Experiment results on several public benchmarks show that the proposed method achieves state-of-the-art performance.",2020,J. Vis. Commun. Image Represent.,,10.1016/j.jvcir.2020.102775,
2a752cbc4868c5ecc4aa80879953433cac9df63b,1,0,0,Optimization of a Tracking System Based on a Network of Cameras,"Tracking the motion of objects in video sequences is an important problem of computer vision that has a wide range of applications. The key points in tracking systems is the detection of an object and, if it was detected repeatedly, its reidentification. A fast correctly working tracking system that uses a number of cameras is described. The system includes detection and segmentation of objects in images, construction of their appearance descriptors, comparison of each new object with earlier collected objects, and making a decision about their reidentification. The basic system configuration is implemented in which the state-of-the art detection algorithms and models for constructing the appearance descriptors are used as the constituent parts. Based on this, the system as a whole and some of its modules are modified. A computational experiment that quantitatively confirms the advantages of the modified system over the basic system is performed.",2020,,,10.1134/s1064230720040127,
2aa0a7d65fb66435308e09ea2bc7fb09b2d484fc,1,0,0,Multiscale Path Metrics for the Analysis of Discrete Geometric Structures,"c. THIS PAGE The public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggesstions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA, 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to any oenalty for failing to comply with a collection of information if it does not display a currently valid OMB control number. PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS.",2017,,,,https://pdfs.semanticscholar.org/2aa0/a7d65fb66435308e09ea2bc7fb09b2d484fc.pdf
2b40fcc5e0019e31d599f167be0b73fbae41face,1,1,0,Re-Identification With Consistent Attentive Siamese Networks,"We propose a new deep architecture for person re-identification (re-id). While re-id has seen much recent progress, spatial localization and view-invariant representation learning for robust cross-view matching remain key, unsolved problems. We address these questions by means of a new attention-driven Siamese learning architecture, called the Consistent Attentive Siamese Network. Our key innovations compared to existing, competing methods include (a) a flexible framework design that produces attention with only identity labels as supervision, (b) explicit mechanisms to enforce attention consistency among images of the same person, and (c) a new Siamese framework that integrates attention and attention consistency, producing principled supervisory signals as well as the first mechanism that can explain the reasoning behind the Siamese framework’s predictions. We conduct extensive evaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and report competitive performance.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1811.07487,10.1109/CVPR.2019.00588,https://arxiv.org/pdf/1811.07487.pdf
2b473f109e23a4edb45c13ba9b78316afaca4f45,1,1,0,C V ] 3 A pr 2 01 9 Hyperbolic Image Embeddings,"Computer vision tasks such as image classification, image retrieval and few-shot learning are currently dominated by Euclidean and spherical embeddings, so that the final decisions about class belongings or the degree of similarity are made using linear hyperplanes, Euclidean distances, or spherical geodesic distances (cosine similarity). In this work, we demonstrate that in many practical scenarios hyperbolic embeddings provide a better alternative.",2019,,,,https://pdfs.semanticscholar.org/2b47/3f109e23a4edb45c13ba9b78316afaca4f45.pdf
2b550d1d44e2dab9e6da210b67fd1218643f7e07,1,0,0,A deep model with combined losses for person re-identification,"Abstract Person re-identification (PReID), which aims to re-identity a pedestrian from multiple non-overlapping cameras, has been significantly improved by deep learning system. There exist two popular deep frameworks used for PReID, i.e., identification and triplet models. Since these two frameworks have different loss functions, they have their own advantages and disadvantages. To combine the both advantages of two frameworks, in this paper, we propose using the triplet and Online Instance Matching (OIM) losses to train the carefully designed network. Given a triplet input images, the combined model can output the identities of the input images and learn a corresponding similarity measurement simultaneously. Experiments on CUHK01, CUHK03, Market-1501, and DukeMTMC-reID datasets demonstrate that the proposed model outperforms the compared state-of-the-art methods in most cases.",2019,Cognitive Systems Research,,10.1016/j.cogsys.2018.04.003,
2be79623e328f1af6f1d557d402af816061df045,1,0,0,STNReID: Deep Convolutional Networks With Pairwise Spatial Transformer Networks for Partial Person Re-Identification,"Partial person re-identification (ReID) is a challenging task because only partial information of person images is available for matching target persons. Few studies, especially on deep learning, have focused on matching partial person images with holistic person images. This study presents a novel deep partial ReID framework based on pairwise spatial transformer networks (STNReID), which can be trained on existing holistic person datasets. STNReID includes a spatial transformer network (STN) module and a ReID module. The STN module samples an affined image (a semantically corresponding patch) from the holistic image to match the partial image. The ReID module extracts the features of the holistic, partial, and affined images. Competition (or confrontation) is observed between the STN module and the ReID module, and two-stage training is applied to acquire a strong STNReID for partial ReID. Experimental results show that our STNReID obtains 66.7% and 54.6% rank-1 accuracies on Partial-ReID and Partial-iLIDS datasets, respectively. These values are at par with those obtained with state-of-the-art methods.",2020,IEEE Transactions on Multimedia,1903.07072,10.1109/TMM.2020.2965491,https://arxiv.org/pdf/1903.07072.pdf
2c0cdb0bd2204af7400de25e08184d2f602762cd,0,1,0,Joint patch and instance discrimination learning for unsupervised person re-identification,"Abstract The unsupervised person re-identification (re-ID) has become increasingly significant in the community because it is more scalable than the supervised method when dealing with the large-scale person re-ID. However, it is difficult to learn discriminative enough features from across-camera images without labelling information. To address this problem, we propose a joint patch and instance discrimination learning (JPIL) framework for the unsupervised person re-ID. The JPIL framework exploits a patch feature extraction model to generate patch-wise features for each input image. Then the patch discrimination learning (PDL) loss is designed to guide the model to mine the patch-wise discriminative information from unlabelled person image patches. On the other hand, we introduce the instance discrimination learning (IDL) loss to provide instance-wise supervision. The IDL loss aims to pull features of the same instance under different transformations closer and push features belonging to different instances away. Finally, we combine the PDL and IDL loss to apply the joint training. Extensive experiments on Market-1501 and DukeMTMC-reID datasets demonstrate the effectiveness of the proposed method for unsupervised person re-ID.",2020,Image Vis. Comput.,,10.1016/j.imavis.2020.104000,
2c1b8d5279cf3beb2778aae4bf61daa58a15865e,1,0,0,Tracking Passengers and Baggage Items using Multi-camera Systems at Security Checkpoints,"We introduce a novel tracking-by-detection framework to track multiple objects in overhead camera videos for airport checkpoint security scenarios where targets correspond to passengers and their baggage items. Our approach improves object detection by employing a test-time data augmentation procedure that provides multiple geometrically transformed images as inputs to a convolutional neural network. We cluster the multiple detections generated by the network using the mean-shift algorithm. The multiple hypothesis tracking algorithm then keeps track of the temporal identifiers of the targets based on the cluster centroids. Our method also incorporates a trajectory association mechanism to maintain the consistency of the temporal identifiers as passengers travel across camera views. Finally, we also introduce a simple distance-based matching mechanism to associate passengers with their luggage. An evaluation of detection, tracking, and association performances on videos obtained from multiple overhead cameras in a realistic airport checkpoint environment demonstrates the effectiveness of the proposed approach.",2020,ArXiv,2007.07924,,https://arxiv.org/pdf/2007.07924.pdf
2c56829c368a4aaa34cc5d24eabac5977057ad98,0,1,0,Video-based Person Re-identification Method Based on GAN and Pose Estimation,"As the government keeps attaching importance to public security, non-overlapping viewsheds surveillance systems have been deployed widely. It has become especially important to recognize pedestrian target through matching cameras with different viewsheds in nowadays. Deep learning relies on big data to solve overfitting. However, the current video-based person re-identification only has small data volume and homogeneous learning features. To solve this, we put forward a method to improve person re-identification based on the video. This method can increase the sample quantity by generating video frame sequence through generative adversarial network. It also adds the feature information of the pedestrian joints, which can improve the model efficiency. The experiment result shows that the modified method discussed in this paper can improve the recognition rate of public datasets effectively. In the experiments on PRID2011 and iLIDS-VID, Rank 1 attained 80.2% and 66.3%, respectively.",2020,,,,http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2020/3/PDF/zdhxb-46-3-576.pdf
2ca735150abeaf0f1312fed8d98880dbfa2bed08,1,0,0,OneShotDA: Online Multi-Object Tracker With One-Shot-Learning-Based Data Association,"Tracking multiple objects in a video sequence can be accomplished by identifying the objects appearing in the sequence and distinguishing between them. Therefore, many recent multi-object tracking (MOT) methods have utilized re-identification and distance metric learning to distinguish between objects by computing the similarity/dissimilarity scores. However, it is difficult to generalize such approaches for arbitrary video sequences, because some important information, such as the number of objects (classes) in a video, is not known in advance. Therefore, in this study, we applied a one-shot learning framework to the MOT problem. Our algorithm tracks objects by classifying newly observed objects into existing tracks, irrespective of the number of objects appearing in a video frame. The proposed method, called OneShotDA, exploits the one-shot learning framework based on an attention mechanism. Our neural network learns to classify unseen data samples using labels from a support set. Once the network has been trained, it predicts correct labels for newly received detection results based on the set of existing tracks. To analyze the effectiveness of our method, it was tested on the MOTchallenge benchmark datasets (MOT16 and MOT17 datasets). The results reveal that the performance of the proposed method was comparable with those of current state-of-the-art methods. In particular, it is noteworthy that the proposed method ranked first among the online trackers on the MOT17 benchmark.",2020,IEEE Access,,10.1109/ACCESS.2020.2975912,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09007402.pdf
2cd9cfe20d7553cccf153ec9df01b916609d8639,0,1,0,Person Identification by Walking Gesture Using Skeleton Sequences,"When coping with person identification problem, previous approaches either directly take raw RGB as inputs or use more sophisticated devices to capture other information. However, most of the approaches are sensitive to the changes of environment and different clothing, little variation may lead to failure identification. Recent research shows that “gait” (i.e., a person’s manner of walking) is a unique trait of a human being. Motivated by this, we propose a novel method to identify people by their gaits. In order to figure out the characteristic of individual gait, we are interested in utilizing skeletal information, which is more robust to the diversification of environment and appearance. To effectively utilize skeletal data, we analyze the spatial relationship of joints and transform the 3D skeleton coordinates into relative distances and angles between joints, and then we use a bidirectional long short-term memory neural network to explore the temporal information of the skeleton sequences. Results show that our proposed method can outperform previous methods on BIWI and IAS-Lab datasets by gaining 10.33% accuracy improvement on average.",2020,ACIVS,,10.1007/978-3-030-40605-9_18,
2d5e551b2091a2ea9f3d16cc6a634b28795a0c24,0,1,0,Local to Global with Multi-Scale Attention Network for Person Re-Identification,"Recently, part-based person re-identification methods attract lots of attention and largely improve the accuracy. However, due to the large variations in camera occlusion, pose change and misalignment, the corresponding part regions of different images from a same person may miss the key cues. In this paper, we proposed a local to global with multi-scale attention network (LGMANet), which sufficiently exploits the contextual information and spacial attention information. Our proposed model includes two branches. One is local to global branch. By pooling operation, an image generates the feature maps of different dimensions. Then, we learn local to global descriptors by partitioning these feature maps with the same scale. The other is multi-scale attention branch, which captures the contextual dependencies from different convolution layers and further improves the discriminative ability of the image feature. Experimental results demonstrate that our method achieves the state-of-the-art results on three benchmark datasets, Market-1501, DukeMTMC-reID and CUHK03.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803292,
2d89bc8037bc6148792566b636b94099ef44d5bf,1,1,0,Person Recognition in Personal Photo Collections,"People nowadays share large parts of their personal lives through social media. Being able to automatically recognise people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task, however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social media photos sets new challenges for computer vision, including non-cooperative subjects (e.g., backward viewpoints, unusual poses) and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1] benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social groups, and events. Compared the conference version of the paper [2] , this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3] ), (2) new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the head viewpoint-wise breakdown of performance, and (5) results on the open-world setup.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2018.2877588,
2d95f0ac271d018352bcb9e73a869cbf2821638e,1,0,0,Automatic Adaptation of Person Association for Multiview Tracking in Group Activities,"Reliable markerless motion tracking of multiple people participating in complex group activity from multiple handheld cameras is challenging due to frequent occlusions, strong viewpoint and appearance variations, and asynchronous video streams. The key to solving this problem is to reliably associate the same person across distant viewpoint and temporal instances. In this work, we combine motion tracking, mutual exclusion constraints, and multiview geometry in a multitask learning framework to automatically adapt a generic person appearance descriptor to the domain videos. Tracking is formulated as a spatiotemporally constrained clustering using the adapted person descriptor. Physical human constraints are exploited to reconstruct accurate and consistent 3D skeletons for every person across the entire sequence. We show significant improvement in association accuracy (up to 18%) in events with up to 60 people and 3D human skeleton reconstruction (5 to 10 times) over the baseline for events captured ""in the wild"".",2018,ArXiv,,,http://www.cs.cmu.edu/~mvo/index_files/Papers/3DTracking.pdf
2dd1b7f699b4987168cf8f2db0ef1d4436a516dd,0,1,0,People Re-Identification by Multi-Branch CNN with Multi-Scale Features,"People re-identification is a retrieval problem to find a person of interest among a gallery of person images from different cameras in various poses or view angles. How to get a strong feature representation for a person image plays an important role in performing people re-identification. In this paper, we present a novel end-to-end framework that extracts both global and local features with multiple scales to generate more discriminative representations. The model we design is a multi-branch network consisting of one global branch to obtain features of the whole input image from different convolutional layers and several local branches to obtain features from horizontal partitions in different granularities. Our method achieves state-of-the-art results on three challenging datasets (Market-1501, CUHK03 and DukeMTMC-reid).",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803796,
2de1931999db8369d78b4d034e916943286141bb,1,1,0,Enforcing Affinity Feature Learning through Self-attention for Person Re-identification,"Person re-identification is the task of recognizing an individual across heterogeneous non-overlapping camera views. It has become a crucial capability needed by many applications in public space video surveillance. However, it remains a challenging task due to the subtle inter-class similarity and large intra-class variation found in person images. Current CNN-based approaches have focused and investigated traditional identification or verification frameworks. Such approaches typically use the whole input image including the background and fail to pay attention to specific body parts, deviating the feature representation learning from informative parts. In this article, we introduce a self-attention mechanism coupled with cross-resolution to improve the feature representation learning of person re-identification task. The proposed self-attention module reinforces the most informative parts from a high-resolution image using its internal representation at the low-resolution. In particular, the model is fed with a pair of images on a different scale and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learns a filtering attention heatmap. The feature maps on the lower branch are subsequently weighted to reflect the importance of each patch of the input image using a softmax operation; whereas, on the upper branch, we apply a max pooling operation to downsample the high-resolution feature map before element-wise multiplied with the attention heatmap. Our attention module helps the network learn the most discriminative visual features of multiple regions of the image and is specifically optimized to attend and enforce feature representation at different scales. Extensive experiments on three large-scale datasets show that network architectures augmented with our self-attention module systematically improve their accuracy and outperform various state-of-the-art models by a large margin.",2020,ACM Trans. Multim. Comput. Commun. Appl.,,10.1145/3377352,
2dfb58a8c7b070a49bff87095a6ee961e2f8cdbf,1,1,0,Generalizing Person Re-Identification by Camera-Aware Invariance Learning and Cross-Domain Mixup,"Despite the impressive performance under the single-domain setup, current fully-supervised models for person re-identification (re-ID) degrade significantly when deployed to an unseen domain. According to the characteristics of cross-domain re-ID, such degradation is mainly attributed to the dramatic variation within the target domain and the severe shift between the source and target domain. To achieve a model that generalizes well to the target domain, it is desirable to take both issues into account. In terms of the former issue, one of the most successful solutions is to enforce consistency between nearest-neighbors in the embedding space. However, we find that the search of neighbors is highly biased due to the discrepancy across cameras. To this end, we improve the vanilla neighborhood invariance approach by imposing the constraint in a camera-aware manner. As for the latter issue, we propose a novel cross-domain mixup scheme. It alleviates the abrupt transfer by introducing the interpolation between the two domains as a transition state. Extensive experiments on three public benchmarks demonstrate the superiority of our method. Without any auxiliary data or models, it outperforms existing state-of-the-arts by a large margin. The code is available at https://github.com/LuckyDC/generalizing-reid.",2020,ECCV,,10.1007/978-3-030-58555-6_14,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600222.pdf
2e192aaed7fe365c26a5e004b0d586370f57080f,1,1,0,Cluster-level Feature Alignment for Person Re-identification,"Instance-level alignment is widely exploited for person re-identification, e.g. spatial alignment, latent semantic alignment and triplet alignment. This paper probes another feature alignment modality, namely cluster-level feature alignment across whole dataset, where the model can see not only the sampled images in local mini-batch but the global feature distribution of the whole dataset from distilled anchors. Towards this aim, we propose anchor loss and investigate many variants of cluster-level feature alignment, which consists of iterative aggregation and alignment from the overview of dataset. Our extensive experiments have demonstrated that our methods can provide consistent and significant performance improvement with small training efforts after the saturation of traditional training. In both theoretical and experimental aspects, our proposed methods can result in more stable and guided optimization towards better representation and generalization for well-aligned embedding.",2020,ArXiv,2008.0681,,https://arxiv.org/pdf/2008.06810.pdf
2e361f170521a7e3f661a8ed2cc266a32349446a,1,1,0,Compact Triplet Loss for person re-identification in camera sensor networks,"Abstract The triplet loss in deep learning has achieved promising results for person re-identification (re-ID) in camera sensor networks. However, it neglects the relationship among pedestrian images captured from different sensors, which results in a relatively large intra-class variation. In this paper, we propose a novel loss function named Compact Triplet Loss (CTL) for training Convolutional Neural Networks (CNNs), which not only decreases the intra-class variation but also increases the inter-class variation to improve the generalization ability of person re-ID model. Specifically, CTL simultaneously considers three aspects for pedestrian representations. It pushes the pedestrian images to be closer to their corresponding centers and meanwhile forces different centers are away from each other. In addition, CTL forces the distance between the positive sample pair is smaller than that of the negative sample pair. Finally, we integrate the proposed CTL and the cross-entropy loss to perform multi-task learning. We evaluate the proposed method on Market1501, DukeMTMCreID and CUHK03, and the experimental results reveal our method exceeds other state-of-the-art methods by a large margin.",2019,Ad Hoc Networks,,10.1016/J.ADHOC.2019.101984,
2e4e3d80e0a789dcf45e61401c8af4e3fa96dfea,1,1,0,Batch DropBlock Network for Person Re-Identification and Beyond,"Since the person re-identification task often suffers from the problem of pose changes and occlusions, some attentive local features are often suppressed when training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network which is a two branch network composed of a conventional ResNet-50 as the global branch and a feature dropping branch.The global branch encodes the global salient representations.Meanwhile, the feature dropping branch consists of an attentive feature learning module called Batch DropBlock, which randomly drops the same region of all input feature maps in a batch to reinforce the attentive feature learning of local regions.The network then concatenates features from both branches and provides a more comprehensive and spatially distributed feature representation. Albeit simple, our method achieves state-of-the-art on person re-identification and it is also applicable to general metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existed works by a large margin (more than 6%).",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00379,
2edebc7062bf3bf51f6248b9436f18fdbc3d5be5,0,1,0,Person Re-Identification Based on Heterogeneous Part-Based Deep Network in Camera Networks,"In this paper, we propose a new deep learning model named heterogeneous part-based deep network for person re-identification in camera networks, which simultaneously learns the alignment and discrimination for parts of pedestrian images. Concretely, several parts are obtained through the uniform partition on the convolutional layer for each pedestrian image. Then, we present part-aligned distances to perform alignment by searching the shortest local distances between image parts in a certain range. Meanwhile, we utilize the batch hard triplet loss and cross-entropy loss to learn more discriminative part-based features in different aspects. Experiments are conducted on three challenging datasets, Market-1501, CUHK03, and DukeMTMC-reID, and we achieve 94.0%, 64.3%, and 83.6% rank-1 accuracy and 81.2%, 58.2%, and 68.0% mAP, outperforming the state-of-the-art methods by a large margin.",2020,IEEE Transactions on Emerging Topics in Computational Intelligence,,10.1109/TETCI.2018.2883348,
2eecdb103cb8bac86aab6ef8fa19d11ce1e77218,0,1,0,A novel deep model with multi-loss and efficient training for person re-identification,"Abstract The purpose of Person re-identification (PReID) is to identify the same individual from the non-overlapping cameras, the task has been greatly promoted by the deep learning system. In this study, we review two widely-used CNN frameworks in the PReID community: identification model and triplet model. We provide a comprehensive overview of the advantages and limitations of the two models and present a hybrid model that combines the advantages of both identification and triplet models. Specifically, the proposed model employs triplet loss, identification loss and center loss to simultaneously train the carefully designed network. Furthermore, the dropout scheme is adopted by its identification subnetwork. Given a triplet unit images, the model can output the identities of the three input images and force the Euclidean distance between the mismatched pairs to be larger than those between the matched pairs as well as reduce the variance of the same class at the same time. Extensive comparative experiments on three PReID benchmark datasets (CUHK01, CUHK03, Market-1501) show that our proposed architecture outperforms many state of the art methods in most cases.",2019,Neurocomputing,,10.1016/j.neucom.2018.03.073,https://www.ee.ryerson.ca/~xzhang/publications/NEUCOM2019-PersonReID.pdf
2f041131ff56b0cc0ed9ec07a186c8f3da4aa359,1,1,0,Celebrities-ReID: A Benchmark for Clothes Variation in Long-Term Person Re-Identification,"This paper considers person re-identification (re-ID) in the case of long-time gap (i.e., long-term re-ID) that concentrates on the challenge of clothes variation of each person. We introduce a new dataset, named Celebrities-reID to handle that challenge. Compared with current datasets, the proposed Celebrities-reID dataset is featured in two aspects. First, it contains 590 persons with 10,842 images, and each person does not wear the same clothing twice, making it the largest clothes variation person re-ID dataset to date. Second, a comprehensive evaluation using state of the arts is carried out to verify the feasibility and new challenge exposed by this dataset. In addition, we propose a benchmark approach to the dataset where a two-step fine-tuning strategy on human body parts is introduced to tackle the challenge of clothes variation. In experiments, we evaluate the feasibility and quality of the proposed Celebrities-reID dataset. The experimental results demonstrate that the proposed benchmark approach is not only able to best tackle clothes variation shown in our dataset but also achieves competitive performance on a widely used person re-ID dataset Market1501, which further proves the reliability of the proposed benchmark approach.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8851957,
2f0c2670d438598e8d096836bcb6d79580afc1c9,1,0,0,Hypothesis Testing Based Tracking With Spatio-Temporal Joint Interaction Modeling,"Data association is one of the key research in tracking-by-detection framework. Due to frequent interactions among targets, there are various relationships among trajectories in crowded scenes which leads to problems in data association, such as association ambiguity, association omission, etc. To handle these problems, we propose hypothesis-testing based tracking (HTBT) framework to build potential associations between target by constructing and testing hypotheses. In addition, a spatio-temporal interaction graph (STIG) model is introduced to describe the basic interaction patterns of trajectories and test the potential hypotheses. Based on network flow optimization, we formulate offline tracking as a MAP problem. Experimental results show that our tracking framework improves the robustness of tracklet association when detection failure occurs during tracking. On the public MOT16, MOT17 and MOT20 benchmark, our method achieves competitive results compared with other state-of-the-art methods.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2020.2988649,https://ieeexplore.ieee.org/ielx7/76/9185141/09072184.pdf
2f596d62458537db24998dd15dd2a367491f081d,0,1,0,Multilevel deep representation fusion for person re-identification,"Abstract. Person re-identification (re-ID) aims at matching two pedestrian images across different cameras. Usually, the main scheme of re-ID based on deep learning includes two phases: feature extraction and metric calculation. We focus on how to extract more discriminative image features for re-ID. To address this problem, we propose a multilevel deep representation fusion (MDRF) model based on the convolutional neural network. Specifically, the MDRF model is designed to extract image features at different network levels through one forward pass. In order to produce the final image representation, these multilevel features are fused by a fusion layer. Then the final image representation is fed into a combined loss of the softmax and the triplet to optimize the model. The proposed method not only utilizes the abstract information of high-level features but also integrates the appearance information of low-level features. Extensive experiments on public datasets including Market-1501, DukeMTMC-reID, and CUHK03 demonstrate the effectiveness of the proposed method for person re-ID.",2020,J. Electronic Imaging,,10.1117/1.JEI.29.2.023005,
2fa567ba525ed267b4af10806b2ec709ae042537,1,0,0,Orientation- and Scale-Invariant Multi-Vehicle Detection and Tracking from Unmanned Aerial Videos,"Along with the advancement of light-weight sensing and processing technologies, unmanned aerial vehicles (UAVs) have recently become popular platforms for intelligent traffic monitoring and control. UAV-mounted cameras can capture traffic-flow videos from various perspectives providing a comprehensive insight into road conditions. To analyze the traffic flow from remotely captured videos, a reliable and accurate vehicle detection-and-tracking approach is required. In this paper, we propose a deep-learning framework for vehicle detection and tracking from UAV videos for monitoring traffic flow in complex road structures. This approach is designed to be invariant to significant orientation and scale variations in the videos. The detection procedure is performed by fine-tuning a state-of-the-art object detector, You Only Look Once (YOLOv3), using several custom-labeled traffic datasets. Vehicle tracking is conducted following a tracking-by-detection paradigm, where deep appearance features are used for vehicle re-identification, and Kalman filtering is used for motion estimation. The proposed methodology is tested on a variety of real videos collected by UAVs under various conditions, e.g., in late afternoons with long vehicle shadows, in dawn with vehicles lights being on, over roundabouts and interchange roads where vehicle directions change considerably, and from various viewpoints where vehicles’ appearance undergo substantial perspective distortions. The proposed tracking-by-detection approach performs efficiently at 11 frames per second on color videos of 2720p resolution. Experiments demonstrated that high detection accuracy could be achieved with an average F1-score of 92.1%. Besides, the tracking technique performs accurately, with an average multiple-object tracking accuracy (MOTA) of 81.3%. The proposed approach also addressed the shortcomings of the state-of-the-art in multi-object tracking regarding frequent identity switching, resulting in a total of only one identity switch over every 305 tracked vehicles.",2019,Remote. Sens.,,10.3390/rs11182155,https://pdfs.semanticscholar.org/5288/1cb86ad2a5dcace5927c49a9d94c7e0cc352.pdf
2fe9a1e797bb0a2ba79b1f68c95a1067dbe402fe,1,0,0,Adaptive L2 Regularization in Person Re-Identification.,"We introduce an adaptive L2 regularization mechanism in the setting of person re-identification. In the literature, it is common practice to utilize hand-picked regularization factors which remain constant throughout the training procedure. Unlike existing approaches, the regularization factors in our proposed method are updated adaptively through backpropagation. This is achieved by incorporating trainable scalar variables as the regularization factors, which are further fed into a scaled hard sigmoid function. Extensive experiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the effectiveness of our framework. Most notably, we obtain state-of-the-art performance on MSMT17, which is the largest dataset for person re-identification. Source code is publicly available at https://github.com/nixingyang/AdaptiveL2Regularization.",2020,,2007.07875,,https://arxiv.org/pdf/2007.07875.pdf
300decb4831f5fa9b91837c05a29c0bd3545d54c,1,0,0,Concentrated Local Part Discovery With Fine-Grained Part Representation for Person Re-Identification,"The attention mechanism for person re-identification has been widely studied with deep convolutional neural networks. This mechanism works as a good complement to the global features extracted from an image of the entire human body. However, existing works mainly focus on discovering local parts with simple feature representations, such as global average pooling. Moreover, these works either require extra supervision, such as labeling of body joints, or pay little attention to the guidance of part learning, resulting in scattered activation of learned parts. Furthermore, existing works usually extract local features from different body parts via global average pooling and then concatenate them together as good global features. We find that local features acquired in this way contribute little to the overall performance. In this paper, we argue the significance of local part description and explore the attention mechanism from both local part discovery and local part representation aspects. For local part discovery, we propose a new constrained attention module to make the activated regions concentrated and meaningful without extra supervision. For local part representation, we propose a statistical-positional-relational descriptor to represent local parts from a fine-grained viewpoint. Extensive experiments are conducted to validate the overall performance, the effectiveness of each component, and the generalization ability. We achieve a rank-1 accuracy of 95.1% on Market1501, 64.7% on CUHK03, 87.1% on DukeMTMC-ReID, and 79.9% on MSMT17, outperforming state-of-the-art methods.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2019.2946486,
3010141db561594cad7325554fbc6d41f88c8eba,0,1,0,Self-Paced Video Data Augmentation with Dynamic Images Generated by Generative Adversarial Networks,"There is an urgent need for an effective video classification method by means of a small number of samples. The deficiency of samples could be effectively alleviated by generating samples through Generative Adversarial Networks (GAN), but the generation of videos on a typical category remains to be underexplored since the complex actions and the changeable viewpoints are difficult to simulate. In this paper, we propose a generative data augmentation method for temporal stream of the Temporal Segment Networks with the dynamic image. The dynamic image compresses the motion information of video into a still image, removing the interference factors such as the background. Thus it is easier to generate images with categorical motion information using GAN. We use the generated dynamic images to enhance the features, with regularization achieved as well, thereby to achieve the effect of video augmentation. In order to deal with the uneven quality of generated images, we propose a Self-Paced Selection (SPS) method, which automatically selects the high-quality generated samples to be added to the network training. Our method is verified on two benchmark datasets, HMDB51 and UCF101. The experimental results show that the method can improve the accuracy of video classification under the circumstance of sample insufficiency and sample imbalance.",2019,ArXiv,1909.12929,,https://arxiv.org/pdf/1909.12929.pdf
304196021200067a838c06002d9e96d6a12a1e46,1,1,1,Similarity-preserving Image-image Domain Adaptation for Person Re-identification,"This article studies the domain adaptation problem in person re-identification (re-ID) under a ""learning via translation"" framework, consisting of two components, 1) translating the labeled images from the source to the target domain in an unsupervised manner, 2) learning a re-ID model using the translated images. The objective is to preserve the underlying human identity information after image translation, so that translated images with labels are effective for feature learning on the target domain. To this end, we propose a similarity preserving generative adversarial network (SPGAN) and its end-to-end trainable version, eSPGAN. Both aiming at similarity preserving, SPGAN enforces this property by heuristic constraints, while eSPGAN does so by optimally facilitating the re-ID model learning. More specifically, SPGAN separately undertakes the two components in the ""learning via translation"" framework. It first preserves two types of unsupervised similarity, namely, self-similarity of an image before and after translation, and domain-dissimilarity of a translated source image and a target image. It then learns a re-ID model using existing networks. In comparison, eSPGAN seamlessly integrates image translation and re-ID model learning. During the end-to-end training of eSPGAN, re-ID learning guides image translation to preserve the underlying identity information of an image. Meanwhile, image translation improves re-ID learning by providing identity-preserving training samples of the target domain style. In the experiment, we show that identities of the fake images generated by SPGAN and eSPGAN are well preserved. Based on this, we report the new state-of-the-art domain adaptation results on two large-scale person re-ID datasets.",2018,ArXiv,1811.10551,,https://arxiv.org/pdf/1811.10551.pdf
308a13fd1d2847d98930a8e5542f773a9651a0ae,1,1,0,Group Consistent Similarity Learning via Deep CRF for Person Re-identification,"Person re-identification benefits greatly from deep neural networks (DNN) to learn accurate similarity metrics and robust feature embeddings. However, most of the current methods impose only local constraints for similarity learning. In this paper, we incorporate constraints on large image groups by combining the CRF with deep neural networks. The proposed method aims to learn the ""local similarity"" metrics for image pairs while taking into account the dependencies from all the images in a group, forming ""group similarities"". Our method involves multiple images to model the relationships among the local and global similarities in a unified CRF during training, while combines multi-scale local similarities as the predicted similarity in testing. We adopt an approximate inference scheme for estimating the group similarity, enabling end-to-end training. Extensive experiments demonstrate the effectiveness of our model that combines DNN and CRF for learning robust multi-scale local similarities. The overall results outperform those by state-of-the-arts with considerable margins on three widely-used benchmarks.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00902,http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf
30ae42109e035f9f6985735f0d0a38aecdb0e110,0,1,0,Deep Domain Knowledge Distillation for Person Re-identification,"Learning generic and robust representations with data from multiple domains is a big challenge in Person ReID. In this paper, we propose an end-to-end framework called Deep Domain Knowledge Distillation (\(D^2KD\)) for leaning more generic and robust features with Convolutional Neural Networks (CNNs). Domain-specific knowledge learned by the auxiliary network is transferred to the domain-free subnetwork and guides the optimization of the feature extractor. While person identity information is transferred to the auxiliary network to further accurately identify domain classes. In the test period, just with a single base model as the feature extractor, we improve the Rank-1 and mAP by a clear margin. Experiments on Market-1501, CUHK03 and DukeMTMC-reID demonstrate the effectiveness of our method.",2019,ICANN,,10.1007/978-3-030-30484-3_55,https://e-nns.org/icann2019/online_posters/373.pdf
310830a4bdeaaaa9b815edeb47b421a1f8e03b08,1,1,0,Data Augmentation Using GANs for 3D Applications,,2020,,,10.4018/978-1-5225-5294-9.ch011,
31443817acd068867c2d40a221f642348a6d0d6c,0,1,0,Partial person re-identification with two-stream network and reconstruction,"Abstract Partial person re-identification is a challenging issue at present. However, affected by occlusions, features in person re-identification cannot be detected and the traditional person re-identification methods can not accurately deal with it. In order to solve this problem, we propose to match query and gallery by combining different modes from two-stream network with sparse reconstruction to realize partial person re-identification. For acquiring features, bilinear pooling is applied to fuse the two different modes from the appearance network and pose network aiming at better performance. For matching query and galley, the robust sparse representation reconstructs the features extracted by the network for flexible solution, using the parameters learned from galley. The reconstruction process achieves arbitrary size images in partial person re-identification. In addition, we extract mid-level feature and fuse it with the high-level feature for more accuracy. Experiments demonstrate the performance of the proposed method better compared with the methods of state-of-the-art person re-identification methods on dataset Market1501, CUHK03, DukeMTMC-reID and partial person dataset Partial-REID, Partial-iLIDS.",2020,Neurocomputing,,10.1016/J.NEUCOM.2019.04.098,
3144c9b3bedb6e3895dcd36998bcb0903271841d,1,0,0,CAN: Composite Appearance Network and a Novel Evaluation Metric for Person Tracking,"Tracking multiple people across multiple cameras is an open problem. It is typically divided into two tasks: (i) single-camera tracking (SCT) - identify trajectories in the same scene, and (ii) inter-camera tracking (ICT) - identify trajectories across cameras for real surveillance scenes. Many methods cater to SCT, while ICT still remains a challenge. In this paper, we present a feature aggregation architecture called Composite Appearance Network (CAN) to address the above problem. The key structure of this architecture is called EvalNet that pays attention to each feature vector and learns to weight them based on gradients it receives for the overall template for optimal re-identification performance. We demonstrate the efficiency of our approach with experiments on the challenging multi-camera tracking dataset, DukeMTMC. We also survey existing tracking measures and present an online error metric called ""Inference Error"" (IE) that provides a better estimate of tracking/re-identification error, by treating SCT and ICT errors uniformly.",2018,ArXiv,1811.06582,,https://arxiv.org/pdf/1811.06582.pdf
315d579e59f119adff04a343faf403811eb0adbb,1,0,0,Dynamic Re-ranking with Deep Features Fusion for Person Re-identification,"State-of-the-art (STOA) person re-identification (re-ID) methods measure features extracted by deep CNNs for final evaluation. In this work, we aim to improve re-ID performance by better utilizing these deep features. Firstly, a Dynamic Re-ranking (DRR) method is proposed, which matches features based on neighborhood structure to utilize contextual information. Different from common re-ranking methods, it finds more matches by adding contextual information. Secondly, to exploit the diverse information embedded in the deep features, we introduce Deep Feature Fusion (DFF), which splits and combines deep features through a diffusion and fusion process. Extensive comparative evaluations on three large re-ID benchmarks and six well-known features show that DRR and DFF are effective and insensitive to parameter setting. With a proper integration strategy, DRR and DFF can achieve STOA re-ID performance.",2019,PRICAI,,10.1007/978-3-030-29911-8_16,
317f5a56519df95884cce81cfba180ee3adaf5a5,1,1,0,Operator-in-the-Loop Deep Sequential Multi-Camera Feature Fusion for Person Re-Identification,"Given a target image as query, person re-identification systems retrieve a ranked list of candidate matches on a per-camera basis. In deployed systems, a human operator scans these lists and labels sighted targets by touch or mouse-based selection. However, classical re-id approaches generate per-camera lists independently. Therefore, target identifications by operator in a subset of cameras cannot be utilized to improve ranking of the target in remaining set of network cameras. To address this shortcoming, we propose a novel sequential multi-camera re-id approach. The proposed approach can accommodate human operator inputs and provides early gains via a monotonic improvement in target ranking. At the heart of our approach is a fusion function which operates on deep feature representations of query and candidate matches. We formulate an optimization procedure custom-designed to incrementally improve query representation. Since existing evaluation methods cannot be directly adopted to our setting, we also propose two novel evaluation protocols. The results on two large-scale re-id datasets (Market-1501, DukeMTMC-reID) demonstrate that our multi-camera method significantly outperforms baselines and other popular feature fusion schemes. Additionally, we conduct a comparative subject-based study of human operator performance. The superior operator performance enabled by our approach makes a compelling case for its integration into deployable video-surveillance systems.",2020,IEEE Transactions on Information Forensics and Security,1807.07295,10.1109/TIFS.2019.2957701,https://arxiv.org/pdf/1807.07295.pdf
319c863ffde8b3af82f375720a7ba8a176cbeda6,1,1,1,Pose-Guided Spatial Alignment and Key Frame Selection for One-Shot Video-Based Person Re-Identification,"One-shot video-based person re-identification exploits the unlabeled data by using a single-labeled sample for each individual to train a model and to reduce the need for laborious labeling. Although recent works focusing on this task have made some achievements, most state-of-the-art models are vulnerable to misalignment, pose variation and corrupted frames. To address these challenges, we propose a one-shot video-based person re-identification model based on pose-guided spatial alignment and KFS. First, a spatial transformer sub-network trained using pose-guided regression is employed to perform the spatial alignment. Second, we propose a novel training strategy based on KFS. Key frames with abruptly changing poses are deliberately identified and selected to make the network adaptive to pose variation. Finally, we propose a frame feature pooling method by incorporating long short-term memory with an attention mechanism to reduce the influence of corrupted frames. Comprehensive experiments are presented based on the MARS and DukeMTMC-VideoReID datasets. The mAP values for these datasets reach 46.5% and 68.4%, respectively, demonstrating that the proposed model achieves significant improvements over state-of-the-art one-shot person re-identification methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2922679,
31da1da2d4e7254dd8f2a4578d887c57e0678438,1,1,1,Unsupervised Person Re-identification,"The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.",2018,ACM Trans. Multim. Comput. Commun. Appl.,1705.10444,10.1145/3243316,https://arxiv.org/pdf/1705.10444.pdf
31e89d17132716fdaf5c749aa7dc14e19e89e725,1,0,0,Metric Learning with Equidistant and Equidistributed Triplet-based Loss for Product Image Search,"Product image search in E-commerce systems is a challenging task, because of a huge number of product classes, low intra-class similarity and high inter-class similarity. Deep metric learning, based on paired distances independent of the number of classes, aims to minimize intra-class variances and inter-class similarity in feature embedding space. Most existing approaches strictly restrict the distance between samples with fixed values to distinguish different classes of samples. However, the distance of paired samples has various magnitudes during different training stages. Therefore, it is difficult to directly restrict absolute distances with fixed values. In this paper, we propose a novel Equidistant and Equidistributed Triplet-based (EET) loss function to adjust the distance between samples with relative distance constraints. By optimizing the loss function, the algorithm progressively maximizes intra-class similarity and inter-class variances. Specifically, 1) the equidistant loss pulls the matched samples closer by adaptively constraining two samples of the same class to be equally distant from another one of a different class in each triplet, 2) the equidistributed loss pushes the mismatched samples farther away by guiding different classes to be uniformly distributed while keeping intra-class structure compact in embedding space. Extensive experimental results on product search benchmarks verify the improved performance of our method. We also achieve improvements on other retrieval datasets, which show superior generalization capacity of our method in image search.",2020,WWW,,10.1145/3366423.3380094,
3211d7ad22116916cf9d0f02ccecf43ba7e123a5,1,1,0,Improving person re-identification by multi-task learning,"We propose a novel Multi-Task Learning Network (MTNET) with four different subtasks for person re-identification mission. At the same time, the attribute recognition mission can be implemented by the same network. We achieve multi-mission by integrating four subtasks, such as identity identification, identity verification, attribute identification, attribute verification. Identity loss and attribute loss can provide complementary information on a different perspective by integrating multi-context information. Identity focuses on the overall contour and appearance, while attribute focuses on local aspects and dresses of one person. Identification loss and verification loss are used to optimize the distance of samples. Identification loss used to construct a robust category space, while verification loss used to optimize the space by minimizing the distance between similar images, and maximizing the distance between dissimilar images. Moreover, an effective verification loss named constraint contrast verification (CCV) is proposed to restrict the distance between feature pair to a foreseeable range that ensures the network has better convergence. The MTNet is an end-to-end deep learning framework, all the parameters and losses can be jointly optimized. We evaluate our approach with the state-of-the-art methods on two famous dataset Market1501 and DukeMTMC-reID. Experiments demonstrate that our MTNet achieves the very competitive results.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-07921-6,
329ed6f9cb32375c72fae2343ecc92ebdec5e444,1,0,0,MOANA: An Online Learned Adaptive Appearance Model for Robust Multiple Object Tracking in 3D,"Multiple object tracking has been a challenging field, mainly due to noisy detection sets an identity switch caused by occlusion and similar appearance among nearby targets. Previous works rely on appearance models that are built on an individual or several selected frames for the comparison of features, but they cannot encode the long-term appearance changes caused by pose, viewing angle, and lighting conditions. In this paper, we propose an adaptive model that learns online a relatively long-term appearance change of each target. The proposed model is compatible with any feature of fixed dimension or their combination, whose learning rates are dynamically controlled by the adaptive update and spatial weighting schemes. To handle occlusion and nearby objects that are sharing a similar appearance, we also design the cross-matching and re-identification schemes based on the application of the proposed adaptive appearance models. In addition, the 3D geometry information is effectively incorporated in our formulation for data association. The proposed method outperforms all the state of the art on the MOTChallenge 3D benchmark and achieves real-time computation with only a standard desktop CPU. It has also shown superior performance over the state of the art on the 2D benchmark of MOTChallenge.",2019,IEEE Access,1901.02626,10.1109/ACCESS.2019.2903121,https://arxiv.org/pdf/1901.02626.pdf
32beed981f2cedeca2dcbcfd30b53a6c9a22881c,1,0,0,Tracking Handball Players with the DeepSORT Algorithm,"In team sports scenes, such as in handball, it is common to have many players on the field performing different actions according to the rules of the game. During practice, each player has their own ball, and sequentially repeats a particular technique in order to adopt it and use it. In this paper, the focus is to detect and track all players on the handball court, so that the performance of a particular athlete, and the adoption of a particular technique can be analyzed. This is a very demanding task of multiple object tracking because players move fast, often change direction, and are often occluded or out of the camera field view. We propose a DeepSort algorithm for player tracking after the players have been detected with YOLOv3 object detector. The effectiveness of the proposed methods is evaluated on a custom set of handball scenes using standard multiple object tracking metrics. Also, common detection problems that have been observed are discussed.",2020,ICPRAM,,10.5220/0009177605930599,https://pdfs.semanticscholar.org/a97e/f13dad8779638f45119a8c249d079a3c2c34.pdf
32d3e9d714ccede8e16fd5fccb828b8aa722a699,1,1,0,Identity Adaptation for Person Re-Identification,"Person re-identification (re-ID), which aims to identify the same individual from a gallery collected with different cameras, has attracted increasing attention in the multimedia retrieval community. Current deep learning methods for person re-ID focus on learning classification models on training identities to obtain an ID-discriminative embedding (IDE) extractor, which is used to extract features from testing images for re-ID. The IDE features of the testing identities might not be discriminative due to that the training identities are different from the testing identities. In this paper, we introduce a new ID-adaptation network (ID-AdaptNet), which aims to improve the discriminative power of the IDE features of the testing identities for better person re-ID. The main idea of the ID-AdaptNet is to transform the IDE features to a common discriminative latent space, where the representations of the “seen” training identities are enforced to adapt to those of the “unseen” training identities. More specifically, the ID-AdaptNet is trained by simultaneously minimizing the classification cross-entropy and the discrepancy between the “seen” and the “unseen” training identities in the hidden space. To calculate the discrepancy, we represent their probability distributions as moment sequences and calculate their distance using their central moments. We further propose a stacking ID-AdaptNet that jointly trains multiple ID-AdaptNets with a regularization method for better re-ID. Experiments show that the ID-AdaptNet and stacking ID-AdaptNet effectively improve the discriminative power of IDE features.",2018,IEEE Access,,10.1109/ACCESS.2018.2867898,
331f465d3f3b1ef4fede376749eb2ba851e996bd,0,1,0,Universal Person Re-Identification,"Most state-of-the-art person re-identification (re-id) methods depend on supervised model learning with a large set of cross-view identity labelled training data. Even worse, such trained models are limited to only the same-domain deployment with significantly degraded cross-domain generalization capability, i.e. ""domain specific"". To solve this limitation, there are a number of recent unsupervised domain adaptation and unsupervised learning methods that leverage unlabelled target domain training data. However, these methods need to train a separate model for each target domain as supervised learning methods. This conventional ""{\em train once, run once}"" pattern is unscalable to a large number of target domains typically encountered in real-world deployments. We address this problem by presenting a ""train once, run everywhere"" pattern industry-scale systems are desperate for. We formulate a ""universal model learning' approach enabling domain-generic person re-id using only limited training data of a ""{\em single}"" seed domain. Specifically, we train a universal re-id deep model to discriminate between a set of transformed person identity classes. Each of such classes is formed by applying a variety of random appearance transformations to the images of that class, where the transformations simulate the camera viewing conditions of any domains for making the model training domain generic. Extensive evaluations show the superiority of our method for universal person re-id over a wide variety of state-of-the-art unsupervised domain adaptation and unsupervised learning re-id methods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and VIPeR.",2019,ArXiv,1907.09511,,https://arxiv.org/pdf/1907.09511.pdf
332fbbcae6661fe8f035a0c1f49f9c3b9f8350b6,1,0,1,Multi-Scale Temporal Cues Learning for Video Person Re-Identification,"Temporal cues embedded in videos provide important clues for person Re-Identification (ReID). To efficiently exploit temporal cues with a compact neural network, this work proposes a novel 3D convolution layer called Multi-scale 3D (M3D) convolution layer. The M3D layer is easy to implement and could be inserted into traditional 2D convolution networks to learn multi-scale temporal cues by end-to-end training. According to its inserted location, the M3D layer has two variants, i.e., local M3D layer and global M3D layer, respectively. The local M3D layer is inserted between 2D convolution layers to learn spatial-temporal cues among adjacent 2D feature maps. The global M3D layer is computed on adjacent frame feature vectors to learn their global temporal relations. The local and global M3D layers hence learn complementary temporal cues. Their combination introduces a fraction of parameters to traditional 2D CNN, but leads to the strong multi-scale temporal feature learning capability. The learned temporal feature is fused with a spatial feature to compose the final spatial-temporal representation for video person ReID. Evaluations on four widely used video person ReID datasets, i.e., MARS, DukeMTMC-VideoReID, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over the state-of-the art. For example, it achieves rank1 accuracy of 88.63% on MARS without re-ranking. Our method also achieves a reasonable trade-off between ReID accuracy and model size, e.g., it saves about 40% parameters of I3D CNN.",2020,IEEE Transactions on Image Processing,1908.10049,10.1109/TIP.2020.2972108,https://arxiv.org/pdf/1908.10049.pdf
3337fedaf7ebc6e22aa5a65e548ed9e29891e32d,1,0,0,Using Panoramic Videos for Multi-Person Localization and Tracking In A 3D Panoramic Coordinate,"3D panoramic multi-person localization and tracking are prominent in many applications, however, conventional methods using LiDAR equipment could be economically expensive and also computationally inefficient due to the processing of point cloud data. In this work, we propose an effective and efficient approach at a low cost. First, we obtain panoramic videos with four normal cameras. Then, we transform human locations from a 2D panoramic image coordinate to a 3D panoramic camera coordinate using camera geometry and human bio-metric property (i.e., height). Finally, we generate 3D tracklets by associating human appearance and 3D trajectory. We verify the effectiveness of our method on three datasets including a new one built by us, in terms of 3D single-view multi-person localization, 3D single-view multi-person tracking, and 3D panoramic multi-person localization and tracking. Our code and dataset are available at https://github.com/fandulu/MPLT.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1911.10535,10.1109/ICASSP40776.2020.9053497,https://arxiv.org/pdf/1911.10535.pdf
334a86ab24ec40b2d084f2e049f4fc1b06b34462,1,0,0,Deep Learning-based Person Search with Visual Attention Embedding,"In this work, we consider the problem of person search, which is a challenging task that requires both person detection and person re-identification r un c oncurrently. In this context, we propose a person search approach based on deep neural networks that incorporates attention mechanisms to perform retrieval more robustly. Global and local features are extracted for person detection and person identification, respectively, boosted by attention layers that allow the extraction of discriminative feature representations, all in an end-to-end manner. We evaluate our approach on three challenging data sets and show that our proposed method improves the state-of-the-art networks.",2020,2020 13th International Conference on Communications (COMM),,10.1109/COMM48946.2020.9141958,
3367dfef9062681c0631b91b4c8b25f5f87d1187,1,0,0,"Vision Meets Drones: Past, Present and Future","Drones, or general UAVs, equipped with cameras have been fast deployed with a wide range of applications, including agriculture, aerial photography, and surveillance. Consequently, automatic understanding of visual data collected from drones becomes highly demanding, bringing computer vision and drones more and more closely. To promote and track the evelopments of object detection and tracking algorithms, we have organized two challenge workshops in conjunction with ECCV 2018, and ICCV 2019, attracting more than 100 teams around the world. We provide a large-scale drone captured dataset, VisDrone, which includes four tracks, i.e., (1) image object detection, (2) video object detection, (3) single object tracking, and (4) multi-object tracking. In this paper, we first presents a thorough review of object detection and tracking datasets and benchmarks, and discuss the challenges of collecting large-scale drone-based object detection and tracking datasets with fully manual annotations. After that, we describe our VisDrone dataset, which is captured over various urban/suburban areas of 14 different cities across China from North to South. Being the largest such dataset ever published, VisDrone enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. We provide a detailed analysis of the current state of the field of large-scale object detection and tracking on drones, and conclude the challenge as well as propose future directions. We expect the benchmark largely boost the research and development in video analysis on drone platforms. All the datasets and experimental results can be downloaded from the website: this https URL",2020,ArXiv,2001.06303,,https://arxiv.org/pdf/2001.06303.pdf
336d6e69b3f71894bc668871c57bb1b7bb73232b,1,1,0,Unsupervised Person Re-identification Using Reliable and Soft Labels,"In this paper, we propose unsupervised person re-identification (ReID) using reliable and soft labels. We provide unsupervised person ReID to consider unknown pedestrian queries. We update ResNet model for person ReID based on reliable and soft labels. First, we perform unsupervised clustering on person images under different cameras, and select samples based similarity between images and cluster centers. Then, we conduct re-clustering for the selected samples and assign labels to them, i.e. reliable labels. We get probability of the unselected samples, i.e. soft labels. Finally, we update ResNet model for person ReID using reliable and soft labels. Experiments on Market-1501 and DukeMTMC-ReID demonstrate that the proposed method outperforms state-of-the-arts for unsupervised person ReID in terms of the cosine distance and accuracy.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683353,
338e6eeb8282346ab9edce8ebf7d909c2c0ae214,0,1,0,Distilled Camera-Aware Self Training for Semi-Supervised Person Re-Identification,"Person re-identification (Re-ID), which is for matching pedestrians across disjoint camera views in surveillance, has made great progress in supervised learning. However, requirement of a large number of labelled identities leads to high cost for large-scale Re-ID systems. Consequently, it is significant to study learning Re-ID with unlabelled data and limited labelled data, that is, semi-supervised person re-identification. When labelled data is limited, the learned model tends to overfit the data and cannot generalize well. Moreover, the scene variations between cameras lead to domain shift in the feature space, which makes mining auxiliary supervision information from unlabelled data more difficult. To address these problems, we propose a Distilled Camera-Aware Self Training framework for semi-supervised person re-identification. To alleviate the overfitting problem for learning from limited labelled data, we propose a Multi-Teacher Selective Similarity Distillation Loss to selectively aggregate the knowledge of multiple weak teacher models trained with different subsets and distill a stronger student model. Then, we exploit the unlabelled data by learning pseudo labels by clustering based on the student model for self training. To alleviate the effect of scene variations between cameras, we propose a Camera-Aware Hierarchical Clustering (CAHC) algorithm to perform intra-camera clustering and cross-camera clustering hierarchically. Experiments show that our method outperformed the state-of-the-art semi-supervised person re-identification methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2950122,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08886397.pdf
33ab5bfdc40212f46da70d0bdfd372ebc51ca2d1,1,0,0,Towards Scalable Video Analytics at the Edge,"Breakthroughs in deep learning, GPUs, and edge computing have paved the way for always-on, live video analytics. However, to achieve real-time performance, a GPU needs to be dedicated amongst a few video feeds. But, GPUs are expensive resources and a large-scale deployment requires supporting hundreds of video cameras – exorbitant cost prohibits widespread adoption. To ease this burden, we propose Tetris, a system comprising of several optimization techniques from computer vision and deep-learning literature blended in a synergistic manner. Tetris is designed to maximize the parallel processing of video feeds on a single GPU, with a marginal drop in inference accuracy. Tetris performs CPU-based tiling of active regions to combine activities across video feeds. resulting in a condensed input volume. It then runs the deep learning model on this condensed volume instead of individual feeds, which significantly improves the GPU utilization. Our evaluation on Duke MTMC dataset reveals that Tetris can process 4x video feeds in parallel compared to any of the existing methods used in isolation.",2019,"2019 16th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",,10.1109/SAHCN.2019.8824876,
33f358f1d2b54042c524d69b20e80d98dde3dacd,1,1,0,Spectral Feature Transformation for Person Re-Identification,"With the surge of deep learning techniques, the field of person re-identification has witnessed rapid progress in recent years. Deep learning based methods focus on learning a discriminative feature space where data points are clustered compactly according to their corresponding identities. Most existing methods process data points individually or only involves a fraction of samples while building a similarity structure. They ignore dense informative connections among samples more or less. The lack of holistic observation eventually leads to inferior performance. To relieve the issue, we propose to formulate the whole data batch as a similarity graph. Inspired by spectral clustering, a novel module termed Spectral Feature Transformation is developed to facilitate the optimization of group-wise similarities. It adds no burden to the inference and can be applied to various scenarios. As a natural extension, we further derive a lightweight re-ranking method named Local Blurring Re-ranking which makes the underlying clustering structure around the probe set more compact. Empirical studies on four public benchmarks show the superiority of the proposed method. Code is available at https://github.com/LuckyDC/SFT_REID.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1811.11405,10.1109/ICCV.2019.00508,https://arxiv.org/pdf/1811.11405.pdf
34066e2c779e9e68c4a808f776afd4f2cb1504ac,1,0,0,Online Layered Multiple Object Tracking Using Residual-Residual Networks,"When dealing with multiple object tracking in the real world, it faces several challenges: (a) The number of targets to be tracked will change over time, (b) The data association of the target at different times will be affected by occlusion, (c) The problem of estimating the continuous state of all targets and deciding whether the targets leave the screen and then stop tracking. In this paper, a novel multiple object tracking method that consists of a residual-residual network and a four-layer data association scheme. The residual-residual network combines a deep residual classification network and a deep residual feature network. The deep residual classification network is used to remove unwanted background noise from the frame and corrects the target position of the missing ones. It can accurately track the position of multiple targets, link their positions in each time period, combine layered target data association method, and stepwise pair the trajectories and target candidates according the features generated from the deep residual feature network. The experiments using MOT16 that is a multi-object tracking database, show that the proposed method leads most existing researches in several evaluation criteria including the accuracy, speed and false positive.",2019,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),,10.1109/APSIPAASC47483.2019.9023151,
34929543127f97737e4e91417165bd147a052168,0,1,0,Adversarial Erasing Attention for Person Re-Identification in Camera Networks Under Complex Environments,"Person re-identification (Re-ID) in camera networks under complex environments has achieved promising performance using deep feature representations. However, most approaches usually ignore to learn features from non-salient parts of pedestrian, which results in an incomplete pedestrian representation. In this paper, we propose a novel person Re-ID method named Adversarial Erasing Attention (AEA) to mine discriminative completed features using an adversarial way. Specifically, the proposed AEA consists of the basic network and the complementary network. On the one hand, original pedestrian images are used to train the basic network in order to extract global and local deep features. On the other hand, to learn features complementary to the basic network, we propose the adversarial erasing operation, that locates non-salient areas with the help of attention map, to generate erased pedestrian images. Then, we utilize them to train the complementary network and adopt the dynamic strategy to match the dynamic status of AEA in the learning process. Hence, the diversity of training samples is enriched and the complementary network could discover new clues when learning deep features. Finally, we combine the features learned from the basic and complementary networks to represent the pedestrian image. Experiments on three databases (Market1501, CUHK03 and DukeMTMC-reID) demonstrate the proposed AEA achieves great performances.",2020,IEEE Access,,10.1109/ACCESS.2020.2982032,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09043556.pdf
34afb8848428f55ef9824037f1c550ac86a746bf,0,1,0,Improving person re-identification via attribute-identity representation and visual attention mechanism,"Person re-identification, which aims to compare a person of interest as seen in a “probe” camera view to a “gallery” of candidates captured from a camera that does not overlap with the probe one, has increased significant attention in computer vision due to its application in surveillance and security. Various methods utilize the global information as the feature descriptors, which neglect the details in an image and may have a low accuracy of person re-identification and not much attention has been paid to suppressing the background information, which has an influence on person re-identification to some extent. Being aware of these problems, this paper concentrate on the appearance of a person by improving feature descriptors that shed light on a combination framework by fusing the attribute-identity discrimination network with the person discrimination network based on the visual attention mechanism. Experimental results on publicly available image benchmark data sets have demonstrated that the proposed combination framework can achieve competitive performances as compared with state-of-the-art algorithms in terms of accuracy and effectiveness.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-08184-x,
34d14bc37d08bc9ce980ac7b769180d2dcc5c1bb,1,0,0,A fast multi-object tracking system using an object detector ensemble,"Multiple-Object Tracking (MOT) is of crucial importance for applications such as retail video analytics and video surveillance. Object detectors are often the computational bottleneck of modern MOT systems, limiting their use for real-time applications. In this paper, we address this issue by leveraging on an ensemble of detectors, each running every f frames. We measured the performance of our system in the MOT16 benchmark. The proposed model surpassed other online entries of the MOT16 challenge in speed, while maintaining an acceptable accuracy.",2019,2019 IEEE Colombian Conference on Applications in Computational Intelligence (ColCACI),1908.04349,10.1109/ColCACI.2019.8781972,https://arxiv.org/pdf/1908.04349.pdf
34f47e8805da7183fc144543e9b7524249ff1932,1,0,0,Fast One-Shot Learning for Identity Classification in Person Re-identification and Tracking,"For video analytics and surveillance applications, person re-identification across multiple camera views remains an open problem. The challenge of being able to determine that two images of people are the same person based solely on their appearance can be difficult, even for human observers. Many recent re-identification methods use deep learning with supervised learning to discriminate between the identity classes. However, the requisite training data is generally not available in real-world scenarios. In this paper, we compare a number of fast classification methods for the purposes of re-identification, taking extracted and pre-processed feature vectors and classifying them into identity classes, focusing on one-shot and unsupervised learning algorithms. We present two novel one-shot learning methods, including Sequential K-means, a computationally efficient algorithm with competitive accuracy. We demonstrate this on an indoor person tracking dataset, and discuss parameter tuning in order to further improve the accuracy of the algorithm.",2018,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",,10.1109/ICARCV.2018.8581291,
3529310a5b3a8b5a576651dcc994ff1755e8560c,0,1,0,Video-Based Person Re-identification by Deep Feature Guided Pooling,"Person re-identification (re-id) aims to match a specific person across non-overlapping views of different cameras, which is currently one of the hot topics in computer vision. Compared with image-based person re-id, video-based techniques could achieve better performance by fully utilizing the space-time information. This paper presents a novel video-based person re-id method named Deep Feature Guided Pooling (DFGP), which can take full advantage of the space-time information. The contributions of the method are in the following aspects: (1) PCA-based convolutional network (PCN), a lightweight deep learning network, is trained to generate deep features of video frames. Deep features are aggregated by average pooling to obtain person deep feature vectors. The vectors are utilized to guide the generation of human appearance features, which makes the appearance features robust to the severe noise in videos. (2) Hand-crafted local features of videos are aggregated by max pooling to reinforce the motion variations of different persons. In this way, the human descriptors are more discriminative. (3) The final human descriptors are composed of deep features and hand-crafted local features to take their own advantages and the performance of identification is promoted. Experimental results show that our approach outperforms six other state-of-the-art video-based methods on the challenging PRID 2011 and iLIDS-VID video-based person re-id datasets.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.188,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w17/papers/Li_Video-Based_Person_Re-Identification_CVPR_2017_paper.pdf
35706db03fadc6183edb84c1e5b8e12692248275,1,1,0,Temporal Continuity Based Unsupervised Learning for Person Re-identification,"Person re-identification (re-id) aims to match the same person from images taken across multiple cameras. Most existing person re-id methods generally require a large amount of identity labeled data to act as discriminative guideline for representation learning. Difficulty in manually collecting identity labeled data leads to poor adaptability in practical scenarios. To overcome this problem, we propose an unsupervised center-based clustering approach capable of progressively learning and exploiting the underlying re-id discriminative information from temporal continuity within a camera. We call our framework Temporal Continuity based Unsupervised Learning (TCUL). Specifically, TCUL simultaneously does center based clustering of unlabeled (target) dataset and fine-tunes a convolutional neural network (CNN) pre-trained on irrelevant labeled (source) dataset to enhance discriminative capability of the CNN for the target dataset. Furthermore, it exploits temporally continuous nature of images within-camera jointly with spatial similarity of feature maps across-cameras to generate reliable pseudo-labels for training a re-identification model. As the training progresses, number of reliable samples keep on growing adaptively which in turn boosts representation ability of the CNN. Extensive experiments on three large-scale person re-id benchmark datasets are conducted to compare our framework with state-of-the-art techniques, which demonstrate superiority of TCUL over existing methods.",2019,ICONIP,2009.00242,10.1007/978-3-030-36802-9_81,https://arxiv.org/pdf/2009.00242.pdf
35780748a07958b9775709bd2f81058284e8acfb,1,1,0,Joint identification-verification for person re-identification: A four stream deep learning approach with improved quartet loss function,"Abstract A deep four-stream convolutional neural network (CNN) is proposed for person re-identification (re-ID) to overcome the poor generalisation of the traditional triplet loss function. Specifically, the proposed method is a four-stream network, taking four input images where two images are from the same identity and the other two are from different identities. The network uses dual identification and verification losses in a single framework to minimise the intra-class distance while maximising the inter-class distance. Extensive experiments illustrate the state-of-the-art performance of the proposed approach on seven challenging person re-ID datasets: VIPeR, CUHK03, CUHK01, PRID2011, i-LIDS, Market-1501, and DukeMTMC-reID. In addition, we build a five-stream network and a four-stream network with an alternate formulation of positive and negative pairs to further explore the performance of the proposed four-stream network. We also demonstrate promising performance when training and testing sets are from different domains, highlighting the real-world applicability of the approach.",2020,Comput. Vis. Image Underst.,,10.1016/j.cviu.2020.102989,
35b066f4bfef8666233a1b0ab6c24e3f12cc7411,1,0,0,Unsupervised Cross-Domain Person Re-Identification: A New Framework,"Although existing person Re-IDentification (ReID) methods have achieved great progress with large-scale labeled data, it is still hard to generalize to unseen scenarios without any la-beled person identities. To alleviate this problem, this paper proposes a new framework to take full advantage of the label information of source domain and the data distribution geometry of unlabeled target domain to improve the ReID performance in the unlabeled target domain. Instead of direct model transfer, the data transfer is first adopted where the identity preserving samples are generated from the labeled source domain to unlabeled target domain. Accordingly, a better initialized target domain adapted ReID model could be obtained with the generated samples. The fine-grained part-level features are then learned instead of global features to better mine new persons in the unlabeled target domain. Finally, the proposed framework iteratively updates the ReID model with the generated persons and the mined persons in last iteration, and explores new persons from the unlabeled target domain. The state-of-the-art experimental results are achieved on Market1501 and DukeMTMC-reID in terms of unsupervised cross-domain person ReID.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8804418,
35e8312d8bdcffb8e0c956d20d5a581cad1c1b8a,0,1,0,Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning,"Semi-supervised learning, i.e. jointly learning from labeled and unlabeled samples, is an active research topic due to its key role on relaxing human supervision. In the context of image classification, recent advances to learn from unlabeled samples are mainly focused on consistency regularization methods that encourage invariant predictions for different perturbations of unlabeled samples. We, conversely, propose to learn from unlabeled data by generating soft pseudo-labels using the network predictions. We show that a naive pseudo-labeling overfits to incorrect pseudo-labels due to the so-called confirmation bias and demonstrate that mixup augmentation and setting a minimum number of labeled samples per mini-batch are effective regularization techniques for reducing it. The proposed approach achieves state-of-the-art results in CIFAR-10/100, SVHN, and Mini-ImageNet despite being much simpler than other methods. These results demonstrate that pseudo-labeling alone can outperform consistency regularization methods, while the opposite was supposed in previous work. Source code is available at https://git.io/fjQsC.",2020,2020 International Joint Conference on Neural Networks (IJCNN),1908.02983,10.1109/IJCNN48605.2020.9207304,http://doras.dcu.ie/24371/1/PID6429195.pdf
35f85ab75cb5f982919c0e6f3c8d0e422356ecd5,1,1,0,Part-Aligned Bilinear Representations for Person Re-identification,"We propose a novel network that learns a part-aligned representation for person re-identification. It handles the body part misalignment problem, that is, body parts are misaligned across human detections due to pose/viewpoint change and unreliable detection. Our model consists of a two-stream network (one stream for appearance map extraction and the other one for body part map extraction) and a bilinear-pooling layer that generates and spatially pools a part-aligned map. Each local feature of the part-aligned map is obtained by a bilinear mapping of the corresponding local appearance and body part descriptors. Our new representation leads to a robust image matching similarity, which is equivalent to an aggregation of the local similarities of the corresponding body parts combined with the weighted appearance similarity. This part-aligned representation reduces the part misalignment problem significantly. Our approach is also advantageous over other pose-guided representations (e.g., extracting representations over the bounding box of each body part) by learning part descriptors optimal for person re-identification. For training the network, our approach does not require any part annotation on the person re-identification dataset. Instead, we simply initialize the part sub-stream using a pre-trained sub-network of an existing pose estimation network, and train the whole network to minimize the re-identification loss. We validate the effectiveness of our approach by demonstrating its superiority over the state-of-the-art methods on the standard benchmark datasets, including Market-1501, CUHK03, CUHK01 and DukeMTMC, and standard video dataset MARS.",2018,ECCV,1804.07094,10.1007/978-3-030-01264-9_25,https://arxiv.org/pdf/1804.07094.pdf
36653f8705b56e39642bcd123494eb680cd1636b,0,1,0,Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.",2018,ICLR,1711.09325,,https://arxiv.org/pdf/1711.09325.pdf
3688e5c9ed0048905783a8a0e98724776d609bce,1,0,0,Active visual tracking in multi-agent scenarios,"We propose an active visual tracker with collision avoidance for camera-equipped robots in dense multi-agent scenarios. The objective of each tracking agent (robot) is to maintain visual fixation on its moving target while updating its velocity to avoid other agents. However, when multiple robots are present or targets intensively intersect each other, robots may have no accessible collision-avoiding paths. We address this problem with an adaptive mechanism that sets the pair-wise responsibilities to increase the total accessible collision-avoiding controls. The final collision-avoiding control accounts for motion smoothness and view performance, i.e. maintaining the target centered in the field of view and at a certain size. We validate the proposed approach under different target-intersecting scenarios and compare it with the Optimal Reciprocal Collision Avoidance and the Reciprocal Velocity Obstacle methods.",2017,2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2017.8078519,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/42804/1/WANG_Yiming_PhD_Final_060618.pdf
368a8fbf6304a192a67f614d032510e5a4100552,0,0,1,Generalizing from a Few Examples,"Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1",2020,ACM Comput. Surv.,1904.05046,10.1145/3386252,https://arxiv.org/pdf/1904.05046.pdf
36bccfb2ad847096bc76777e544f305813cd8f5b,1,0,0,WILDTRACK: A Multi-camera HD Dataset for Dense Unscripted Pedestrian Detection,"People detection methods are highly sensitive to occlusions between pedestrians, which are extremely frequent in many situations where cameras have to be mounted at a limited height. The reduction of camera prices allows for the generalization of static multi-camera set-ups. Using joint visual information from multiple synchronized cameras gives the opportunity to improve detection performance. In this paper, we present a new large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated frames for detection at a rate of 2 frames per second. This results in over 40 000 bounding boxes delimiting every person present in the area of interest, for a total of more than 300 individuals. We provide a series of benchmark results using baseline algorithms published over the recent months for multi-view detection with deep neural networks, and trajectory estimation using a non-Markovian model.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00528,http://www.idiap.ch/~fleuret/papers/chavdarova-et-al-cvpr2018.pdf
37041e54a5da61d648893f2f9d1d6239b7221972,0,1,0,An Empirical Study of Person Re-Identification with Attributes,"Person re-identification aims to identify a person from an image collection, given one image of that person as the query. There is, however, a plethora of real-life scenarios where we may not have a priori library of query images and therefore must rely on information from other modalities. In this paper, an attribute-based approach is proposed where the person of interest (POI) is described by a set of visual attributes, which are used to perform the search. We compare multiple algorithms and analyze how the quality of attributes impacts the performance. While prior work mostly relies on high precision attributes annotated by experts, we conduct a human-subject study and reveal that certain visual attributes could not be consistently described by human observers, making them less reliable in real applications. A key conclusion is that the performance achieved by non-expert attributes, instead of expert-annotated ones, is a more faithful indicator of the status quo of attribute-based approaches for person re-identification.",2019,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),2002.03752,10.1109/RO-MAN46459.2019.8956459,https://arxiv.org/pdf/2002.03752.pdf
370a9d9f9ec5b8ed5782c8ad28224d8d0240cd8e,1,1,1,Unsupervised Person Re-identification via Cross-Camera Similarity Exploration,"Most person re-identification (re-ID) approaches are based on supervised learning, which requires manually annotated data. However, it is not only resource-intensive to acquire identity annotation but also impractical for large-scale data. To relieve this problem, we propose a cross-camera unsupervised approach that makes use of unsupervised style-transferred images to jointly optimize a convolutional neural network (CNN) and the relationship among the individual samples for person re-ID. Our algorithm considers two fundamental facts in the re-ID task, i.e., variance across diverse cameras and similarity within the same identity. In this paper, we propose an iterative framework which overcomes the camera variance and achieves across-camera similarity exploration. Specifically, we apply an unsupervised style transfer model to generate style-transferred training images with different camera styles. Then we iteratively exploit the similarity within the same identity from both the original and the style-transferred data. We start with considering each training image as a different class to initialize the Convolutional Neural Network (CNN) model. Then we measure the similarity and gradually group similar samples into one class, which increases similarity within each identity. We also introduce a diversity regularization term in the clustering to balance the cluster distribution. The experimental results demonstrate that our algorithm is not only superior to state-of-the-art unsupervised re-ID approaches, but also performs favorably compared with other competing unsupervised domain adaptation methods (UDA) and semi-supervised learning methods.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2982826,
3724cb3b3db2b33bfd3f4ee8bd8df7bf8e59608a,0,1,0,Data augmentation in fault diagnosis based on the Wasserstein generative adversarial network with gradient penalty,"Abstract Fault detection and diagnosis in industrial process is an extremely essential part to keep away from undesired events and ensure the safety of operators and facilities. In the last few decades various data based machine learning algorithms have been widely studied to monitor machine condition and detect process faults. However, the faulty datasets in industrial process are hard to acquire. Thus low-data of faulty data or imbalanced data distributions are common to see in industrial processes, resulting in the difficulty to accurately identify different faults for many algorithms. Therefore, in this paper, Wasserstein generative adversarial network with gradient penalty (WGAN-GP) based data augmentation approaches are researched to generate data samples to supplement low-data input set in fault diagnosis field and help improve the fault diagnosis accuracies. To verify its efficient, various classifiers are used and three industrial benchmark datasets are involved to evaluate the performance of GAN based data augmentation ability. The results show the fault diagnosis accuracies for classifiers are increased in all datasets after employing the GAN-based data augmentation techniques.",2020,Neurocomputing,,10.1016/J.NEUCOM.2018.10.109,
372c72f602c71b3e3805b13b3ab4ae2e1e079176,0,1,0,Proxy Task Learning For Cross-Domain Person Re-Identification,"Person re-identification (ReID) has achieved rapid improvement recently. However, exploiting the model in a new scene is always faced with huge performance drop. The cause lies in distribution discrepancy between domains, including both low-level (e.g. image quality) and high-level (e.g. pedestrian attribute) variance. To alleviate the problem of domain shift, we propose a novel framework Proxy Task Learning (PTL), which performs body perception tasks on target-domain images while training source-domain ReID, in a multi-task manner. The backbone is shared between tasks and domains, hence both low- and high-level distributions are deeply aligned. We experimentally verify two proxy tasks, i.e. human parsing and attribute recognition, that prominently enhance generalization of the model. When integrating our method into an existing cross-domain pipeline, we achieve state-of-the-art performance on large-scale benchmarks.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME46284.2020.9102898,
37586e83d36006a7e00c1325fc5f9194c94ca055,0,1,0,Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN,"Studies show that Deep Neural Network (DNN)-based image classification models are vulnerable to maliciously constructed adversarial examples. However, little effort has been made to investigate how DNN-based image retrieval models are affected by such attacks. In this paper, we introduce Unsupervised Adversarial Attacks with Generative Adversarial Networks (UAA-GAN) to attack deep feature-based image retrieval systems. UAA-GAN is an unsupervised learning model that requires only a small amount of unlabeled data for training. Once trained, it produces query-specific perturbations for query images to form adversarial queries. The core idea is to ensure that the attached perturbation is barely perceptible to human yet effective in pushing the query away from its original position in the deep feature space. UAA-GAN works with various application scenarios that are based on deep features, including image retrieval, person Re-ID and face search. Empirical results show that UAA-GAN cripples retrieval performance without significant visual changes in the query images. UAA-GAN generated adversarial examples are less distinguishable because they tend to incorporate subtle perturbations in textured or salient areas of the images, such as key body parts of human, dominant structural patterns/textures or edges, rather than in visually insignificant areas (e.g., background and sky). Such tendency indicates that the model indeed learned how to toy with both image retrieval systems and human eyes.",2019,ArXiv,1907.05793,,https://arxiv.org/pdf/1907.05793.pdf
37ee2f860950669b5441ab2e873a67ab4971df35,0,1,0,BASED PSEUDO-LABELING FOR SEMI-SUPERVISED PERSON RE-IDENTIFICATION,"Generative Adversarial Networks (GAN) have shown promising results on data modeling and can generate high quality synthetic samples from the data distribution. However, how to effectively use the generated data for improved feature learning still remains an open question. This work proposes a Center based Pseudo-Labeling (CPL) method dedicated to this purpose. The network is trained with both labeled real data and unlabeled synthetic data, under a joint supervision of cross-entropy loss together with a center regularization term, which simultaneously predicts pseudo-labels for unlabeled synthetic data. Experimental results on two standard benchmarks show our approach achieves superior performance over closely related competitors and comparable results with stateof-the-art methods.",,,,,https://pdfs.semanticscholar.org/37ee/2f860950669b5441ab2e873a67ab4971df35.pdf
37fa0c718526121395f6f0025b6e349ba74c68a9,1,0,0,Multi Target Tracking by Learning from Generalized Graph Differences,"Formulating the multi object tracking problem as a network flow optimization problem is a popular choice. In this paper an efficient way of learning the weights of such a network is presented. It separates the problem into one embedding of feasible solutions into a one dimensional feature space and one optimization problem. The embedding can be learned using standard SGD type optimization without relying on an additional optimizations within each step. Training data is produced by performing small perturbations of ground truth tracks and representing them using generalized graph differences, which is an efficient way introduced to represent the difference between two graphs. The proposed method is evaluated on DukeMTMCT with competitive results.",2019,ArXiv,1908.06646,,https://arxiv.org/pdf/1908.06646.pdf
38259235a1c7b2c68ca09f3bc0930987ae99cf00,1,1,0,Deep Feature Ranking for Person Re-Identification,"Person re-identification plays a critical part in many surveillance applications. Due to complicated illumination environments and various viewpoints, it is still a challenging problem to extract robust features. To solve this issue, we propose a novel deep feature ranking scheme. Our main contribution is to rank achieved deep features, which are obtained by classic deep learning model, and set the sort order number as our feature vector, named as ordinal deep features (ODFs). Person re-identification results are acquired by ranking person candidates by measuring distance based on ODFs. Since applying for rank orders rather than original feature values, our method achieves robust results, especially under the situation of viewpoints shift. Comprehensive experiments are carried out to demonstrate the significance of the proposed feature. Meanwhile, comparative experiments are applied over the publicly available dataset, our method achieves promising performance and outperforms the state of the art methods. Moreover, we applied the proposed feature in the scenario of image classification and discussed the effectiveness.",2019,IEEE Access,,10.1109/ACCESS.2019.2894347,
382f034532418722974eb01c0047c70874d1d934,1,1,1,Feature Space Regularization for Person Re-identification with One Sample,"Few Shot Learning is a solution to relieve the huge annotation cost in Person Re-Identification. We concentrate on one sample setting in this work, where each identity has only one labeled sample along with many unlabeled samples. Training with one sample setting, the model is easily biased towards certain identities. Moreover, a reliable pseudo-label estimation scheme can greatly improve the final performance of the model. Targeting to solve the issues above, we propose two simple and effective solutions. (a) We design the Feature Space Regularization (FSR) Loss to adjust the distribution of samples in feature space. The FSR loss make the difference in distance of all labeled samples to unlabeled samples as small as possible. (b) We propose combining the Nearest Neighbor distance with inter-class distance to estimate pseudo-label for unlabeled data, which we called Joint-Distance. Notably, the Rank-1 accuracy of our method outperforms the state of the art method by a large margin of 12.1 points (absolute, i.e., 67.9% vs. 55.8%) on Market-1501, and 10.1 points (absolute, i.e., 58.9% vs. 48.8%) on DukeMTMC-reID, respectively. We will release all the code in https://github.com/Freedomxt/Feature_Space_Regularization_for_person_Re-Identification_with_One_Sample.",2019,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),,10.1109/ICTAI.2019.00208,
3842ebd34058b00b2b606496e49acf9366f9943f,1,1,0,Improving Online Multi-Person Tracking Occlusion : Scale Loss for Deep ReID Feature Learning,"Occlusion and crossing in Multi-Person Tracking always influence the tracking results. In this paper, we show how deep Re-Identification (ReID), which aims at matching pedestrians across non-overlapping video cameras, can be used to improve the occlusion problem on tracking. The learned ReID feature is more robust than other features used in traditional trackers because the training set is collected from different cameras which includes different parts of the same person. This also helps to solve the occlusion problem in tracking. We train a neural network with the designed scale loss which normalizes both weight vectors and output features to remove the effect of their scale variations on a large Person ReID dataset offline to learn the deep ReID model and build a framework combining detector and tracker to meet real-world application requirements. During the online tracking stage, the data association is solved by calculating the cosine distance cost matrix according to the learned ReID feature vectors. Experiments show that using ReID features can effectively reduce the occlusion index data on MOTChallenge, and the scale loss performs well. Overall our method achieves competitive performance on MOTChallenge, and the framework guarantees the running speed in real-time.",2018,,,,https://pdfs.semanticscholar.org/3842/ebd34058b00b2b606496e49acf9366f9943f.pdf
384908bfad5b9e81d605344abcb9e99d8b0f4027,0,1,0,Improving Deep Models of Person Re-identification for Cross-Dataset Usage,"Person re-identification (Re-ID) is the task of matching humans across cameras with non-overlapping views that has important applications in visual surveillance. Like other computer vision tasks, this task has gained much with the utilization of deep learning methods. However, existing solutions based on deep learning are usually trained and tested on samples taken from same datasets, while in practice one need to deploy Re-ID systems for new sets of cameras for which labeled data is unavailable. Here, we mitigate this problem for one state-of-the-art model, namely, metric embedding trained with the use of the triplet loss function, although our results can be extended to other models. The contribution of our work consists in developing a method of training the model on multiple datasets, and a method for its online practically unsupervised fine-tuning. These methods yield up to 19.1% improvement in Rank-1 score in the cross-dataset evaluation.",2018,AIAI,1807.08526,10.1007/978-3-319-92007-8_7,https://arxiv.org/pdf/1807.08526.pdf
3849c22a21e475d7ecba0afd80b9129eb006a72e,1,1,0,Video Person Re-identification with Competitive Snippet-Similarity Aggregation and Co-attentive Snippet Embedding,"In this paper, we address video-based person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding. Our approach divides long person sequences into multiple short video snippets and aggregates the top-ranked snippet similarities for sequence-similarity estimation. With this strategy, the intra-person visual variation of each sample could be minimized for similarity estimation, while the diverse appearance and temporal information are maintained. The snippet similarities are estimated by a deep neural network with a novel temporal co-attention for snippet embedding. The attention weights are obtained based on a query feature, which is learned from the whole probe snippet by an LSTM network, making the resulting embeddings less affected by noisy frames. The gallery snippet shares the same query feature with the probe snippet. Thus the embedding of gallery snippet can present more relevant features to compare with the probe snippet, yielding more accurate snippet similarity. Extensive ablation studies verify the effectiveness of competitive snippet-similarity aggregation as well as the temporal co-attentive embedding. Our method significantly outperforms the current state-of-the-art approaches on multiple datasets.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00128,http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1036.pdf
3852738e4baa0a3bf57fb4e3d6d19435e764000e,1,0,0,VisDrone-VDT2018: The Vision Meets Drone Video Detection and Tracking Challenge Results,"Drones equipped with cameras have been fast deployed to a wide range of applications, such as agriculture, aerial photography, fast delivery, and surveillance. As the core steps in those applications, video object detection and tracking attracts much research effort in recent years. However, the current video object detection and tracking algorithms are not usually optimal for dealing with video sequences captured by drones, due to various challenges, such as viewpoint change and scales. To promote and track the development of the detection and tracking algorithms with drones, we organized the Vision Meets Drone Video Detection and Tracking (VisDrone-VDT2018) challenge, which is a subtrack of the Vision Meets Drone 2018 challenge workshop in conjunctiohe 15th European Conference on Computer Vision (ECCV 2018). Specifically, this workshop challenge consists of two tasks, (1) video object detection, and (2) multi-object tracking. We present a large-scale video object detection and tracking dataset, which consists of 79 video clips with about 1.5 million annotated bounding boxes in 33, 366 frames. We also provide rich annotations, including object categories, occlusion, and truncation ratios for better data usage. Being the largest such dataset ever published, the challenge enables extensive evaluation, investigation and tracking the progress of object detection and tracking algorithms on the drone platform. We present the evaluation protocol of the VisDrone-VDT2018 challenge and the results of the algorithms on the benchmark dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We hope the challenge largely boost the research and development in related fields.",2018,ECCV Workshops,,10.1007/978-3-030-11021-5_29,http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Zhu_VisDrone-VDT2018_The_Vision_Meets_Drone_Video_Detection_and_Tracking_Challenge_ECCVW_2018_paper.pdf
385e82409b93826261a79b63349b5bc0b507476b,1,0,0,Tracking Road Users using Constraint Programming,"In this paper, we aim at improving the tracking of road users in urban scenes. We present a constraint programming (CP) approach for the data association phase found in the tracking-by-detection paradigm of the multiple object tracking (MOT) problem. Such an approach can solve the data association problem more efficiently than graph-based methods and can handle better the combinatorial explosion occurring when multiple frames are analyzed. Because our focus is on the data association problem, our MOT method only uses simple image features, which are the center position and color of detections for each frame. Constraints are defined on these two features and on the general MOT problem. For example, we enforce color appearance preservation over trajectories and constrain the extent of motion between frames. Filtering layers are used in order to eliminate detection candidates before using CP and to remove dummy trajectories produced by the CP solver. Our proposed method was tested on a motorized vehicles tracking dataset and produces results that outperform the top methods of the UA-DETRAC benchmark.",2020,ArXiv,2003.04468,,https://arxiv.org/pdf/2003.04468.pdf
386ce3ac3d2db91290c8c2fa984a5e76dd364169,0,1,0,Person Re-Identification by Multi-Camera Networks for Internet of Things in Smart Cities,"As one of the most important areas of public safety and security, intelligent video surveillance is an indispensable part of the urban Internet of Things infrastructure. Person re-identification (person re-ID), which aims to track and recognize a person in a multi-camera scene, is mostly viewed as an image retrieval problem, and this task has been greatly boosted by deep convolutional neural networks (CNNs) in recent years. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images, and CNNs are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. We incorporate the deformable convolution module to the traditional baseline to enhance the transformation modeling capability without additional supervision. The new module can readily replace their plain counterparts in the existing CNNs and can be easily trained end-to-end by standard backpropagation. Experiments on two large-scale re-ID datasets confirm the performance of our approach. The experiments also show that learning dense spatial transformation in deep CNNs is effective for person re-ID task and has a bright future in the intelligent video surveillance area.",2018,IEEE Access,,10.1109/ACCESS.2018.2883560,
388b03244e7cdf28c750d7f6d4b4eb64219c3e7a,1,1,0,Optimizing Speed/Accuracy Trade-Off for Person Re-identification via Knowledge Distillation,"Finding a person across a camera network plays an important role in video surveillance. For a real-world person re-identification application, in order to guarantee an optimal time response, it is crucial to find the balance between accuracy and speed. We analyse this trade-off, comparing a classical method, that comprises hand-crafted feature description and metric learning, in particular, LOMO and XQDA, with state-of-the-art deep learning techniques, using image classification networks, ResNet and MobileNets. Additionally, we propose and analyse network distillation as a learning strategy to reduce the computational cost of the deep learning approach at test time. We evaluate both methods on the Market-1501 and DukeMTMC-reID large-scale datasets.",2020,Eng. Appl. Artif. Intell.,1812.02937,10.1016/j.engappai.2019.103309,https://arxiv.org/pdf/1812.02937.pdf
388f159cbf897beadb07d1d0af65332b95810cb8,1,1,0,Person Re-identification by Contour Sketch under Moderate Clothing Change,"The majority of existing models of person re-identification(re-id) that matches pedestrian images across different camera views are largely dependent on color appearance. However, this can be an issue if that person changes his/her clothes, causing most existing methods to fail. We call person re-id under clothing change the ""cross-clothes person re-id"". Particularly, as a first attempt at solving this problem using visible light images, we assume a person only changes his/her clothes moderately when the weather doesn't change substantially within a short period so that the shape of a person would not change significantly. We perform cross-clothes person re-id based on a contour sketch of person image to take advantage of the shape of human body instead of color information. To select more discriminative curve patterns on a body contour sketch, we introduce a learning-based spatial polar transformation(SPT) layer in convolutional neural network(CNN) to transform contour sketch images into a polar coordinate space. An angle-specific extractor(ASE) is applied in the CNN to extract more fine-grained discriminant angle-specific features. Finally, we develop a multistream network for aggregating multi-granularity features. We also contribute a cross-clothes dataset consisting of 33698 images from 221 identities. Our experiments demonstrate the effectiveness of our proposed method.",2019,IEEE transactions on pattern analysis and machine intelligence,2002.02295,10.1109/TPAMI.2019.2960509,https://arxiv.org/pdf/2002.02295.pdf
38c95599e373d1fb01c2c5c1d6eb37f845620c91,0,1,0,Frustratingly Easy Person Re-Identification: Generalizing Person Re-ID in Practice,"Contemporary person re-identification (\reid) methods usually require access to data from the deployment camera network during training in order to perform well. This is because contemporary \reid{} models trained on one dataset do not generalise to other camera networks due to the domain-shift between datasets. This requirement is often the bottleneck for deploying \reid{} systems in practical security or commercial applications, as it may be impossible to collect this data in advance or prohibitively costly to annotate it. This paper alleviates this issue by proposing a simple baseline for domain generalizable~(DG) person re-identification. That is, to learn a \reid{} model from a set of source domains that is suitable for application to unseen datasets out-of-the-box, without any model updating. Specifically, we observe that the domain discrepancy in \reid{} is due to style and content variance across datasets and demonstrate appropriate Instance and Feature Normalization alleviates much of the resulting domain-shift in Deep \reid{} models. Instance Normalization~(IN) in early layers filters out style statistic variations and Feature Normalization~(FN) in deep layers is able to further eliminate disparity in content statistics. Compared to contemporary alternatives, this approach is extremely simple to implement, while being faster to train and test, thus making it an extremely valuable baseline for implementing \reid{} in practice. With a few lines of code, it increases the rank 1 \reid{} accuracy by {11.8\%, 33.2\%, 12.8\% and 8.5\%} on the VIPeR, PRID, GRID, and i-LIDS benchmarks respectively. Source codes are available at \url{this https URL}.",2019,BMVC,1905.03422,,https://arxiv.org/pdf/1905.03422.pdf
38cd49b7fc37a2e85822490b4517122ff7936960,1,1,0,Person Re-identification by Deep Learning Multi-scale Representations,"Existing person re-identification (re-id) methods depend mostly on single-scale appearance information. This not only ignores the potentially useful explicit information of other different scales, but also loses the chance of mining the implicit correlated complementary advantages across scales. In this work, we demonstrate the benefits of learning multi-scale person appearance features using Convolutional Neural Networks (CNN) by aiming to jointly learn discriminative scale-specific features and maximise multiscale feature fusion selections in image pyramid inputs. Specifically, we formulate a novel Deep Pyramid Feature Learning (DPFL) CNN architecture for multi-scale appearance feature fusion optimised simultaneously by concurrent per-scale re-id losses and interactive cross-scale consensus regularisation in a closed-loop design. Extensive comparative evaluations demonstrate the re-id advantages of the proposed DPFL model over a wide range of state-of-the-art re-id methods on three benchmarks Market-1501, CUHK03, and DukeMTMC-reID.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.304,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/36238/1/Gong%20Person%20re-identification%202017%20Accepted.pdf
38e82f59b7ea81014687193946b70f133360925c,0,0,1,Continual egocentric object recognition,"We present a framework capable of tackilng the problem of continual object recognition in a setting which resembles that under whichhumans see and learn. This setting has a set of unique characteristics:it assumes an egocentric point-of-view bound to the needs of a singleperson, which implies a relatively low diversity of data and a coldstart with no data; it requires to operate in an open world, where newobjects can be encounteredat any time; supervision is scarce and hasto be solicited to the user, and completelyunsupervised recognitionof new objects should be possible. Note that this setting differs fromthe one addressed in the open world recognition literature, where supervised feedback is always requested to be able to incorporate newobjects. We propose a first solution to this problem in the form ofa memory-based incremental framework that is capable of storinginformation of each and any object it encounters, while using the supervision of the user to learn to discriminate between known and unknown objects. Our approach is based on four main features: the useof time and space persistence (i.e., the appearance of objects changesrelatively slowly), the use of similarity as the main driving principlefor object recognition and novelty detection, the progressive introduction of new objects in a developmental fashion and the selectiveelicitation of user feedback in an online active learning fashion. Experimental results show the feasibility of open world, generic objectrecognition, the ability to recognize, memorize and re-identify newobjects even in complete absence of user supervision, and the utilityof persistence and incrementality in boosting performance.",2020,ECAI,1912.05029,10.3233/FAIA200210,https://arxiv.org/pdf/1912.05029.pdf
39153bcd060957a5e79e511cd4d39ec8ada4ed6b,0,1,0,Scalable and Effective Deep CCA via Soft Decorrelation,"Recently the widely used multi-view learning model, Canonical Correlation Analysis (CCA) has been generalised to the non-linear setting via deep neural networks. Existing deep CCA models typically first decorrelate the feature dimensions of each view before the different views are maximally correlated in a common latent space. This feature decorrelation is achieved by enforcing an exact decorrelation constraint; these models are thus computationally expensive due to the matrix inversion or SVD operations required for exact decorrelation at each training iteration. Furthermore, the decorrelation step is often separated from the gradient descent based optimisation, resulting in sub-optimal solutions. We propose a novel deep CCA model Soft CCA to overcome these problems. Specifically, exact decorrelation is replaced by soft decorrelation via a mini-batch based Stochastic Decorrelation Loss (SDL) to be optimised jointly with the other training objectives. Extensive experiments show that the proposed soft CCA is more effective and efficient than existing deep CCA models. In addition, our SDL loss can be applied to other deep models beyond multi-view learning, and obtains superior performance compared to existing decorrelation losses.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00161,
39334eea31ff15406005ce9dc0be97c30298dc81,1,1,0,Joint Learning of Body and Part Representation for Person Re-Identification,"Person re-identification (ReID), aiming to identify people among multiple camera views, has attracted an increasing attention due to the potential of application in surveillance security. Large variations in subjects’ postures, view angles, and illuminating conditions as well as non-ideal human detection significantly increase the difficulty of person ReID. Learning a robust metric for measuring the similarity between different person images is another under-addressed problem. In this paper, following the recent success of part-based models, in order to generate a discriminative and robust feature representation, we first propose to learn global and weighted local body-part features from pedestrian images. Then, in the training phase, angular loss and part-level classification loss are employed jointly as a similarity measure to train the network, which significantly improves the robustness of the resultant network against feature variance. Experimental results on several benchmark data sets demonstrate that our method outperforms the state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2864588,
39bf8e4256c3ed75545a1b10e669cb42625b3198,0,1,0,A Hybrid Deep Model for Person Re-Identification,"In this study, we present a hybrid model that combines the advantages of the identification, verification and triplet models for person re-identification. Specifically, the proposed model simultaneously uses Online Instance Matching (OIM), verification and triplet losses to train the carefully designed network. Given a triplet images, the model can output the identities of the three input images and the similarity score as well as make the L-2 distance between the mismatched pair larger than the one between the matched pair. Experiments on two benchmark datasets (CUHK01 and Market-1501) show that the proposed method can achieve favorable accuracy while compared with other state of the art methods.",2018,ICIC,,10.1007/978-3-319-95957-3_25,
39c99000839d66101bddcfc00aa93476245a7f58,1,0,0,Diversity-Achieving Slow-DropBlock Network for Person Re-Identification,"A big challenge of person re-identification (Re-ID) using a multi-branch network architecture is to learn diverse features from the ID-labeled dataset. The 2-branch Batch DropBlock (BDB) network was recently proposed for achieving diversity between the global branch and the feature-dropping branch. In this paper, we propose to move the dropping operation from the intermediate feature layer towards the input (image dropping). Since it may drop a large portion of input images, this makes the training hard to converge. Hence, we propose a novel double-batch-split co-training approach for remedying this problem. In particular, we show that the feature diversity can be well achieved with the use of multiple dropping branches by setting individual dropping ratio for each branch. Empirical evidence demonstrates that the proposed method performs superior to BDB on popular person Re-ID datasets, including Market-1501, DukeMTMC-reID and CUHK03 and the use of more dropping branches can further boost the performance.",2020,ArXiv,2002.04414,,https://arxiv.org/pdf/2002.04414.pdf
39df6ca15f41e5a674ed8cd1654e699dbc8b8c11,1,0,0,Human tracking over camera networks: a review,"In recent years, automated human tracking over camera networks is getting essential for video surveillance. The tasks of tracking human over camera networks are not only inherently challenging due to changing human appearance, but also have enormous potentials for a wide range of practical applications, ranging from security surveillance to retail and health care. This review paper surveys the most widely used techniques and recent advances for human tracking over camera networks. Two important functional modules for the human tracking over camera networks are addressed, including human tracking within a camera and human tracking across non-overlapping cameras. The core techniques of human tracking within a camera are discussed based on two aspects, i.e., generative trackers and discriminative trackers. The core techniques of human tracking across non-overlapping cameras are then discussed based on the aspects of human re-identification, camera-link model-based tracking and graph model-based tracking. Our survey aims to address existing problems, challenges, and future research directions based on the analyses of the current progress made toward human tracking techniques over camera networks.",2017,EURASIP J. Adv. Signal Process.,,10.1186/s13634-017-0482-z,
39fe64ac062e9d1117f6ce71ef2df5e6b55a8d89,0,1,0,AP-GAN: Adversarial patch attack on content-based image retrieval systems,"Key Smart City applications such as traffic management and public security rely heavily on the intelligent processing of video and image data, often in the form of visual retrieval tasks, such as person Re-IDentification (ReID) and vehicle re-identification. For these tasks, Deep Neural Networks (DNNs) have been the dominant solution for the past decade, for their remarkable ability in learning discriminative features from images to boost retrieval performance. However, it is been discovered that DNNs are broadly vulnerable to maliciously constructed adversarial examples. By adding small perturbations to a query image, the returned retrieval results will be completely dissimilar from the query image. This poses serious challenges to vital systems in Smart City applications that depend on the DNN-based visual retrieval technology, as in the physical world, simple camouflage can be added on the subject (a few patches on the body or car), and turn the subject completely untrackable by person or vehicle Re-ID systems. To demonstrate the potential of such threats, this paper proposes a novel adversarial patch generative adversarial network (AP-GAN) to generate adversarial patches instead of modifying the entire image, which also causes the DNNs-based image retrieval models to return incorrect results. AP-GAN is trained in an unsupervised way that requires only a small amount of unlabeled data for training. Once trained, it produces query-specific perturbations for query images to form adversarial queries. Extensive experiments show that the AP-GAN achieves excellent attacking performance with various application scenarios that are based on deep features, including image retrieval, person ReID and vehicle ReID. The results of this study provide a warning that when deploying a DNNs-based image retrieval system, its security and robustness needs to be thoroughly considered.",2020,GeoInformatica,,10.1007/s10707-020-00418-7,
3a0c2cf66bf8b33907aeee15a9e2d3b8eb4f3191,0,1,0,Cross Domain Person Re-Identification With Large Scale Attribute Annotated Datasets,"We propose a novel deep convolutional neural network framework called deep augmented attribute network (DAAN) to learn augmented attribute features for cross-domain person re-identification (person Re-ID) task. We observed that in some cases, different persons could have similar attributes (e.g., wearing similar clothes). It motivates us to distinguish such pedestrians by further learning complementary image features. We thus construct a deep neural network with three branches: 1) the attribute branch predicts the attributes of the input image; 2) the augmentation branch generates complementary features that are fused with the output of the attribute branch to form the augmented attribute features; and 3) the reconstruction branch to refine augmented attribute features on the target dataset. In order to learn precise and detailed attributes for pedestrian, we manually labeled two large datasets (CUHK03 and Market-1501) with 25 pre-defined mid-level semantic attributes. We evaluate the DAAN on a series of cross-domain person Re-ID tasks, where DAAN demonstrates superior performance (around 6%) to the prior state-of-the-art cross-domain algorithms.",2019,IEEE Access,,10.1109/ACCESS.2019.2896663,
3a8d40c750d147ce8f88b674aedd141b1448cca1,1,0,0,Learning to Align via Wasserstein for Person Re-Identification,"Existing successful person re-identification (Re-ID) models often employ the part-level representation to extract the fine-grained information, but commonly use the loss that is particularly designed for global features, ignoring the relationship between semantic parts. In this paper, we present a novel triplet loss that emphasizes the salient parts and also takes the consideration of alignment. This loss is based on the crossing-bing matching metric that also known as Wasserstein Distance. It measures how much effort it would take to move the embeddings of local features to align two distributions, such that it is able to find an optimal transport matrix to re-weight the distance of different local parts. The distributions in support of local parts is produced via a new attention mechanism, which is calculated by the inner product between high-level global feature and local features, representing the importance of different semantic parts w.r.t. identification. We show that the obtained optimal transport matrix can not only distinguish the relevant and misleading parts, and hence assign different weights to them, but also rectify the original distance according to the learned distributions, resulting in an elegant solution for the mis-alignment issue. Besides, the proposed method is easily implemented in most Re-ID learning system with end-to-end training style, and can obviously improve their performance. Extensive experiments and comparisons with recent Re-ID methods manifest the competitive performance of our method.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2998931,
3ac08ced28e2de43c51397dfabeb1158f3f2b326,1,0,0,Multi-view Person Re-identification in a Fisheye Camera Network with Different Viewing Directions,"Person re-identification is a crucial component for multi-camera networks in different real-world applications such as surveillance, automation, and business analytics. Despite considerable recent progress, the performance in practice is still not satisfactory due to the high intra-person variation and significant complexity of the task, including differences in scale, viewing direction, and illumination. We propose a novel approach for person re-identification, which exploits multi-view information of a fisheye camera looking downwards from the ceiling. To handle this highly variable multi-view information, we build a generic pipeline for processing fisheye camera imagery based on geometric sensor modelling and deep learning. The proposed approach is evaluated on a re-mapped version of the publicly available Market-1501 dataset, and, in addition, on a new fisheye dataset. Significant improvements are shown in our experiments: our approach achieves more than 97% rank#1 recognition rate if applied on the re-mapped Market-1501 dataset; on the new fisheye dataset we find an improvement of about 12% compared to random-view fusion.",2019,"PFG – Journal of Photogrammetry, Remote Sensing and Geoinformation Science",,10.1007/s41064-019-00083-y,https://link.springer.com/content/pdf/10.1007%2Fs41064-019-00083-y.pdf
3b24dcb3a1ff4811386b3467943c0ccad266bc99,1,0,0,Adaptive Re-ranking of Deep Feature for Person Re-identification,"Typical person re-identification (re-ID) methods train a deep CNN to extract deep features and combine them with a distance metric for the final evaluation. In this work, we focus on exploiting the full information encoded in the deep feature to boost the re-ID performance. First, we propose a Deep Feature Fusion (DFF) method to exploit the diverse information embedded in a deep feature. DFF treats each sub-feature as an information carrier and employs a diffusion process to exchange their information. Second, we propose an Adaptive Re-Ranking (ARR) method to exploit the contextual information encoded in the features of neighbors. ARR utilizes the contextual information to re-rank the retrieval results in an iterative manner. Particularly, it adds more contextual information after each iteration automatically to consider more matches. Third, we propose a strategy that combines DFF and ARR to enhance the performance. Extensive comparative evaluations demonstrate the superiority of the proposed methods on three large benchmarks.",2018,ArXiv,1811.08561,,https://arxiv.org/pdf/1811.08561.pdf
3b311a1ce30f9c0f3dc1d9c0cf25f13127a5e48c,1,1,0,A Coarse-to-fine Pyramidal Model for Person Re-identification via Multi-Loss Dynamic Training,"Most existing Re-IDentification (Re-ID) methods are highly dependent on precise bounding boxes that enable images to be aligned with each other. However, due to the inevitable challenging scenarios, current detection models often output inaccurate bounding boxes yet, which inevitably worsen the performance of these Re-ID algorithms. In this paper, to relax the requirement, we propose a novel coarse-to-fine pyramid model that not only incorporates local and global information, but also integrates the gradual cues between them. The pyramid model is able to match the cues at different scales and then search for the correct image of the same identity even when the image pair are not aligned. In addition, in order to learn discriminative identity representation, we explore a dynamic training scheme to seamlessly unify two losses and extract appropriate shared information between them. Experimental results clearly demonstrate that the proposed method achieves the state-of-the-art results on three datasets and it is worth noting that our approach exceeds the current best method by 9.5% on the most challenging dataset CUHK03.",2018,ArXiv,1810.12193,,https://arxiv.org/pdf/1810.12193.pdf
3b8a6782601acd444d00c9bcecddf5f076a7dfce,0,1,0,Self-supervised data augmentation for person re-identification,"Abstract Although person re-identification (ReID) has been intensively studied over the past few years, the shortage of annotated training data stands as an obstacle for further performance improvement. To address this issue, many data augmentation methods have been successfully applied to person ReID, such as random scaling, flipping and cropping, which mainly operate on a single image, whilst overlooking the relationship between images. Recently, generative adversarial networks (GANs) have been widely used for data augmentation and smoothly migrated to person ReID. However, the cost of training GAN is expensive and the performance improvement is often limited. Moreover, all these methods focus on augmenting samples for existing IDs. This paper proposes a simple yet effective data augmentation strategy based on self-supervised learning to handle these problems, which consists of the offline ID augmentation that can generate new categories and the online instance augmentation. These two components are integrated i nto a unified framework for boosting the ReID performance. Furthermore, a novel Siamese-like network is developed for ReID in conjunction with the proposed data augmentation method. Extensive experiments on three benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art by a clear margin, which verifies the robustness and the effectiveness of our method. Code will be released at:  https://github.com/flychen321/data_aug_reid .",2020,Neurocomputing,,10.1016/j.neucom.2020.07.087,
3b9e32152a5ffceafc63da356ad73a8e7d1e2623,0,1,0,An Automatic System for Generating Artificial Fake Character Images,"Due to the introduction of deep learning for text detection and recognition in natural scenes, and the increase in detecting fake images in crime applications, automatically generating fake character images has now received greater attentions. This paper presents a new system named Fake Character GAN (FCGAN). It has the ability to generate fake and artificial scene characters that have similar shapes and colors with the existing ones. The proposed method first extracts shapes and colors of character images. Then, it constructs the FCGAN, which consists of a series of convolution, residual and transposed convolution blocks. The extracted features are then fed to the FCGAN to generate fake characters and verify the quality of the generated characters simultaneously. The proposed system chooses characters from the benchmark ICDAR 2015 dataset for training, and further validated by conducting text detection and recognition experiments on input and generated fake images to show its effectiveness.",2019,MMM,,10.1007/978-3-030-05716-9_24,
3bb4242420255590eeaf45fe0b7c3b489fa87303,0,1,0,A Richly Annotated Pedestrian Dataset for Person Retrieval in Real Surveillance Scenarios,"Retrieving specific persons with various types of queries, e.g., a set of attributes or a portrait photo has great application potential in large-scale intelligent surveillance systems. In this paper, we propose a richly annotated pedestrian (RAP) dataset which serves as a unified benchmark for both attribute-based and image-based person retrieval in real surveillance scenarios. Typically, previous datasets have three improvable aspects, including limited data scale and annotation types, heterogeneous data source, and controlled scenarios. Differently, RAP is a large-scale dataset which contains 84928 images with 72 types of attributes and additional tags of viewpoint, occlusion, body parts, and 2589 person identities. It is collected in the real uncontrolled scene and has complex visual variations in pedestrian samples due to the change of viewpoints, pedestrian postures, and cloth appearance. Towards a high-quality person retrieval benchmark, an amount of state-of-the-art algorithms on pedestrian attribute recognition and person re-identification (ReID), are performed for quantitative analysis with three evaluation tasks, i.e., attribute recognition, attribute-based and image-based person retrieval, where a new instance-based metric is proposed to measure the dependency of the prediction of multiple attributes. Finally, some interesting problems, e.g., the joint feature learning of attribute recognition and ReID, and the problem of cross-day person ReID, are explored to show the challenges and future directions in person retrieval.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2018.2878349,
3be6fc6b39325854697bc583cf28c59742e23baf,1,0,0,Traffic-Aware Multi-Camera Tracking of Vehicles Based on ReID and Camera Link Model,"Multi-target multi-camera tracking (MTMCT), i.e., tracking multiple targets across multiple cameras, is a crucial technique for smart city applications. In this paper, we propose an effective and reliable MTMCT framework for vehicles, which consists of a traffic-aware single camera tracking (TSCT) algorithm, a trajectory-based camera link model (CLM) for vehicle re-identification (ReID), and a hierarchical clustering algorithm to obtain the cross camera vehicle trajectories. First, the TSCT, which jointly considers vehicle appearance, geometric features, and some common traffic scenarios, is proposed to track the vehicles in each camera separately. Second, the trajectory-based CLM is adopted to facilitate the relationship between each pair of adjacently connected cameras and add spatio-temporal constraints for the subsequent vehicle ReID with temporal attention. Third, the hierarchical clustering algorithm is used to merge the vehicle trajectories among all the cameras to obtain the final MTMCT results. Our proposed MTMCT is evaluated on the CityFlow dataset and achieves a new state-of-the-art performance with IDF1 of 74.93%.",2020,ACM Multimedia,2008.09785,10.1145/3394171.3413863,https://arxiv.org/pdf/2008.09785.pdf
3c4cb62be288c1881326b26089f3c11e9498632f,1,1,0,"Pedestrian Models for Autonomous Driving Part I: low level models, from sensing to tracking","Abstract—Autonomous vehicles (AVs) must share space with pedestrians, both in carriageway cases such as cars at pedestrian crossings and off-carriageway cases such as delivery vehicles navigating through crowds on pedestrianized high-streets. Unlike static obstacles, pedestrians are active agents with complex, inter- active motions. Planning AV actions in the presence of pedestrians thus requires modelling of their probable future behaviour as well as detecting and tracking them. This narrative review article is Part I of a pair, together surveying the current technology stack involved in this process, organising recent research into a hierarchical taxonomy ranging from low-level image detection to high-level psychology models, from the perspective of an AV designer. This self-contained Part I covers the lower levels of this stack, from sensing, through detection and recognition, up to tracking of pedestrians. Technologies at these levels are found to be mature and available as foundations for use in high-level systems, such as behaviour modelling, prediction and interaction control.",2020,ArXiv,2002.11669,10.1109/TITS.2020.3006768,https://arxiv.org/pdf/2002.11669.pdf
3c83af7ecdf477386a5829e95513a7d66fb0eeaa,1,0,0,AdaptiveReID: Adaptive L2 Regularization in Person Re-Identification,"We introduce an adaptive L2 regularization mechanism termed AdaptiveReID, in the setting of person re-identification. In the literature, it is common practice to utilize hand-picked regularization factors which remain constant throughout the training procedure. Unlike existing approaches, the regularization factors in our proposed method are updated adaptively through backpropagation. This is achieved by incorporating trainable scalar variables as the regularization factors, which are further fed into a scaled hard sigmoid function. Extensive experiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the effectiveness of our framework. Most notably, we obtain state-of-the-art performance on MSMT17, which is the largest dataset for person re-identification. Source code will be published at this https URL.",2020,ArXiv,2007.07875,,https://arxiv.org/pdf/2007.07875.pdf
3c89455d9a91560eb08e59237dbc4f9fac16ff09,1,1,0,Foreground-Aware Pyramid Reconstruction for Alignment-Free Occluded Person Re-Identification,"Re-identifying a person across multiple disjoint camera views is important for intelligent video surveillance, smart retailing and many other applications. However, existing person re-identification methods are challenged by the ubiquitous occlusion over persons and suffer performance degradation. This paper proposes a novel occlusion-robust and alignment-free model for occluded person ReID and extends its application to realistic and crowded scenarios. The proposed model first leverages the fully convolution network (FCN) and pyramid pooling to extract spatial pyramid features. Then an alignment-free matching approach namely Foreground-aware Pyramid Reconstruction (FPR) is developed to accurately compute matching scores between occluded persons, regardless of their different scales and sizes. FPR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two persons. More importantly, we design a occlusion-sensitive foreground probability generator that focuses more on clean human body parts to robustify the similarity computation with less contamination from occlusion. The FPR is easily embedded into any end-to-end person ReID models. The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30%), Partial iLIDS (68.08%), Occluded REID (81.00%), and three benchmark person datasets: Market1501 (95.42%), DukeMTMC (88.64%), CUHK03 (76.08%).",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.04975,10.1109/ICCV.2019.00854,https://arxiv.org/pdf/1904.04975.pdf
3ca227ec7b6fc1eeadcbf6d84a04ff77db0da290,1,0,0,A Bayesian 3D Multi-view Multi-object Tracking Filter,"This paper proposes an online multi-camera multi-object tracker that only requires monocular detector training, independent of the multi-camera configurations, allowing seamless extension/deletion of cameras without (retraining) effort. The proposed algorithm has a linear complexity in the total number of detections across the cameras, and hence scales gracefully with the number of cameras. It operates in 3D world frame, and provides 3D trajectory estimates of the objects. The key innovation is a high fidelity yet tractable 3D occlusion model, amenable to optimal Bayesian multi-view multi-object filtering, which seamlessly integrates, into a single Bayesian recursion, the sub-tasks of track management, state estimation, clutter rejection, and occlusion/misdetection handling. The proposed algorithm is evaluated on the latest WILDTRACKS dataset, and demonstrated to work in very crowded scenes on a new dataset.",2020,ArXiv,,,
3cb1103dd1fedddc7524888eed519ebe09ac7f0c,0,1,0,Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild,"With the rapid development of neural architecture search (NAS), researchers found powerful network architectures for a wide range of vision tasks. However, it remains unclear if the searched architecture can transfer across different types of tasks as manually designed ones did. This paper puts forward this problem, referred to as NAS in the wild, which explores the possibility of finding the optimal architecture in a proxy dataset and then deploying it to mostly unseen scenarios.  We instantiate this setting using a currently popular algorithm named differentiable architecture search (DARTS), which often suffers unsatisfying performance while being transferred across different tasks. We argue that the accuracy drop originates from the formulation that uses a super-network for search but a sub-network for re-training. The different properties of these stages have resulted in a significant optimization gap, and consequently, the architectural parameters ""over-fit"" the super-network. To alleviate the gap, we present a progressive method that gradually increases the network depth during the search stage, which leads to the Progressive DARTS (P-DARTS) algorithm. With a reduced search cost (7 hours on a single GPU), P-DARTS achieves improved performance on both the proxy dataset (CIFAR10) and a few target problems (ImageNet classification, COCO detection and three ReID benchmarks). Our code is available at \url{this https URL}.",2019,ArXiv,1912.10952,10.1007/s11263-020-01396-x,https://arxiv.org/pdf/1912.10952.pdf
3cbf60c4a73fadd05b59c3abd19df032303e8577,1,1,0,Incremental Deep Hidden Attribute Learning,"Person re-identifcation is a key technique to match person images captured in non-overlapping camera views. Due to the sensitivity of visual features to environmental changes, semantic attributes, such as ""short-hair"" or ""long-hair"", begin to be investigated to represent person's appearance to improve the re-identifcation performance. Generally, training semantic attribute representations requires massive annotated samples, which limits the applicability on the large-scale practical applications. To alleviate the reliance on annotation efforts, we propose a new person representation with hidden attributes by mining latent information from visual feature in an unsupervised manner. In particular, an auto-encoder model is plugged-in to the deep learning network to compose a Deep Hidden Attribute Network (DHA-Net). The learnt hidden attribute representation preserves the robustness of semantic attributes and simultaneously inherits the discrimination ability of visual features. Experiments conducted on public datasets have validated the effectiveness of DHA-Net. On two large-scale datasets, i.e., Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.",2018,ACM Multimedia,,10.1145/3240508.3240510,
3d402f1ef2185695a33ca0a6cc21985f4dea4b3e,1,1,0,Illumination-Invariant Person Re-Identification,"Due to the effect of weak illumination, person images captured by surveillance cameras usually contain various degradations such as color shift, low contrast and noise. These degradations result in severe discriminant information loss, which makes the person re-identification (re-id) more challenging. However, existing person re-identification approaches are designed based on the assumption that the pedestrians images are under well lighting conditions, which is impractical in real-world scenarios. Inspired by the Retinex theory, we propose a illumination-invariant person re-identification framework which is able to simultaneously achieve Retinex illumination decomposition and person re-identification. We first verify that directly using weak illuminated images can greatly reduce the performance of person re-id. We then design a bottom-up attention network to remove the effect of weak illumination and obtain the enhanced image without introducing over-enhancement. To effectively connect low-level and high-level vision tasks, a joint training strategy is further introduced to boost the performance of person re-id under weak illumination conditions. Experiments have demonstrated the advantages of our method on benchmarks with severe lighting changes and low light conditions.",2019,ACM Multimedia,,10.1145/3343031.3350994,
3d46d366d060ed799fd62e1c1bf6b3b160bef0dc,0,1,0,Adaptive Transfer Network for Cross-Domain Person Re-Identification,"Recent deep learning based person re-identification approaches have steadily improved the performance for benchmarks, however they often fail to generalize well from one domain to another. In this work, we propose a novel adaptive transfer network (ATNet) for effective cross-domain person re-identification. ATNet looks into the essential causes of domain gap and addresses it following the principle of ""divide-and-conquer"". It decomposes the complicated cross-domain transfer into a set of factor-wise sub-transfers, each of which concentrates on style transfer with respect to a certain imaging factor, e.g., illumination, resolution and camera view etc. An adaptive ensemble strategy is proposed to fuse factor-wise transfers by perceiving the affect magnitudes of various factors on images. Such ""decomposition-and-ensemble"" strategy gives ATNet the capability of precise style transfer at factor level and eventually effective transfer across domains. In particular, ATNet consists of a transfer network composed by multiple factor-wise CycleGANs and an ensemble CycleGAN as well as a selection network that infers the affects of different factors on transferring each image. Extensive experimental results on three widely-used datasets, i.e., Market-1501, DukeMTMC-reID and PRID2011 have demonstrated the effectiveness of the proposed ATNet with significant performance improvements over state-of-the-art methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00737,http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Adaptive_Transfer_Network_for_Cross-Domain_Person_Re-Identification_CVPR_2019_paper.pdf
3db9031ce4465fd8cab23910099d99f483718a81,0,1,0,Person Re-identification in Aerial Imagery,"Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles (UAVs), visual surveillance by utilizing the UAV platform has been very attractive. Most of the research works for UAV captured visual data are mainly focused on the tasks of object detection and tracking. However, limited attention has been paid to the task of person Re-identification (ReID) which has been widely studied in ordinary surveillance cameras with fixed emplacements. In this paper, to facilitate the research of person ReID in aerial imagery, we collect a large scale airborne person ReID dataset named as Person ReID for Aerial Imagery (PRAI-1581), which consists of 39,461 images of 1581 person identities. The images of the dataset are shot by two DJI consumer UAVs flying at an altitude ranging from 20 to 60 meters above the ground, which covers most of the real UAV surveillance scenarios. In addition, we propose to utilize subspace pooling of convolution feature maps to represent the input person images. Our method can learn a discriminative and compact feature representation for ReID in aerial imagery and can be trained in an end-to-end fashion efficiently. We conduct extensive experiments on the proposed dataset and the experimental results demonstrate that re-identify persons in aerial imagery is a challenging problem, where our method performs favorably against state of the arts. Our dataset can be accessed via \url{this https URL}.",2019,ArXiv,1908.05024,10.1109/TMM.2020.2977528,https://arxiv.org/pdf/1908.05024.pdf
3dd74e372e791e1cb1a17ce40642d00f46d1d6f6,1,1,1,Unsupervised Person Re-identification by Combining Appearance Features with Spatial-temporal Features,"Most of the existing person re-identification methods usually follow a supervised learning framework and train models based on a large number of labeled pedestrian images. However, directly deploying these trained models in real scenes will lead to poor performances, because the target domain data may be completely different from the training data, thus the model parameters cannot be well fitted. Furthermore, it is very time-consuming and impractical to label a large number of data. In order to solve these problems, we propose a simple and effective strategy for segmentation based on key parts aiming to obtain the discriminative appearance features. Simultaneously, we constructs a hybrid Gaussian model by calculating the time difference of pedestrian groups to acquire spatial-temporal features. Finally, a measure fusion model is used to combine the appearance measure matrix and spatial-temporal distance matrix, which greatly improves the performance of the unsupervised person re-identification. We conduct extensive experiments on the large-scale image datasets, including Market-1501 and DukeMTMC-reID. The experimental results demonstrate that our algorithm is superior to state-of-the-art unsupervised re-identification approaches.",2020,MM 2020,,10.1145/3404716.3404723,
3de037485131a6ff30dd1620a98b9315d26eda27,0,1,0,An Intelligent System for Coffee Grading and Disease Identification. (Un système intelligent pour le classement du café et l'identification des maladies),"One of the key factors in the success of deep learning models is their ability to learn important representations from the input data automatically, without the need for human experts designing task specific features. However, to learn these representations, deep learning methods usually require large amounts of data, which are expensive to obtain especially because of the efforts required for gathering and labeling data. This thesis investigates the applicability of deep learning for real world situations where data exist in small amounts, collected at different locations (labs), using different acquisition techniques and with minimally controlled conditions. There are two main contribution. First, we tackle the problem of coffee plant disease detection, which up to now was not approached in the literature, using a dataset containing images of coffee plant disease downloaded from the internet and that we captured in the farm, and a transfer-learning approach. It was possible to design a model that classifies coffee plant diseases with test accuracy of 90.18% using only 562 images of coffee plant. Second, we tackle the problem of coffee beans grading. A dataset for coffee beans grading is created by capturing images of coffee beans at the Jimma branch of Ethiopia's Commodity Exchange (ECX) in two rounds. In an attempt to solve the dataset shift that occurred in the two sets because of illumination and camera differences, color correction algorithms were added to the existing image processing and data augmentation pipeline. However, these techniques did not improve the performance of the model when compared to the commonly used preprocessing methods. This indicates that the difference in the image acquisition techniques was not the only reason for the dataset shift. We proposed and evaluated a network architecture that combined with data augmentation and ensemble learning techniques, led to an improved classifier (89.1% accuracy on the test dataset) that grades coffee beans, which performs better than the classical machine learning approaches (25.47% improvement) and off-the-shelf deep learning models (18% improvement). The coffee beans dataset and the models will be made publicly available to support further research on these important topics.",2020,,,,https://hal.archives-ouvertes.fr/tel-02506162/file/Serawork-thesis.pdf
3dffacda086689c1bcb01a8dad4557a4e92b8205,1,0,0,Multiple Object Tracking: A Literature Review,"Multiple Object Tracking (MOT) is an important computer vision problem which has gained increasing attention due to its academic and commercial potential. Although different kinds of approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in a multiple object tracking system, including formulation, categorization, key principles, evaluation of an MOT are discussed. 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks. 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative comparisons. We also point to some interesting discoveries by analyzing these results. 4) We provide a discussion about issues of MOT research, as well as some interesting directions which could possibly become potential research effort in the future.",2014,,,,http://arxiv.org/pdf/1409.7618v3.pdf
3e05ff3d86dd0abf8acd333414c24916c2ba9712,1,0,0,Learning multi-level domain invariant features for sketch re-identification,"Abstract Person re-identification (Re-ID) aims to match the photos of suspects in the gallery database by a query photo. Under Re-ID, the input is usually a query photo of the target person, however, in a realistic setting, such a photo is not always available. In this paper, we study the problem of Sketch Re-ID, which is not based on a person's photo but the professional sketch of the target person. We use a full-body sketch of a target person drawn by a professional to find the photo with the same ID in the gallery. This problem is very challenging because photos and sketches belong to two completely different domains. Specifically, a sketch is a highly abstract description of a person, which only contains some rough outline information. We address the Sketch Re-ID problem by proposing a novel framework to jointly model photos and sketches into a common embedding space. Our framework uses a triplet classification network as a base network. We propose to use a spatial attention module and combine high-level and mid-level output features of CNN to represent the input images. Moreover, we design a novel domain-invariant feature by using a gradient reverse layer (GRL) to solve the domain gap problem. We validated our approach on the Sketch Re-ID dataset, which contains 200 persons, each of whom has a sketch and two photos from different cameras associated. To evaluate the generalization of our method, we also performed experiments on some other public Sketch-based Image Retrieval (SBIR) datasets. The extensive experimental results show that our method can get higher performance than state-of-the-art models.",2020,Neurocomputing,,10.1016/j.neucom.2020.04.060,
3e0e32ca5db5975d6c34b1dc4cd1f0ead1e7ba21,1,1,1,Torchreid: A Library for Deep Learning Person Re-Identification in Pytorch,"Person re-identification (re-ID), which aims to re-identify people across different camera views, has been significantly advanced by deep learning in recent years, particularly with convolutional neural networks (CNNs). In this paper, we present Torchreid, a software library built on PyTorch that allows fast development and end-to-end training and evaluation of deep re-ID models. As a general-purpose framework for person re-ID research, Torchreid provides (1) unified data loaders that support 15 commonly used re-ID benchmark datasets covering both image and video domains, (2) streamlined pipelines for quick development and benchmarking of deep re-ID models, and (3) implementations of the latest re-ID CNN architectures along with their pre-trained models to facilitate reproducibility as well as future research. With a high-level modularity in its design, Torchreid offers a great flexibility to allow easy extension to new datasets, CNN models and loss functions.",2019,ArXiv,1910.10093,,https://arxiv.org/pdf/1910.10093.pdf
3e7454b1332d9117f376d53c266ac917dbe29010,0,1,0,Improve Person Re-Identification With Part Awareness Learning,"Person re-identification (ReID) aims to predict whether two images from different cameras belong to the same person. Due to low image quality and variance in view point and body pose, it remains a difficult task. To solve the task, a model is supposed to appropriately capture features that describe body regions for identification. With the simple intuition that explicitly incorporating ReID model with part awareness could be beneficial for learning a more discriminative feature space, we propose part segmentation as an assistant body perception task during the training of a ReID model. Specifically, we add a lightweight segmentation head to the backbone of ReID model during training, which is supervised with part labels. Note that our segmentation head is only introduced during training and that it does not change network input or the way of extracting ReID feature. Experiments show that part segmentation considerably improves the performance of ReID. Through quantitative and qualitative analyses, we further reveal that body part perception helps ReID model to capture a set of more diverse features from the body, with decreased similarity between part features and increased focus on different body regions. We experiment with various representative ReID models and achieve consistent improvement on several large-scale datasets including Market1501, CUHK03, DukeMTMC-reID and MSMT17. E.g. on MSMT17, our method increases Rank-1 Accuracy of GlobalPool-ResNet-50, PCB and MGN by 2.3%, 2.9% and 3.9%, respectively. Incorporated with MGN, our model achieves state-of-the-art performance, with Rank-1 Accuracy 95.8%, 78.8%, 90.0% and 84.0% on four datasets, respectively.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3003442,
3ea6de39fcd31a446883f3ff0852c24f13f5de46,1,0,0,Viewpoint-robust Person Re-identification via Deep Residual Equivariant Mapping and Fine-grained Features,"Existing person re-identification methods usually directly calculate the similarities of person pictures regardless of their viewpoints. However, matching persons under different viewpoints is difficult since it is intrinsically hard to directly learn a representation which is geometrically invariant to large viewpoint variations. In this paper, we explicitly take viewpoint information into account and propose a novel Deep Residual Equivariant Mapping and Fine-grained Features (DREMFF) approach for viewpoint-robust person re-identification. Specifically, DREMFF hypothesizes that there exists inherent mapping between different viewpoints of a person, and consequently, the global representation discrepancy of a person under different viewpoints will be bridged through equivariant mapping by adaptively adding residuals to original representation according corresponding angle deviation. What’s more, based on attention mechanism, DREMFF extracts fine-grained features for each image from multiple salient regions as well as different scales. These captured information is capable of providing assistant decision-making at lower granularities. The mapped global features and the learned fine-grained features work collaboratively to enable viewpoint-robust person re-identification. Experiments on three challenging benchmarks consistently demonstrate the effectiveness of the proposed approach.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852125,
3eaa1ada0667245f32d507d702c9d143a55d7c1a,1,0,0,Heterogeneous Association Graph Fusion for Target Association in Multiple Object Tracking,"Tracking-by-detection is one of the most popular approaches to tracking multiple objects in which the detector plays an important role. Sometimes, detector failures caused by occlusions or various poses are unavoidable and lead to tracking failure. To cope with this problem, we construct a heterogeneous association graph that fuses high-level detections and low-level image evidence for target association. Compared with other methods using low-level information, our proposed heterogeneous association fusion (HAF) tracker is less sensitive to particular parameters and is easier to extend and implement. We use the fused association graph to build track trees for HAF and solve them by the multiple hypotheses tracking framework, which has been proven to be competitive by introducing efficient pruning strategies. In addition, the novel idea of adaptive weights is proposed to analyze the contribution between motion and appearance. We also evaluated our results on the MOT challenge benchmarks and achieved state-of-the-art results on the MOT Challenge 2017.",2019,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2018.2882192,https://ieeexplore.ieee.org/ielx7/76/8886618/08540450.pdf
3eb2ed57124aad74badc7534513ba8299cbe66d9,1,0,0,A novel multi-view pedestrian detection database for collaborative Intelligent Transportation Systems,"Abstract Recent advances in machine-learning, especially in deep neural networks have significantly accelerated the development and deployment of transport-oriented intelligent designs with increasingly high efficiency. While these technologies are exceptionally promising towards revolutionizing our current mobility and reducing the number of road accidents, the way to safe Intelligent Transportation Systems (ITS) remains long. Since pedestrians are the most vulnerable road users, designing accurate pedestrian detection methods is a priority task. However, traditional monocular pedestrian detection methods are limited, especially in occlusion handling. Hence, a collaborative perception scheme in which vehicles no longer restrict their input data to their immediate embedded sensors and rather exploit data from remote sensors is necessary to achieve a more comprehensive environment perception. In this work, we propose a novel public dataset: Infrastructure to Vehicle Multi-View Pedestrian Detection Database (I2V-MVPD) that combines synchronized images from both a mobile camera embedded in a car and a static camera in the road infrastructure. We also propose a new multi-view pedestrian detection framework based on collaborative intelligence between vehicles and infrastructure. Our results show a significant improvement in detection performance over monocular detection.",2020,Future Gener. Comput. Syst.,,10.1016/j.future.2020.07.025,
3ec22eb4c4c8f2f389730f1e4ab5aab8ecc5b350,1,1,0,Improving person re-identification performance by customized dataset and person detection,"For person re-identification (re-ID), nearly all person re-ID algorithms use public person re-ID datasets, where these datasets all consist of predefined image crops containing a single person. Unfortunately, these image crops are not optimal for video analysis, so that the person detection becomes suboptimal and person re-ID obtains a lower performance score. In this work, several techniques are presented that customize the person images of a popular public person re-ID dataset. These techniques consist of customization algorithms based on postprocessing the person-detection bounding boxes using the original frames, resulting in several customized datasets to better facilitate person re-identification. We have evaluated five different ways for customization, based on widening the image crops, various aspect ratios and resolutions, and person instance segmentation. We have obtained a significant increase in performance with widened image crops, yielding a convincing performance increase of nearly 3% in the resulting Rank-1 score. Furthermore, when the applied random-cropping process is further optimized to this customization technique, an increase of even more than 4% is obtained. Both performance gains are a strong indication that any future person re-ID system may benefit from customizations based on the original video frames or from specializing the person detector.",2019,Image Processing: Algorithms and Systems,,10.2352/issn.2470-1173.2019.11.ipas-268,https://pdfs.semanticscholar.org/aaf1/e774a7df84cc70fc39529a6247c8879be7b2.pdf
3ed490a0f9c22fce4e1d56bd3933d39f97afd914,1,1,0,Progressive Bilateral-Context Driven Model for Post-Processing Person Re-Identification,"Most existing person re-identification methods compute pairwise similarity by extracting robust visual features and learning the discriminative  metric. Owing to visual ambiguities, these content-based methods that determine the pairwise relationship only based on the similarity between  them, inevitably produce a suboptimal ranking list. Instead, the pairwise similarity can be estimated more accurately along the geodesic path of  the underlying data manifold by exploring the rich contextual information of the sample. In this paper, we propose a lightweight post-processing  person re-identification method in which the pairwise measure is determined by the relationship between the sample and the counterpart's  context in an unsupervised way. We translate the point-to-point comparison into the bilateral point-to-set comparison. The sample's context is  composed of its neighbor samples with two different definition ways: the first order context and the second order context, which are used to  compute the pairwise similarity in sequence, resulting in a progressive post-processing model. The experiments on four large-scale person  re-identification benchmark datasets indicate that (1) the proposed method can consistently achieve higher accuracies by serving as a  post-processing procedure after the content-based person re-identification methods, showing its state-of-the-art results, (2) the proposed  lightweight method only needs about 6 milliseconds for optimizing the ranking results of one sample, showing its high-efficiency. Code is available  at: https://github.com/123ci/PBCmodel.",2020,ArXiv,2009.03098,10.1109/tmm.2020.2994524,https://arxiv.org/pdf/2009.03098.pdf
3ef7e092003b66d1e66f2d9570defcb5ac866ba8,1,0,0,PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks,,2020,ArXiv,2005.02154,,https://arxiv.org/pdf/2005.02154.pdf
3f32a548caece94efd444b5fee175d2d94917e30,1,0,0,Push for Center Learning via Orthogonalization and Subspace Masking for Person Re-Identification,"Person re-identification aims to identify whether pairs of images belong to the same person or not. This problem is challenging due to large differences in camera views, lighting and background. One of the mainstream in learning CNN features is to design loss functions which reinforce both the class separation and intra-class compactness. In this paper, we propose a novel Orthogonal Center Learning method with Subspace Masking for person re-identification. We make the following contributions: (i) we develop a center learning module to learn the class centers by simultaneously reducing the intra-class differences and inter-class correlations by orthogonalization; (ii) we introduce a subspace masking mechanism to enhance the generalization of the learned class centers; and (iii) we devise to integrate the average pooling and max pooling in a regularizing manner that fully exploits their powers. Extensive experiments show that our proposed method consistently outperforms the state-of-the-art methods on the large-scale ReID datasets including Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17.",2019,,1908.10535,,https://arxiv.org/pdf/1908.10535.pdf
3f3b88dd9856f00d02ac69505d4f940775e28ec8,1,0,0,How to Train Your Deep Multi-Object Tracker,"The recent trend in vision-based multi-object tracking (MOT) is heading towards leveraging the representational power of deep learning to jointly learn to detect and track objects. However, existing methods train only certain sub-modules using loss functions that often do not correlate with established tracking evaluation measures such as Multi-Object Tracking Accuracy (MOTA) and Precision (MOTP). As these measures are not differentiable, the choice of appropriate loss functions for end-to-end training of multi-object tracking methods is still an open research problem. In this paper, we bridge this gap by proposing a differentiable proxy of MOTA and MOTP, which we combine in a loss function suitable for end-to-end training of deep multi-object trackers. As a key ingredient, we propose a Deep Hungarian Net (DHN) module that approximates the Hungarian matching algorithm. DHN allows estimating the correspondence between object tracks and ground truth objects to compute differentiable proxies of MOTA and MOTP, which are in turn used to optimize deep trackers directly. We experimentally demonstrate that the proposed differentiable framework improves the performance of existing multi-object trackers, and we establish a new state of the art on the MOTChallenge benchmark. Our code is publicly available from https://github.com/yihongXU/deepMOT.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1906.06618,10.1109/CVPR42600.2020.00682,https://arxiv.org/pdf/1906.06618.pdf
3f6139a3017f3948b357eb41d41aee4d7a47eabb,1,0,0,Bilinear Attention Networks for Person Retrieval,"This paper investigates a novel Bilinear attention (Bi-attention) block, which discovers and uses second order statistical information in an input feature map, for the purpose of person retrieval. The Bi-attention block uses bilinear pooling to model the local pairwise feature interactions along each channel, while preserving the spatial structural information. We propose an Attention in Attention (AiA) mechanism to build inter-dependency among the second order local and global features with the intent to make better use of, or pay more attention to, such higher order statistical relationships. The proposed network, equipped with the proposed Bi-attention is referred to as Bilinear ATtention network (BAT-net). Our approach outperforms current state-of-the-art by a considerable margin across the standard benchmark datasets (e.g., CUHK03, Market-1501, DukeMTMC-reID and MSMT17).",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00812,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.pdf
3fa21735bc0f080601a352505bf543b984443bab,1,1,0,Compressed Self-Attention for Deep Metric Learning with Low-Rank Approximation,"In this paper, we aim to boost the performance of deep metric learning by using the self-attention (SA) mechanism. However, due to the pairwise similarity measurement, the cost of storing and manipulating the complete attention maps makes it infeasible for large inputs such as images. To solve this problem, we propose a compressed selfattention with low-rank approximation (CSALR) module, which significantly reduces the computation and memory costs without sacrificing the accuracy. In CSALR, the original attention map is decomposed into a landmark attention map and a combination coefficient map with a small number of landmark feature vectors sampled from the input feature map by average pooling. Thanks to the efficiency of CSALR, we can apply CSALR to highresolution shallow convolutional layers and implement a multi-head form of CSALR, which further boosts the performance. We evaluate the proposed CSALR on person re-identification which is a typical metric learning task. Extensive experiments shows the effectiveness and efficiency of CSALR in deep metric learning and its superiority over the baselines.",2020,IJCAI,,10.24963/ijcai.2020/285,https://www.ijcai.org/Proceedings/2020/0285.pdf
3ff74b685615f50736e10294811281c41de3d61e,1,1,0,Dissecting Person Re-Identification From the Viewpoint of Viewpoint,"Variations in visual factors such as viewpoint, pose, illumination and background, are usually viewed as important challenges in person re-identification (re-ID). In spite of acknowledging these factors to be influential, quantitative studies on how they affect a re-ID system are still lacking. To derive insights in this scientific campaign, this paper makes an early attempt in studying a particular factor, viewpoint. We narrow the viewpoint problem down to the pedestrian rotation angle to obtain focused conclusions. In this regard, this paper makes two contributions to the community. First, we introduce a large-scale synthetic data engine, PersonX. Composed of hand-crafted 3D person models, the salient characteristic of this engine is “controllable”. That is, we are able to synthesize pedestrians by setting the visual variables to arbitrary values. Second, on the 3D data engine, we quantitatively analyze the influence of pedestrian rotation angle on re-ID accuracy. Comprehensively, the person rotation angles are precisely customized from 0 to 360, allowing us to investigate its effect on the training, query, and gallery sets. Extensive experiment helps us have a deeper understanding of the fundamental problems in person re-ID. Our research also provides useful insights for dataset building and future practical usage, e.g., a person of a side view makes a better query.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1812.02162,10.1109/CVPR.2019.00070,https://arxiv.org/pdf/1812.02162.pdf
3ff8dbba650f69b0a9015de40750983545015207,1,1,0,Densely Semantically Aligned Person Re-Identification,"We propose a densely semantically aligned person re-identification (re-ID) framework. It fundamentally addresses the body misalignment problem caused by pose/viewpoint variations, imperfect person detection, occlusion, etc.. By leveraging the estimation of the dense semantics of a person image, we construct a set of densely semantically aligned part images (DSAP-images), where the same spatial positions have the same semantics across different person images. We design a two-stream network that consists of a main full image stream (MF-Stream) and a densely semantically-aligned guiding stream (DSAG-Stream). The DSAG-Stream, with the DSAP-images as input, acts as a regulator to guide the MF-Stream to learn densely semantically aligned features from the original image. In the inference, the DSAG-Stream is discarded and only the MF-Stream is needed, which makes the inference system computationally efficient and robust. To our best knowledge, we are the first to make use of fine grained semantics for addressing misalignment problems for re-ID. Our method achieves rank-1 accuracy of 78.9% (new protocol) on the CUHK03 dataset, 90.4% on the CUHK01 dataset, and 95.7% on the Market1501 dataset, outperforming state-of-the-art methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1812.08967,10.1109/CVPR.2019.00076,https://arxiv.org/pdf/1812.08967.pdf
3ffcfb957d71f185c3710ede8515d96dae1fafca,0,0,1,Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification,"Video-based person re-identification (re-ID) is an important research topic in computer vision. The key to tackling the challenging task is to exploit both spatial and temporal clues in video sequences. In this work, we propose a novel graph-based framework, namely Multi-Granular Hypergraph (MGH), to pursue better representational capabilities by modeling spatiotemporal dependencies in terms of multiple granularities. Specifically, hypergraphs with different spatial granularities are constructed using various levels of part-based features across the video sequence. In each hypergraph, different temporal granularities are captured by hyperedges that connect a set of graph nodes (i.e., part-based features) across different temporal ranges. Two critical issues (misalignment and occlusion) are explicitly addressed by the proposed hypergraph propagation and feature aggregation schemes. Finally, we further enhance the overall video representation by learning more diversified graph-level representations of multiple granularities based on mutual information minimization. Extensive experiments on three widely-adopted benchmarks clearly demonstrate the effectiveness of the proposed framework. Notably, 90.0% top-1 accuracy on MARS is achieved using MGH, outperforming the state-of-the-arts.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00297,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Learning_Multi-Granular_Hypergraphs_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf
40848b442cd52cbb9a440e413d5edea23a6ac26d,1,0,0,Semi-automatic Data Annotation Tool for Person Re-identification Across Multi Cameras,"Person re-identification is an important technique towards automatic search of a person’s presence in a surveillance video. It is becoming a hot research topic due to its value in both machine learning research and video surveillance applications. Considering the current success of deep learning, having tons of person images with identity labels are important and helpful for learning effective person matchers. However, collecting labeled images for person re-identification is more difficult than other similar tasks such as face recognition due to complex intra-class variations in illumination, pose, viewpoint, blur, low resolution, and occlusion. Although the volume of surveillance videos has become larger and larger today, it is time-consuming and costs lots of human labors in labeling a large dataset for person re-identification. In this paper, we propose a semi-automatic data annotation tool to accelerate annotation of person images across multi cameras. This tool consists of automatic person detection and tracking algorithms for person image collection, and an ad-hoc person matcher for automatic person matching suggestions across multi cameras. Moreover, we further utilize background and video sequence information for identity confirmation during annotation, which is also a good intuition for the future design of person re-identification algorithms.",2018,2018 IEEE International Conference on Big Data (Big Data),,10.1109/BigData.2018.8622366,
41285650d62c3768e98282b8a82b72488f458ce7,0,1,0,Person Re-Identification by Ranking Ensemble Representations,"Existing deep learning algorithms for person re-identification (re-id) typically rely on single-sample classification or pairwise matching constraints. This indicates a breach of deployment due to ignoring the probe-specific matching information against the gallery set encoded in ranking lists. In this work, we address this problem by exploring the idea of RANkinG Ensembles (RANGE) that learns such information from the ranking lists. Specifically, given an off-the-self deep re-id feature representation model, we construct per-probe ranking lists and exploit them to learn inter ranking ensemble representation. To mitigate the harm of inevitable false gallery positives, we further introduce a complementary intra ranking ensemble representation. Extensive experiments show that both supervised and unsupervised re-id benefit from the proposed RANGE method on four challenging benchmarks: MSMT17, Market-1501, DukeMTMC-ReID, and CUHK03.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803280,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/64522/2/Gong%20Person%20Re-Identification%20by%202019%20Accepted.pdf
41c1dd74db853c6256c00df1826290981dac56ec,1,0,0,Online multi-object tracking with pedestrian re-identification and occlusion processing,"Tracking-by-detection is a common approach for online multi-object tracking problem. At present, the following challenges still exist in the multi-object tracking scenarios: (1) The result of object re-tracking after full occlusion is not ideal; (2) The predicted position of object is not accurate enough in the complicated video scenarios. Aiming at these two problems, this paper proposes a multi-object tracking framework called DROP (Deep Re-identification Occlusion Processing). The framework consists of object detection, fast pedestrian re-identification, and a confidence-based data association algorithm. A lightweight convolutional neural network that can solve the re-tracking problem is constructed by increasing and learning the affinity of appearance features of the same object in different frames. And this paper proposes to judge the occlusion of the object that can solve inaccurate position predicted by Kalman filter by using the data association result of the appearance features of the object, and to reduce the matching error by improving the data association formula. The experimental results on the multi-object tracking datasets MOT15 and MOT16 show that the proposed method can improve the precision while ensure the real-time tracking performance.",2020,The Visual Computer,,10.1007/s00371-020-01854-0,
424baa07e5edc55e0358d02461f54f52cb3e1be3,1,0,0,Candidate Selection-based Deep Affinity Network for Multi-object Tracking,"Deep Affinity Network (DAN) is a novel approach in multi-object tracking (MOT) designed to jointly modeling object appearances and affinities end to end. But tracking accuracy of DAN tracker is greatly limited since it neglects unreliable detection. Exploiting predictions of tracks has emerged as a popular approach to tackle the task of tracking-by-detection. However, it's observed that missing detection has not been solved well enough which would significantly influence tracking accuracy. Thus, obtaining more reliable tracking candidates is concerned to further address the problem of missing detection. In this paper, we propose Candidate Selection-based Deep Affinity Network (CSDAN) tracker for MOT. It collects candidates from detection, predictions of tracks and backward tracking simultaneously so that they can complement each other in different scenarios. Moreover, we propose a deep learned candidate selection model (DCSM) with a unified scoring function suitable for CSDAN, which can well handle candidates from three sources separately and select those for data association. Experiments conducted on MOT17 benchmark demonstrate that our extensions can significantly address the unreliable detection problem in DAN tracker, and our CSDAN tracker demonstrates competitive tracking performance.",2020,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),,10.1109/ICAICA50127.2020.9182622,
424c6beb7c0a0a8baac8b7f033edc04c6c261fbd,0,1,0,A Self-adaptive Fuzzy Network for Prediction in Non-stationary Environments,"Prediction in non-stationary environments, where data streams are ever-changing at very high speeds, has become more and more important in real-world applications. The uncertainty in data streams caused by changes in data distribution is described as concept drift. The appearance of concept drift in a data stream results in inconsistencies between the existing data and incoming data. Such inconsistencies pose a great challenge to conventional machine learning methods, given they are built on the assumption of independent and identically distributed data and cannot adapt to unpredictable changes in knowledge patterns. To solve such data stream uncertainty problem, this paper presents a window-based self-adaptive fuzzy network called adaptive fuzzy network (AFN), which can continuously modify the network through identifying new knowledge from the previous data samples. Three components are embedded in ANF: a drift detection module to identify whether the current window of data samples presents different pattern from the previous; a drift adaption module to retain useful knowledge in previous samples; and a fuzzy inference system, which integrates the detection and adaption modules for prediction. ANF has been evaluated through a set of experiments on non-stationary data streams. The experimental results show a good effectiveness of our method.",2018,2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),,10.1109/FUZZ-IEEE.2018.8491572,
42656cf2b75dccc7f8f224f7a86c2ea4de1ae671,1,0,0,Unsupervised Domain Adaptive Re-Identification: Theory and Practice,"We study the problem of unsupervised domain adaptive re-identification (re-ID) which is an active topic in computer vision but lacks a theoretical foundation. We first extend existing unsupervised domain adaptive classification theories to re-ID tasks. Concretely, we introduce some assumptions on the extracted feature space and then derive several loss functions guided by these assumptions. To optimize them, a novel self-training scheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively makes guesses for unlabeled target data based on an encoder and trains the encoder based on the guessed labels. Extensive experiments on unsupervised domain adaptive person re-ID and vehicle re-ID tasks with comparisons to the state-of-the-arts confirm the effectiveness of the proposed theories and self-training framework. Our code is available at \url{this https URL}.",2020,Pattern Recognit.,1807.11334,10.1016/j.patcog.2019.107173,https://arxiv.org/pdf/1807.11334.pdf
42af47b99e972f2f5f1897f00082f37496aeb7e8,1,0,0,Precise Regression for Bounding Box Correction for Improved Tracking Based on Deep Reinforcement Learning,"In this paper, we propose a precise regression approach for correcting imprecise bounding box using deep reinforcement learning. Object tracking task essentially builds trajectory of a moving object based on detection and tracking algorithms and its current state is indicated by having the object encapsulated with a bounding box corresponding to its position and size. However due to the imperfect detection and tracking algorithms operating in complex scene, it is difficult to obtain the precise bounding box as errors frequently occur producing oversized, partial, and false bounding box, respectively. To correct the error, we train an intelligent agent that move the bounding box to the right position and scale it to its correct size matching to that of the true target. The agent is trained by deep Q-Iearning and evaluated on several state-of-the-art multiple object tracking approaches. The experimental results demonstrate that our proposed framework can effectively eliminate the object tracking bounding box error and its robustness is verified by realizing improved tracking performance in complex scene.",2018,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2018.8462063,http://mirlab.org/conference_papers/International_Conference/ICASSP%202018/pdfs/0001643.pdf
42c254517647750a96ebed55a053dfe7ab32f79e,0,1,0,Adversarial Learning for Fine-Grained Image Search,"Fine-grained image search is still a challenging problem due to the difficulty in capturing subtle differences regardless of pose variations of objects from fine-grained categories. In practice, a dynamic inventory with new fine-grained categories adds another dimension to this challenge. In this work, we propose an end-to-end network, called FGGAN, that learns discriminative representations by implicitly learning a geometric transformation from multi-view images for fine-grained rigid object retrieval. We integrate a generative adversarial network (GAN) that can automatically handle complex view and pose variations by converting them to a canonical view without any predefined transformations. Moreover, in an open-set scenario, our network is able to better match rigid objects from unseen and unknown fine-grained categories. Extensive experiments on the public CompCars dataset and a newly collected dataset have demonstrated the effectiveness of the proposed method in both closed-set and open-set scenarios.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),1807.02247,10.1109/ICME.2019.00091,https://arxiv.org/pdf/1807.02247.pdf
42c80a8f23cfa9ee19ad90e86ea305c7eeb0c614,0,0,1,Unsupervised Learning of Human Action Categories in Still Images with Deep Representations,"In this article, we propose a novel method for unsupervised learning of human action categories in still images. In contrast to previous methods, the proposed method explores distinctive information of actions directly from unlabeled image databases, attempting to learn discriminative deep representations in an unsupervised manner to distinguish different actions. In the proposed method, action image collections can be used without manual annotations. Specifically, (i) to deal with the problem that unsupervised discriminative deep representations are difficult to learn, the proposed method builds a training dataset with surrogate labels from the unlabeled dataset, then learns discriminative representations by alternately updating convolutional neural network (CNN) parameters and the surrogate training dataset in an iterative manner; (ii) to explore the discriminatory information among different action categories, training batches for updating the CNN parameters are built with triplet groups and the triplet loss function is introduced to update the CNN parameters; and (iii) to learn more discriminative deep representations, a Random Forest classifier is adopted to update the surrogate training dataset, and more beneficial triplet groups then can be built with the updated surrogate training dataset. Extensive experiments on four benchmark datasets demonstrate the effectiveness of the proposed method.",2019,TOMM,,10.1145/3362161,
42dc432f58adfaa7bf6af07e5faf9e75fea29122,0,1,0,Sequence-based Person Attribute Recognition with Joint CTC-Attention Model,"Attribute recognition has become crucial because of its wide applications in many computer vision tasks, such as person re-identification. Like many object recognition problems, variations in viewpoints, illumination, and recognition at far distance, all make this task challenging. In this work, we propose a joint CTC-Attention model (JCM), which maps attribute labels into sequences to learn the semantic relationship among attributes. Besides, this network uses neural network to encode images into sequences, and employs connectionist temporal classification (CTC) loss to train the network with the aim of improving the encoding performance of the network. At the same time, it adopts the attention model to decode the sequences, which can realize aligning the sequences and better learning the semantic information from attributes. Extensive experiments on three public datasets, i.e., Market-1501 attribute dataset, Duke attribute dataset and PETA dataset, demonstrate the effectiveness of the proposed method.",2018,ArXiv,1811.08115,,https://arxiv.org/pdf/1811.08115.pdf
42e5ce74677cc10144b2d352f7f7cc37fd54f0bc,1,0,0,Applications of Generative Adversarial Networks (GANs): An Updated Review,"Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This paper aims to present an overview of GANs, its different variants, and potential application in various domains. The paper attempts to identify GANs’ advantages, disadvantages and significant challenges to the successful implementation of GAN in different application areas. The main intention of this paper is to explore and present a comprehensive review of the crucial applications of GANs covering a variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects.",2019,,,10.1007/s11831-019-09388-y,
42f81abff3a2a3c6594f167d51347e4aeb3ee01d,0,0,1,ASTA-Net: Adaptive Spatio-Temporal Attention Network for Person Re-Identification in Videos,"The attention mechanism has been widely applied to enhance pedestrian representation for person re-identification in videos. However, most existing methods learn the spatial and temporal attention separately, and thus ignore the correlation between them. In this work, we propose a novel Adaptive Spatio-Temporal Attention Network (ASTA-Net) to adaptively aggregate the spatial and temporal attention features into discriminative pedestrian representation for person re-identification in videos. Specifically, multiple Adaptive Spatio-Temporal Fusion modules within ASTA-Net are designed for exploring precise spatio-temporal attention on multi-level feature maps. They first obtain the preliminary spatial and temporal attention features via the spatial semantic relations for each frame and temporal dependencies among inconsecutive frames, then adaptively aggregate the preliminary attention features on the basis of their correlation. Moreover, an Adjacent-Frame Motion module is designed to explicitly extract motion patterns according to the feature-level variation among adjacent frames. Extensive experiments on the three widely-used datasets, i.e., MARS, iLIDS-VID and PRID2011, have demonstrated the effectiveness of the proposed approach.",2020,ACM Multimedia,,10.1145/3394171.3413843,
4303c361832bfc9cb5adf43d9d9b363077d0ddd9,0,1,0,Center Based Pseudo-Labeling For Semi-Supervised Person Re-Identification,"Generative Adversarial Networks (GAN) have shown promising results on data modeling and can generate high quality synthetic samples from the data distribution. However, how to effectively use the generated data for improved feature learning still remains an open question. This work proposes a Center based Pseudo-Labeling (CPL) method dedicated to this purpose. The network is trained with both labeled real data and unlabeled synthetic data, under a joint supervision of cross-entropy loss together with a center regularization term, which simultaneously predicts pseudo-labels for unlabeled synthetic data. Experimental results on two standard benchmarks show our approach achieves superior performance over closely related competitors and comparable results with state-of-the-art methods.",2018,2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),,10.1109/ICMEW.2018.8551541,
43270f621fc8ea87fbdffbed3b4c4dbbde5b3bce,1,1,0,State-Aware Re-Identification Feature for Multi-Target Multi-Camera Tracking,"Multi-target Multi-camera Tracking (MTMCT) aims to extract the trajectories from videos captured by a set of cameras. Recently, the tracking performance of MTMCT is significantly enhanced with the employment of re-identification (Re-ID) model. However, the appearance feature usually becomes unreliable due to the occlusion and orientation variance of the targets. Directly applying Re-ID model in MTMCT will encounter the problem of identity switches (IDS) and tracklet fragment caused by occlusion. To solve these problems, we propose a novel tracking framework in this paper. In this framework, the occlusion status and orientation information are utilized in Re-ID model with human pose information considered. In addition, the tracklet association using the proposed fused tracking feature is adopted to handle the fragment problem. The proposed tracker achieves 81.3% IDF1 on the multiple-camera hard sequence, which outperforms all other reference methods by a large margin.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1906.01357,10.1109/CVPRW.2019.00192,https://arxiv.org/pdf/1906.01357.pdf
4327934807725afb7592eb77c264acefc49c9347,1,1,0,Person Reidentification via Structural Deep Metric Learning,"Despite the promising progress made in recent years, person reidentification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. This paper proposes to tackle this task by jointly learning feature representation and distance metric in an end-to-end manner. Existing deep metric learning-based re-ID methods usually encounter the following two weaknesses: 1) most works based on pairwise or triplet constraints often suffer from slow convergence and poor local optima, partially because they use very limited samples for each update and 2) hard negative sample mining has been widely applied in existing works. However, hard positive samples, which also contribute to the training of network, have not received enough attention. To alleviate these problems, we develop a novel structural metric learning objective for person re-ID, in which each positive pair is allowed to be compared against all negative pairs in a minibatch and each positive pair is adaptively assigned a hardness-aware weight to modulate its contribution. The introduced positive pair weighting strategy enables the algorithm to focus more on the hard positive samples. Furthermore, we propose to enhance the proposed loss function by adding a global loss term to reduce the variances of positive/negative pair distances, which is able to improve the generalization capability of the network model. By this approach, person images can be nonlinearly mapped into a low-dimensional embedding space where similar samples are kept closer and dissimilar samples are pushed farther apart. We implement the proposed algorithm using the inception architecture and evaluate it on three large-scale re-ID data sets. Experiment results demonstrate that our approach is able to outperform most state of the arts while using much lower dimensional deep features.",2019,IEEE Transactions on Neural Networks and Learning Systems,,10.1109/TNNLS.2018.2861991,
4327d23ffa232f753964bdc7ba07991d7dd23cc9,1,0,0,Attention in Multimodal Neural Networks for Person Re-identification,"In spite of increasing interest from the research community, person re-identification remains an unsolved problem. Correctly deciding on a true match by comparing images of a person, captured by several cameras, requires extraction of discriminative features to counter challenges such as changes in lighting, viewpoint and occlusion. Besides devising novel feature descriptors, the setup can be changed to capture persons from an overhead viewpoint rather than a horizontal. Furthermore, additional modalities can be considered that are not affected by similar environmental changes as RGB images. In this work, we present a Multimodal ATtention network (MAT) based on RGB and depth modalities. We combine a Convolution Neural Network with an attention module to extract local and discriminative features that are fused with globally extracted features. Attention is based on correlation between the two modalities and we finally also fuse RGB and depth features to generate a joint multilevel RGB-D feature. Experiments conducted on three datasets captured from an overhead view show the importance of attention, increasing accuracies by 3.43%, 2.01% and 2.13% on OPR, DPI-T and TVPR, respectively.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00055,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Lejbolle_Attention_in_Multimodal_CVPR_2018_paper.pdf
43629a7de0618700b1612479d9d98a3581e0dd3d,1,1,0,A Mask-Pooling Model With Local-Level Triplet Loss for Person Re-Identification,"Person Re-Identification (ReID) is an important yet challenging task in computer vision. Background clutter is one of the greatest challenges to overcome. In this paper, we propose a Mask-pooling model with local-level triplet loss (MPM-LTL) to tackle this problem and improve person ReID performance. Specifically, we present a novel pooling method, called mask pooling (MP), to gradually remove background features in feature maps through deep convolutional network. With mask pooling, the network can learn the most crucial person features. Moreover, we raise a novel local-level triplet loss for discriminative feature training. Furthermore, we propose a new hard triplets selection algorithm named Mask-guided TriHard. The method is based on human outline information, which is, to our best knowledge, employed for the first time for hard triplets selection. We achieve the state-of-the-art results on three benchmark person datasets Market-1501 <xref ref-type=""bibr"" rid=""ref1"">[1]</xref>, CUHK03 <xref ref-type=""bibr"" rid=""ref2"">[2]</xref> and DukeMTMC-reID <xref ref-type=""bibr"" rid=""ref3"">[3]</xref>, <xref ref-type=""bibr"" rid=""ref4"">[4]</xref>.",2020,IEEE Access,,10.1109/access.2020.3011961,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09149590.pdf
4370dd65f7f2d71bfa8ef4f64932790e3123e7ef,0,1,0,Person Re-Identification via Feature Representation Learning Based on Verification Sample Constrain,"In person re-identification (ReID) task, the variance between the samples are quite large, and there is no standard sample as a comparison. The current common method is to implement it as a classification task, which can get better results than the classical methods in verification task such as contrastive loss. However, the identification loss used in classification task only seeks the boundary of the classification, the intra-class distance between samples is still large so that it is insufficient for ReID task. In this paper, we consider to overcome these difficulties by proposing a joint loss with an identification loss and constrains of modeling the Euclidean distances between samples and their corresponding eigen data which is constructed by Principal Component Analysis method (PCA), we call it EigenPerson. The entire loss is formed in a linear combination. This work is mainly motivated by the center loss for face recognition problems, of which regularizations are restricted by a simultaneously learned center. We substitute the center with EigenPerson which we constructed offline as an auxiliary training sample. The learned model with our proposed method is evaluated on the benchmark of Market1501 and CUHK03 and achieve comparable results to those methods proposed in the same period.",2019,2019 International Conference on Machine Learning and Cybernetics (ICMLC),,10.1109/ICMLC48188.2019.8949262,
439604727275549e0fab80dcd7242a36953fe9e3,1,1,0,Towards Visually Explaining Similarity Models,"We consider the problem of visually explaining similarity models, i.e., explaining why a model predicts two images to be similar in addition to producing a scalar score. While much recent work in visual model interpretability has focused on gradient-based attention, these methods rely on a classification module to generate visual explanations. Consequently, they cannot readily explain other kinds of models that do not use or need classification-like loss functions (e.g., similarity models trained with a metric learning loss). In this work, we bridge this crucial gap, presenting the first method to generate gradient-based visual explanations for image similarity predictors. By relying solely on the learned feature embedding, we show that our approach can be applied to any kind of CNN-based similarity architecture, an important step towards generic visual explainability. We show that our resulting visual explanations serve more than just interpretability; they can be infused into the model learning process itself with new trainable constraints based on our similarity explanations. We show that the resulting similarity models perform, and can be visually explained, better than the corresponding baseline models trained without our explanation constraints. We demonstrate our approach using extensive experiments on three different kinds of tasks: generic image retrieval, person re-identification, and low-shot semantic segmentation.",2020,ArXiv,2008.06035,,https://arxiv.org/pdf/2008.06035.pdf
43d0754d2ddfed61041397559b2ec537c0eaa1c1,0,1,0,Deep Learning for Retail Product Recognition: Challenges and Techniques,"Taking time to identify expected products and waiting for the checkout in a retail store are common scenes we all encounter in our daily lives. The realization of automatic product recognition has great significance for both economic and social progress because it is more reliable than manual operation and time-saving. Product recognition via images is a challenging task in the field of computer vision. It receives increasing consideration due to the great application prospect, such as automatic checkout, stock tracking, planogram compliance, and visually impaired assistance. In recent years, deep learning enjoys a flourishing evolution with tremendous achievements in image classification and object detection. This article aims to present a comprehensive literature review of recent research on deep learning-based retail product recognition. More specifically, this paper reviews the key challenges of deep learning for retail product recognition and discusses potential techniques that can be helpful for the research of the topic. Next, we provide the details of public datasets which could be used for deep learning. Finally, we conclude the current progress and point new perspectives to the research of related fields.",2020,Computational intelligence and neuroscience,,10.1155/2020/8875910,
43eb56d0dfdb1d5c78a0d2acb43b436f2d5c4ecf,1,0,0,"The Unmanned Aerial Vehicle Benchmark: Object Detection, Tracking and Baseline","With the increasing popularity of Unmanned Aerial Vehicles (UAVs) in computer vision-related applications, intelligent UAV video analysis has recently attracted the attention of an increasing number of researchers. To facilitate research in the UAV field, this paper presents a UAV dataset with 100 videos featuring approximately 2700 vehicles recorded under unconstrained conditions and 840k manually annotated bounding boxes. These UAV videos were recorded in complex real-world scenarios and pose significant new challenges, such as complex scenes, high density, small objects, and large camera motion, to the existing object detection and tracking methods. These challenges have encouraged us to define a benchmark for three fundamental computer vision tasks, namely, object detection, single object tracking (SOT) and multiple object tracking (MOT), on our UAV dataset. Specifically, our UAV benchmark facilitates evaluation and detailed analysis of state-of-the-art detection and tracking methods on the proposed UAV dataset. Furthermore, we propose a novel approach based on the so-called Context-aware Multi-task Siamese Network (CMSN) model that explores new cues in UAV videos by judging the consistency degree between objects and contexts and that can be used for SOT and MOT. The experimental results demonstrate that our model could make tracking results more robust in both SOT and MOT, showing that the current tracking and detection methods have limitations in dealing with the proposed UAV benchmark and that further research is indeed needed.",2019,International Journal of Computer Vision,,10.1007/s11263-019-01266-1,
43ed5ad50b1cde7e6452f8e0796083e2e06b2112,0,1,0,An Implicit Attention Mechanism for Deep Learning Pedestrian Re-identification Frameworks,"Attention is defined as the preparedness for the mental selection of certain aspects in a physical environment. In the computer vision domain, this mechanism is of most interest, as it helps to define the segments of an image/video that are critical for obtaining a specific decision. This paper introduces one 'implicit' attentional mechanism for deep learning frameworks, that provides simultaneously: 1) masks-free; and 2) foreground-focused samples for the inference phase. The main idea is to generate synthetic data composed of interleaved segments from the original learning set, while using class information only from specific segments. During the learning phase, the newly generated samples feed the network, keeping their label exclusively consistent with the identity from where the region-of-interest was cropped. Hence, as the model receives images of each identity with inconsistent unwanted areas, it naturally pays the most attention to the label consistent consistent regions, which we observed to be equivalent to learn an effective receptive field. During the test phase, samples are provided without any mask, and the network naturally disregards the detrimental information, which is the insight for the observed improvements in performance. As a proof-of-concept, we consider the challenging problem of pedestrian re-identification and compare the effectiveness of our solution to the state-of-the-art techniques in the well known Richly Annotated Pedestrian (RAP) dataset. The code is available at this https URL",2020,ArXiv,2001.11267,,https://arxiv.org/pdf/2001.11267.pdf
44238993b9bae74878f0d3aac2c0cd4779e77fc4,1,0,0,Tracking without bells and whistles Supplementary Material,"The supplementary material complements our work with the pseudocode representation of Tracktor and additional implementation and training details of its object detector and tracking extensions. In addition, we provide more details on our experiments and analysis including the MOTChallenge benchmark results of our Tracktor++ tracker for each sequence and set of public detections.",2019,,,,https://pdfs.semanticscholar.org/4423/8993b9bae74878f0d3aac2c0cd4779e77fc4.pdf
44891b9216f8dc8d2daaaa690e95bcb72817a9cb,0,1,0,Harmonious attention network for person re-identification via complementarity between groups and individuals,"Abstract Person re-identification (Re-ID) is of important capability for artificial intelligence and human-computer interaction. The main challenge of person Re-ID lies in limited data to precisely capture a wide range of appearance variations over multiple viewpoints. Furthermore, compared to the Re-ID between the single person, the person groups contain information about the relationship between pedestrians that can potentially help identify certain identities. The Re-ID combining groups and individuals remain to be a promising task under rare study. In this paper, we propose a harmonious attention network for person re-identification, in which we jointly consider the complementarity between person groups and individuals. Concretely, first we propose a two-stream attentive network (TSAN) to respectively learn the information from the person groups and individuals. TSAN consists of a spatial-temporal fusion network for the group Re-ID, as well as a deep network for the traditionally individual person Re-ID. To jointly consider the contributions of the groups and individuals, then we propose a novel re-ranking algorithm (GIRK) based on the learned features to associate the group and individual information. We also propose a new group Re-ID dataset DukeGroupVid to evaluate the performance of our approach. Comprehensive experimental results on the proposed dataset and other Re-ID datasets demonstrate the effectiveness of our model.",2020,,,10.1016/j.neucom.2020.07.118,
448ef67eafb2780a155a677b9da54c92b8f3e540,0,1,0,Pseudo Label Based on Multiple Clustering for Unsupervised Cross-Domain Person Re-Identification,"Person re-identification (Re-ID) has achieved great improvement with the development of deep learning. However, domain adaptation in unsupervised Re-ID has always been a challenging task. Most existing works based on clustering only cluster once, which may lead to pseudo labels of poor quality. In this letter, we propose a Pseudo Label based on Multiple Clustering (PLMC) approach, which makes full advantage of multiple clustering to obtain more robust pseudo labels. In particular, our PLMC framework consists of two stages, namely, global training stage, and local training stage. We adopt the training strategy that combines the information learned from global features, and local features by training two stages alternately. Extensive experiments are carried out on three standard benchmark datasets (e.g., Maket1501, DukeMTMC-ReID, CUHK03). The results demonstrate that our PLMC method is superior to the previous methods based on single clustering, and achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.3016528,
449d2e25f1da780e9c68feaba9cc630fdf1b7331,0,1,0,Multiple Back Propagation Network and Metric Fusion for Person Re-identification,"Person re-identification (Re-ID) is a research focus in pattern recognition, which is to identify a person from another camera view. Many researches have studied feature representations and metric distances of person images, which are robust to changes of view angle and illumination. In this paper, we propose a Multiple Back Propagation (MBP) network and Metric Fusion (MF) for person Re-ID. The proposed MBP network is based on DenseNet or ResNet. Each Dense-conv layer or Conv-ID block is linked by a MBP layer. Each MBP layer is divided into two sub-streams. One sub-stream is connected to softmax loss and the other sub-stream is transferred to a convolution layer followed by triplet loss. A Metric Fusion (MF) method with an optimized weighting scheme is proposed for deep feature fusion. Furthermore, we propose a new metric Re-ranking Euclidean distance joining metric fusion. Experiments on three large-scale person Re-ID benchmark datasets, including Market1501, CUHK03 and DukeMTMC-reID, show that the proposed MBPMF method can achieve state-of-the-art performances.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852031,
449df573e41217c584e55e602428c2e3849b82e0,0,1,0,Üretilmesi Generating Person Images Based on Attributes,"Attribute based person image generation is a problem which considers generating realistic person images using attributes like pose, gender, clothes, whether a bag is present or not etc. and it has wide variety of applications on computer vision. Realization of that generation process is quite difficult due to several reasons such as foreground/background, partial occlusion, stance of a person, camera angle and distance, complex relationships between attributes, unbalanced and poor quality data etc. Synthetic images are generated in related works which have worked on relatively easier datasets using less attributes with more complex models for more specific purposes. In this work, a more controversial goal was set up and a model named DCGAN-C is developed based on conditional generative adversarial networks and it can produce sythetic person images with both multi-class and multi-labels. Consequently, both quantitative and qualitative experiments were performed on the PA-100K dataset and the performance of the model was demonstrated. Keywords—Conditional generative adversarial networks, synthetic person images",2018,,,,https://web.cs.hacettepe.edu.tr/~aykut/papers/siu2018_3.pdf
44a60c1739d355c9d8f73573e8124a9f82d995e5,1,0,0,Appearance features for online multiple camera multiple target tracking,"Multiple object tracking methods in the state-of-the-art are challenged by appearance variation, environment changes and long-term occlusions. Exploiting multiple calibrated and frame synchronized cameras holds the promise of alleviating these problems, in particular, the one pertaining to occlusion. The practical realization of this idea faces the problem that the appearance of the same target can change through different cameras. Thus, particular care should be taken in order to enhance the computation of appearance distances between targets in multiple cameras. In this paper, we tackle the problem of multiple object multiple camera tracking by adopting a Markov Decision Process framework. We concentrate on the effect of the affinity function by discussing different possible implementations and validating their performance, in terms of the MOT metric and the ID measure, on the PETS 2009 and EPFL datasets. Our experimental result shows a significant improvement of multiple cameras approaches with a sufficiently large overlapping zone compared to single camera ones.",2020,SAC,,10.1145/3341105.3373960,
44aa2104bff800f8b43c637342abd356cd1388f6,1,0,0,Multi Target Tracking from Drones by Learning from Generalized Graph Differences,Formulating the multi object tracking problem as a network flow optimization problem is a popular choice. The weights of such network flow problem can be learnt efficiently from training data using a recently introduced concept called Generalized Graph Differences (GGD). This allows a general tracker implementation to be specialized to drone videos by training it on the VisDrone dataset. Two modifications to the original GGD is introduced in this paper and a result with an average precision of 23.09 on the test set of VisDrone 2019 was achieved.,2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00012,http://openaccess.thecvf.com/content_ICCVW_2019/papers/VISDrone/Ardo_Multi_Target_Tracking_from_Drones_by_Learning_from_Generalized_Graph_ICCVW_2019_paper.pdf
44e49f07d1c4f7b44133eb379ed1277cb8d03281,1,0,0,Non-local attention association scheme for online multi-object tracking,"Abstract Online multi-object tracking (MOT) is a fundamental problem in video analysis and multimedia applications. The major challenge in the popular tracking-by-detection framework is knowing how to associate candidate detections results with existing tracklets. In this, we propose a non-local attention association approach and apply it to a unified online MOT framework that integrates the merits of single object tracking and data association methods. Specifically, we use non-local attention association networks (NAAN) to incorporate both spatial and temporal characteristics to associate new detections. The non-local attention mechanism generates global attention maps across space and time, enabling the network to focus on the whole tracklet information, as opposed to the local attention mechanism to overcome the problems of noisy detections, occlusion, and frequent interactions between targets. Experimental results on MOT benchmark datasets show that the proposed algorithm performs favorably against various online trackers on the basis of identity-preserving metrics.",2020,Image Vis. Comput.,,10.1016/j.imavis.2020.103983,
44eca323a85ac3f50ea6804571d2fe83111bcaa5,1,0,0,End-to-End Multi-Object Tracking with Global Response Map,"Most existing Multi-Object Tracking (MOT) approaches follow the Tracking-by-Detection paradigm and the data association framework where objects are firstly detected and then associated. Although deep-learning based method can noticeably improve the object detection performance and also provide good appearance features for cross-frame association, the framework is not completely end-to-end, and therefore the computation is huge while the performance is limited. To address the problem, we present a completely end-to-end approach that takes image-sequence/video as input and outputs directly the located and tracked objects of learned types. Specifically, with our introduced multi-object representation strategy, a global response map can be accurately generated over frames, from which the trajectory of each tracked object can be easily picked up, just like how a detector inputs an image and outputs the bounding boxes of each detected object. The proposed model is fast and accurate. Experimental results based on the MOT16 and MOT17 benchmarks show that our proposed on-line tracker achieved state-of-the-art performance on several tracking metrics.",2020,ArXiv,2007.06344,,https://arxiv.org/pdf/2007.06344.pdf
44ed4e9845c24532bad13c118a03ee4437089ae1,1,0,0,Efficient Multi-View Multi-Target Tracking Using a Distributed Camera Network,"In this paper, we propose a multi-target tracking method using a distributed camera network, which can effectively handle the occlusion and reidenfication problems by combining advanced deep learning and distributed information fusion. The targets are first detected using a fast object detection method based on deep learning. We then combine the deep visual feature information and spatial trajectory information in the Hungarian algorithm for robust targets association. The deep visual feature information is extracted from a convolutional neural network, which is pre-trained using a large-scale person reidentification dataset. The spatial trajectories of multiple targets in our framework are derived from a multiple view information fusion method, which employs an information weighted consensus filter for fusion and tracking. In addition, we also propose an efficient track processing method for ID assignment using multiple view information. The experiments on public datasets show that the proposed method is robust to solve the occlusion problem and reidentification problem, and can achieve superior performance compared to the state of the art methods.",2020,IEEE Sensors Journal,,10.1109/JSEN.2019.2949385,http://orca.cf.ac.uk/126304/1/Ji%20Z%20Efficient%20multi-view%20multi%20....pdf
450df4347f77f4b5bb74fe108c0f8dcec937791d,0,1,0,CGAN-TM: A Novel Domain-to-Domain Transferring Method for Person Re-Identification,"Person re-identification (re-ID) is a technique aiming to recognize person cross different cameras. Although some supervised methods have achieved favorable performance, they are far from practical application owing to the lack of labeled data. Thus, unsupervised person re-ID methods are in urgent need. Generally, the commonly used approach in existing unsupervised methods is to first utilize the source image dataset for generating a model in supervised manner, and then transfer the source image domain to the target image domain. However, images may lose their identity information after translation, and the distributions between different domains are far away. To solve these problems, we propose an image domain-to-domain translation method by keeping pedestrian’s identity information and pulling closer the domains’ distributions for unsupervised person re-ID tasks. Our work exploits the CycleGAN to transfer the existing labeled image domain to the unlabeled image domain. Specially, a Self-labeled Triplet Net is proposed to maintain the pedestrian identity information, and maximum mean discrepancy is introduced to pull the domain distribution closer. Extensive experiments have been conducted and the results demonstrate that the proposed method performs superiorly than the state-of-the-art unsupervised methods on DukeMTMC-reID and Market-1501.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2985545,
4564cd62e826c48267e3cbd49ca7f2afa15b1f0f,0,0,1,Human Tracking for Children Behavior Analysis in Nursery Schools,"Children's care and education is an eternally important element of society. In nursery schools, because children are active and sensitive, it is always difficult to effectively analyze the natural behavior of children if an observer (a person holding a camera) exists. In this paper, we collect a large amount of video data from different nursery schools by using stationary cameras without attaching motion sensors on children. We construct a system for children's behavior analysis based on human tracking from the videos in nursery schools. To achieve effective human tracking in videos of nursery schools, we combine multiple computer version technologies such as person detection, object tracking, and person re-identification. For each technique, we comprehensively compare various methods and identified the best one. With the system, we analyze some behavioral patterns of children in nursery schools. Specifically, we identify the popular areas in the room, the amount of exercise, and the gregariousness of each child.",2019,2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),,10.1109/SITIS.2019.00044,
45b952a1cd736ef6413de5dadd55763b6d51776a,1,0,0,Multiple Fisheye Camera Tracking via Real-Time Feature Clustering,"Recently, Multi-Target Multi-Camera Tracking (MTMC) makes a breakthrough due to the release of DukeMTMC and show the feasibility of related applications. However, most of the existing MTMC methods focus on the batch methods which attempt to find the global optimal solution from the entire image sequence and thus are not suitable for the real-time applications, e.g., customer tracking in unmanned stores. In this paper, we propose a low-cost online tracking algorithm, namely, Deep Multi-Fisheye-Camera Tracking (DeepMFCT) to identify the customers and locate the corresponding positions from multiple overlapping fisheye cameras. Based on any single camera tracking algorithm (e.g., Deep SORT), our proposed algorithm establishes the correlation between different single camera tracks. Owing to the lack of well-annotated multiple overlapping fisheye cameras dataset, the main challenge of this issue is to efficiently overcome the domain gap problem between normal cameras and fisheye cameras based on existed deep learning based model. To address this challenge, we integrate a single camera tracking algorithm with cross camera clustering including location information that achieves great performance on the unmanned store dataset and Hall dataset. Experimental results show that the proposed algorithm improves the baselines by at least 7% in terms of MOTA on the Hall dataset.",2019,MMAsia,,10.1145/3338533.3366581,
45f4294a9ca31ca77a3393ba4e9499096d3eacc9,1,1,0,Omni-Scale Feature Learning for Person Re-Identification,"As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We callse features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional feature streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses both pointwise and depthwise convolutions. By stacking such blocks layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, our OSNet achieves state-of-the-art performance on six person-ReID datasets. Code and models are available at: https://github.com/KaiyangZhou/deep-person-reid.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1905.00953,10.1109/ICCV.2019.00380,https://arxiv.org/pdf/1905.00953.pdf
46133074cf77921239c97d6bd886e053e7ae0943,1,1,0,Filter Grafting for Deep Neural Networks,"This paper proposes a new learning paradigm called filter grafting, which aims to improve the representation capability of Deep Neural Networks (DNNs). The motivation is that DNNs have unimportant (invalid) filters (e.g., l1 norm close to 0). These filters limit the potential of DNNs since they are identified as having little effect on the network. While filter pruning removes these invalid filters for efficiency consideration, filter grafting re-activates them from an accuracy boosting perspective. The activation is processed by grafting external information (weights) into invalid filters. To better perform the grafting process, we develop an entropy-based criterion to measure the information of filters and an adaptive weighting strategy for balancing the grafted information among networks. After the grafting operation, the network has very few invalid filters compared with its untouched state, enpowering the model with more representation capacity. We also perform extensive experiments on the classification and recognition tasks to show the superiority of our method. For example, the grafted MobileNetV2 outperforms the non-grafted MobileNetV2 by about 7 percent on CIFAR-100 dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2001.05868,10.1109/cvpr42600.2020.00663,https://arxiv.org/pdf/2001.05868.pdf
462e13f050e89fe10180d843838b9a679ee08739,1,0,0,Exploit the Connectivity: Multi-Object Tracking with TrackletNet,"Multi-object tracking (MOT) is an important topic and critical task related to both static and moving camera applications, such as traffic flow analysis, autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works exploit spatial and temporal information for MOT, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by associating detection results frame by frame with the help of the appearance similarity and the spatial consistency. To compensate camera movement, epipolar constraints are taken into consideration in the association. Then, for every pair of two tracklets, the similarity, called the connectivity in the paper, is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups and each group represents a unique object ID. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieves promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.",2019,ACM Multimedia,1811.07258,10.1145/3343031.3350853,https://arxiv.org/pdf/1811.07258.pdf
463809529d6db0b0ccc2ee8d36a35dab0ac0869d,0,1,1,Dispersion based Clustering for Unsupervised Person Re-identification,"The cumbersome acquisition of large-scale annotations for person re-identification task makes its deployment difficult in real-world scenarios. It is necessary to teach models to learn without explicit supervision. This paper proposes a simple but effective clustering approach for unsupervised person re-identification. We explore a basic concept in statistics, namely dispersion, to achieve a robust clustering criterion. Dispersion reflects the compactness of a cluster when assessed within and reveals the separation when measured at the inter-cluster level. Based on this insight, we propose a Dispersion based Clustering (DBC) approach which performs better at discovering the underlying data patterns. The approach can automatically prioritize standalone data points and prevents poor clustering. Our extensive experimental results demonstrate that the proposed methodology outperforms the state-of-the-art unsupervised methods on person re-identification.",2019,BMVC,,,https://bmvc2019.org/wp-content/uploads/papers/0459-paper.pdf
46800f04189e5bb57c5ec825b95a2064ed2537df,1,0,0,Human Attribute Recognition: A Comprehensive Survey,"Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR’s main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.",2020,,,10.20944/preprints202007.0055.v1,https://pdfs.semanticscholar.org/9fc4/56fca45f3c51d32b8dc2b1b6e8e47d7a3dd6.pdf
468c12b1ff6a5c4f2630cdbaca214e6df0c935cc,0,0,1,Appearance-Preserving 3D Convolution for Video-based Person Re-identification,"Due to the imperfect person detection results and posture changes, temporal appearance misalignment is unavoidable in video-based person re-identification (ReID). In this case, 3D convolution may destroy the appearance representation of person video clips, thus it is harmful to ReID. To address this problem, we propose AppearancePreserving 3D Convolution (AP3D), which is composed of two components: an Appearance-Preserving Module (APM) and a 3D convolution kernel. With APM aligning the adjacent feature maps in pixel level, the following 3D convolution can model temporal information on the premise of maintaining the appearance representation quality. It is easy to combine AP3D with existing 3D ConvNets by simply replacing the original 3D convolution kernels with AP3Ds. Extensive experiments demonstrate the effectiveness of AP3D for video-based ReID and the results on three widely used datasets surpass the state-of-the-arts. Code is available at: this https URL.",2020,ECCV,2007.08434,10.1007/978-3-030-58536-5_14,https://arxiv.org/pdf/2007.08434.pdf
474c8f4e31a51e2cb3c1e9fed83202b4483efb35,0,1,0,Computer Vision – ECCV 2018,"360◦ panoramas are a rich medium, yet notoriously difficult to visualize in the 2D image plane. We explore how intelligent rotations of a spherical image may enable content-aware projection with fewer perceptible distortions. Whereas existing approaches assume the viewpoint is fixed, intuitively some viewing angles within the sphere preserve high-level objects better than others. To discover the relationship between these optimal snap angles and the spherical panorama’s content, we develop a reinforcement learning approach for the cubemap projection model. Implemented as a deep recurrent neural network, our method selects a sequence of rotation actions and receives reward for avoiding cube boundaries that overlap with important foreground objects. We show our approach creates more visually pleasing panoramas while using 5x less computation than the baseline.",2018,Lecture Notes in Computer Science,,10.1007/978-3-030-01228-1,
4782214c4adbc6df05d850e4c17810bfb9f1bc5d,1,0,1,Channel Recurrent Attention Networks for Video Pedestrian Retrieval,"Full attention, which generates an attention value per element of the input feature maps, has been successfully demonstrated to be beneficial in visual tasks. In this work, we propose a fully attentional network, termed {\it channel recurrent attention network}, for the task of video pedestrian retrieval. The main attention unit, \textit{channel recurrent attention}, identifies attention maps at the frame level by jointly leveraging spatial and channel patterns via a recurrent neural network. This channel recurrent attention is designed to build a global receptive field by recurrently receiving and learning the spatial vectors. Then, a \textit{set aggregation} cell is employed to generate a compact video representation. Empirical experimental results demonstrate the superior performance of the proposed deep network, outperforming current state-of-the-art results across standard video person retrieval benchmarks, and a thorough ablation study shows the effectiveness of the proposed units.",2020,ArXiv,2010.03108,,https://arxiv.org/pdf/2010.03108.pdf
4794f699a50ae06eb74907674f27cbbe337d9475,0,1,0,Attention Deep Model with Multi-Scale Deep Supervision for Person Re-Identification,"In recent years, person re-identification (PReID) has become a hot topic in computer vision duo to it is an important part in intelligent surveillance. Many state-of-the-art PReID methods are attention-based or multi-scale feature learning deep models. However, introducing attention mechanism may lead to some important feature information losing issue. Besides, most of the multi-scale models embedding the multi-scale feature learning block into the feature extraction deep network, which reduces the efficiency of inference network. To address these issue, in this study, we introduce an attention deep architecture with multi-scale deep supervision for PReID. Technically, we contribute a reverse attention block to complement the attention block, and a novel multi-scale layer with deep supervision operator for training the backbone network. The proposed block and operator are only used for training, and discard in test phase. Experiments have been performed on Market-1501, DukeMTMC-reID and CUHK03 datasets. All the experiment results show that the proposed model significantly outperforms the other competitive state-of-the-art methods.",2019,ArXiv,1911.10335,,https://arxiv.org/pdf/1911.10335.pdf
47f4dec5f733e933c8b9a8fdcda9419741f2bf62,0,1,0,Adversarial Metric Attack for Person Re-identification,"Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance, especially considering the highly strict requirement of public safety.  Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks, which can effectively generate adversarial examples for re-ID. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Moreover, by benchmarking various adversarial settings, we expect that our work can facilitate the development of robust feature learning with the experimental conclusions we have drawn.",2019,ArXiv,,,
4851bcfc0e2ae7b417e38fcb00b7a44ffdd117d0,1,0,0,MagnifierNet: Towards Semantic Regularization and Fusion for Person Re-identification,"Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this paper, we propose MagnifierNet, a novel network which accurately mines details for each semantic region and selectively fuse all semantic feature representations. Apart from conventional global branch, our proposed network is composed of a Semantic Regularization Branch (SRB) as learning regularizer and a Semantic Fusion Branch (SFB) towards selectively semantic fusion. The SRB learns with limited number of semantic regions randomly sampled in each batch, which forces the network to learn detailed representation for each semantic region, and the SFB selectively fuses semantic region information in a sequential manner, focusing on beneficial information while neglecting irrelevant features or noises. In addition, we introduce a novel loss function ""Semantic Diversity Loss"" (SD Loss) to facilitate feature diversity and improves regularization among all semantic regions. State-of-the-art performance has been achieved on multiple datasets by large margins. Notably, we improve SOTA on CUHK03-Labeled Dataset by 12.6% in mAP and 8.9% in Rank-1. We also outperform existing works on CUHK03-Detected Dataset by 13.2% in mAP and 7.8% in Rank-1 respectively, which demonstrates the effectiveness of our method.",2020,ArXiv,2002.10979,,https://arxiv.org/pdf/2002.10979.pdf
485952ce2b2b36cd0a6d00d2ae4655f0f70b86cd,1,0,0,Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are We There Yet?,"Intelligent video-surveillance (IVS) is currently an active research field in computer vision and machine learning and provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one of the most critical problems in IVS, and it consists of recognizing whether or not an individual has already been observed over a camera in a network. Solutions to PReID have myriad applications including retrieval of video-sequences showing an individual of interest or even pedestrian tracking over multiple camera views. Different techniques have been proposed to increase the performance of PReID in the literature, and more recently researchers utilized deep neural networks (DNNs) given their compelling performance on similar vision problems and fast execution at test time. Given the importance and wide range of applications of re-identification solutions, our objective herein is to discuss the work carried out in the area and come up with a survey of state-of-the-art DNN models being used for this task. We present descriptions of each model along with their evaluation on a set of benchmark datasets. Finally, we show a detailed comparison among these models, which are followed by some discussions on their limitations that can work as guidelines for future research.",2020,ArXiv,2005.00355,,https://arxiv.org/pdf/2005.00355.pdf
48d185de8563add6af7ace6fd2740c717db2df9d,0,1,0,Person Re-identification with Soft Biometrics Through Deep Learning,"Re-identification of persons is usually based on primary biometric features such as their faces, fingerprints, iris or gait. However, in most existing video surveillance systems, it is difficult to obtain these features due to the low resolution of surveillance footages and unconstrained real-world environments. As a result, most of the existing person re-identification techniques only focus on overall visual appearance. Recently, the use of soft biometrics has been proposed to improve the performance of person re-identification. Soft biometrics such as height, gender, age are physical or behavioural features, which can be described by humans. These features can be obtained from low-resolution videos at a distance ideal for person re-identification application. In addition, soft biometrics are traits for describing an individual with human-understandable labels. It allows human verbal descriptions to be used in the person re-identification or person retrieval systems. In some deep learning based person re-identification methods, soft biometrics attributes are integrated into the network to boot the robustness of the feature representation. Biometrics can also be utilised as a domain adaptation bridge for addressing the cross-dataset person re-identification problem. This chapter will review the state-of-the-art deep learning methods involving soft biometrics from three perspectives: supervised, semi-supervised and unsupervised approaches. In the end, we discuss the existing issues that are not addressed by current works.",2020,,,10.1007/978-3-030-32583-1_2,
48e4d6d901d90574b6be900c95a80f167d36de90,1,0,0,Training Algorithms for Multiple Object Tracking,"Multiple object tracking is a crucial Computer Vision Task. It aims at locating objects of interest in the image sequences, maintaining their identities, and identifying their trajectories over time. A large portion of current research focuses on tracking pedestrians, and other types of objects, that often exhibit predictable behaviours, that allow us, as humans, to track those objects. Nevertheless, most existing approaches rely solely on simple affinity or appearance cues to maintain the identities of the tracked objects, ignoring their behaviour. This presents a challenge when objects of interest are invisible or indistinguishable for a long period of time. In this thesis, we focus on enhancing the quality of multiple object trackers by learning and exploiting the long ranging models of object behaviour. Such behaviours come in different forms, be it a physical model of the ball motion, model of interaction between the ball and the players in sports or motion patterns of pedestrians or cars, that is specific to a particular scene. In the first part of the thesis, we begin with the task of tracking the ball and the players in team sports. We propose a model that tracks both types of objects simultaneously, while respecting the physical laws of ball motion when in free fall, and interaction constraints that appear when players are in the possession of the ball. We show that both the presence of the behaviour models and the simultaneous solution of both tasks aids the performance of tracking, in basketball, volleyball, and soccer. In the second part of the thesis, we focus on motion models of pedestrian and car behaviour that emerge in the outdoor scenes. Such motion models are inherently global, as they determine where people starting from one location tend to end up much later in time. Imposing such global constraints while keeping the tracking problem tractable presents a challenge, which is why many approaches rely on local affinity measures. We formulate a problem of simultaneously tracking the objects and learning their behaviour patterns. We show that our approach, when applied in conjunction with a number of state-of-the-art trackers, improves their performance, by forcing their output to follow the learned motion patterns of the scene. In the last part of the thesis, we study a new emerging class of models for multiple object tracking, that appeared recently due to availability of large scale datasets sequence models for multiple object tracking. While such models could potentially learn arbitrarily long ranging behaviours, training them presents several challenges. We propose a training scheme and a loss function",2019,,,10.5075/EPFL-THESIS-9203,https://pdfs.semanticscholar.org/48e4/d6d901d90574b6be900c95a80f167d36de90.pdf
4982495604c95a243eea13b4d8639172bb24ef38,0,1,0,Deep Constrained Dominant Sets for Person Re-Identification,"In this work, we propose an end-to-end constrained clustering scheme to tackle the person re-identification (re-id) problem. Deep neural networks (DNN) have recently proven to be effective on person re-identification task. In particular, rather than leveraging solely a probe-gallery similarity, diffusing the similarities among the gallery images in an end-to-end manner has proven to be effective in yielding a robust probe-gallery affinity. However, existing methods do not apply probe image as a constraint, and are prone to noise propagation during the similarity diffusion process. To overcome this, we propose an intriguing scheme which treats person-image retrieval problem as a constrained clustering optimization problem, called deep constrained dominant sets (DCDS). Given a probe and gallery images, we re-formulate person re-id problem as finding a constrained cluster, where the probe image is taken as a constraint (seed) and each cluster corresponds to a set of images corresponding to the same person. By optimizing the constrained clustering in an end-to-end manner, we naturally leverage the contextual knowledge of a set of images corresponding to the given person-images. We further enhance the performance by integrating an auxiliary net alongside DCDS, which employs a multi-scale ResNet. To validate the effectiveness of our method we present experiments on several benchmark datasets and show that the proposed method can outperform state-of-the-art methods.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.11397,10.1109/ICCV.2019.00995,https://arxiv.org/pdf/1904.11397.pdf
49b911544ebfca12bed8cbf09b4f48679a8a8ae2,1,0,0,"Efficient Inference on Video, In Real-Time and At Scale","Efficient Inference on Video, In Real-Time and At Scale",2020,,,,https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-35.pdf
4a91be40e6b382c3ddf3385ac44062b2399336a8,1,1,0,Random Occlusion-recovery for Person Re-identification,"As a basic task of multi-camera surveillance system, person re-identification aims to re-identify a query pedestrian observed from non-overlapping cameras or across different time with a single camera. Recently, Deep learning based person re-identification models have achieved great success in many benchmarks. However, these supervised models require a large amount of labeled image data and the process of manual labeling spends much manpower and time. In this study, we introduce a method to automatically synthesize labeled person images and adopt them to increase the sample number for per identity in datasets. Specifically, we use the block rectangles to occlude the random parts of the persons in the images. Then, a generative adversarial network (GAN) model is proposed to use paired occlusion and original images to synthesize the de-occluded images that similar but not identical to the original images. Afterwards, we annotate the de-occluded images with the same labels of their corresponding raw image and use them to augment the training samples. We use the augmented datasets to train baseline model. The experiment results on Market-1501 and CUHK03 datasets show that the effectiveness of the proposed method.",2018,ArXiv,1809.0997,10.2352/j.imagingsci.technol.2019.63.3.030405,https://arxiv.org/pdf/1809.09970.pdf
4ad8abdb22733a4f40017a2178a2419b9461660d,0,1,0,Asymmetric Distance Learning for Unsupervised Video Person Re-Identification with Tracklet Neighborhood Re-Ranking,"The gruelling human-annotation and lack of sufficient labeled data make unsupervised person re-identification (re-ID) an important component in research. This paper proposes a re-ID system for unsupervised video-based re-ID, which mainly contains an asymmetric distance learning approach and a re-ranking meth-od. Specifically, using the sequence information provided by video, asymmetric learning makes a distinctive projection for features in each view, while label estimation makes this procedure efficient and effective. To further refine the results of the ranking list, an unsupervised re-ranking technique based on the already computed distance is introduced to the system. We show that both of our asymmetric distance learning and re-ranking method have achieved state-of-the-art performance on PRID-2011, iLIDS-VID and MARS datasets, meanwhile restrains the computational costs. The experiments show that our asymmetric learning method is suitable for video-based re-ID with multiple cameras, and the proposed re-ranking method is a good solution to refine the ranking list for video-based re-ID.",2018,DMIP '18,,10.1145/3299852.3299861,
4b5bcd02d3630e01979060d669faea1fa3fb4670,0,1,0,Synthetic Convolutional Features for Improved Semantic Segmentation,"Recently, learning-based image synthesis has enabled to generate high-resolution images, either applying popular adversarial training or a powerful perceptual loss. However, it remains challenging to successfully leverage synthetic data for improving semantic segmentation with additional synthetic images. Therefore, we suggest to generate intermediate convolutional features and propose the first synthesis approach that is catered to such intermediate convolutional features. This allows us to generate new features from label masks and include them successfully into the training procedure in order to improve the performance of semantic segmentation. Experimental results and analysis on two challenging datasets Cityscapes and ADE20K show that our generated feature improves performance on segmentation tasks.",2020,ArXiv,2009.08849,,https://arxiv.org/pdf/2009.08849.pdf
4bb3764e1fc2e200ae23b3ea72a31e19a7d66ee2,0,1,0,Network Improved by Auxiliary Part Features for Person Re-identification,"Person re-identification (ReID) is an important issue in computer vision area. It focuses on identifying people under different scenarios. In this paper, we test the contributions of local part features in ReID system. With the auxiliary of local part features, our model achieves significantly improvements, which achieves rank-1 accuracy of 91.7% on market1501 dataset and 82.6% on MARS dataset. We also test the feasibility of using densenet as backbone model in ReID system. With densenet as our backbone model, our method achieves state-of-art performance and simultaneously reduces the model size enormously.",2018,ICCSIP,,10.1007/978-981-13-7986-4_20,
4bdf4fe5ec737d40c8904d35f9ba80f0631fe9fa,0,1,0,COCAS: A Large-Scale Clothes Changing Person Dataset for Re-Identification,"Recent years have witnessed great progress in person re-identification (re-id). Several academic benchmarks such as Market1501, CUHK03 and DukeMTMC play important roles to promote the re-id research. To our best knowledge, all the existing benchmarks assume the same person will have the same clothes. While in real-world scenarios, it is very often for a person to change clothes. To address the clothes changing person re-id problem, we construct a novel large-scale re-id benchmark named Clothes Changing Person Set (COCAS), which provides multiple images of the same identity with different clothes. COCAS totally contains 62,382 body images from 5,266 persons. Based on COCAS, we introduce a new person re-id setting for clothes changing problem, where the query includes both a clothes template and a person image taking another clothes. Moreover, we propose a two-branch network named Biometric-Clothes Network (BC-Net) which can effectively integrate biometric and clothes feature for re-id under our setting. Experiments show that it is feasible for clothes changing re-id with clothes templates.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2005.07862,10.1109/cvpr42600.2020.00346,https://arxiv.org/pdf/2005.07862.pdf
4c4815776b4d3b82e8f8e1b24bb6fdf5735bfc63,1,0,0,A Bayesian Filter for Multi-view 3D Multi-object Tracking with Occlusion Handling,,2020,,2001.04118,,https://arxiv.org/pdf/2001.04118.pdf
4c903009e7b963f1cd4f02482ea4b242d71e8058,1,1,0,Camera Adversarial Transfer for Unsupervised Person Re-Identification,"Unsupervised person re-identification (Re-ID) methods consist of training with a carefully labeled source dataset, followed by generalization to an unlabeled target dataset, i.e. person-identity information is unavailable. Inspired by domain adaptation techniques, these methods avoid a costly, tedious and often unaffordable labeling process. This paper investigates the use of camera-index information, namely which camera captured which image, for unsupervised person Re-ID. More precisely, inspired by domain adaptation adversarial approaches, we develop an adversarial framework in which the output of the feature extractor should be useful for person Re-ID and in the same time should fool a camera discriminator. We refer to the proposed method as camera adversarial transfer (CAT). We evaluate adversarial variants and, alongside, the camera robustness achieved for each variant. We report cross-dataset ReID performance and we compare the variants of our method with several state-of-the-art methods, thus showing the interest of exploiting camera-index information within an adversarial framework for the unsupervised person Re-ID.",2019,ArXiv,,,
4c9112d0bdb0f13938144a3d8a5bf9c2916f4c6e,1,0,0,Compact Network Training for Person ReID,"The task of person re-identification (ReID) has attracted growing attention in recent years leading to improved performance, albeit with little focus on real-world applications. Most SotA methods are based on heavy pre-trained models, e.g. ResNet50 (~25M parameters), which makes them less practical and more tedious to explore architecture modifications. In this study, we focus on a small-sized randomly initialized model that enables us to easily introduce architecture and training modifications suitable for person ReID. The outcomes of our study are a compact network and a fitting training regime. We show the robustness of the network by outperforming the SotA on both Market1501 and DukeMTMC. Furthermore, we show the representation power of our ReID network via SotA results on a different task of multi-object tracking.",2020,ICMR,1910.07038,10.1145/3372278.3390686,https://arxiv.org/pdf/1910.07038.pdf
4c9cd3d642d80fe39c07fee8fe3868eee7a7962c,0,1,0,Human skeleton mutual learning for person re-identification,"Abstract Person re-identification refers to matching people across non-overlapping camera views on different locations and at different times. In the case of changes in perspective, light, background, veil, and person's clothing, traditional method cannot achieve person recognition effectively and reliably. In this paper, we propose a novel biometric metric learning method named Human Skeleton Mutual Learning person re-identification (HSMLP-Reid). The purpose of HSML person re-identification method (HSMLP-Reid) largely aims to use the new pedestrian local segmentation method proposed in this paper combined with the global skeleton information to solve the influence of background and local posture change. Firstly, bottom-up method is used to estimate the pedestrian posture and skeleton and the joint points of the pedestrians will be marked in this process. A new local segmentation method proposed in this paper named joint segmentation is used to locally segment pedestrians and perform local block matching. Furthermore, we learn the global skeleton information by defined joint distances from the pedestrian 2D skeleton estimation by the bottom-up method and use global skeleton information to global skeleton matching. Finally, we use local match and global skeleton match for mutual learning. Local match based on pedestrian nodes and global skeleton match based on pedestrian skeleton are based on biometrics. We learn the classification loss and metric learning loss to train model. Metric loss includes global skeletal distance and local block metric distance. Extensive experimental results on the large-scale Market1501, CUHK03 and CUHK-SYSU data sets demonstrate that the proposed method achieves consistently superior performance and outperforms most of the state-of-the-art methods.",2020,Neurocomputing,,10.1016/j.neucom.2019.12.120,
4cd07662e23da2bc9592b595d1ba008e621a5173,1,1,0,Simulating Content Consistent Vehicle Datasets with Attribute Descent,"This paper uses a graphic engine to simulate a large amount of training data with free annotations. Between synthetic and real data, there is a two-level domain gap, i.e., content level and appearance level. While the latter has been widely studied, we focus on reducing the content gap in attributes like illumination and viewpoint. To reduce the problem complexity, we choose a smaller and more controllable application, vehicle re-identification (re-ID). We introduce a large-scale synthetic dataset VehicleX. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes. We propose an attribute descent approach to let VehicleX approximate the attributes in real-world datasets. Specifically, we manipulate each attribute in VehicleX, aiming to minimize the discrepancy between VehicleX and real data in terms of the Frechet Inception Distance (FID). This attribute descent algorithm allows content domain adaptation (DA) orthogonal to existing appearance DA methods. We mix the optimized VehicleX data with real-world vehicle re-ID datasets, and observe consistent improvement. With the augmented datasets, we report competitive accuracy. We make the dataset, engine and our codes available at this https URL.",2020,ECCV,1912.08855,10.1007/978-3-030-58539-6_46,https://arxiv.org/pdf/1912.08855.pdf
4d024deee31516076ff95c807f5d62b8ad11d022,1,1,0,Unsupervised Region Attention Network for Person Re-Identification,"As supervised person re-identification (Re-Id) requires massive labeled pedestrian data and it is very difficult to collect sufficient labeled data in reality, unsupervised Re-Id approaches attract much more attention than the former. Existing unsupervised person Re-Id models learn global features of pedestrian from whole images or several constant patches. These models ignore the difference of each region in the whole pedestrian images for feature representation, such as occluded and pose invariant regions, and thus reduce the robustness of models for cross-view feature learning. To solve these issues, we propose an Unsupervised Region Attention Network (URAN) that can learn the cross-view region attention features from the cropped pedestrian images, fixed by region importance weights on images. The proposed URAN designs a Pedestrian Region Biased Enhance (PRBE) loss to produce high attention weights for most important regions in pedestrian images. Furthermore, the URAN employs a first neighbor relation grouping algorithm and a First Neighbor Relation Constraint (FNRC) loss to provide the training direction of the unsupervised region attention network, such that the region attention features are discriminant enough for unsupervised person Re-Id task. In experiments, we consider two popular datasets, Market1501 and DukeMTMC-reID, as evaluation of PRBE and FNRC loss, and their balance parameter to demonstrate the effectiveness and efficiency of the proposed URAN, and the experimental results show that the URAN provides better performance than the-state-of-the-arts (higher than existing methods at least 1.1%).",2019,IEEE Access,,10.1109/ACCESS.2019.2953280,
4d799f6e09f442bde583a50a0a9f81131ef707bb,1,1,0,TAR: Enabling Fine-Grained Targeted Advertising in Retail Stores,"Mobile advertisements influence customers' in-store purchases and boost in-store sales for brick-and-mortar retailers. Targeting mobile ads has become significantly important to compete with online shopping. The key to enabling targeted mobile advertisement and service is to learn shoppers' interest during their stay in the store. Precise shopper tracking and identification are essential to gain the insights. However, existing sensor-based or vision-based solutions are neither practical nor accurate; no commercial solutions today can be readily deployed in a large store. On the other hand, we recognize that most retail stores have the installation of surveillance cameras, and most shoppers carry Bluetooth-enabled smartphones. Thus, in this paper, we propose TAR to learn shoppers' in-store interest via accurate multi-camera people tracking and identification. TAR leverages widespread camera deployment and Bluetooth proximity information to accurately track and identify shoppers in the store. TAR is composed of four novel design components: (1) a deep neural network (DNN) based visual tracking, (2) a user trajectory estimation by using shopper visual and BLE proximity trace, (3) an identity matching and assignment to recognize shopper's identity, and (4) a cross-camera calibration algorithm. TAR carefully combines these components to track and identify shoppers in real-time. TAR achieves 90% accuracy in two different real-life deployments, which is 20% better than the state-of-the-art solution.",2018,MobiSys,,10.1145/3210240.3210342,https://nslcsusc.files.wordpress.com/2018/06/mobisys18-tar.pdf
4d8347a69e77cc02c1e1aba3a8b6646eac1a0b3d,1,1,0,Re-ID done right: towards good practices for person re-identification,"Training a deep architecture using a ranking loss has become standard for the person re-identification task. Increasingly, these deep architectures include additional components that leverage part detections, attribute predictions, pose estimators and other auxiliary information, in order to more effectively localize and align discriminative image regions. In this paper we adopt a different approach and carefully design each component of a simple deep architecture and, critically, the strategy for training it effectively for person re-identification. We extensively evaluate each design choice, leading to a list of good practices for person re-identification. By following these practices, our approach outperforms the state of the art, including more complex methods with auxiliary components, by large margins on four benchmark datasets. We also provide a qualitative analysis of our trained representation which indicates that, while compact, it is able to capture information from localized and discriminative regions, in a manner akin to an implicit attention mechanism.",2018,ArXiv,1801.05339,,https://arxiv.org/pdf/1801.05339.pdf
4d866d9c1cd7f756c1bda334e9e4342dcd489aef,1,0,0,Eye in the Sky: Drone-Based Object Tracking and 3D Localization,"Drones, or general UAVs, equipped with a single camera have been widely deployed to a broad range of applications, such as aerial photography, fast goods delivery and most importantly, surveillance. Despite the great progress achieved in computer vision algorithms, these algorithms are not usually optimized for dealing with images or video sequences acquired by drones, due to various challenges such as occlusion, fast camera motion and pose variation. In this paper, a drone-based multi-object tracking and 3D localization scheme is proposed based on the deep learning based object detection. We first combine a multi-object tracking method called TrackletNet Tracker (TNT) which utilizes temporal and appearance information to track detected objects located on the ground for UAV applications. Then, we are also able to localize the tracked ground objects based on the group plane estimated from the Multi-View Stereo technique. The system deployed on the drone can not only detect and track the objects in a scene, but can also localize their 3D coordinates in meters with respect to the drone camera. The experiments have proved our tracker can reliably handle most of the detected objects captured by drones and achieve favorable 3D localization performance when compared with the state-of-the-art methods.",2019,ACM Multimedia,1910.08259,10.1145/3343031.3350933,https://arxiv.org/pdf/1910.08259.pdf
4d8ba694859f8a222341882e421292f0764cb47f,1,0,0,Reference-oriented Loss for Person Re-identification,"Deep metric learning methods are quite effective in exploring discriminative feature embeddings, among which triplet loss and its variants are widely utilized. However, in existing methods, the tightness information for intra-class samples is ignored, leading to large intra-class divergence and severe inter-class overlapping problem. To address this issue, a novel loss function called reference-oriented triplet loss is proposed in this paper. The proposed method introduces several reference images to guide training. More specifically, distances between the reference image and images of the same identity are required to be as similar as possible. By introducing reference images, images from the same class become much closer with each other and the inter-class overlapping problem is alleviated. Comparing to baseline batch hard triplet loss, the mAP accuracy increases by 3.75%/5.69% on person re-ID datasets Market1501 and DukeMTMC-Reid. Comparison results with state-of-the-art algorithms also demonstrate effectiveness of the proposed algorithm.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8851996,
4dc34746158da585a66d4bf1a6240d816705ac17,1,1,1,Long-Short Temporal–Spatial Clues Excited Network for Robust Person Re-identification,"Directly benefiting from the rapid advancement of deep learning methods, person re-identification (Re-ID) applications have been widespread with remarkable successes in recent years. Nevertheless, cross-scene Re-ID is still hindered by large view variation, since it is challenging to effectively exploit and leverage the temporal clues due to heavy computational burden and the difficulty in flexibly incorporating discriminative features. To alleviate, we articulate a long-short temporal–spatial clues excited network (LSTS-NET) for robust person Re-ID across different scenes. In essence, our LSTS-NET comprises a motion appearance model and a motion-refinement aggregating scheme. Of which, the former abstracts temporal clues based on multi-range low-rank analysis both in consecutive frames and in cross-camera videos, which can augment the person-related features with details while suppressing the clutter background across different scenes. In addition, to aggregate the temporal clues with spatial features, the latter is proposed to automatically activate the person-specific features by incorporating personalized motion-refinement layers and several motion-excitation CNN blocks into deep networks, which expedites the extraction and learning of discriminative features from different temporal clues. As a result, our LSTS-NET can robustly distinguish persons across different scenes. To verify the improvement of our LSTS-NET, we conduct extensive experiments and make comprehensive evaluations on 8 widely-recognized public benchmarks. All the experiments confirm that, our LSTS-NET can significantly boost the Re-ID performance of existing deep learning methods, and outperforms the state-of-the-art methods in terms of robustness and accuracy.",2020,International Journal of Computer Vision,,10.1007/s11263-020-01349-4,
4dcbb68106dee0b5f68f6060c836b40cd7c053c3,1,0,0,Unsupervised Person Re-Identification via Multi-Label Classification,"The challenge of unsupervised person re-identification (ReID) lies in learning discriminative features without true labels. This paper formulates unsupervised person ReID as a multi-label classification task to progressively seek true labels. Our method starts by assigning each person image with a single-class label, then evolves to multi-label classification by leveraging the updated ReID model for label prediction. The label prediction comprises similarity computation and cycle consistency to ensure the quality of predicted labels. To boost the ReID model training efficiency in multi-label classification, we further propose the memory-based multi-label classification loss (MMCL). MMCL works with memory-based non-parametric classifier and integrates multi-label classification and single-label classification in an unified framework. Our label prediction and MMCL work iteratively and substantially boost the ReID performance. Experiments on several large-scale person ReID datasets demonstrate the superiority of our method in unsupervised person ReID. Our method also allows to use labeled person images in other domains. Under this transfer learning setting, our method also achieves state-of-the-art performance.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.09228,10.1109/CVPR42600.2020.01099,https://arxiv.org/pdf/2004.09228.pdf
4dcc3f86c0ca68c23e2d0b0f5ebffd5d3fdc8f8a,1,0,0,PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks,"Despite significant progress of applying deep learning methods to the field of content-based image retrieval, there has not been a software library that covers these methods in a unified manner. In order to fill this gap, we introduce PyRetri, an open source library for deep learning based unsupervised image retrieval. The library encapsulates the retrieval process in several stages and provides functionality that covers various prominent methods for each stage. The idea underlying its design is to provide a unified platform for deep learning based image retrieval research, with high usability and extensibility. The project source code, with usage examples, sample data and pre-trained models are available at https://github.com/PyRetri/.",2020,ACM Multimedia,2005.02154,10.1145/3394171.3414537,https://arxiv.org/pdf/2005.02154.pdf
4e0a735ee8f7606dd13633a88de15f6dfe3348ac,1,1,0,Deep Meta Metric Learning,"In this paper, we present a deep meta metric learning (DMML) approach for visual recognition. Unlike most existing deep metric learning methods formulating the learning process by an overall objective, our DMML formulates the metric learning in a meta way, and proves that softmax and triplet loss are consistent in the meta space. Specifically, we sample some subsets from the original training set and learn metrics across different subsets. In each sampled sub-task, we split the training data into a support set as well as a query set, and learn the set-based distance, instead of sample-based one, to verify the query cell from multiple support cells. In addition, we introduce hard sample mining for set-based distance to encourage the intra-class compactness. Experimental results on three visual recognition applications including person re-identification, vehicle re-identification and face verification show that the proposed DMML method outperforms most existing approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00964,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Deep_Meta_Metric_Learning_ICCV_2019_paper.pdf
4e479987696bcb229cb4779b896187bc0d3497e2,0,1,1,Temporal Coherence or Temporal Motion: Which Is More Critical for Video-Based Person Re-identification?,"Video-based person re-identification aims to match pedestrians with the consecutive video sequences. While a rich line of work focuses solely on extracting the motion features from pedestrian videos, we show in this paper that the temporal coherence plays a more critical role. To distill the temporal coherence part of video representation from frame representations, we propose a simple yet effective Adversarial Feature Augmentation (AFA) method, which highlights the temporal coherence features by introducing adversarial augmented temporal motion noise. Specifically, we disentangle the video representation into the temporal coherence and motion parts and randomly change the scale of the temporal motion features as the adversarial noise. The proposed AFA method is a general lightweight component that can be readily incorporated into various methods with negligible cost. We conduct extensive experiments on three challenging datasets including MARS, iLIDS-VID, and DukeMTMC-VideoReID, and the experimental results verify our argument and demonstrate the effectiveness of the proposed method.",2020,ECCV,,10.1007/978-3-030-58598-3_39,https://raoyongming.github.io/files/temporal_coherence.pdf
4e4e3ddb55607e127a4abdef45d92adf1ff78de2,1,0,0,Non-Markovian Globally Consistent Multi-object Tracking,"Many state-of-the-art approaches to multi-object tracking rely on detecting them in each frame independently, grouping detections into short but reliable trajectory segments, and then further grouping them into full trajectories. This grouping typically relies on imposing local smoothness constraints but almost never on enforcing more global ones on the trajectories.,,In this paper, we propose a non-Markovian approach to imposing global consistency by using behavioral patterns to guide the tracking algorithm. When used in conjunction with state-of-the-art tracking algorithms, this further increases their already good performance on multiple challenging datasets. We show significant improvements both in supervised settings where ground truth is available and behavioral patterns can be learned from it, and in completely unsupervised settings.",2017,2017 IEEE International Conference on Computer Vision (ICCV),,10.1109/ICCV.2017.278,http://www.idiap.ch/~fleuret/papers/maksai-et-al-iccv2017.pdf
4e5c09ace0cbb3a442bb4e32a22fba23c12ac063,0,1,0,Person Re-identification with Hierarchical Deep Learning Feature and efficient XQDA Metric,"Feature learning and metric learning are two important components in person re-identification (re-id). In this paper, we utilize both aspects to refresh the current State-Of-The-Arts (SOTA). Our solution is based on a classification network with label smoothing regularization (LSR) and multi-branch tree structure. The insight is that some middle network layers are found surprisingly better than the last layers on the re-id task. A Hierarchical Deep Learning Feature (HDLF) is thus proposed by combining such useful middle layers. To learn the best metric for the high-dimensional HDLF, an efficient eXQDA metric is proposed to deal with the large-scale big-data scenarios. The proposed HDLF and eXQDA are evaluated with current SOTA methods on five benchmark datasets. Our methods achieve very high re-id results, which are far beyond state-of-the-art solutions. For example, our approach reaches 81.6%, 96.1% and 95.6% Rank-1 accuracies on the ILIDS-VID, PRID2011 and Market-1501 datasets. Besides, the code and related materials (lists of over 1800 re-id papers and 170 top conference re-id papers) are released for research purposes.",2018,ACM Multimedia,,10.1145/3240508.3240717,
4e7be14130623a2859944b12028f7101c0508712,1,0,1,Exploiting Temporal Coherence for Self-Supervised One-shot Video Re-identification,"While supervised techniques in re-identification are extremely effective, the need for large amounts of annotations makes them impractical for large camera networks. One-shot re-identification, which uses a singular labeled tracklet for each identity along with a pool of unlabeled tracklets, is a potential candidate towards reducing this labeling effort. Current one-shot re-identification methods function by modeling the inter-relationships amongst the labeled and the unlabeled data, but fail to fully exploit such relationships that exist within the pool of unlabeled data itself. In this paper, we propose a new framework named Temporal Consistency Progressive Learning, which uses temporal coherence as a novel self-supervised auxiliary task in the one-shot learning paradigm to capture such relationships amongst the unlabeled tracklets. Optimizing two new losses, which enforce consistency on a local and global scale, our framework can learn learn richer and more discriminative representations. Extensive experiments on two challenging video re-identification datasets - MARS and DukeMTMC-VideoReID - demonstrate that our proposed method is able to estimate the true labels of the unlabeled data more accurately by up to $8\%$, and obtain significantly better re-identification performance compared to the existing state-of-the-art techniques.",2020,ECCV,2007.11064,10.1007/978-3-030-58583-9_16,https://arxiv.org/pdf/2007.11064.pdf
4ece8b73a0a713731d063a59a6c0936c58a2c239,1,0,0,Deep Pyramidal Pooling With Attention for Person Re-Identification,"Learning discriminative, view-invariant and multi-scale representations of object appearance with different semantic levels is of paramount importance for person Re-Identification (ReID). Recently, the community has focused on learning deep Re-ID models to capture a single holistic representation. To improve the achieved results, additional visual attributes and object part-driven models have been considered, inevitably introducing additional human annotation labor or computational efforts. In this paper, we argue that pyramid-inspired methods capturing multi-scale information may overcome such requirements. Precisely, multi-scale pooled regions representing visual information of an object are integrated within a novel deep architecture factorizing them into discriminative features at multiple semantic levels. These are exploited through an attention mechanism later considered in an identification-similarity multi-task loss, trained by means of a curriculum learning strategy. Extensive results on three person ReID benchmarks demonstrate that better performance than existing methods are achieved. Code is available at https://github.com/iN1k1.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3000904,
4f83ef534c164bd7fbd1e71fe6a3d09a30326b26,0,1,0,Cross-Resolution Person Re-identification with Deep Antithetical Learning,"Images with different resolutions are ubiquitous in public person re-identification (ReID) datasets and real-world scenes, it is thus crucial for a person ReID model to handle the image resolution variations for improving its generalization ability. However, most existing person ReID methods pay little attention to this resolution discrepancy problem. One paradigm to deal with this problem is to use some complicated methods for mapping all images into an artificial image space, which however will disrupt the natural image distribution and requires heavy image preprocessing. In this paper, we analyze the deficiencies of several widely-used objective functions handling image resolution discrepancies and propose a new framework called deep antithetical learning that directly learns from the natural image space rather than creating an arbitrary one. We first quantify and categorize original training images according to their resolutions. Then we create an antithetical training set and make sure that original training images have counterparts with antithetical resolutions in this new set. At last, a novel Contrastive Center Loss(CCL) is proposed to learn from images with different resolutions without being interfered by their resolution discrepancies. Extensive experimental analyses and evaluations indicate that the proposed framework, even using a vanilla deep ReID network, exhibits remarkable performance improvements. Without bells and whistles, our approach outperforms previous state-of-the-art methods by a large margin.",2018,ACCV,1810.10221,10.1007/978-3-030-20893-6_15,https://arxiv.org/pdf/1810.10221.pdf
4f89223d0fca71d3ab3a459a6856b9aebd9183c3,1,0,0,Multi-camera Tracking Exploiting Person Re-ID Technique,"Multi-target multi-camera tracking is an important issue in image processing. It is meaningful to improve matching performance across cameras with high computational efficiency. In this paper, we apply high performance feature representation LOMO and metric learning XQDA in person re-identification across cameras to improve tracking performance. We also exploit direction information of trajectories to handle viewpoint variation. Experiments on DukeMTMCT dataset show that the proposed method improves tracking performance and is also competitive in running time.",2017,ICONIP,,10.1007/978-3-319-70090-8_41,
4fb7bd7d68f1f07f787aa2824cc8948a2d6820d8,1,1,0,Multi-Target Multi-Camera Tracking with Human Body Part Semantic Features,"Recently, Multi-Target Multi-Camera Tracking (MTMCT) has gained more and more attention. It is a challenging task with major problems including occlusion, background clutter, poses and camera point of view variations. Compared to single camera tracking, which takes advantage of location information and strict time constraints, good appearance features are more important to MTMCT. This drives us to extract robust and discriminative features for MTMCT. We propose MTMCT\_HS which uses human body part semantic features to overcome the above challenges. We use a two-stream deep neural network to extract the global appearance features and human body part semantic maps separately, and employ aggregation operations to generate final features. We argue that these features are more suitable for affinity measurement, which can be seen as the average of appearance similarity weighted by the corresponding human body part similarity. Next, our tracker adopts a hierarchical correlation clustering algorithm, which combines targets' appearance feature similarity with motion correlation for data association. We validate the effectiveness of our MTMCT\_HS method by demonstrating its superiority over the state-of-the-art method on DukeMTMC benchmark. Experiments show that the extracted features with human body part semantics are more effective for MTMCT compared with the methods solely employing global appearance features.",2019,CIKM,,10.1145/3357384.3358029,
4fce128b9579fe6c466846a3cdcf20f1dfbcfbec,0,1,0,Person Re-identification Based on Camera Style Adaptation,"Person re-identification is a popular topic in computer vision, aiming to retrieve a given pedestrian image across the camera. In this paper, a new Person re-identification based on camera style (CamStyle) adaptation is proposed to solve the problem of lack of data and lack of information in pedestrian feature extraction. In the stage of image preprocessing, CamStyle can serve as a data augmentation approach that transforms the camera style of image. In the training stage, using ID loss and Triplet loss to supervise training of eigenvectors. The experimental results show that the recognition accuracy of the method is improved greatly on Market1501, and the validity of the method is verified.",2020,AINA Workshops,,10.1007/978-3-030-44038-1_45,
4fdbce56b288c8e160641bda6d2af29eadbefc42,1,1,1,Progressive Unsupervised Person Re-identification by Tracklet Association with Spatio-Temporal Regularization,"Existing methods for person re-identification (Re-ID) are mostly based on supervised learning which requires numerous manually labeled samples across all camera views for training. Such a paradigm suffers the scalability issue since in real-world Re-ID application, it is difficult to exhaustively label abundant identities over multiple disjoint camera views. To this end, we propose a progressive deep learning method for unsupervised person Re-ID in the wild by Tracklet Association with Spatio-Temporal Regularization (TASTR). In our approach, we first collect tracklet data within each camera by automatic person detection and tracking. Then, an initial Re-ID model is trained based on within-camera triplet construction for person representation learning. After that, based on the person visual feature and spatio-temporal constraint, we associate cross-camera tracklets to generate cross-camera triplets and update the Re-ID model. Lastly, with the refined Re-ID model, better visual feature of person can be extracted, which further promote the association of cross-camera tracklets. The last two steps are iterated multiple times to progressively upgrade the Re-ID model.",2019,ArXiv,1910.1156,10.1109/tmm.2020.2985525,https://arxiv.org/pdf/1910.11560.pdf
4fef7fe1d8422c6d8a6d5d144470b089419e3b82,0,1,0,MOTS: Multiple Object Tracking for General Categories Based On Few-Shot Method,"Most modern Multi-Object Tracking (MOT) systems typically apply REID-based paradigm to hold a balance between computational efficiency and performance. In the past few years, numerous attempts have been made to perfect the systems. Although they presented favorable performance, they were constrained to track specified category. Drawing on the ideas of few shot method, we pioneered a new multi-target tracking system, named MOTS, which is based on metrics but not limited to track specific category. It contains two stages in series: In the first stage, we design the self-Adaptive-matching module to perform simple targets matching, which can complete 88.76% assignments without sacrificing performance on MOT16 training set. In the second stage, a Fine-match Network was carefully designed for unmatched targets. With a newly built TRACK-REID data-set, the Fine-match Network can perform matching of 31 category targets, even generalizes to unseen categories.",2020,ArXiv,2005.09167,,https://arxiv.org/pdf/2005.09167.pdf
5061fe2817edf066307576d0de16143a57dc7e8b,0,1,0,Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification,"Extracting effective and discriminative features is very important for addressing the challenging person re-identification (re-ID) task. Prevailing deep convolutional neural networks (CNNs) usually use high-level features for identifying pedestrian. However, some essential spatial information resided in low-level features such as shape, texture and color will be lost when learning the high-level features, due to extensive padding and pooling operations in the training stage. In addition, most existing person re-ID methods are mainly based on hand-craft bounding boxes where images are precisely aligned. It is unrealistic in practical applications, since the exploited object detection algorithms often produce inaccurate bounding boxes. This will inevitably degrade the performance of existing algorithms. To address these problems, we put forward a novel person re-ID model that fuses high- and low-level embeddings to reduce the information loss caused in learning high-level features. Then we divide the fused embedding into several parts and reconnect them to obtain the global feature and more significant local features, so as to alleviate the affect caused by the inaccurate bounding boxes. In addition, we also introduce the spatial and channel attention mechanisms in our model, which aims to mine more discriminative features related to the target. Finally, we reconstruct the feature extractor to ensure that our model can obtain more richer and robust features. Extensive experiments display the superiority of our approach compared with existing approaches. Our code is available at this https URL.",2020,ArXiv,2009.07536,,https://arxiv.org/pdf/2009.07536.pdf
507210135fa0c86e58713f15d295a596d4ad87e8,1,1,0,Semantically Selective Augmentation for Deep Compact Person Re-Identification,"We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clustering-based network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces.",2018,,,,https://research-information.bris.ac.uk/files/203125984/Full_text_PDF_accepted_author_manuscript_.pdf
50a5f148edee525becde78d97b248b61944ba8ad,1,1,0,Semantically Selective Augmentation for Deep Compact Person Re-Identification,"We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clustering-based network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces.",2018,ECCV Workshops,1806.04074,10.1007/978-3-030-11012-3_41,https://arxiv.org/pdf/1806.04074.pdf
50e5ca477ee8cc2b9dd58de78ceacf37cc085d18,0,0,1,Learning Person Re-identification Models from Videos with Weak Supervision,"Most person re-identification methods, being supervised techniques, suffer from the burden of massive annotation requirement. Unsupervised methods overcome this need for labeled data, but perform poorly compared to the supervised alternatives. In order to cope with this issue, we introduce the problem of learning person re-identification models from videos with weak supervision. The weak nature of the supervision arises from the requirement of video-level labels, i.e. person identities who appear in the video, in contrast to the more precise framelevel annotations. Towards this goal, we propose a multiple instance attention learning framework for person re-identification using such video-level labels. Specifically, we first cast the video person re-identification task into a multiple instance learning setting, in which person images in a video are collected into a bag. The relations between videos with similar labels can be utilized to identify persons, on top of that, we introduce a co-person attention mechanism which mines the similarity correlations between videos with person identities in common. The attention weights are obtained based on all person images instead of person tracklets in a video, making our learned model less affected by noisy annotations. Extensive experiments demonstrate the superiority of the proposed method over the related methods on two weakly labeled person re-identification datasets.",2020,ArXiv,2007.10631,,https://arxiv.org/pdf/2007.10631.pdf
50e94e13947f3f2b7cc93379be88a9cab2b2d320,0,1,0,Exploiting Images for Video Recognition: Heterogeneous Feature Augmentation via Symmetric Adversarial Learning,"Training deep models of video recognition usually requires sufficient labeled videos in order to achieve good performance without over-fitting. However, it is quite labor-intensive and time-consuming to collect and annotate a large amount of videos. Moreover, training deep neural networks on large-scale video datasets always demands huge computational resources which further hold back many researchers and practitioners. To resolve that, collecting and training on annotated images are much easier. However, thoughtlessly applying images to help recognize videos may result in noticeable performance degeneration due to the well-known domain shift and feature heterogeneity. This proposes a novel symmetric adversarial learning approach for heterogeneous image-to-video adaptation, which augments deep image and video features by learning domain-invariant representations of source images and target videos. Primarily focusing on an unsupervised scenario where the labeled source images are accompanied by unlabeled target videos in the training phrase, we present a data-driven approach to respectively learn the augmented features of images and videos with superior transformability and distinguishability. Starting with learning a common feature space (called image-frame feature space) between images and video frames, we then build new symmetric generative adversarial networks (Sym-GANs) where one GAN maps image-frame features to video features and the other maps video features to image-frame features. Using the Sym-GANs, the source image feature is augmented with the generated video-specific representation to capture the motion dynamics while the target video feature is augmented with the image-specific representation to take the static appearance information. Finally, the augmented features from the source domain are fed into a network with fully connected layers for classification. Thanks to an end-to-end training procedure of the Sym-GANs and the classification network, our approach achieves better results than other state-of-the-arts, which is clearly validated by experiments on two video datasets, i.e., the UCF101 and HMDB51 datasets.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2917867,
5114e46720e3eeee3def2e6d73aad4049e2c2a78,1,1,0,Interpretable and Generalizable Deep Image Matching with Adaptive Convolutions,"For image matching tasks, like face recognition and person re-identification, existing deep networks often focus on representation learning. However, without domain adaptation or transfer learning, the learned model is fixed as is, which is not adaptable to handle various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and result is interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of such an image matching architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. The proposed method is preliminarily validated on the person re-identification task. Through direct cross-dataset evaluation without further transfer learning, it achieves better results than many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method is proposed, which improves the performance to a further extent, resulting in state-of-the-art results in cross-dataset evaluation.",2019,ArXiv,,,
5137ca9f0a7cf4c61f2254d4a252a0c56e5dcfcc,1,1,0,Batch Feature Erasing for Person Re-identification and Beyond,"This paper presents a new training mechanism called Batch Feature Erasing (BFE) for person re-identification. We apply this strategy to train a novel network with two branches and employing the ResNet-50 as the backbone. The two branches consist of a conventional global branch and a feature erasing branch where the BFE strategy is applied. When training the feature erasing branch, we randomly erase the same region of all the feature maps in a batch. The network then concatenates features from the two branches for person re-identification. Albeit simple, our method achieves state-of-the-art on person re-identification and is applicable to general metric learning tasks in image retrieval problems. For instance, we achieve 75.4% Rank1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existed works by a large margin (more than 6%).",2018,ArXiv,1811.0713,,https://arxiv.org/pdf/1811.07130.pdf
513d1979a56bfd03b825c7f3b2a3ba70869315d1,1,0,0,Self-supervised Multi-view Person Association and Its Applications.,,2020,IEEE transactions on pattern analysis and machine intelligence,1805.08717,10.1109/TPAMI.2020.2974726,https://arxiv.org/pdf/1805.08717.pdf
51491596bd2017d77b7d6b91b68a86169b6b2aa2,1,0,0,Combine Coarse and Fine Cues: Multi-grained Fusion Network for Video-Based Person Re-identification,"Video-based person re-identification aims to precisely match video sequences of pedestrian across non-overlapped cameras. Existing methods deal with this task by encoding each frame and aggregating them along time. In order to increase the discriminative ability of video features, we propose an end-to-end framework called Multi-grained Fusion Network (MGFN) which aims to keep both global and local information by combining frame-level representations with different granularities. The final video features are generated by aggregating multi-grained representations on both spatial and temporal. Experiments indicate our method achieves excellent performance on three widely used datasets named PRID-2011, iLIDS-VID, and MARS. Especially on MARS, MGFN surpass state-of-the-art result by \(11.5\%\).",2018,KSEM,,10.1007/978-3-319-99365-2_16,
51ea7f870a69d56bea93cf3fcfca06d9c09a53cf,0,0,1,Camera On-Boarding for Person Re-Identification Using Hypothesis Transfer Learning,"Most of the existing approaches for person re-identification consider a static setting where the number of cameras in the network is fixed. An interesting direction, which has received little attention, is to explore the dynamic nature of a camera network, where one tries to adapt the existing re-identification models after on-boarding new cameras, with little additional effort. There have been a few recent methods proposed in person re-identification that attempt to address this problem by assuming the labeled data in the existing network is still available while adding new cameras. This is a strong assumption since there may exist some privacy issues for which one may not have access to those data. Rather, based on the fact that it is easy to store the learned re-identifications models, which mitigates any data privacy concern, we develop an efficient model adaptation approach using hypothesis transfer learning that aims to transfer the knowledge using only source models and limited labeled data, but without using any source camera data from the existing network. Our approach minimizes the effect of negative transfer by finding an optimal weighted combination of multiple source models for transferring the knowledge. Extensive experiments on four challenging benchmark datasets with variable number of cameras well demonstrate the efficacy of our proposed approach over state-of-the-art methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2007.11149,10.1109/cvpr42600.2020.01216,https://arxiv.org/pdf/2007.11149.pdf
52450161beabec336a3fe2ad13958573c5f654c0,0,1,0,Data Enhancement for Plant Disease Classification Using Generated Lesions,"Deep learning has recently shown promising results in plant lesion recognition. However, a deep learning network requires a large amount of data for training, but because some plant lesion data is difficult to obtain and very similar in structure, we must generate complete plant lesion leaf images to augment the dataset. To solve this problem, this paper proposes a method to generate complete and scarce plant lesion leaf images to improve the recognition accuracy of the classification network. The advantages of our study include: (i) proposing a binary generator network to solve the problem of how a generative adversarial network (GAN) generates a lesion image with a specific shape and (ii) using the edge-smoothing and image pyramid algorithm to solve the problem that occurs when synthesizing a complete lesion leaf image where the synthetic edge pixels are different and the network output size is fixed but the real lesion size is random. Compared with the recognition accuracy of human experts and AlexNet, it was shown that our method can effectively expand the plant lesion dataset and improve the recognition accuracy of a classification network.",2020,,,10.3390/app10020466,https://pdfs.semanticscholar.org/c77f/c9d257a145f1f4db41520f5b18354040af12.pdf
5247bcf4d7c80cd854c44f30f6976223d09fd44b,1,1,0,Color-Sensitive Person Re-Identification,"Recent deep Re-ID models mainly focus on learning high-level semantic features, while failing to explicitly explore color information which is one of the most important cues for person Re-ID model. In this paper, we propose a novel Color-Sensitive Re-ID to take full advantage of color information. On one hand, we train our model with real and fake images. By using the extra fake images, more color information can be exploited and it can avoid over-fitting during training. On the other hand, we also train our model with images of the same person with different colors. By doing so, features can be forced to focus on the color difference in regions. To generate fake images with specified colors, we propose a novel Color Translation GAN (CTGAN) to learn mappings between different clothing colors and preserve identity consistency among the same person with the clothing color. Extensive evaluations on two benchmark datasets show that our approach significantly outperforms state-of-the-art Re-ID models.",2019,IJCAI,,10.24963/ijcai.2019/131,https://pdfs.semanticscholar.org/738e/a9b470627d1d6ad5f7bd1bd20698662d79f7.pdf
5258df049df03bdd38f01609fa351f1bd9f3e493,0,1,0,Augmented Hard Example Mining for Generalizable Person Re-Identification,"Although the performance of person re-identification (Re-ID) has been much improved by using sophisticated training methods and large-scale labelled datasets, many existing methods make the impractical assumption that information of a target domain can be utilized during training. In practice, a Re-ID system often starts running as soon as it is deployed, hence training with data from a target domain is unrealistic. To make Re-ID systems more practical, methods have been proposed that achieve high performance without information of a target domain. However, they need cumbersome tuning for training and unusual operations for testing. In this paper, we propose augmented hard example mining, which can be easily integrated to a common Re-ID training process and can utilize sophisticated models without any network modification. The method discovers hard examples on the basis of classification probabilities, and to make the examples harder, various types of augmentation are applied to the examples. Among those examples, excessively augmented ones are eliminated by a classification based selection process. Extensive analysis shows that our method successfully selects effective examples and achieves state-of-the-art performance on publicly available benchmark datasets.",2019,ArXiv,1910.0528,,https://arxiv.org/pdf/1910.05280.pdf
5266b3be002cebb72267d6477deeaab22515cb74,0,1,0,CDPM: Convolutional Deformable Part Models for Semantically Aligned Person Re-Identification,"Part-level representations are essential for robust person re-identification. However, common errors that arise during pedestrian detection frequently result in severe misalignment problems for body parts, which degrade the quality of part representations. Accordingly, to deal with this problem, we propose a novel model named Convolutional Deformable Part Models (CDPM). CDPM works by decoupling the complex part alignment procedure into two easier steps: first, a vertical alignment step detects each body part in the vertical direction, with the help of a multi-task learning model; second, a horizontal refinement step based on attention suppresses the background information around each detected body part. Since these two steps are performed orthogonally and sequentially, the difficulty of part alignment is significantly reduced. In the testing stage, CDPM is able to accurately align flexible body parts without any need for outside information. Extensive experimental results demonstrate the effectiveness of the proposed CDPM for part alignment. Most impressively, CDPM achieves state-of-the-art performance on three large-scale datasets: Market-1501, DukeMTMC-ReID, and CUHK03.",2020,IEEE Transactions on Image Processing,1906.04976,10.1109/TIP.2019.2959923,https://arxiv.org/pdf/1906.04976.pdf
527523ccb136d4ffc4f31a362a629e913655a687,0,1,0,A Survey on Multi-source Person Re-identification,"Person re-identification (Re-ID) has been a popular and well-investigated topic in computer vision community. However, current researches have a relatively ideal assumption that person images are captured under a sufficient light condition and with high-resolution. Although most researches can achieve very exciting performances, they are not suitable for practical applications. Since practical conditions are a little complicated, and there are multiple sources to represent persons' appearance. In this paper, we focus on the multi-source person Re-ID, which refers to the problem of using multiple sources of data for person re-identification. Compared with general person Re-ID methods, multi-source person Re-ID researches are more practical, yet more challenging in reality. We need to face challenges caused by domain gap among different data sources, such as low-resolution images, infrared images, depth images, text information and sketch images. In this paper, we start with a brief introduction of general person Re-ID. The differences between general and multi-source person Re-ID are then compared. Five types of multi-source person Re-ID are further analyzed and summarized. From these discussions, it will become evident that several advantages exist in multi-source person Re-ID over general person Re-ID methods, as the former can make full use of data sources to learn cross-modality feature transformation. Finally, the future trends of multi-source person Re-ID are discussed.",2020,,,,http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2020/9/PDF/AAS-CN-2019-0278.pdf
52d3bb5cbd4c96ac56075aa0408a0fe66f8a13aa,1,0,0,Person Re-Identification of Cross-Domain Based on Costume Characteristics,"Cross-domain person re-identification (ReID) task has always been a research hotspot. The domain transfer technique based on deep learning alleviates the problem caused by the gap among data distributions. However, the generalization of such kinds of person ReID models are not robust. This paper proposes a novel ReID method to optimize the robustness from the perspective of persons' costume. It can extract hierarchical knowledge features from costume information and apply the feature model to the task of ReID. Specifically, the method first constructs hierarchical labels according to the attribute of costume data. It builds the knowledge model by using the multi-task learning, and performs similarity measurement on multiple ReID datasets. The experimental results show that the proposed method outperforms the latest ReID methods. Therefore, it provides a novel technique for practical applications.",2019,,,10.1145/3349341.3349433,
534a1c607bf3a8412f9986a724c3bbbf783342bd,0,1,0,Re-identification = Retrieval + Verification: Back to Essence and Forward with a New Metric,"Re-identification (re-ID) is currently investigated as a closed-world image retrieval task, and evaluated by retrieval based metrics. The algorithms return ranking lists to users, but cannot tell which images are the true target. In essence, current re-ID overemphasizes the importance of retrieval but underemphasizes that of verification, i.e., all returned images are considered as the target. On the other hand, re-ID should also include the scenario that the query identity does not appear in the gallery. To this end, we go back to the essence of re-ID, i.e., a combination of retrieval and verification in an open-set setting, and put forward a new metric, namely, Genuine Open-set re-ID Metric (GOM). GOM explicitly balances the effect of performing retrieval and verification into a single unified metric. It can also be decomposed into a family of sub-metrics, enabling a clear analysis of re-ID performance. We evaluate the effectiveness of GOM on the re-ID benchmarks, showing its ability to capture important aspects of re-ID performance that have not been taken into account by established metrics so far. Furthermore, we show GOM scores excellent in aligning with human visual evaluation of re-ID performance. Related codes are available at https://github.com/ YuanXinCherry/Person-reID-Evaluation.",2020,ArXiv,2011.11506,,https://arxiv.org/pdf/2011.11506.pdf
5351feb51da379af28164fbaa93fad7fd9b8d5b7,1,1,0,Cross-Entropy Adversarial View Adaptation for Person Re-Identification,"Person re-identification (re-ID) is a task of matching pedestrians under disjoint camera views. To recognize paired snapshots, it has to cope with large cross-view variations caused by the camera view shift. The supervised deep neural networks are effective in producing a set of non-linear projections that can transform cross-view images into a common feature space. However, they typically impose a symmetric architecture, leaving the network ill-conditioned on its optimization. In this paper, we learn view-invariant subspace for person re-ID, and its corresponding similarity metric using an adversarial view adaptation approach. The main contribution is to learn coupled asymmetric mappings regarding view characteristics which are adversarially trained to address the view discrepancy by optimizing the cross-entropy view confusion objective. To determine the similarity value, the network is empowered with a similarity discriminator to promote features that are highly discriminant in distinguishing positive and negative pairs. The other contribution includes an adaptive weighing on the most difficult samples to address the imbalance of within-/between-identity pairs. Our approach achieves notably improved performance in comparison with the state-of-the-arts on benchmark datasets.",2020,IEEE Transactions on Circuits and Systems for Video Technology,1904.01755,10.1109/TCSVT.2019.2909549,https://arxiv.org/pdf/1904.01755.pdf
535655e26faafc09341e878e636313edcad1a225,1,1,1,Domain-Camera Adaptation for Unsupervised Person Re-Identification,"Although supervised person re-identification (Re-ID) performance has been significantly improved in recent years, it is still a challenge for unsupervised person Re-Iddue to its absence of labels across disjoint camera views. On the other hand, Re-Idmodels trained on source domain usually offer poor performance when they are tested on target domain due to inter-domain bias e.g. different classes and intra-domain difference e.g camera variance. To overcome this problem, given a labeled source training domain and an unlabeled target training domain, we propose an unsupervised transfer method, Domain-Camera Adaptation model, to generate a pseudo target domain by bridging inter-domain bias and intra-domain difference. The idea is to fill the absence of labels in target domain by transferring labeled images of source domain to target domain across cameras. Then we propose a cross-domain classification loss to extract discriminative representation across domains. The intuition is to think of unsupervised learning as semi-supervised learning in target domain. We evaluate our deep model on Market-1501 and DukeMTMC-reID and the results show our model outperforms the state-of-art unsupervised Re-ID methods by large margins.",2019,"2019 6th International Conference on Behavioral, Economic and Socio-Cultural Computing (BESC)",,10.1109/BESC48373.2019.8963072,
536210d839dd439b14637354e23aabd7689afae0,0,1,0,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-identification,"We address the problem of person re-identification from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our first contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances re-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.",2018,ECCV,,10.1007/978-3-030-01228-1_44,
53829cd71916382dd9dc8cbf1ec369ce2bd7f0a4,0,1,0,A New Discriminative Feature Learning for Person Re-Identification Using Additive Angular Margin Softmax Loss,"In this paper, a new end-to-end framework is proposed for person re-identification (re-ID) by combining metric learning and classification. In this new framework, the Additive Angular Margin Softmax is used which imposes an additive angular margin constraint to the target logit on hypersphere manifold. This is aimed to improve the similarity of the intra-class features and the dissimilarity of the inter-class features simultaneously. Compard with the three popular used softmax-based-loss methods, the experiments show that the proposed approach has achieved improved performance on Market1501 and DukeMTMC-reID datasets for person re-ID.",2019,2019 UK/ China Emerging Technologies (UCET),,10.1109/UCET.2019.8881838,
5386c181d0b294d54123f001678d1125ca7b76dd,0,1,0,Multi-pseudo Regularized Label for Generated Samples in Person Re-Identification,"Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for labelling large number of images, the amount of available training data is always limited. To produce more data for training a deep network, Generative Adversarial Network (GAN) can be used to generate artificial sample data. However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this work is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48% improvements in rank-1 accuracy over a strong CNN baseline on the five datasets respectively, and outperforms state-of-the-art methods.",2018,ArXiv,,,
539a21c34669e68336ef7eaaf8840688c52b7d4b,0,0,1,Few-Shot Deep Adversarial Learning for Video-Based Person Re-Identification,"Video-based person re-identification (re-ID) refers to matching people across camera views from arbitrary unaligned video footages. Existing methods rely on supervision signals to optimise a projected space under which the distances between inter/intra-videos are maximised/minimised. However, this demands exhaustively labelling people across camera views, rendering them unable to be scaled in large networked cameras. Also, it is noticed that learning effective video representations with view invariance is not explicitly addressed for which features exhibit different distributions otherwise. Thus, matching videos for person re-ID demands flexible models to capture the dynamics in time-series observations and learn view-invariant representations with access to limited labeled training samples. In this paper, we propose a novel few-shot deep learning approach to video-based person re-ID, to learn comparable representations that are discriminative and view-invariant. The proposed method is developed on the variational recurrent neural networks (VRNNs) and trained adversarially to produce latent variables with temporal dependencies that are highly discriminative yet view-invariant in matching persons. Through extensive experiments conducted on three benchmark datasets, we empirically show the capability of our method in creating view-invariant temporal features and state-of-the-art performance achieved by our method.",2020,IEEE Transactions on Image Processing,1903.12395,10.1109/TIP.2019.2940684,https://arxiv.org/pdf/1903.12395.pdf
545e3f4e684f82eaf211bde9558d00a0dad946d2,1,1,0,Pyramidal Person Re-IDentification via Multi-Loss Dynamic Training,"Most existing Re-IDentification (Re-ID) methods are highly dependent on precise bounding boxes that enable images to be aligned with each other. However, due to the challenging practical scenarios, current detection models often produce inaccurate bounding boxes, which inevitably degenerate the performance of existing Re-ID algorithms. In this paper, we propose a novel coarse-to-fine pyramid model to relax the need of bounding boxes, which not only incorporates local and global information, but also integrates the gradual cues between them. The pyramid model is able to match at different scales and then search for the correct image of the same identity, even when the image pairs are not aligned. In addition, in order to learn discriminative identity representation, we explore a dynamic training scheme to seamlessly unify two losses and extract appropriate shared information between them. Experimental results clearly demonstrate that the proposed method achieves the state-of-the-art results on three datasets. Especially, our approach exceeds the current best method by 9.5% on the most challenging CUHK03 dataset.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00871,
546703ba427c3bade6b06526ea578a97a7970ee1,1,1,0,Double-Domain Imaging and Adaption for Person Re-Identification,"Person re-identification (re-ID) performance has been significantly boosted in recent works, but re-ID model trained on one dataset usually cannot work effectively on another one. To address this problem, we proposed a novel framework—double-domain translation generative adversarial network (DTGAN) that can train images between two domains and generalize the model trained on one domain well to another domain. We divide this paper into two steps. First, the source images are translated in an unsupervised manner, and the translated images retain the style of target images and the ID labels in the source domain. Second, the translated images are used as the training data for supervised feature learning. Besides, with the purpose of moderating the influence of noise, we employ the strategy of label smoothing regularization (LSR). In our experiments, we observe that the images generated by the DTGAN are of high quality and more appropriate for domain adaption. In addition, the re-ID accuracy of the DTGAN is competitive to the state-of-the-art methods on Market-1501 and DukeMTMC-reID.",2019,IEEE Access,,10.1109/ACCESS.2019.2930865,
54a0f303fba2b79868bfbf2941b23d970ab5ce0a,0,1,0,Generating person images based on attributes,"Attribute based person image generation is a problem which considers generating realistic person images using attributes like pose, gender, clothes, whether a bag is present or not etc. and it has wide variety of applications on computer vision. Realization of that generation process is quite difficult due to several reasons such as foreground/background, partial occlusion, stance of a person, camera angle and distance, complex relationships between attributes, unbalanced and poor quality data etc. Synthetic images are generated in related works which have worked on relatively easier datasets using less attributes with more complex models for more specific purposes. In this work, a more controversial goal was set up and a model named DCGAN-C is developed based on conditional generative adversarial networks and it can produce sythetic person images with both multi-class and multi-labels. Consequently, both quantitative and qualitative experiments were performed on the PA-100K dataset and the performance of the model was demonstrated.",2018,2018 26th Signal Processing and Communications Applications Conference (SIU),,10.1109/SIU.2018.8404426,
54c28bf64debbdb21c246795182f97d4f7917b74,1,0,1,STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification,"In this work, we propose a novel Spatial-Temporal Attention (STA) approach to tackle the large-scale person reidentification task in videos. Different from the most existing methods, which simply compute representations of video clips using frame-level aggregation (e.g. average pooling), the proposed STA adopts a more effective way for producing robust clip-level feature representation. Concretely, our STA fully exploits those discriminative parts of one target person in both spatial and temporal dimensions, which results in a 2-D attention score matrix via inter-frame regularization to measure the importances of spatial parts across different frames. Thus, a more robust clip-level feature representation can be generated according to a weighted sum operation guided by the mined 2-D attention score matrix. In this way, the challenging cases for video-based person re-identification such as pose variation and partial occlusion can be well tackled by the STA. We conduct extensive experiments on two large-scale benchmarks, i.e. MARS and DukeMTMCVideoReID. In particular, the mAP reaches 87.7% on MARS, which significantly outperforms the state-of-the-arts with a large margin of more than 11.6%.",2019,AAAI,1811.04129,10.1609/aaai.v33i01.33018287,https://arxiv.org/pdf/1811.04129.pdf
54d3e211a5c3137ba359731af43d22429608dada,1,0,0,The 4th AI City Challenge,"The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leverage city-scale real traffic data and high-quality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The general leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2004.14619,10.1109/CVPRW50498.2020.00321,https://arxiv.org/pdf/2004.14619.pdf
54d78ad2ed30557474fabd1d3a9e5db1c76fbeaa,1,0,0,Deep Person Re-identification for Probabilistic Data Association in Multiple Pedestrian Tracking,"We present a data association method for vision-based multiple pedestrian tracking, using deep convolutional features to distinguish between different people based on their appearances. These re-identification (re-ID) features are learned such that they are invariant to transformations such as rotation, translation, and changes in the background, allowing consistent identification of a pedestrian moving through a scene. We incorporate re-ID features into a general data association likelihood model for multiple person tracking, experimentally validate this model by using it to perform tracking in two evaluation video sequences, and examine the performance improvements gained as compared to several baseline approaches. Our results demonstrate that using deep person re-ID for data association greatly improves tracking robustness to challenges such as occlusions and path crossings.",2018,ArXiv,1810.08565,,https://arxiv.org/pdf/1810.08565.pdf
54e0f75bb609fb9745d199891573efba3fcef3a3,0,1,0,Data Augmentation for Enhancing EEG-based Emotion Recognition with Deep Generative Models,"OBJECTIVE The data scarcity problem in emotion recognition from electroencephalography (EEG) leads to difficulty in building an affective model with high accuracy using machine learning algorithms or deep neural networks. Inspired by emerging deep generative models, we propose three methods for augmenting EEG training data to enhance the performance of emotion recognition models.   APPROACH Our proposed methods are based on two deep generative models, variational autoencoder (VAE) and generative adversarial network (GAN), and two data augmentation ways, full and partial usage strategies. For the full usage strategy, all of the generated data are augmented to the training dataset without judging the quality of the generated data, while for the partial usage, only high-quality data are selected and appended to the training dataset. These three methods are called conditional Wasserstein GAN (cWGAN), selective VAE (sVAE), and selective WGAN (sWGAN).   MAIN RESULTS To evaluate the effectiveness of these proposed methods, we perform a systematic experimental study on two public EEG datasets for emotion recognition, namely, SEED and DEAP. We first generate realistic-like EEG training data in two forms: power spectral density and differential entropy. Then, we augment the original training datasets with a different number of generated realistic-like EEG data. Finally, we train support vector machines and deep neural networks with shortcut layers to build affective models using the original and augmented training datasets. The experimental results demonstrate that our proposed data augmentation methods based on generative models outperform the existing data augmentation approaches such as conditional VAE, Gaussian noise, and rotational data augmentation. We also observe that the number of generated data should be less than 10 times of the original training dataset to achieve the best performance.   SIGNIFICANCE The augmented training datasets produced by our proposed sWGAN method significantly enhance the performance of EEG-based emotion recognition models.",2020,Journal of neural engineering,2006.05331,10.1088/1741-2552/abb580,https://arxiv.org/pdf/2006.05331.pdf
55315bcd0a391fc7339e01011bde7b9c27efca79,1,0,0,Pedestrian Motion Model Using Non-Parametric Trajectory Clustering and Discrete Transition Points,"This letter presents a pedestrian motion model that includes both low level trajectory patterns, and high level discrete transitions. The inclusion of both levels creates a more general predictive model, allowing for more meaningful prediction and reasoning about pedestrian trajectories, as compared to the current state of the art. The model uses an iterative clustering algorithm with Dirichlet Process Gaussian Processes to cluster trajectories into continuous motion patterns and hypothesis testing to identify discrete transitions in the data called transition points. The model iteratively splits full trajectories into sub-trajectory clusters based on transition points, where pedestrians make discrete decisions. State transition probabilities are then learned over the transition points and trajectory clusters. The model is for online prediction of motions, and detection of anomalous trajectories. The proposed model is validated on the Duke Multi-Target, Multi-Camera Tracking Project (Duke MTMC) dataset to demonstrate identification of low level trajectory clusters and high level transitions, and the ability to predict pedestrian motion and detect anomalies online with high accuracy.",2019,IEEE Robotics and Automation Letters,2001.10571,10.1109/LRA.2019.2898464,https://arxiv.org/pdf/2001.10571.pdf
55355b0317f6e0c5218887441de71f05da4b42f6,1,1,0,Parameter-Free Spatial Attention Network for Person Re-Identification,"Global average pooling (GAP) allows to localize discriminative information for recognition [40]. While GAP helps the convolution neural network to attend to the most discriminative features of an object, it may suffer if that information is missing e.g. due to camera viewpoint changes. To circumvent this issue, we argue that it is advantageous to attend to the global configuration of the object by modeling spatial relations among high-level features. We propose a novel architecture for Person Re-Identification, based on a novel parameter-free spatial attention layer introducing spatial relations among the feature map activations back to the model. Our spatial attention layer consistently improves the performance over the model without it. Results on four benchmarks demonstrate a superiority of our model over the state-of-the-art achieving rank-1 accuracy of 94.7% on Market-1501, 89.0% on DukeMTMC-ReID, 74.9% on CUHK03-labeled and 69.7% on CUHK03-detected.",2018,ArXiv,1811.1215,,https://arxiv.org/pdf/1811.12150.pdf
553d7b7f8eff287ad860129385ce420e47905b84,0,1,0,Person Re-Identification with a Body Orientation-Specific Convolutional Neural Network,"Person re-identification consists in matching images of a particular person captured in a network of cameras with non-overlapping fields of view. The challenges in this task arise from the large variations of human appearance. In particular, the same person could show very different appearances from different points of view. To address this challenge, in this paper we propose an Orientation-Specific Convolutional Neural Network (OSCNN) framework which jointly performs body orientation regression and extracts orientation-specific deep representations for person re-identification. A robust joint embedding is obtained by combining feature representations under different body orientations. We experimentally show on two public benchmarks that taking into account body orientations improves the person re-identification performance. Moreover, our approach outperforms most of the previous state-of-the-art re-identification methods on these benchmarks.",2018,ACIVS,,10.1007/978-3-030-01449-0_3,https://hal.archives-ouvertes.fr/hal-01895374/file/ACIVS_OSCNN.pdf
5555a3289bf089cbeb551258b53a55458a6c6c25,1,1,0,SafeNet: Scale-normalization and Anchor-based Feature Extraction Network for Person Re-identification,"Person Re-identification (ReID) is a challenging retrieval task that requires matching a person’s image across non-overlapping camera views. The quality of fulfilling this task is largely determined on the robustness of the features that are used to describe the person. In this paper, we show the advantage of jointly utilizing multi-scale abstract information to learn powerful features over full body and parts. A scale normalization module is proposed to balance different scales through residual-based integration. To exploit the information hidden in non-rigid body parts, we propose an anchor-based method to capture the local contents by stacking convolutions of kernels with various aspect ratios, which focus on different spatial distributions. Finally, a well-defined framework is constructed for simultaneously learning the representations of both full body and parts. Extensive experiments conducted on current challenging large-scale person ReID datasets, including Market1501, CUHK03 and DukeMTMC, demonstrate that our proposed method achieves the state-of-the-art results.",2018,IJCAI,,10.24963/ijcai.2018/156,https://pdfs.semanticscholar.org/5555/a3289bf089cbeb551258b53a55458a6c6c25.pdf
556cd5b25fa8ff25513352e9016e095428f4db44,1,0,0,City-Scale Multi-Camera Vehicle Tracking by Semantic Attribute Parsing and Cross-Camera Tracklet Matching,"This paper focuses on the Multi-Target Multi-Camera Tracking (MTMCT) task in a city-scale multi-camera network. As the trajectory of each target is naturally split into multiple sub-trajectories (namely local tracklets) in different cameras, the key issue of MTMCT is how to match local tracklets belonging to the same target across different cameras. To this end, we propose an efficient two-step MTMCT approach to robustly track vehicles in a camera network. It first generates all local tracklets and then matches the ones belonging to the same target across different cameras. More specifically, in the local tracklet generation phase, we follow the tracking-by-detection paradigm and link the detections to local tracklets by graph clustering. In the cross-camera tracklet matching phase, we first develop a spatial-temporal attention mechanism to produce robust tracklet representations. We then prune false matching candidates by traffic topology reasoning and match tracklets across cameras using the recently proposed TRACklet-to-Target Assignment (TRACTA) algorithm. The proposed method is evaluated on the City-Scale Multi-Camera Vehicle Tracking task at the 2020 AI City Challenge and achieves the second-best results.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00296,
557ba3d8f7bba0bbf00cf6363f3263484b541d6f,1,1,0,A Strong Baseline and Batch Normalization Neck for Deep Person Re-Identification,"This study proposes a simple but strong baseline for deep person re-identification (ReID). Deep person ReID has achieved great progress and high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global- and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods. Our codes and models are available at: https://github.com/michuanhaohao/reid-strong-baseline",2020,IEEE Transactions on Multimedia,1906.08332,10.1109/TMM.2019.2958756,https://arxiv.org/pdf/1906.08332.pdf
55bea5a55277ee3310fbd96bded8dc8012b2656c,1,0,0,Attention-aware scoring learning for person re-identification,"Abstract Person re-identification (re-ID) refers to matching people across multiple camera views at different times and locations. The challenge is mainly about the huge variance of visual appearance of a specific pedestrian owing to pose variations, illumination changes and various camera-styles. In this paper, an Attention-Aware Scoring Learning (AASL) framework is proposed to address these issues. The proposed AASL framework consists of two attention modules and a score learning head. Specifically, the two modules, Spatial Attention Grid and Channel Attention Grid, embedded respectively in the shallow and deep layer in the convolutional neural network, are put forward to help the network learn the most discriminative visual features. Furthermore, an adaptive module termed score learning head is proposed to optimize the parameters of the attention modules. The present paper carries out extensive experiments on three large-scale datasets, including Market-1501, DukeMTMC-reID and CUHK03, after which it is found that our Attention-Aware Scoring Learning framework significantly outperforms the baseline model and achieves a competitive performance compared with the state-of-the-art person re-ID methods.",2020,Knowl. Based Syst.,,10.1016/j.knosys.2020.106154,
55dab895fcbb0b8af60488c7c4ac177db359b60e,1,0,0,SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking,"We present a novel self quality evaluation metric SQE for parameters optimization in the challenging yet critical multi-object tracking task. Current evaluation metrics all require annotated ground truth, thus will fail in the test environment and realistic circumstances prohibiting further optimization after training. By contrast, our metric reflects the internal characteristics of trajectory hypotheses and measures tracking performance without ground truth. We demonstrate that trajectories with different qualities exhibit different single or multiple peaks over feature distance distribution, inspiring us to design a simple yet effective method to assess the quality of trajectories using a two-class Gaussian mixture model. Experiments mainly on MOT16 Challenge data sets verify the effectiveness of our method in both correlating with existing metrics and enabling parameters self-optimization to achieve better performance. We believe that our conclusions and method are inspiring for future multi-object tracking in practice.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.07472,10.1109/cvpr42600.2020.00833,https://arxiv.org/pdf/2004.07472.pdf
55e615bbbb5ed7af3cc785f3dcf751946e0303c2,0,1,0,Improving Skin Lesion Segmentation with Generative Adversarial Networks,"This paper proposes a novel strategy that employs Generative Adversarial Networks (GANs) to augment data in the image segmentation field, and a Convolutional-Deconvolutional Neural Network (CDNN) to automatically generate lesion segmentation mask from dermoscopic images. Training the CDNN with our GAN generated data effectively improves the state-of-the-art.",2018,2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS),,10.1109/CBMS.2018.00086,https://iris.unimore.it/bitstream/11380/1161448/1/CBMS_2018_paper_177.pdf
55ffaaf51dbeefae6137b97eb1856c15192c7d2e,1,0,0,Attention: A Big Surprise for Cross-Domain Person Re-Identification,"In this paper, we focus on model generalization and adaptation for cross-domain person re-identification (Re-ID). Unlike existing cross-domain Re-ID methods, leveraging the auxiliary information of those unlabeled target-domain data, we aim at enhancing the model generalization and adaptation by discriminative feature learning, and directly exploiting a pre-trained model to new domains (datasets) without any utilization of the information from target domains. To address the discriminative feature learning problem, we surprisingly find that simply introducing the attention mechanism to adaptively extract the person features for every domain is of great effectiveness. We adopt two popular type of attention mechanisms, long-range dependency based attention and direct generation based attention. Both of them can perform the attention via spatial or channel dimensions alone, even the combination of spatial and channel dimensions. The outline of different attentions are well illustrated. Moreover, we also incorporate the attention results into the final output of model through skip-connection to improve the features with both high and middle level semantic visual information. In the manner of directly exploiting a pre-trained model to new domains, the attention incorporation method truly could enhance the model generalization and adaptation to perform the cross-domain person Re-ID. We conduct extensive experiments between three large datasets, Market-1501, DukeMTMC-reID and MSMT17. Surprisingly, introducing only attention can achieve state-of-the-art performance, even much better than those cross-domain Re-ID methods utilizing auxiliary information from the target domain.",2019,ArXiv,1905.1283,,https://arxiv.org/pdf/1905.12830.pdf
5634576f5cbac41f0329e8d6161f9bfdff6f7de9,0,0,1,Learning adaptively from the unknown for few-example video person re-ID,"This paper mainly studies one-example and few-example video person re-identification. A multi-branch network PAM that jointly learns local and global features is proposed. PAM has high accuracy, few parameters and converges fast, which is suitable for few-example person re-identification. We iteratively estimates labels for unlabeled samples, incorporates them into training sets, and trains a more robust network. We propose the static relative distance sampling(SRD) strategy based on the relative distance between classes. For the problem that SRD can not use all unlabeled samples, we propose adaptive relative distance sampling (ARD) strategy. For one-example setting, We get 89.78\%, 56.13\% rank-1 accuracy on PRID2011 and iLIDS-VID respectively, and 85.16\%, 45.36\% mAP on DukeMTMC and MARS respectively, which exceeds the previous methods by large margin.",2019,ArXiv,1908.0934,,https://arxiv.org/pdf/1908.09340.pdf
5639de0355d61075388f1400cdf3c5f7d598bfbf,1,0,0,Rapid Pedestrian Detection Based on Deep Omega-Shape Features with Partial Occlusion Handing,"Region-based Fully ConvNet (R-FCN) designed for general object detection is difficult to be directly applied for pedestrian detection, due to being with large human pose and scale changes, and even with partial occlusion in surveillance scenarios. This paper presents a rapid pedestrian detection method with partial occlusion handling, which builds on the framework of R-FCN. We introduce a deep Omega-shape feature learning and multi-paths detection to make our detector be robust to human pose and scale changes. A novel predicted boxes fusion strategy is proposed to reduce the number of false negatives caused by partial occlusion in crowded environment. Our end-to-end approach achieved 95.35% mAP on the Caltech dataset, 96.22% mAP on DukeMTMC dataset and 97.43% mAP on Bronze dataset at a test-time speed of approximate 86 ms per image.",2018,Neural Processing Letters,,10.1007/s11063-018-9837-1,
56423685e039d82d3cc88f797fc2b73f2d93e200,1,1,0,A Unified Generative Adversarial Framework for Image Generation and Person Re-identification,"Person re-identification (re-id) aims to match a certain person across multiple non-overlapping cameras. It is a challenging task because the same person's appearance can be very different across camera views due to the presence of large pose variations. To overcome this issue, in this paper, we propose a novel unified person re-id framework by exploiting person poses and identities jointly for simultaneous person image synthesis under arbitrary poses and pose-invariant person re-identification. The framework is composed of a GAN based network and two Feature Extraction Networks (FEN), and enjoys following merits. First, it is a unified generative adversarial model for person image generation and person re-identification. Second, a pose estimator is utilized into the generator as a supervisor in the training process, which can effectively help pose transfer and guide the image generation with any desired pose. As a result, the proposed model can automatically generate a person image under an arbitrary pose. Third, the identity-sensitive representation is explicitly disentangled from pose variations through the person identity and pose embedding. Fourth, the learned re-id model can have better generalizability on a new person re-id dataset by using the synthesized images as auxiliary samples. Extensive experimental results on four standard benchmarks including Market-1501 [69], DukeMTMC-reID [40], CUHK03 [23], and CUHK01 [22] demonstrate that the proposed model can perform favorably against state-of-the-art methods.",2018,ACM Multimedia,,10.1145/3240508.3240573,http://www.jdl.link/doc/2011/201917_A%20Unified%20Generative%20Adversarial%20Framework%20for%20Image.pdf
5645f567d83bacf318c9b8c4adb5e508b713e466,1,1,0,Deep Sequential Multi-camera Feature Fusion for Person Re-identification,"Given a target image as query, person re-identification systems retrieve a ranked list of candidate matches from other camera field-of-views. Re-identification is typically performed independently for each gallery camera across the network. However, images of the same target from a subset of cameras often provide complementary appearance information which, if aggregated judiciously, can lead to better re-id performance in the remaining cameras. This motivates us to investigate the novel problem of multi-camera feature fusion for person re-id. We propose an intelligent sequential fusion technique, designed to not only improve re-id accuracy but to also learn increasingly better feature representations as observations from additional cameras are fused. The fusion function is modeled using a Gated Recurrent Unit (GRU) with modification on default GRU training regime to achieve the aforementioned improvements. Extensive experimentation validates that the proposed fusion scheme substantially boosts identification performance when combined with off-the-shelf feature extraction methods for re-id.",2018,ArXiv,,,
566a2ede36a6493010ea42a7df49916739e00c9d,0,1,0,Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis,"Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoderdecoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-theart results on large pose face recognition.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1704.04086,10.1109/ICCV.2017.267,https://arxiv.org/pdf/1704.04086.pdf
567ba1e44ec7c4107e670174eabd32eb487a4c05,1,0,0,MessyTable: Instance Association in Multiple Camera Views,"We present an interesting and challenging dataset that features a large number of scenes with messy tables captured from multiple camera views. Each scene in this dataset is highly complex, containing multiple object instances that could be identical, stacked and occluded by other instances. The key challenge is to associate all instances given the RGB image of all views. The seemingly simple task surprisingly fails many popular methods or heuristics that we assume good performance in object association. The dataset challenges existing methods in mining subtle appearance differences, reasoning based on contexts, and fusing appearance with geometric cues for establishing an association. We report interesting findings with some popular baselines, and discuss how this dataset could help inspire new problems and catalyse more robust formulations to tackle real-world instance association problems. Project page: $\href{this https URL}{\text{MessyTable}}$",2020,ArXiv,2007.14878,,https://arxiv.org/pdf/2007.14878.pdf
568635a4965ee42af44b3b84bb2fff27bf54acd1,0,1,0,SPL: Exploiting Unlabeled Data for Multi-label Image Classification,"The utilization of the unlabeled data provides a beneficial attempt for improving the generalization ability of the convolutional neural network (CNN) model, just as what is applied in person re-identification task. Different from that, multi-label image classification aims to predict multiple labels for each given image. The unlabeled data should be properly assigned multiple labels for regularizing the training process of CNN model. To make full use of the unlabeled data, this paper proposes a soft pseudo labeling (SPL) method for multi-label image classification. Specifically, the unlabeled samples are first generated by DCGAN and WGAN-GP. Then, the virtual multiple labels of the generated unlabeled samples are assigned based on an initial confidence value by SoftMax function. Finally, both the generated samples and original training samples are fed into the network as input, in order to learn a CNN model with stronger generalization ability. On three public multi-label image classification datasets (i.e., WIDER-Attribute, NUS-WIDE and MS-COCO), SPL provides a stable improvement over the baseline and produces a competitive performance compared with some existing multi-label image classification methods.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME.2019.00035,
5696e95cb9538fe5f5cc27cd47543b62761021b7,0,1,0,A loss combination based deep model for person re-identification,"The Convolutional Neural Network (CNN) has significantly improved the state-of-the-art in person re-identification (re-ID). In the existing available identification CNN model, the softmax loss function is employed as the supervision signal to train the CNN model. However, the softmax loss only encourages the separability of the learned deep features between different identities. The distinguishing intra-class variations have not been considered during the training process of CNN model. In order to minimize the intra-class variations and then improve the discriminative ability of CNN model, this paper combines a new supervision signal with original softmax loss for person re-ID. Specifically, during the training process, a center of deep features is learned for each pedestrian identity and the deep features are subtracted from the corresponding identity centers, simultaneously. So that, the deep features of the same identity to the center will be pulled efficiently. With the combination of loss functions, the inter-class dispersion and intra-class aggregation can be constrained as much as possible. In this way, a more discriminative CNN model, which has two key learning objectives, can be learned to extract deep features for person re-ID task. We evaluate our method in two identification CNN models (i.e., CaffeNet and ResNet-50). It is encouraging to see that our method has a stable improvement compared with the baseline and yields a competitive performance to the state-of-the-art person re-ID methods on three important person re-ID benchmarks (i.e., Market-1501, CUHK03 and MARS).",2017,Multimedia Tools and Applications,,10.1007/s11042-017-5009-y,
56e8d2e6170d90046b98f8d80694e5f3a2749346,0,1,0,Mask-Guided Region Attention Network for Person Re-Identification,"Person re-identification (ReID) is an important and practical task which identifies pedestrians across non-overlapping surveillance cameras based on their visual features. In general, ReID is an extremely challenging task due to complex background clutters, large pose variations and severe occlusions. To improve its performance, a robust and discriminative feature extraction methodology is particularly crucial. Recently, the feature alignment technique driven by human pose estimation, that is, matching two person images with their corresponding parts, increases the effectiveness of ReID to a certain extent. However, we argue that there are still a few problems among these methods such as imprecise handcrafted segmentation of body parts, and some improvements can be further achieved. In this paper, we present a novel framework called Mask-Guided Region Attention Network (MGRAN) for person ReID. MGRAN consists of two major components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). MRA aims to generate spatial attention masks and meanwhile mask out the background clutters and occlusions. Moreover, the generated masks are utilized for region-level feature alignment in the MA module. We then evaluate the proposed method on three public datasets, including Market-1501, DukeMTMC-reID and CUHK03. Extensive experiments with ablation analysis show the effectiveness of this method.",2020,PAKDD,,10.1007/978-3-030-47436-2_22,https://link.springer.com/content/pdf/10.1007%2F978-3-030-47436-2_22.pdf
576dd6fd076114b86c108f5364e7f0f2aed21ad7,0,1,0,Progressive Pose Attention Transfer for Person Image Generation,"This paper proposes a new generative adversarial network to the problem of pose transfer, i.e., transferring the pose of a given person to a target one. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.03349,10.1109/CVPR.2019.00245,https://arxiv.org/pdf/1904.03349.pdf
577ed1325eea7f47f03bb6c6cc3f67d299927f0d,0,1,0,Deep Learning Research With an Expectation-Maximization Model for Person Re-Identification,"In existing person re-identification methods based on deep learning, the extraction of good features is still a key step. Some efforts divide the image of a person into multiple parts to extract more detailed information from semantically coherent parts but ignore their correlation with each other. Others adopt self attention to reallocate weights of pixels for learning the association between different regions. This association can improve the accuracy of the person re-identification task, but the features obtained by this type of algorithm have high redundancy, which is not conducive to the expression of feature information. In order to address the above challenges, we propose a feature extraction method based on a novel attention mechanism which combines the expectation maximization (EM) algorithm and non-local operation. We embed the attention module into the ResNet50 backbone network. The attention module captures the correlation between different regional features through non-local operation and then reconstructs these features through the EM algorithm. In addition, we divide the network into a global branch and a local branch, where the global branch extracts the complete features, and the local branch uses the Batch DropBlock method to erase a portion of the features to achieve feature diversity. Finally, extensive experiments validate the superiority of the proposed model for person re-ID over a wide variety of state-of-the-art methods on three large-scale benchmarks, including DukeMTMC-ReID, Market-1501 and CUHK03.",2020,IEEE Access,,10.1109/ACCESS.2020.3019100,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09174970.pdf
583cfbc89a5ed2d3343ba4e5b08fbe1b2151b80a,0,1,0,End-To-End Chromosome Karyotyping with Data Augmentation Using GAN,"Classifying human chromosomes from input cell images, i.e., karyotyping, requires domain expertise and quantity of manual effort to perform. In this paper, we propose an end-to-end chromosome karyotyping method, which can automatically detect, segment and classify chromosomes from cell images. During detection, we explore Extremal Regions (ER) to obtain chromosome candidates in input images. During segmentation, we segment overlapping chromosome candidates by approximating chromosome shapes with eclipses. In classification, we first propose Multiple Distribution Generative Advertising Network (MD-GAN) to effectively cover diverse data modes and generate more labeled samples for data augmentation. Then, we finetune pre-trained convolutional neural network (CNN) to classify chromosomes with samples generated by MD-GAN. We demonstrate the accuracy of the proposed end-to-end method in detecting, segmenting and classifying by experiments on a self-collected dataset. Experiments also prove data augmentation with MD-GAN could improve classification performance of CNN.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451041,
5868eb2658f22a71860afb38b9e964600b30fbce,1,1,0,A Novel Method for Person Re-Identification: Conditional Translated Network Based on GANs,"The main challenge of person re-identification (re-id) lies in the strikingly discrepancy between different camera views, including illumination, background and human pose. Existing person re-id methods rely mostly on implicit solutions, such as seeking robust features or designing discriminative distance metrics. Compared to these methods, human solutions are more straightforward. That is, imagine the appearance of the target person under different camera views before matching target person. The key idea is that human can intuitively implement viewpoint transfer, noting the association of the target person under different camera views but the machine failed. In this paper, we attempt to imitate such human behavior that transfer person image to certain camera views before matching. In practice, we propose a conditional transfer network (cTransNet) that conditionally implement viewpoint transfer, which transfers image to the viewpoint with the biggest domain gap through a variant of Generative Adversarial Networks (GANs). After that, we obtain hybrid person representation by fusing the feature of original image with the transferred image then perform similarity ranking according to cosine distance. Compared with former methods, we propose a human-like approach and obtains consistent improvement of the rank-1 precision over the baseline in Market-1501, DukeMTMC-ReID and MSMT17 dataset by 3%,4%,4%, respectively.",2020,IEEE Access,,10.1109/ACCESS.2019.2962301,
587cf6001c8265dea75e42a204446fb51895e78e,1,0,0,Deep Generative Models and Applications,"Over the past few years, there have been fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. The amount of annotated data drastically increased and supervised deep discriminative models exceeded human-level performances in certain object detection tasks [Russakovsky et al., 2015, He et al., 2015]. The increasing availability in quantity and complexity of unlabelled data also opens up exciting possibilities for the development of unsupervised learning methods. Among the family of unsupervised methods, deep generative models find numerous applications. Moreover, as real-world applications include high dimensional data, the ability of generative models to automatically learn semantically meaningful subspaces makes their advancement an essential step toward developing more efficient algorithms. Generative Adversarial Networks (GANs) are a family of unsupervised generative algorithms that have demonstrated impressive performance for data synthesis and are now used in a wide range of computer vision tasks. Despite this success, they gained a reputation for being difficult to train, which results in a time-consuming and human-involved development process to use them. In the first part of this thesis, we focus on improving the stability and the performances of GANs. Foremost, we consider an alternative training process to the standard one, named SGAN, in which several adversarial “local” pairs of networks are trained independently so that a “global” supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well. Next, to further reduce the computational footprint while maintaining the stability and performance advantages of SGAN, we focus on training single pair of adversarial networks using variance reduced gradient. More precisely, we study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with two stochastic variance-reduced gradient and extragradient optimization algorithms for GANs, named SVRG-GAN and SVRE, respectively. As batch extragradient is the only method that converges for simple examples of games, our analyses focus on SVRE, which method for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method",2020,,,10.5075/EPFL-THESIS-10257,
58a06477685794544e34d03a9190ca20a1a8e505,1,1,1,Context-Interactive CNN for Person Re-Identification,"Despite growing progresses in recent years, cross-scenario person re-identification remains challenging, mainly due to the pedestrians commonly surrounded by highly-complex environment contexts. In reality, the human perception mechanism could adaptively find proper contextualized spatial-temporal clues towards pedestrian recognition. However, conventional methods fall short in adaptively leveraging the long-term spatial-temporal information due to ever-increasing computational cost. Moreover, CNN-based deep learning methods are hard to conduct optimization due to the non-differentiable property of the built-in context search operation. To ameliorate, this paper proposes a novel Context-Interactive CNN (CI-CNN) to dynamically find both spatial and temporal contexts by embedding multi-task Reinforcement Learning (MTRL). The CI-CNN streamlines the multi-task reinforcement learning by using an actor-critic agent to capture the temporal-spatial context simultaneously, which comprises a context-policy network and a context-critic network. The former network learns policies to determine the optimal spatial context region and temporal sequence range. Based on the inferred temporal-spatial cues, the latter one focuses on the identification task and provides feedback for the policy network. Thus, CI-CNN can simultaneously zoom in/out the perception field in spatial and temporal domain for the context interaction with the environment. By fostering the collaborative interaction between the person and context, our method could achieve outstanding performance on various public benchmarks, which confirms the rationality of our hypothesis, and verifies the effectiveness of our CI-CNN framework.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2953587,
58cf45b4c1bc070535c2f3c1004b2fdc6fe8c907,1,0,0,Tracking Without Bells and Whistles,"The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1903.05625,10.1109/ICCV.2019.00103,https://arxiv.org/pdf/1903.05625.pdf
58f508f6d123f9fc37ae9234d2939a939eed3cd9,1,0,0,Grad-Cam Guided Progressive Feature CutMix for Classification,"Image features from a small local region often give strong evidence in the classification task. However, CNN suffers from paying too much attention only on these local areas, thus ignoring other discriminative regions. This paper deals with this issue by performing the attentive feature cutmix in a progressive manner, among the multi-branch classifier trained on the same task. Specifically, we build the several sequential head branches, with the first global branch fed the original features without any constrains, and other following branches given the attentive cutmix features. The grad-CAM is employed to guide input features of them, so that discriminative region blocks in the current branch are intentionally cut and replaced by those from other images, hence preventing the model from relying on only the small regions and forcing it to gradually focus on large areas. Extensive experiments have been carried out on reID datasets such as the Market1501, DukeMTMC and CUHK03, showing that the proposed algorithm can boost the classification performance significantly.",2020,ArXiv,2007.08779,,https://arxiv.org/pdf/2007.08779.pdf
5914c5306b42a54476f1e74ad700c73b515bf8db,1,1,0,Multi-view Based Pose Alignment Method for Person Re-identification,"This paper proposes a Multi-View based Pose Alignment (MVPA) method for person re-identification (re-id). Most recent methods solve re-id as a matching process based on single image. However, when poses vary or viewpoints change, the performance seriously deteriorates. This paper aims to learn a representation insensitive to view and pose. Specifically, we establish a set of Multi-view based Person Pose Templates (MPPT) and propose a Pose-Guided Person image Generation (iPG2) model to synthesize multi-view and uniform-pose based images. The representation learned from multi-view images can significantly enhances the accuracy of re-id. We evaluate our method on two popular datasets, i.e., Market-1501 and DukeMTMC-reID. The results show that our framework promotes the performance of re-id a lot and surpass other methods.",2019,,,10.1007/978-981-32-9050-1_50,
592906dc3d4909adca9981fea78a01ef4b925964,0,1,0,Mobile person re-identification with a lightweight trident CNN,"Engineering Research Center of Hubei Province for Clothing Information, School of Mathematics and Computer Science, Wuhan Textile University, Wuhan 430200, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan 430072, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an 710068, China",2020,Science China Information Sciences,,10.1007/s11432-019-2782-3,http://scis.scichina.com/en/2020/219102-supplementary.pdf
59455e1752f7587a0d7aea053ecd81cc359e3faf,1,0,0,Robust Person Re-Identification by Modelling Feature Uncertainty,"We aim to learn deep person re-identification (ReID) models that are robust against noisy training data. Two types of noise are prevalent in practice: (1) label noise caused by human annotator errors and (2) data outliers caused by person detector errors or occlusion. Both types of noise pose serious problems for training ReID models, yet have been largely ignored so far. In this paper, we propose a novel deep network termed DistributionNet for robust ReID. Instead of representing each person image as a feature vector, DistributionNet models it as a Gaussian distribution with its variance representing the uncertainty of the extracted features. A carefully designed loss is formulated in DistributionNet to unevenly allocate uncertainty across training samples. Consequently, noisy samples are assigned large variance/uncertainty, which effectively alleviates their negative impacts on model fitting. Extensive experiments demonstrate that our model is more effective than alternative noise-robust deep models. The source code is available at: https://github.com/TianyuanYu/DistributionNet",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00064,https://www.pure.ed.ac.uk/ws/files/111974444/Robust_Person_Re_identification_YU_DoA220719_AFV.pdf
59a4cec1afb2804eeff1774c4eb315701443af76,0,1,0,Relation-Aware Global Attention,"Attention mechanism aims to increase the representation power by focusing on important features and suppressing unnecessary ones. For convolutional neural networks (CNNs), attention is typically learned with local convolutions, which ignores the global information and the hidden relation. How to efficiently exploit the long-range context to globally learn attention is underexplored. In this paper, we propose an effective Relation-Aware Global Attention (RGA) module for CNNs to fully exploit the global correlations to infer the attention. Specifically, when computing the attention at a feature position, in order to grasp information of global scope, we propose to stack the relations, i.e., its pairwise correlations/affinities with all the feature positions, and the feature itself together for learning the attention with convolutional operations. Given an intermediate feature map, we have validated the effectiveness of this design across both the spatial and channel dimensions. When applied to the task of person re-identification, our model achieves the state-of-the-art performance. Extensive ablation studies demonstrate that our RGA can significantly enhance the feature representation power. We further demonstrate the general applicability of RGA to vision tasks by applying it to the scene segmentation and image classification tasks resulting in consistent performance improvement.",2019,ArXiv,1904.02998,,https://arxiv.org/pdf/1904.02998.pdf
59f357015054bab43fb8cbfd3f3dbf17b1d1f881,1,0,0,Unsupervised Multi-Object Detection for Video Surveillance Using Memory-Based Recurrent Attention Networks,"Nowadays, video surveillance has become ubiquitous with the quick development of artificial intelligence. Multi-object detection (MOD) is a key step in video surveillance and has been widely studied for a long time. The majority of existing MOD algorithms follow the “divide and conquer” pipeline and utilize popular machine learning techniques to optimize algorithm parameters. However, this pipeline is usually suboptimal since it decomposes the MOD task into several sub-tasks and does not optimize them jointly. In addition, the frequently used supervised learning methods rely on the labeled data which are scarce and expensive to obtain. Thus, we propose an end-to-end Unsupervised Multi-Object Detection framework for video surveillance, where a neural model learns to detect objects from each video frame by minimizing the image reconstruction error. Moreover, we propose a Memory-Based Recurrent Attention Network to ease detection and training. The proposed model was evaluated on both synthetic and real datasets, exhibiting its potential.",2018,Symmetry,,10.3390/sym10090375,https://pdfs.semanticscholar.org/59f3/57015054bab43fb8cbfd3f3dbf17b1d1f881.pdf
59f8d07bba3967f2909d36fe12d03f8a515aa01a,1,1,0,An Introduction to Person Re-identification with Generative Adversarial Networks,"Person re-identification is a basic subject in the field of computer vision. The traditional methods have several limitations in solving the problems of person illumination like occlusion, pose variation and feature variation under complex background. Fortunately, deep learning paradigm opens new ways of the person re-identification research and becomes a hot spot in this field. Generative Adversarial Nets (GANs) in the past few years attracted lots of attention in solving these problems. This paper reviews the GAN based methods for person re-identification focuses on the related papers about different GAN based frameworks and discusses their advantages and disadvantages. Finally, it proposes the direction of future research, especially the prospect of person re-identification methods based on GANs.",2019,ArXiv,1904.05992,,https://arxiv.org/pdf/1904.05992.pdf
5a6c5b5d45fc4cbb4893cc67385517add187d910,1,0,1,Attribute-aware Identity-hard Triplet Loss for Video-based Person Re-identification,"Video-based person re-identification (Re-ID) is an important computer vision task. The batch-hard triplet loss frequently used in video-based person Re-ID suffers from the Distance Variance among Different Positives (DVDP) problem. In this paper, we address this issue by introducing a new metric learning method called Attribute-aware Identity-hard Triplet Loss (AITL), which reduces the intra-class variation among positive samples via calculating attribute distance. To achieve a complete model of video-based person Re-ID, a multi-task framework with Attribute-driven Spatio-Temporal Attention (ASTA) mechanism is also proposed. Extensive experiments on MARS and DukeMTMC-VID datasets shows that both the AITL and ASTA are very effective. Enhanced by them, even a simple light-weighted video-based person Re-ID baseline can outperform existing state-of-the-art approaches. The codes has been published on this https URL.",2020,ArXiv,2006.07597,,https://arxiv.org/pdf/2006.07597.pdf
5aa62121aeed6908bb90a4871a9dc7425477befb,1,0,0,Real-Time Online Multi-Object Tracking in Compressed Domain,"Recent online multi-object tracking (MOT) methods have achieved desirable tracking performance. However, the tracking speed of most existing methods is rather slow. Inspired from the fact that the adjacent frames are highly relevant and redundant, we divide the frames into key and non-key frames and track objects in the compressed domain. For the key frames, the RGB images are restored for detection and data association. To make data association more reliable, an appearance convolutional neural network (CNN) which can be jointly trained with the detector is proposed. For the non-key frames, the objects are directly propagated by a tracking CNN based on the motion information provided in the compressed domain. Compared with the state-of-the-art online MOT methods, our tracker is about $6\times $ faster while maintaining a comparable tracking performance.",2019,IEEE Access,,10.1109/ACCESS.2019.2921975,
5b062562a8067baae045df1c7f5a8455d0363b5a,1,0,0,SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and Partial Person Re-Identification,"Holistic person re-identification (ReID) has received extensive study in the past few years and achieves impressive progress. However, persons are often occluded by obstacles or other persons in practical scenarios, which makes partial person re-identification non-trivial. In this paper, we propose a spatial-channel parallelism network (SCPNet) in which each channel in the ReID feature pays attention to a given spatial part of the body. The spatial-channel corresponding relationship supervises the network to learn discriminative feature for both holistic and partial person re-identification. The single model trained on four holistic ReID datasets achieves competitive accuracy on these four datasets, as well as outperforms the state-of-the-art methods on two partial ReID datasets without training.",2018,ACCV,1810.06996,10.1007/978-3-030-20890-5_2,https://arxiv.org/pdf/1810.06996.pdf
5b499d14c732f8acbd1234d8c5e1cca695b0e1df,1,0,0,"Automated Individual Pig Localisation, Tracking and Behaviour Metric Extraction Using Deep Learning","Individual pig tracking is key to stepping away from group-level treatment and towards individual pig care. By doing so we can monitor individual pig behaviour changes over time and use these as indicators of health and well-being, which, in turn, will assist in the early detection of disease allowing for earlier and more effective intervention. However, it is a much more computationally challenging than performing this task at group level; mistakes in identification and tracking accumulate and, over time, provide noise measures. We combine a deep CNN object localisation method, Faster Region-based convolutional neural network (R-CNN), with two potential real-time multi-object tracking methods in order to create a complete system that can autonomously localise and track individual pigs allowing for the extraction of metrics pertaining to individual pig behaviours from RGB cameras. We evaluate two different transfer learning strategies to adapt Faster R-CNN to our pig detection dataset that is more challenging than conventional tracking benchmark datasets. We are able to localise pigs in individual frames with 0.901 mean average precision (mAP), which then allows us to track individual pigs across video footage with 92% Multi-Object Tracking Accuracy (MOTA) and 73.4% Identity F1-Score (IDF1), and re-identify them after occlusions and dropped frames with 0.862 mAP (0.788 Rank 1 cumulative matching characteristic (CMC)). From these tracks we extract individual behavioural metrics for total distance travelled, time spent idle, and average speed with less than 0.015 mean squared error (MSE) for each. Changes in all these behavioural metrics have value in the detection of pig health and wellbeing.",2019,IEEE Access,,10.1109/ACCESS.2019.2933060,
5b710064b70ab210556918becc24614d135f3646,0,1,0,Semi-Supervised Feature Learning for Off-Line Writer Identifications,"Conventional approaches used supervised learning to estimate off-line writer identifications. In this study, we improved the off-line writer identifica- tions by semi-supervised feature learning pipeline, which trained the extra unla- beled data and the original labeled data simultaneously. In specific, we proposed a weighted label smoothing regularization (WLSR) method, which assigned the weighted uniform label distribution to the extra unlabeled data. We regularized the convolutional neural network (CNN) baseline, which allows learning more discriminative features to represent the properties of different writing styles. Based on experiments on ICDAR2013, CVL and IAM benchmark datasets, our results showed that semi-supervised feature learning improved the baseline meas- urement and achieved better performance compared with existing writer identifications approaches.",2018,ArXiv,,,
5b7186cb31497c305ac029804642c699925aff61,1,0,1,Weakly Supervised Person Re-Identification,"In the conventional person re-id setting, it is assumed that the labeled images are the person images within the bounding box for each individual; this labeling across multiple nonoverlapping camera views from raw video surveillance is costly and time-consuming. To overcome this difficulty, we consider weakly supervised person re-id modeling. The weak setting refers to matching a target person with an untrimmed gallery video where we only know that the identity appears in the video without the requirement of annotating the identity in any frame of the video during the training procedure. Hence, for a video, there could be multiple video-level labels. We cast this weakly supervised person re-id challenge into a multi-instance multi-label learning (MIML) problem. In particular, we develop a Cross-View MIML (CV-MIML) method that is able to explore potential intraclass person images from all the camera views by incorporating the intra-bag alignment and the cross-view bag alignment. Finally, the CV-MIML method is embedded into an existing deep neural network for developing the Deep Cross-View MIML (Deep CV-MIML) model. We have performed extensive experiments to show the feasibility of the proposed weakly supervised setting and verify the effectiveness of our method compared to related methods on four weakly labeled datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.03832,10.1109/CVPR.2019.00085,https://arxiv.org/pdf/1904.03832.pdf
5b7686cbd9ac8c191b5b1e2acaf47667eb6b2ed9,0,1,0,PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised Cross-View Person Re-identification,"Person re-identification (person Re-Id) aims to retrieve the pedestrian images of a same person that captured by disjoint and non-overlapping cameras. Lots of researchers recently focuse on this hot issue and propose deep learning based methods to enhance the recognition rate in a supervised or unsupervised manner. However, two limitations that cannot be ignored: firstly, compared with other image retrieval benchmarks, the size of existing person Re-Id datasets are far from meeting the requirement, which cannot provide sufficient pedestrian samples for the training of deep model; secondly, the samples in existing datasets do not have sufficient human motions or postures coverage to provide more priori knowledges for learning. In this paper, we introduce a novel unsupervised pose augmentation cross-view person Re-Id scheme called PAC-GAN to overcome these limitations. We firstly present the formal definition of cross-view pose augmentation and then propose the framework of PAC-GAN that is a novel conditional generative adversarial network (CGAN) based approach to improve the performance of unsupervised corss-view person Re-Id. Specifically, The pose generation model in PAC-GAN called CPG-Net is to generate enough quantity of pose-rich samples from original image and skeleton samples. The pose augmentation dataset is produced by combining the synthesized pose-rich samples with the original samples, which is fed into the corss-view person Re-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the CPG-Net to improve the quality of new generated samples. To the best of our knowledge, we are the first try to enhance the unsupervised cross-view person Re-Id by pose augmentation, and the results of extensive experiments show that the proposed scheme can combat the state-of-the-arts.",2020,Neurocomputing,1906.01792,10.1016/j.neucom.2019.12.094,https://arxiv.org/pdf/1906.01792.pdf
5bab0828e5dfbc13d2c2e3f0532edbb002995e64,0,1,0,Vehicle Re-identification with the Space-Time Prior,"Vehicle re-identification (Re-ID) is fundamentally challenging due to the difficulties in data labeling, visual domain mismatch between datasets and diverse appearance of the same vehicle. We propose the adaptive feature learning technique based on the space-time prior to address these issues. The idea is demonstrated effectively in both the human Re-ID and the vehicle Re-ID tasks. We train a vehicle feature extractor in a multi-task learning manner on three existing vehicle datasets and fine-tune the feature extractor with the adaptive feature learning technique on the target domain. We then develop a vehicle Re-ID system based on the learned vehicle feature extractor. Finally, our meticulous system design leads to the second place in the 2018 NVIDIA AI City Challenge Track 3.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00024,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Wu_Vehicle_Re-Identification_With_CVPR_2018_paper.pdf
5bc85c6fa9712f842058d4694135f88768a00c66,0,1,0,Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization,"Cross-view geo-localization is to spot images of the same geographic target from different platforms, e.g., drone-view cameras and satellites. It is challenging in the large visual appearance changes caused by extreme viewpoint variations. Existing methods usually concentrate on mining the fine-grained feature of the geographic target in the image center, but underestimate the contextual information in neighbor areas. In this work, we argue that neighbor areas can be leveraged as auxiliary information, enriching discriminative clues for geo-localization. Specifically, we introduce a simple and effective deep neural network, called Local Pattern Network (LPN), to take advantage of contextual information in an end-to-end manner. Without using extra part estimators, LPN adopts a square-ring feature partition strategy, which provides the attention according to the distance to the image center. It eases the part matching and enables the part-wise representation learning. Owing to the square-ring partition design, the proposed LPN has good scalability to rotation variations and achieves competitive results on two prevailing benchmarks, i.e., University-1652 and CVUSA. Besides, we also show the proposed LPN can be easily embedded into other frameworks to further boost performance.",2020,ArXiv,2008.11646,,https://arxiv.org/pdf/2008.11646.pdf
5bc8f2fd72b3ffa0432a971c4097dc370f6d6222,1,0,0,SoDA: Multi-Object Tracking with Soft Data Association,"Robust multi-object tracking (MOT) is a prerequisite fora safe deployment of self-driving cars. Tracking objects, however, remains a highly challenging problem, especially in cluttered autonomous driving scenes in which objects tend to interact with each other in complex ways and frequently get occluded. We propose a novel approach to MOT that uses attention to compute track embeddings that encode the spatiotemporal dependencies between observed objects. This attention measurement encoding allows our model to relax hard data associations, which may lead to unrecoverable errors. Instead, our model aggregates information from all object detections via soft data associations. The resulting latent space representation allows our model to learn to reason about occlusions in a holistic data-driven way and maintain track estimates for objects even when they are occluded. Our experimental results on the Waymo OpenDataset suggest that our approach leverages modern large-scale datasets and performs favorably compared to the state of the art in visual multi-object tracking.",2020,ArXiv,2008.07725,,https://arxiv.org/pdf/2008.07725.pdf
5be74c6fa7f890ea530e427685dadf0d0a371fc1,1,1,0,Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification,"Person re-identification (re-ID) requires rapid, flexible yet discriminant representations to quickly generalize to unseen observations on-the-fly and recognize the same identity across disjoint camera views. Recent effective methods are developed in a pair-wise similarity learning system to detect a fixed set of features from distinct regions which are mapped to their vector embeddings for the distance measuring. However, the most relevant and crucial parts of each image are detected independently without referring to the dependency conditioned on one and another. Also, these region based methods rely on spatial manipulation to position the local features in comparable similarity measuring. To combat these limitations, in this paper we introduce the Deep Co-attention based Comparators (DCCs) that fuse the co-dependent representations of the paired images so as to focus on the relevant parts of both images and produce their \textit{relative representations}. Given a pair of pedestrian images to be compared, the proposed model mimics the foveation of human eyes to detect distinct regions concurrent on both images, namely co-dependent features, and alternatively attend to relevant regions to fuse them into the similarity learning. Our comparator is capable of producing dynamic representations relative to a particular sample every time, and thus well-suited to the case of re-identifying pedestrians on-the-fly. We perform extensive experiments to provide the insights and demonstrate the effectiveness of the proposed DCCs in person re-ID. Moreover, our approach has achieved the state-of-the-art performance on three benchmark data sets: DukeMTMC-reID \cite{DukeMTMC}, CUHK03 \cite{FPNN}, and Market-1501 \cite{Market1501}.",2018,ArXiv,1804.11027,,https://arxiv.org/pdf/1804.11027.pdf
5c3de4d180020d9e0c5469bd575161ea07b79aa7,1,1,0,Unified Multifaceted Feature Learning for Person Re-Identification,"Person re-identification (ReID) aims at re-identifying persons from different viewpoints across multiple cameras, of which it is of great importance to learn multifaceted features expressed in different parts of a person, e.g., clothes, bags, and other accessories in the main body, appearance in the head, and shoes in the foot. To learn such features, existing methods are focused on the striping-based approach that builds multi-branch neural networks to learn local features in each part of the identities, with one-branch network dedicated to one part. This results in complex models with a large number of parameters. To address this issue, this paper proposes to learn the multifaceted features in a simple unified single-branch neural network. The Unified Multifaceted Feature Learning (UMFL) framework is introduced to fulfill this goal, which consists of two key collaborative modules: compound batch image erasing (including batch constant erasing and random erasing) and hierarchical structured loss. The loss structures the augmented images resulted by the two types of image erasing in a two-level hierarchy and enforces multifaceted attention to different parts. As we show in the extensive experimental results on four benchmark person ReID datasets, despite the use of significantly simplified network structure, our method performs substantially better than state-of-the-art competing methods. Our method can also effectively generalize to vehicle ReID, achieving similar improvement on two vehicle ReID datasets.",2019,ArXiv,1911.08651,,https://arxiv.org/pdf/1911.08651.pdf
5c5f18feaa12fa62cd7c8abf1778230b9cc53601,1,0,0,Contextual Multi-Scale Feature Learning for Person Re-Identification,"Representing features at multiple scales is significant for person re-identification (Re-ID). Most existing methods learn the multi-scale features by stacking streams and convolutions without considering the cooperation of multiple scales at a granular level. However, most scales are more discriminative only when they integrate other scales as contextual information. We termed that contextual multi-scale. In this paper, we proposed a novel architecture, namely contextual multi-scale network (CMSNet), for learning common and contextual multi-scale representations simultaneously. The building block of CMSNet obtains contextual multi-scale representations by bidirectionally hierarchical connection groups: the forward hierarchical connection group for stepwise inter-scale information fusion and the backward hierarchical connection group for leap-frogging inter-scale information fusion. Too rich scale features without a selection will confuse the discrimination. Additionally, we introduced a new channel-wise scale selection module to dynamically select scale features for corresponding input image. To the best of our knowledge, CMSNet is the most lightweight model for person Re-ID and it achieves state-of-the-art performance on four commonly used Re-ID datasets, surpassing most large-scale models.",2020,ACM Multimedia,,10.1145/3394171.3414038,
5c5f7438db565e169a5f91bc6fab0232bfc0233a,1,1,0,SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification,"Cross-domain person re-identification (re-ID) is challenging due to the bias between training and testing domains. We observe that if backgrounds in the training and testing datasets are very different, it dramatically introduces difficulties to extract robust pedestrian features, and thus compromises the cross-domain person re-ID performance. In this paper, we formulate such problems as a background shift problem. A Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to generate images with suppressed backgrounds. Unlike simply removing backgrounds using binary masks, SBSGAN allows the generator to decide whether pixels should be preserved or suppressed to reduce segmentation errors caused by noisy foreground masks. Additionally, we take ID-related cues, such as vehicles and companions into consideration. With high-quality generated images, a Densely Associated 2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection (ISDC) modules to strengthen the complementarity of the generated data and ID-related cues. The experiments show that the proposed method achieves competitive performance on three re-ID datasets, i.e., Market-1501, DukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.09086,10.1109/ICCV.2019.00962,https://arxiv.org/pdf/1908.09086.pdf
5c9f448bfb87e9a3db4f2c73a93be271cca0c932,1,0,0,Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking,"Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors for tracking-by-detection.This results in deep models that are detector biased and evaluations that are detector influenced. To resolve this issue, we introduce Deep Motion Modeling Network (DMM-Net) that can estimate multiple objects' motion parameters to perform joint detection and association in an end-to-end manner. DMM-Net models object features over multiple frames and simultaneously infers object classes, visibility, and their motion parameters. These outputs are readily used to update the tracklets for efficient MOT. DMM-Net achieves PR-MOTA score of 12.80 @ 120+ fps for the popular UA-DETRAC challenge, which is better performance and orders of magnitude faster. We also contribute a synthetic large-scale public dataset Omni-MOT for vehicle tracking that provides precise ground-truth annotations to eliminate the detector influence in MOT evaluation. This 14M+ frames dataset is extendable with our public script (Code at Dataset , Dataset Recorder , Omni-MOT Source ). We demonstrate the suitability of Omni-MOT for deep learning with DMMNet and also make the source code of our network public.",2020,ArXiv,2008.08826,,https://arxiv.org/pdf/2008.08826.pdf
5caf6a46dc6469692d124ec8866f4a3b41a964ab,1,0,0,How Trustworthy are the Existing Performance Evaluations for Basic Vision Tasks?,"Performance evaluation is indispensable to the advancement of machine vision, yet its consistency and rigour have not received proportionate attention. This paper examines performance evaluation criteria for basic vision tasks namely, object detection, instance-level segmentation and multi-object tracking. Specifically, we advocate the use of criteria that are (i) consistent with mathematical requirements such as the metric properties, (ii) contextually meaningful in sanity tests, and (iii) robust to hyper-parameters for reliability. We show that many widely used performance criteria do not fulfill these requirements. Moreover, we explore alternative criteria for detection, segmentation, and tracking, using metrics for sets of shapes, and assess them against these requirements.",2020,ArXiv,2008.03533,,https://arxiv.org/pdf/2008.03533.pdf
5cd513cfef98047661ea4176ac690fcf3471f45c,0,1,0,Regularizing Deep Hashing Networks Using GAN Generated Fake Images,"Recently, deep-networks-based hashing (deep hashing) has become a leading approach for large-scale image retrieval. It aims to learn a compact bitwise representation for images via deep networks, so that similar images are mapped to nearby hash codes. Since a deep network model usually has a large number of parameters, it may probably be too complicated for the training data we have, leading to model over-fitting. To address this issue, in this paper, we propose a simple two-stage pipeline to learn deep hashing models, by regularizing the deep hashing networks using fake images. The first stage is to generate fake images from the original training set without extra data, via a generative adversarial network (GAN). In the second stage, we propose a deep architec- ture to learn hash functions, in which we use a maximum-entropy based loss to incorporate the newly created fake images by the GAN. We show that this loss acts as a strong regularizer of the deep architecture, by penalizing low-entropy output hash codes. This loss can also be interpreted as a model ensemble by simultaneously training many network models with massive weight sharing but over different training sets. Empirical evaluation results on several benchmark datasets show that the proposed method has superior performance gains over state-of-the-art hashing methods.",2018,ArXiv,1803.09466,,https://arxiv.org/pdf/1803.09466.pdf
5ce1b44f7e8a7d6a1f2a56b2e7be575e77163d74,1,1,0,Learning Cross Camera Invariant Features with CCSC Loss for Person Re-identification,"Person re-identification (re-ID) is mainly deployed in the multi-camera surveillance scene, which means that learning cross camera invariant features is highly required. In this paper, we propose a novel loss named Cross Camera Similarity Constraint loss (CCSC loss), which makes full use of the camera ID information and the person ID information simultaneously to construct cross camera image pairs and performs cosine similarity constraint on them. The proposed CCSC loss effectively reduces the intra-class variance through forcing the whole network to extract cross camera invariant features, and it can be unified with identification loss in a multi-task manner. Extensive experiments implemented on the standard benchmark datasets including CUHK03, DukeMTMC-reid, Market-1501 and MSMT17 indicate that the proposed CCSC loss can bring a large performance boost on the strong baseline and it is also superior to other metric learning methods such as hard triplet loss and center loss. For instance, on the most challenging dataset CUHK03-Detect, Rank-1 accuracy and mAP are improved by 10.0% and 10.2% than the baseline respectively and simultaneously obtain a comparable performance with the state-of-the-art method.",2019,ICIG,,10.1007/978-3-030-34120-6_35,
5ce4562c9ebf6c6957121b69e13efe730c83aacc,0,1,1,PurifyNet: A Robust Person Re-Identification Model With Noisy Labels,"Person re-identification (Re-ID) has been widely studied by learning a discriminative feature representation with a set of well-annotated training data. Existing models usually assume that all the training samples are correctly annotated. However, label noise is unavoidable due to false annotations in large-scale industrial applications. Different from the label noise problem in image classification with abundant samples, the person Re-ID task with label noise usually has very limited annotated samples for each identity. In this paper, we propose a robust deep model, namely PurifyNet, to address this issue. PurifyNet is featured in two aspects: 1) it jointly refines the annotated labels and optimizes the neural networks by progressively adjusting the predicted logits, which reuses the wrong labels rather than simply filtering them; 2) it can simultaneously reduce the negative impact of noisy labels and pay more attention to hard samples with correct labels by developing a hard-aware instance re-weighting strategy. With limited annotated samples for each identity, we demonstrate that hard sample mining is crucial for label corrupted Re-ID task, while it is usually ignored in existing robust deep learning methods. Extensive experiments on three datasets demonstrate the robustness of PurifyNet over the competing methods under various settings. Meanwhile, we show that it consistently improves the unsupervised/video-based Re-ID methods. Code is available at: https://github.com/mangye16/ReID-Label-Noise.",2020,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.2970590,
5ced2bc1c3815bc8ef2def1a8b28424004747db2,1,1,0,MagnifierNet: Towards Semantic Adversary and Fusion for Person Re-identification,"Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying the occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this work, we propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts. Firstly, the holistic salient features are encoded by a global branch. Secondly, to enhance detailed representation for each semantic region, the ""Semantic Adversarial Branch"" is designed to learn from dynamically generated semantic-occluded samples during training. Meanwhile, we introduce ""Semantic Fusion Branch"" to filter out irrelevant noises by selectively fusing semantic region information sequentially. To further improve feature diversity, we introduce a novel loss function ""Semantic Diversity Loss"" to remove redundant overlaps across learned semantic representations. State-of-the-art performance has been achieved on three benchmarks by large margins. Specifically, the mAP score is improved by 6% and 5% on the most challenging CUHK03-L and CUHK03-D benchmarks.",2020,,,,
5d010ab2e8b491dd8257e6305bd7f1af0371fde1,0,1,0,Person re-identification based on multi-scale constraint network,"Abstract Combining features of different scales to learn a more discriminative model is an essential solution for person re-identification (Re-ID) tasks. Most existing multi-scale methods are based on the fusion of features from different scales, which cannot exploit information throughly at each scale and cause gradient chaos in optimizing. To address this problem, in this paper we propose an end-to-end multi-scale constraint network(MSCN) to capture detailed information from multiple scales which can independently train each scale and integrate the features of each scale for prediction. In order to retain more information at different scales, we uniformly divide the feature maps into several parts, and vary the number of parts in different scales, then concatenate all the parts in each scale as the entire feature for training. We use both classification loss and metric loss to optimize the network from different aspects. Extensive experiments on three datasets demonstrate that our method achieves very competitive performance. Especially on the CUHK03 dataset, our approach achieves the state-of-the-art results outperforming the current best method by 2.4%/2.0% in Rank-1/mAP.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.08.012,
5d0665c5203da9e19926bf6582d3fe9373664618,0,1,0,2 Datasets and Evaluation 2 . 1 Datasets with Soft Biometrics Annotations,"Re-identification of persons is usually based on primary biometric features such as their faces, fingerprints, iris or gait. However, in most existing video surveillance systems, it is difficult to obtain these features due to the low resolution of surveillance footages and unconstrained real-world environments. As a result, most of the existing person re-identification techniques only focus on overall visual appearance. Recently, the use of soft biometrics has been proposed to improve the performance of person re-identification. Soft biometrics such as height, gender, age are physical or behavioural features, which can be described by humans. These features can be obtained from low-resolution videos at a distance ideal for person re-identification application. In addition, soft biometrics are traits for describing an individual with human-understandable labels. It allows human verbal descriptions to be used in the person re-identification or person retrieval systems. In some deep learning based person re-identification methods, soft biometrics attributes are integrated into the network to boot the robustness of the feature representation. Biometrics can also be utilised as a domain adaptation bridge for addressing the cross-dataset person re-identification problem. This chapter will review the stateof-the-art deep learning methods involving soft biometrics from three perspectives: supervised, semi-supervised and unsupervised approaches. In the end, we discuss the existing issues that are not addressed by current works. Shan Lin University of Warwick, Coventry, UK, e-mail: shan.lin@warwick.ac.uk Chang-Tsun Li University of Warwick, Coventry, UK, e-mail: c-t.li@warwick.ac.uk Deakin University, Melbourne, Australia, e-mail: c-t.li@deakin.edu.au",2019,,,,https://warwick.ac.uk/fac/sci/dcs/people/research/xuuldl/deepbiometrics.pdf
5d28769da39bc9dcb0ff19b48977914e15bc794d,1,0,0,Enriched and Discriminative Human Features for Person Re-Identification Based on Explainable Behaviors of Convolutional Neural Networks,"Understanding pedestrian behaviors such as their movement patterns in urban areas could contribute to the design of pedestrian-friendly facilities. With the commonly deployed surveillance cameras, pedestrian movement in a wide region could be identified by the person re-identification (ReID) technique across multiple cameras. Convolutional neural networks (CNNs) have been widely studied to automate the ReID task. CNN models equipped with deep learning techniques could extract discriminative human features from images and show promising ReID performance. However, some common challenges such as occlusion and appearance variation are still unsolved. Specifically, our study infers that over-relying on discriminative features only may compromise ReID performance. Therefore, this paper proposes a new model that extracts enriched features, which is more reliable against those ReID challenges. By adding a feature dropping strategy during model training, our model learns to focus on rich human features from different body parts. Moreover, this paper presents an explainable approach of model design, by visualizing which human parts a deep learning model focuses on. Based on an intuitive interpretation of model behaviors that lead to inaccurate results, specific improvement of model architecture is inspired. Our improved results suggest that making existing models explainable could effectively shed light on designing more robust models.",2020,,,10.1007/978-3-030-51295-8_5,
5d28a54b1b27280482463df85bb66bc2914ff893,1,0,0,Multi-Object Tracking with Correlation Filter for Autonomous Vehicle,"Multi-object tracking is a crucial problem for autonomous vehicle. Most state-of-the-art approaches adopt the tracking-by-detection strategy, which is a two-step procedure consisting of the detection module and the tracking module. In this paper, we improve both steps. We improve the detection module by incorporating the temporal information, which is beneficial for detecting small objects. For the tracking module, we propose a novel compressed deep Convolutional Neural Network (CNN) feature based Correlation Filter tracker. By carefully integrating these two modules, the proposed multi-object tracking approach has the ability of re-identification (ReID) once the tracked object gets lost. Extensive experiments were performed on the KITTI and MOT2015 tracking benchmarks. Results indicate that our approach outperforms most state-of-the-art tracking approaches.",2018,Sensors,,10.3390/s18072004,https://pdfs.semanticscholar.org/5d28/a54b1b27280482463df85bb66bc2914ff893.pdf
5d4b7764e8e88a0a61fea0cfe240038b9abb09d0,1,0,0,Fish Tracking and Segmentation From Stereo Videos on the Wild Sea Surface for Electronic Monitoring of Rail Fishing,"Electronic monitoring of fishery activities has drawn increasing attention. Deformable objects, noise from the wild sea surface, and dynamic background, however, make conventional tracking and segmentation methods unreliable. In this paper, we present an online 3D tracking and segmentation system for stereo video-based monitoring of rail fish catching on the wild sea surface. Based on the result of a pre-trained image object (fish) detector, a Kalman filtering-based tracking system overcomes the issues of low detection scores of deformed objects and of unreliable bounding boxes by rescoring multiple object proposals using spatial information in 3D. A clustering-and-scoring strategy is then applied on the depth map so that a plane classification method can effectively segment the objects from the dynamic background without any prior modeling. The object segmentation is further refined using fully connected conditional random fields based on color and geometric features. Using the segmentation results, we can measure the 3D lengths of objects and update the positions of bounding boxes to help tracking. Experimental results show that a reliable tracking and measurement performance under noisy and dynamic sea surface environment can be achieved.",2019,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2018.2872575,
5d52bdbb478f31749f33ead669cab87107af6b52,1,0,0,BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving,"In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/.",2019,2019 International Conference on Robotics and Automation (ICRA),1903.06405,10.1109/ICRA.2019.8793523,https://arxiv.org/pdf/1903.06405.pdf
5d8337ddffa5757bebf72c31de58c131fce599e6,1,1,0,A Survey of Pruning Methods for Efficient Person Re-identification Across Domains,"Recent years have witnessed a substantial increase in the deep learning architectures proposed for visual recognition tasks like person re-identification, where individuals must be recognized over multiple distributed cameras. Although deep Siamese networks have greatly improved the state-of-the-art accuracy, the computational complexity of the CNNs used for feature extraction remains an issue, hindering their deployment on platforms with with limited resources, or in applications with real-time constraints. Thus, there is an obvious advantage to compressing these architectures without significantly decreasing their accuracy. This paper provides a survey of state-of-the-art pruning techniques that are suitable for compressing deep Siamese networks applied to person re-identification. These techniques are analysed according to their pruning criteria and strategy, and according to different design scenarios for exploiting pruning methods to fine-tuning networks for target applications. Experimental results obtained using Siamese networks with ResNet feature extractors, and multiple benchmarks re-identification datasets, indicate that pruning can considerably reduce network complexity while maintaining a high level of accuracy. In scenarios where pruning is performed with large pre-training or fine-tuning datasets, the number of FLOPS required by the ResNet feature extractor is reduced by half, while maintaining a comparable rank-1 accuracy (within 1\% of the original model). Pruning while training a larger CNNs can also provide a significantly better performance than fine-tuning smaller ones.",2019,ArXiv,1907.02547,,https://arxiv.org/pdf/1907.02547.pdf
5d8ab5c473eb9e083ceb35ebeb00a062114ee6ac,1,0,0,A Reinforcement Learning Approach to Target Tracking in a Camera Network,"Target tracking in a camera network is an important task for surveillance and scene understanding. The task is challenging due to disjoint views and illumination variation in different cameras. In this direction, many graph-based methods were proposed using appearance-based features. However, the appearance information fades with high illumination variation in the different camera FOVs. We, in this paper, use spatial and temporal information as the state of the target to learn a policy that predicts the next camera given the current state. The policy is trained using Q-learning and it does not assume any information about the topology of the camera network. We will show that the policy learns the camera network topology. We demonstrate the performance of the proposed method on the NLPR MCT dataset.",2018,ArXiv,1807.10336,,https://arxiv.org/pdf/1807.10336.pdf
5d997aea77ef72bca575730241b7386c28811f70,0,1,0,Unsupervised domain adaption for image-to-video person re-identification,"Recently, person re-identification technique has been successfully applied to many fields, such as suspect tracking and lost human location. As video always contains more valuable information, more and more researchers focus on video based person re-identification, especially in image-to-video person re-identification (IVPR). However, most of existing IVPR models are under the supervised framework. In fact, marking enough training samples will cost numbers of labors, which limits the practical value of them. At the same time, the 2D features extracted from pedestrian image and 3D features extracted from pedestrian video are heterogeneous, which brings significant challenge for IVPR task. To effective solve the above problems, we propose an unsupervised domain adaption image-to-video person re-identification model by cross-modal feature generating and target information preserving transfer network (CMGTN). On one hand, the designed generator in our model can not only transform target domain unlabeled sample features into source domain feature space, but also can preserve target identity information. On the other hand, we eliminate the gap between pedestrian images and videos by embedding a cross-modal loss term. To evaluate the performance of our approach, we conduct extensive experiments on PRID-2011, iLIDS-VID and MARS datasets, and compare our approach with existing state-of-the-art IVPR models including four unsupervised methods and three supervised methods. Experimental results demonstrate the effectiveness of our approach.",2020,,,10.1007/s11042-019-08550-9,
5dae01134bf11b48f08fb1cad0163aee7d123ded,0,1,0,Light Person Re-Identification by Multi-Cue Tiny Net,"Nowadays, person re-identification (re-id) receives much attraction and it is for matching person images across disjoint camera views. Although many methods are developed, none of the state-of-the-art models (especially for the deep models) can be deployed by a camera's chip due to limited memory and weak computation. To address this problem, we propose a framework called Multi-Cue Tiny Net, which is a combination of tiny convolutional neural networks (CNNs) with small model size. And we call the person re-identification based on limited memory and weak computation the Light Person Re-Identification. Three different tiny nets are used to learn the complementary features and four cues of images are used for learning features with different information. All the features will be concatenated to form a fusion feature. Besides, to reduce the dimension of the features, and keep the small model size and high performance accuracy, we pre-trained the tiny nets and then retrained them after adding a fully connected layer to the global average pooling layer. Experimental results on challenging person re-identification dataset show that our approach yields promising accuracy with light memory and computation.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451738,
5de14b6b89666ab6ca9fd13e089e9ca18173a046,0,1,0,Feature Re-Learning with Data Augmentation for Video Relevance Prediction,"Predicting the relevance between two given videos with respect to their visual content is a key component for content-based video recommendation and retrieval. Thanks to the increasing availability of pre-trained image and video convolutional neural network models, deep visual features are widely used for video content representation. However, as how two videos are relevant is task-dependent, such off-the-shelf features are not always optimal for all tasks. Moreover, due to varied concerns including copyright, privacy and security, one might have access to only pre-computed video features rather than original videos. We propose in this paper feature re-learning for improving video relevance prediction, with no need of revisiting the original video content. In particular, re-learning is realized by projecting a given deep feature into a new space by an affine transformation. We optimize the re-learning process by a novel negative-enhanced triplet ranking loss. In order to generate more training data, we propose a new data augmentation strategy which works directly on frame-level and video-level features. Extensive experiments in the context of the Hulu Content-based Video Relevance Prediction Challenge 2018 justify the effectiveness of the proposed method and its state-of-the-art performance for content-based video relevance prediction.",2020,ArXiv,2004.03815,10.1109/TKDE.2019.2947442,https://arxiv.org/pdf/2004.03815.pdf
5e47ada58f5b7817acc46240f1090ba51dd0c637,0,1,0,Computer Vision – ECCV 2018,"Deep convolutional networks (CNNs) have exhibited their potential in image inpainting for producing plausible results. However, in most existing methods, e.g., context encoder, the missing parts are predicted by propagating the surrounding convolutional features through a fully connected layer, which intends to produce semantically plausible but blurry result. In this paper, we introduce a special shift-connection layer to the U-Net architecture, namely Shift-Net, for filling in missing regions of any shape with sharp structures and fine-detailed textures. To this end, the encoder feature of the known region is shifted to serve as an estimation of the missing parts. A guidance loss is introduced on decoder feature to minimize the distance between the decoder feature after fully connected layer and the ground-truth encoder feature of the missing parts. With such constraint, the decoder feature in missing region can be used to guide the shift of encoder feature in known region. An end-to-end learning algorithm is further developed to train the Shift-Net. Experiments on the Paris StreetView and Places datasets demonstrate the efficiency and effectiveness of our Shift-Net in producing sharper, fine-detailed, and visually plausible results. The codes and pre-trained models are available at https://github.com/Zhaoyi-Yan/Shift-Net.",2018,Lecture Notes in Computer Science,,10.1007/978-3-030-01264-9,
5ef052b669059a9ada8e25bc2bf40c9d75c8ad6d,0,1,0,A heterogeneous branch and multi-level classification network for person re-identification,"Abstract Convolutional neural networks with multiple branches have recently been proved highly effective in person re-identification (re-ID). Researchers design multi-branch networks using part models, yet they always attribute the effectiveness to multiple parts. In addition, existing multi-branch networks always have isomorphic branches, which lack structural diversity. In order to improve this problem, we propose a novel Heterogeneous Branch and Multi-level Classification Network (HBMCN), which is designed based on the pre-trained ResNet-50 model. A new heterogeneous branch, SE-Res-Branch, is proposed based on the SE-Res module, which consists of the Squeeze-and-Excitation block and the residual block. Furthermore, a new multi-level classification joint objective function is proposed for the supervised learning of HBMCN, whereby multi-level features are extracted from multiple high-level layers and concatenated to represent a person. Based on three public person re-ID benchmarks (Market1501, DukeMTMC-reID and CUHK03), experimental results show that the proposed HBMCN reaches 94.4%, 85.7% and 73.8% in Rank-1, and 85.7%, 74.6% and 69.0% in mAP, achieving a state-of-the-art performance. Further analysis demonstrates that the specially designed heterogeneous branch performs better than an isomorphic branch, and multi-level classification provides more discriminative features compared to single-level classification. As a result, HBMCN provides substantial further improvements in person re-ID tasks.",2020,Neurocomputing,2006.01367,10.1016/j.neucom.2020.05.007,https://arxiv.org/pdf/2006.01367.pdf
5efd24909e06e02a40990baf5f844788504e993b,0,1,0,Similarity Learning with Listwise Ranking for Person Re-Identification,"Person re- identification is an important task in video surveillance systems. It consists in matching an image of a probe person among a gallery image set of people detected from a network of surveillance cameras with non-overlapping fields of view. The main challenge of person re- identification is to find image representations that are discriminating the persons' identities and that are robust to the viewpoint, body pose, illumination changes and partial occlusions. In this paper, we proposed a metric learning approach based on a deep neural network using a novel loss function which we call the Rank- Triplet loss. This proposed loss function is based on the predicted and ground truth ranking of a list of instances instead of pairs or triplets and takes into account the improvement of evaluation measures during training. Through our experiments on two person re- identification datasets, we show that the new loss outperforms other common loss functions and that our approach achieves state-of-the-art results on these two datasets.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451628,https://hal.archives-ouvertes.fr/hal-01895355/file/paper_2279.pdf
5f21bccbc65af943ce4c12c6eee7ae7d7d680a2a,0,0,1,Cross Attention Network for Few-shot Classification,"Few-shot classification aims to recognize unlabeled samples from unseen classes given only few labeled samples. The unseen classes and low-data problem make few-shot classification very challenging. Many existing approaches extracted features from labeled and unlabeled samples independently, as a result, the features are not discriminative enough. In this work, we propose a novel Cross Attention Network to address the challenging problems in few-shot classification. Firstly, Cross Attention Module is introduced to deal with the problem of unseen classes. The module generates cross attention maps for each pair of class feature and query sample feature so as to highlight the target object regions, making the extracted feature more discriminative. Secondly, a transductive inference algorithm is proposed to alleviate the low-data problem, which iteratively utilizes the unlabeled query set to augment the support set, thereby making the class features more representative. Extensive experiments on two benchmarks show our method is a simple, effective and computationally efficient framework and outperforms the state-of-the-arts.",2019,NeurIPS,1910.07677,,https://arxiv.org/pdf/1910.07677.pdf
5f55c68d1928c77ce550bbd39b1a663a6e99d0f6,1,1,0,Exploring Latent Information for Unsupervised Person Re-Identification by Discriminative Learning Networks,"For unsupervised domain adaption in person re-identification (Re-ID) tasks, the generally used label estimation approaches simply use the global features or the uniform part features. They often neglect the variations of samples having the same identity caused by occlusion, misalignment and uncontrollable camera settings. In this paper, we propose a discriminative learning network with target domain latent information (LatentDLN) to enhance the generalization ability of the Re-ID model. Specifically, to generate a discriminative and robust representation, two types of latent information in the samples from the target domain are explored by the multi-branch deep structure. First, the key points based valid region information is used to leverage the local and global cues in human body, and then a heuristic distance metric learning method based on the global features and the local features is proposed to effectively evaluate the similarity between different images. Second, the camera style transferred images are used as augmentation data to bridge the gap between different cameras in target domains. Moreover, the re-rank mechanism based on reciprocal neighbors is designed to improve the quality of the label estimation. Experimental results on Market-1501, DukeMTMC-ReID and MSMT17 datasets validate the significant effectiveness of the proposed LatentDLN for unsupervised Re-ID.",2020,IEEE Access,,10.1109/ACCESS.2020.2978407,
5f59b1deae09152b2a1aff2312e558d8db3cb1f6,1,1,0,Unsupervised person re-identification by hierarchical cluster and domain transfer,"Person re-identification (re-ID) has recently been tremendously boosted due to the advancement of deep convolutional neural networks. Unfortunately, the majority of deep re-ID methods focus on supervised, single-domain re-ID task, while less attention is paid on unsupervised domain adaptation. Therefore, these methods always fail to generalize well to real-world scenarios, which have attracted much attention from academia. To address this challenge, we propose a joint unsupervised domain adaptive re-ID method, named HCTL, which is aided by Hierarchical Clustering and Transfer Learning. Specifically, our method performs camera invariance learning using iStarGAN by transferring style of reliable images, which is mined by hierarchical clustering, to the style of other cameras in target domain. During training stage, HCTL integrates TriHard loss on top of ResNet-50 to reduce intra-class variance among dataset and enforce connectedness simultaneously between source domain and target domain. Comprehensive experiments based on Market-1501, DukeMTMC-reID and CUHK03 are conducted, results indicate that our method robustly achieves state-of-the-art performances with only a few reliable samples in target domain and outperform any existing approaches by a large margin.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-08723-x,
5f6cfa804054fd0301cbf7f9f6051994b425677e,1,0,0,Dynamic Scaling of Video Analytics for Wide-Area Tracking in Urban Spaces,"Smart City deployments typically have thousands to even hundreds of thousands of Surveillance cameras. Rapid advancements in computer vision techniques due to Deep Neural Networks enable using these camera feeds for performing non-trivial analytics. Tracking a moving object of interest using a large network of cameras, also known as object reidentification, is one such analytic that empowers city administration with capabilities such as finding missing people or prioritizing emergency vehicles. We have built Anveshak, a framework for distributed wide-area tracking. Anveshak fills in the shortcomings of existing Big Data and Deep Learning frameworks by - exposing an intuitive and composable programming model; automating application deployment and orchestration across edge, fog and cloud resources and providing knobs to the user for managing the application performance. The knobs lend the application the ability to scale potentially to thousands of cameras. In this proposal we have designed two representative applications; missing person tracking and priority signalling for emergency vehicles. We empirically verify that the application scales to 1000 cameras on a Cloud-only deployment of 10 Azure VMs with 8 cores and 32GB RAM each. Alternatively, it scales to 500 cameras on a simulated setup of 100 edge, 30 fog, and 1 Cloud VM. We also highlight the effect of the knobs on the application performance. designed two representative applications; missing person tracking and priority signalling for emergency vehicles. We empirically verify that the application scales to 1000 cameras on a Cloud-only deployment of 10 Azure VMs with 8 cores and 32GB RAM each. Alternatively, it scales to 500 cameras on a simulated setup of 100 edge, 30 fog, and 1 Cloud VM. We also highlight the effect of the knobs on the application performance.",2019,"2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",,10.1109/CCGRID.2019.00018,
5fa86c59b00d3b0c975abfa87154eed926ebfa4a,1,1,0,Unsupervised Person Re-Identification by Deep Asymmetric Metric Embedding,"Person re-identification (Re-ID) aims to match identities across non-overlapping camera views. Researchers have proposed many supervised Re-ID models which require quantities of cross-view pairwise labelled data. This limits their scalabilities to many applications where a large amount of data from multiple disjoint camera views is available but unlabelled. Although some unsupervised Re-ID models have been proposed to address the scalability problem, they often suffer from the view-specific bias problem which is caused by dramatic variances across different camera views, e.g., different illumination, viewpoints and occlusion. The dramatic variances induce specific feature distortions in different camera views, which can be very disturbing in finding cross-view discriminative information for Re-ID in the unsupervised scenarios, since no label information is available to help alleviate the bias. We propose to explicitly address this problem by learning an unsupervised asymmetric distance metric based on cross-view clustering. The asymmetric distance metric allows specific feature transformations for each camera view to tackle the specific feature distortions. We then design a novel unsupervised loss function to embed the asymmetric metric into a deep neural network, and therefore develop a novel unsupervised deep framework named the DEep Clustering-based Asymmetric MEtric Learning (DECAMEL). In such a way, DECAMEL jointly learns the feature representation and the unsupervised asymmetric metric. DECAMEL learns a compact cross-view cluster structure of Re-ID data, and thus help alleviate the view-specific bias and facilitate mining the potential cross-view discriminative information for unsupervised Re-ID. Extensive experiments on seven benchmark datasets whose sizes span several orders show the effectiveness of our framework.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,1901.10177,10.1109/TPAMI.2018.2886878,https://arxiv.org/pdf/1901.10177.pdf
5fffb5d73616b3f9ae698470feb2f516fd561dd0,1,1,0,High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification,"Occluded person re-identification (ReID) aims to match occluded person images to holistic ones across dis-joint cameras. In this paper, we propose a novel framework by learning high-order relation and topology information for discriminative features and robust alignment. At first, we use a CNN backbone to learn feature maps and key-points estimation model to extract semantic local features. Even so, occluded images still suffer from occlusion and outliers. Then, we view the extracted local features of an image as nodes of a graph and propose an adaptive direction graph convolutional (ADGC) layer to pass relation information between nodes. The proposed ADGC layer can automatically suppress the message passing of meaningless features by dynamically learning direction and degree of linkage. When aligning two groups of local features, we view it as a graph matching problem and propose a cross-graph embedded-alignment (CGEA) layer to joint learn and embed topology information to local features, and straightly predict similarity score. The proposed CGEA layer can both take full use of alignment learned by graph matching and replace sensitive one-to-one alignment with a robust soft one. Finally, extensive experiments on occluded, partial, and holistic ReID tasks show the effectiveness of our proposed method. Specifically, our framework significantly outperforms state-of-the-art by $6.5\%$ mAP scores on Occluded-Duke dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.08177,10.1109/CVPR42600.2020.00648,https://arxiv.org/pdf/2003.08177.pdf
60127088b81232dd965795556fb9fedc07004d6f,0,1,0,Discriminative Spatial Feature Learning for Person Re-Identification,"Person re-identification (ReID) aims to match detected pedestrian images from multiple non-overlapping cameras. Most existing methods employ a backbone CNN to extract a vectorized feature representation by performing some global pooling operations (such as global average pooling and global max pooling) on the 3D feature map (i.e., the output of the backbone CNN). Although simple and effective in some situations, the global pooling operation only focuses on the statistical properties and ignores the spatial distribution of the feature map. Hence, it can not distinguish two feature maps when they have similar response values located in totally different positions. To handle this challenge, a novel method is proposed to learn the discriminative spatial features. Firstly, a self-constrained spatial transformer network (SC-STN) is introduced to handle the misalignments caused by detection errors. Then, based on the prior knowledge that the spatial structure of a pedestrian often keeps robust in vertical orientation of images, a novel vertical convolution network (VCN) is proposed to extract the spatial feature in vertical. Extensive experimental evaluations on several benchmarks demonstrate that the proposed method achieves state-of-the-art performances by introducing only a few parameters to the backbone.",2020,ACM Multimedia,,10.1145/3394171.3413730,
603a98ffcc72eb3a313e9be4e1165284bdf8af6e,1,1,0,In Defense of the Classification Loss for Person Re-Identification,"The recent research for person re-identification has been focused on two trends. One is learning the part-based local features to form more informative feature descriptors. The other is designing effective metric learning loss functions such as the triplet loss family. We argue that learning global features with classification loss could achieve the same goal, even with some simple and cost-effective architecture design. In this paper, we first explain why the person re-id framework with standard classification loss usually has inferior performance compared to metric learning. Based on that, we further propose a person re-id framework featured by channel grouping and multi-branch strategy, which divides global features into multiple channel groups and learns the discriminative channel group features by multi-branch classification layers. The extensive experiments show that our framework outperforms prior state-of-the-arts in terms of both accuracy and inference speed.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1809.05864,10.1109/CVPRW.2019.00194,https://arxiv.org/pdf/1809.05864.pdf
606804ae0b7036384a6a27bd474b060f74f75e08,0,1,0,Night Person Re-Identification and a Benchmark,"Person re-identification is an important problem in computer vision fields due to its widely application. However, most of existing person re-identification methods are evaluated in daytime scenarios which is still far from real applications. In this paper, we pay attention to the night scenario person re-identification problem which most of works are not focused on. For this purpose, we contribute a large and real-scenario person re-identification dataset for night scenario named KnightReid, which aims to bridge the gap between theoretical research and practical application. To the best of our knowledge, the KnightReid dataset is the first night scenario dataset for the person re-identification which distinguishes existing works. Furthermore, by carefully examining the properties of night scenario data, we propose to combine image denoising networks with common used person re-identification networks to adapt to this kind of problem. Besides, we provide a comprehensive benchmark result that is evaluated on the dataset. The extensive experiments convince the effectiveness of the proposed model.",2019,IEEE Access,,10.1109/ACCESS.2019.2929854,
60c2712705c5560f3c99062e4ec3f9dcb8f43cba,0,1,0,Improving the generalization performance of deep networks by dual pattern learning with adversarial adaptation,"Abstract In this paper, we present a dual pattern learning network architecture with adversarial adaptation (DPLAANet). Unlike conventional networks, the proposed network has two input branches and two loss functions. This architecture forces the network to learn robust features by analysing dual inputs. The dual input structure allows the network to have a considerably large number of image pairs, which can help address the overfitting issue due to limited training data. In addition, we propose to associate the two input branches with two random interest values during training. As a stochastic regularization technique, this method can improve the generalization performance. Moreover, we introduce to use the adversarial training approach to reduce the domain difference between fused image features and single image features. Extensive experiments on CIFAR-10, CIFAR-100, FI-8, the Google commands dataset, and MNIST demonstrate that our DPLAANets exhibit better performance than the baseline networks. The experimental results on subsets of CIFAR-10, CIFAR-100, and MNIST demonstrate that DPLAANets have a good generalization performance on small datasets. The proposed architecture can be easily extended to have more than two input branches. The experimental results on subsets of MNIST show that the architecture with three branches outperforms two branches when the training set is extremely small.",2020,Knowl. Based Syst.,,10.1016/j.knosys.2020.106016,
61160966edc40c5526fa2cae631c31314a5339a6,0,1,0,Learning deep embedding with mini-cluster loss for person re-identification,"Recently, the triplet loss is commonly used in many deep person re-identification (ReID) frameworks to learn an embedding space in which similar data points are close and dissimilar data points are far away. However, the triplet loss simply focuses on the relative orders of points. This may lead to a relatively large intra-class variance and then a weak generalization capacity on the test set. In this paper, we propose a mini-cluster loss, which regards images belonging to the same identity as a mini-cluster and treats them as a whole during the training instead of considering them separately. For each mini-cluster in a batch, we define the largest distance between points in a mini-cluster as its inner divergence and the shortest distance with outer points as its outer divergence. By constraining the outer divergence larger than the inner divergence, our framework with the mini-cluster loss achieves the more compact mini-clusters while keeping the diversity distributions of the classes. As a result, a better generalization ability and a higher performance can be obtained. In the extensive experiments, our proposed framework achieves a state-of-the-art performance on two large-scale person ReID datasets (Market1501, DukeMTMC-reID) which clearly demonstrates its effectiveness. Specifically, 72.44% mAP and 87.05% rank-1 score are achieved on the Market1501 dataset with single query setting, 78.17% mAP and 91.05% rank-1 score with multiply query setting, and on the DukeMTMC-reID dataset, 60.19% mAP and 77.20% rank-1 score are obtained.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-7446-2,
61470aaccabd195a361112d38d80d3779498f784,0,1,0,Stepwise Metric Promotion for Unsupervised Video Person Re-identification,"The intensive annotation cost and the rich but unlabeled data contained in videos motivate us to propose an unsupervised video-based person re-identification (re-ID) method. We start from two assumptions: 1) different video tracklets typically contain different persons, given that the tracklets are taken at distinct places or with long intervals; 2) within each tracklet, the frames are mostly of the same person. Based on these assumptions, this paper propose a stepwise metric promotion approach to estimate the identities of training tracklets, which iterates between cross-camera tracklet association and feature learning. Specifically, We use each training tracklet as a query, and perform retrieval in the cross-camera training set. Our method is built on reciprocal nearest neighbor search and can eliminate the hard negative label matches, i.e., the cross-camera nearest neighbors of the false matches in the initial rank list. The tracklet that passes the reciprocal nearest neighbor check is considered to have the same ID with the query. Experimental results on the PRID 2011, ILIDS-VID, and MARS datasets show that the proposed method achieves very competitive re-ID accuracy compared with its supervised counterparts.",2017,2017 IEEE International Conference on Computer Vision (ICCV),,10.1109/ICCV.2017.266,http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf
619a1ac232af715ef7a8f76cf88bddb77a8ee5f7,0,1,0,The Multi-Layer Constrained Loss for Cross-Modality Person Re-Identification,"Person Re-identification (Re-ID) has great potential in video surveillance and security. At present, Re-ID in the visible light field has reached a relatively high precision, but the related research on infrared-visible cross-modality ReID problem for night monitoring still has plenty of room for improvement. Current methods mainly focus on extracting the features of thermal(infrared) and visible person images using two models, and then classifying and measuring learning through partial sharing weights and the same loss regression layer. We found that the main difference between infrared image and visible light lies in color and edge sharpening. In this paper, we proposed a simple and effective end-to-end crossmodality Re-ID model named cmPCB by using a Part-based Convolution-al Baseline (PCB) model for feature learning. We use Resnet network to extract network features, and we proposed a Multi-Layer Constrained (MLC) Loss, and extend the distance between near infrared and visible modes. Experimental results demonstrate that our model has superi-or performance compared to the state-of-the-arts, i.e. on SYSU- MM01 dataset(All-search multi-shot), Rank1 is improved from 43.86% to 65.83%, mAP is improved from 30.48% to 53.34%; on RegDB dataset, Rank1 is improved from 50.85% to 81.02%, mAP is improved from 48.67% to 78.73%.",2020,2020 International Conference on Artificial Intelligence and Signal Processing (AISP),,10.1109/AISP48273.2020.9073293,
61da6978e29f417465f66bd418b631ca1951056c,1,0,0,Text Attribute Aggregation and Visual Feature Decomposition for Person Search,Person search is the task of retrieving a pedestrian image given a list of text attributes. We investigate a novel mechanism that operates in feature embedding space for matching data across visual and text modalities. We propose a framework (TAVD) with two complementary modules: Text attribute feature aggregation (TA) that aggregates multiple semantic attributes in a bimodal space for globally matching text descriptions with images and Visual feature decomposition (VD) which performs feature embedding for locally matching image regions with text attributes. The results and comparisons to the state of the art on three standard benchmarks demonstrate that our solution is an effective strategy for retrieving person images while retaining the semantic of each query text attribute.,2020,,,,https://www.bmvc2020-conference.com/assets/papers/0266.pdf
61ddaea545235e75f3c666bef0472ab0d8d74e8f,1,0,0,Ranking and Classification driven Feature Learning for Person Re_identification,"Person re-identification has attracted many researchers' attention for its wide application, but it is still a very challenging task because only part of the image information can be used for personnel matching. Most of current methods uses CNN to learn to embeddings that can capture semantic similarity information among data points. Many of the state-of-the-arts methods use complex network structures with multiple branches that fuse multiple features while training or testing, using classification loss, Triplet loss or a combination of the two as loss function. However, the method that using Triplet loss as loss function converges slowly, and the method in which pull features of the same class as close as possible in features space leads to poor feature stability. This paper will combine the ranking motivated structured loss, proposed a new metric learning loss function that make the features of the same class are sparsely distributed into the range of small hyperspheres and the features of different classes are uniformly distributed at a clearly angle. And adopted a new single-branch network structure that only using global feature can also get great performance. The validity of our method is verified on the Market1501 and DukeMTMC-ReID person re-identification datasets. Finally acquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1 accuracy and 88.7% mAP on Market1501. Codes and models are available in Github.this https URL.",2019,ArXiv,1912.1163,,https://arxiv.org/pdf/1912.11630.pdf
628bb228e42f25f33e5bbe6413c3d42edde39577,1,0,0,Person re-identification using Hybrid Task Convolutional Neural Network in camera sensor networks,"Abstract This paper proposes a new framework called Hybrid Task Convolutional Neural Network (HTCNN) which combines the advantages of ranking and classification tasks for person re-identification (re-ID) in camera sensor networks. As for the ranking task, we propose Weighted Triplet Loss (WTL) to optimize global features of pedestrians, and meanwhile WTL emphasizes the foreground of pedestrian image and weakens the background in order to enhance the feature discrimination. As for the classification task, we evenly divide the convolutional activation map into several horizontal parts and utilize average pooling to obtain local features of pedestrians. We evaluate our method on public person re-ID datasets, and the results indicate HTCNN exceeds the state-of-the-art re-ID methods.",2020,Ad Hoc Networks,,10.1016/j.adhoc.2019.102018,
629a259eb5f2606af6ce33cd9691c094c4fa6758,0,1,0,Faster Person Re-Identification,"Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a $F_{\beta}$ score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only 8% more accurate but also 5x faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is $50\times$ faster with comparable accuracy. Code is available at this https URL.",2020,ECCV,2008.06826,10.1007/978-3-030-58598-3_17,https://arxiv.org/pdf/2008.06826.pdf
629fe22d190488ee83cc1e7488de68482b0d0a29,1,1,0,Complementation-Reinforced Attention Network for Person Re-Identification,"Fine-grained information has been proved helpful for person re-identification, and multi-head attention mechanism offers a feasible solution for this. However, we observe severe redundancy among the multiple branches, which might make the learned representation over-emphasize certain discriminative regions and correspondingly ignore other potentially informative regions. Therefore, we tackle this issue by two aspects yielding the so-called Complementation-Reinforced Attention Network (CRAN). One is the redundancy among branches, and we propose to impose complementing constraints among multiple attention heads. The constraints are two-fold: on the one hand, it encourages each branch to attend to complementary attention regions; on the other hand, it enforces orthogonality among the learned features of different regions in the embedding space. The other is the redundancy among query positions for each attention head. So we simplify the attention block by sparsifying the query positions. Besides, in order to achieve efficient retrieval, we propose an adaptive feature fusion method for dimensional reduction. Compared with the commonly used feature ensemble, our method effectively reduces the dimensionality while keeping the discriminative ability. We demonstrate the effectiveness of our method on MSMT17, Market-1501, DukeMTMC-reID, and CUHK03 datasets.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2957467,
62a0dfc60457ff69c077def93e5027c453d9f8b9,1,1,1,Neighbor similarity and soft-label adaptation for unsupervised cross-dataset person re-identification,"Abstract Most of the existing person re-identification algorithms rely on supervised model learning from a large number of labeled training data per-camera-pair. However, the manual annotations often require expensive human labor, which limits the application of supervised methods for large-scale real-world deployments. To address this problem, we formulate a Neighbor Similarity and Soft-label Adaptation (NSSA) algorithm to transfer the supervised information from source domain to a new unlabeled target dataset. Specifically, we introduce a distance metric on the target domain, which incorporates inner-domain neighbor similarity and inter-domain soft-label adapted from source domain. The unlabeled samples which are close in this metric are considered to share the same pseudo-id and further selected to fine-tune the model. The training is performed iteratively to incorporate more credible sample pairs and incrementally improve the model. Extensive experimental results validate the superiority of our proposed NESSA algorithm, which significantly outperforms the state-of-the-art unsupervised and domain adaptation re-identification methods.",2020,Neurocomputing,,10.1016/j.neucom.2019.12.115,
62ce48ff409cd4d334dac639d04191531e6a25ff,1,0,0,Robust Online Multi-target Visual Tracking using a HISP Filter with Discriminative Deep Appearance Learning,"We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like multiple hypothesis tracking (MHT) and point-process-based approaches like probability hypothesis density (PHD) filter, and it has a linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep convolutional neural networks (CNN) appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal (motion) information that can improve the tracker's performance. In addition, we solve the problem of two or more targets having identical label taking into account the weight propagated with each confirmed hypothesis. Finally, we carry out extensive experiments on Multiple Object Tracking 2016 (MOT16) and 2017 (MOT17) benchmark data sets and find out that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy.",2019,ArXiv,1908.03945,,https://arxiv.org/pdf/1908.03945.pdf
62dd6c64548e345d58cf2d98dd09375ceadb3611,0,1,0,Progressive Cross-Camera Soft-Label Learning for Semi-Supervised Person Re-Identification,"In this paper, we focus on the semi-supervised person re-identification (Re-ID) case, which only has the intra-camera (within-camera) labels but not inter-camera (cross-camera) labels. In real-world applications, these intra-camera labels can be readily captured by tracking algorithms or few manual annotations, when compared with cross-camera labels. In this case, it is very difficult to explore the relationships between cross-camera persons in the training stage due to the lack of cross-camera label information. To deal with this issue, we propose a novel Progressive Cross-camera Soft-label Learning (PCSL) framework for the semi-supervised person Re-ID task, which can generate cross-camera soft-labels and utilize them to optimize the network. Concretely, we calculate an affinity matrix based on person-level features and adapt them to produce the similarities between cross-camera persons (i.e., cross-camera soft-labels). To exploit these soft-labels to train the network, we investigate the weighted cross-entropy loss and the weighted triplet loss from the classification and discrimination perspectives, respectively. Particularly, the proposed framework alternately generates progressive cross-camera soft-labels and gradually improves feature representations in the whole learning course. Extensive experiments on five large-scale benchmark datasets show that PCSL significantly outperforms the state-of-the-art unsupervised methods that employ labeled source domains or the images generated by the GANs-based models. Furthermore, the proposed method even has a competitive performance with respect to deep supervised Re-ID methods.",2020,IEEE Transactions on Circuits and Systems for Video Technology,1908.05669,10.1109/TCSVT.2020.2983600,https://arxiv.org/pdf/1908.05669.pdf
62e407152ca4733ca24269dc66a5be16d2242b66,1,0,0,Pedestrian Re-ID Based on Improved Triplet Loss,"Person re-identification technology is an important foundation for security, pedestrian tracking and other fields, and is the key to building a safe and smart city. In recent years, a large number of researchers have trained pedestrian re-identification networks through triplet loss, especially the triplet loss with batch hard mining (BHTri loss) has greatly improved person re-identification networks on accuracy. However, triplet loss with batch hard mining for an anchor sample only select the hardest positive sample and the hardest negative sample to calculate the loss, ignoring the influence of other samples on network parameters. In response to the above issues, this paper proposes a variant of triplet loss with batch hard mining, which is called adaptive weight triplet loss with batch hard mining. After the training dataset extracts features from the backbone network, in the phase of calculating loss, it takes the average of the sum of the distances between an anchor sample and all corresponding positive samples as a threshold, and both positive samples with an anchor point distance greater than the threshold and negative samples smaller than the threshold are retained, then based on The distance of the anchor sample is given the corresponding sample weight for calculating the loss. Compared with BHTri, mAP have improved 1.79%, 2.04%, and 1.25% respectively on the Market1501, DukeMTMC-reID and CUHK03 datasets, indicating that the proposed algorithm is effective.",2020,,,10.12783/dtetr/mcaee2020/35016,https://pdfs.semanticscholar.org/329f/9cceb5a4c3c8d8c88c61adb3a01e6257f275.pdf
63054ae85026a93b7f2f0da494a8c24c3b1931d4,1,1,0,Pedestrian Image Generation with Target Pose Based on the Improved Cyclegan,"Person image generation mainly contains two parts: The first part is to learn people's appearance information, and the second part is to learn people's posture information. this paper is to generate pedestrian images by using the improved Cycle-Consistent Adversarial Networks (CycleGAN[2]). The network uses a loop structure consisting of two generators G1, G2 and two discriminators D1, D2. In order to preserve the appearance and target pose information of the original pedestrian, we used paired images of the same pedestrian during training and a jump connection is used in the generators G1, G2. The experimental results show that the network structure can generate pedestrian images with original appearance feature information and target pose information.",2019,2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing,,10.1109/ICCWAMTIP47768.2019.9067578,
632329f488e49f5217880abd56da905d044209ca,1,0,0,Enhanced Multiple-Object Tracking Using Delay Processing and Binary-Channel Verification,"Tracking objects over time, i.e., identity (ID) consistency, is important when dealing with multiple object tracking (MOT). Especially in complex scenes with occlusion and interaction of objects this is challenging. Significant improvements in single object tracking (SOT) methods have inspired the introduction of SOT to MOT to improve the robustness, that is, maintaining object identities as long as possible, as well as helping alleviate the limitations from imperfect detections. SOT methods are constantly generalized to capture appearance changes of the object, and designed to efficiently distinguish the object from the background. Hence, simply extending SOT to a MOT scenario, which consists of a complex scene with spatially mixed, occluded, and similar objects, will encounter problems in computational efficiency and drifted results. To address this issue, we propose a binary-channel verification model that deeply excavates the potential of SOT in refining the representation while maintaining the identities of the object. In particular, we construct an integrated model that jointly processes the previous information of existing objects and new incoming detections, by using a unified correlation filter through the whole process to maintain consistency. A delay processing strategy consisting of the three parts—attaching, re-initialization, and re-claiming—is proposed to tackle drifted results caused by occlusion. Avoiding the fuzzy appearance features of complex scenes in MOT, this strategy can improve the ability to distinguish specific objects from each other without contaminating the fragile training space of a single object tracker, which is the main cause of the drift results. We demonstrate the effectiveness of our proposed approach on the MOT17 challenge benchmarks. Our approach shows better overall ID consistency performance in comparison with previous works.",2019,,,10.3390/app9224771,https://pdfs.semanticscholar.org/585b/a125d4e45b3679124a91b4c1f9629940f3bd.pdf
6429e901283ab2b5ce6ae51dc1179475fce0338c,1,0,0,DoT-GNN: Domain-Transferred Graph Neural Network for Group Re-identification,"Most person re-identification (ReID) approaches focus on retrieving a person-of-interest from a database of collected individual images. In addition to the individual ReID task, matching a group of persons across different camera views also plays an important role in surveillance applications. This kind of Group Re-identification (GReID) task is very challenging since we face the obstacles not only from the appearance changes of individuals, but also from the group layout and membership changes. In order to obtain robust representation for the group image, we design a Domain-Transferred Graph Neural Network (DoT-GNN) method. The merits are three aspects: 1) Transferred Style. Due to the lack of training samples, we transfer the labeled ReID dataset to the G-ReID dataset style, and feed the transferred samples to the deep learning model. Taking the superiority of deep learning models, we achieve a discriminative individual feature model. 2) Graph Generation. We treat a group as a graph, where each node denotes the individual feature and each edge represents the relation of a couple of individuals. We propose a graph generation strategy to create sufficient graph samples. 3) Graph Neural Network. Employing the generated graph samples, we train the GNN so as to acquire graph features which are robust to large graph variations. The key to the success of DoT-GNN is that the transferred graph addresses the challenge of the appearance change, while the graph representation in GNN overcomes the challenge of the layout and membership change. Extensive experimental results demonstrate the effectiveness of our approach, outperforming the state-of-the-art method by 1.8% CMC-1 on Road Group dataset and 6.0% CMC-1 on DukeMCMT dataset respectively.",2019,ACM Multimedia,,10.1145/3343031.3351027,
64366fd28a9cecdf3156b1fa84d1613ec8161f1c,1,0,0,Multi-Camera Tracking of Vehicles based on Deep Features Re-ID and Trajectory-Based Camera Link Models,"Due to the exponential growth of traffic camera networks, the need for multi-camera tracking (MCT) for intelligent transportation has received more and more attention. The challenges of MCT include similar vehicle models, significant feature variation in different orientations, color variation of the same car due to lighting conditions, small object sizes and frequent occlusion, as well as the varied resolutions of videos. In this work, we propose an MCT system, which combines single-camera tracking (SCT) and inter-camera tracking (ICT) which includes trajectory-based camera link model and deep feature reidentification.For SCT, we use a TrackletNet Tracker (TNT), which effectively generates the moving trajectories of all detected vehicles by exploiting temporal and appearance information of multiple tracklets that are created by associating bounding boxes of detected vehicles. The tracklets are generated based on CNN feature matching and intersectionover-union (IOU) in every single-camera view. In terms of deep feature re-identification, we exploit the temporal attention model to extract the most discriminant feature of each trajectory. In addition, we propose the trajectorybased camera link models with order constraint to efficiently leverage the spatial and temporal information for ICT. The proposed method is evaluated on CVPR AI City Challenge2019 City Flow dataset, achieving IDF1 70.59%, which outperforms competing methods.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/6436/6fd28a9cecdf3156b1fa84d1613ec8161f1c.pdf
6438a73bade949bd2858ec0e25de52b59be1fdb9,1,0,0,Re-identification for Online Person Tracking by Modeling Space-Time Continuum,"We present a novel approach to multi-person multi-camera tracking based on learning the space-time continuum of a camera network. Some challenges involved in tracking multiple people in real scenarios include a) ensuring reliable continuous association of all persons, and b) accounting for presence of blind-spots or entry/exit points. Most of the existing methods design sophisticated models that require heavy tuning of parameters and it is a nontrivial task for deep learning approaches as they cannot be applied directly to address the above challenges. Here, we deal with the above points in a coherent way by proposing a discriminative spatio-temporal learning approach for tracking based on person re-identification using LSTM networks. This approach is more robust when no a-priori information about the aspect of an individual or the number of individuals is known. The idea is to identify detections as belonging to the same individual by continuous association and recovering from past errors in associating different individuals to a particular trajectory. We exploit LSTM's ability to infuse temporal information to predict the likelihood that new detections belong to the same tracked entity by jointly incorporating visual appearance features and location information. The proposed approach gives a 50% improvement in the error rate compared to the previous state-of-the-art method on the CamNeT dataset and 18% improvement as compared to the baseline approach on DukeMTMC dataset.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00193,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w29/Narayan_Re-Identification_for_Online_CVPR_2018_paper.pdf
643eb627d2e25fa1281616d729d0b2bd0b483546,0,0,1,Temporal Knowledge Propagation for Image-to-Video Person Re-Identification,"In many scenarios of Person Re-identification (Re-ID), the gallery set consists of lots of surveillance videos and the query is just an image, thus Re-ID has to be conducted between image and videos. Compared with videos, still person images lack temporal information. Besides, the information asymmetry between image and video features increases the difficulty in matching images and videos. To solve this problem, we propose a novel Temporal Knowledge Propagation (TKP) method which propagates the temporal knowledge learned by the video representation network to the image representation network. Specifically, given the input videos, we enforce the image representation network to fit the outputs of video representation network in a shared feature space. With back propagation, temporal knowledge can be transferred to enhance the image features and the information asymmetry problem can be alleviated. With additional classification and integrated triplet losses, our model can learn expressive and discriminative image and video features for image-to-video re-identification. Extensive experiments demonstrate the effectiveness of our method and the overall results on two widely used datasets surpass the state-of-the-art methods by a large margin.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.03885,10.1109/ICCV.2019.00974,https://arxiv.org/pdf/1908.03885.pdf
64426bc27c9ec5d6fb50c73c4745cb2e1020b3c7,0,1,0,Exploiting Category Similarity-Based Distributed Labeling for Fine-Grained Visual Classification,"The fine-grained visual classification (FGVC) which aims to distinguish subtle differences among subcategories is an important computer vision task. However, one issue that limits model performance is the problem of diversity within subcategories. To this end, we propose a simple yet effective approach named category similarity-based distributed labeling (CSDL) to tackle this problem. Specifically, we first obtain the feature centers for various subcategories and utilize them to initialize the label distributions. Then we replace the ground-truth labels in a Deep Neural Network (DNN) with the distributed labels to calculate the loss and perform the optimization. Finally, the joint supervision of a softmax loss and a center loss is adopted to update the parameters of the DNN, the deep feature centers, and the distributed labels for learning discriminative deep features. Comprehensive experiments on three publicly available FGVC datasets demonstrate the superiority of our proposed approach.",2020,IEEE Access,,10.1109/ACCESS.2020.3030249,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09220917.pdf
645e1dac0ad9e37b9dfc43768be7e24ef2d04675,1,0,0,Improved Multi-object Tracking Algorithm for Forward Looking Sonar Based on Rotation Estimation,"Multi-object tracking algorithm for forward-looking sonar (FLS) often requires carrier motion information, in order to correct the tracking error caused by the carrier motion. However, it is sometimes difficult to obtain carrier information or synchronize the sonar image with carrier attitude sensor data. Therefore, it is still meaningful to study tracking multiple objects without using carrier motion information. In this paper we focus on improving the performance of multi object tracking without navigation data when the sonar carrier is rotating. The traditional detection-by-tracking framework was improved by rotation estimation. A linear motion model in polar coordinate system was used, and both detection and data association were performed in polar coordinates system. Phase correction was used to estimate the rotation velocity. The velocity was added to the motion model as a control. The improved method was tested on real sonar sequence obtained in conditions which carrier rotated several times. For evaluating the algorithm, we presented a simple approach for object detection. Finally, the results of several tracking metrics show better performance compared with conventional tracking method.",2019,ICIRA,,10.1007/978-3-030-27532-7_15,
64951ddae88d1023b49f9ea74c55c0950d2149a1,0,1,0,Unifying Person and Vehicle Re-Identification,"Person and vehicle re-identification (re-ID) are important challenges for the analysis of the burgeoning collection of urban surveillance videos. To efficiently evaluate such videos, which are populated with both vehicles and pedestrians, it would be preferable to have one unified framework with effective performance across both domains. Unfortunately, due to the contrasting composition of humans and vehicles, no architecture has yet been established that can adequately perform both tasks. We release a Person and Vehicle Unified Data Set (PVUD) comprising of both pedestrians and vehicles from popular existing re-ID data sets, in order to better model the data that we would expect to find in the real world. We exploit the generalisation ability of metric learning to propose a re-ID framework that can learn to re-identify humans and vehicles simultaneously. We design our network, MidTriNet, to harness the power of mid-level features to develop better representations for the re-ID tasks. We help the system to handle mixed data by appending unification terms with additional hard negative and hard positive mining to MidTriNet. We attain comparable accuracy training on PVUD to training on the comprising data sets separately, supporting the system’s generalisation power. To further demonstrate the effectiveness of our framework, we also obtain results better than, or competitive with, the state-of-the-art on each of the Market-1501, CUHK03, VehicleID and VeRi data sets.",2020,IEEE Access,,10.1109/ACCESS.2020.3004092,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09121997.pdf
64993fd5e6fead52722fc6189ab1184482c12a44,1,0,0,Person Re-Identification Using Additive Distance Constraint With Similar Labels Loss,"Despite the promising progress made in recent years, person re-identification (Re-ID) remains a challenging task due to the intra-class variations. Most of the current studies used the traditional Softmax loss for solutions, but its discriminative capability encounters a bottleneck. Therefore, how to improve person Re-ID performance is still a challenging task. To address this problem, we proposed a novel loss function, namely additive distance constraint with similar labels loss (ADCSLL). Specifically, we reformulated the Softmax loss by adding a distance constraint to the ground truth label, based on which similar labels were introduced to enhance the learned features to be much more stable and centralized. Experimental evaluations were conducted on two popular datasets (Market-1501 and DukeMTMC-reID) to examine the effectiveness of our proposed method. The results showed that our proposed ADCSLL was more discriminative than most of the other compared state-of-the-art methods. The rank-1 accuracy and the mAP on Market-1501 were 95.0% and 87.0%, respectively. The numbers were 88.6% and 77.2% on DukeMTMC-reID, respectively.",2020,IEEE Access,,10.1109/ACCESS.2020.3023948,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195852.pdf
649b7ffe51065c4d3579084812c0676335fbc777,0,1,0,Generation of synthetic traffic sign images using conditional generative adversarial networks,"В работе рассматривается метод генерации синтетических обучающих выборок для задачи классификации дорожных знаков. Метод основан на использовании порождающих конкурирующих нейросетей и метрики Васерштейна. Исследуется метод условной генерации изображений, когда на вход порождающей нейросети подается случайный шум и метка класса изображения, которое нужно сгенерировать. Для обучения такой нейросети предлагается использовать перекрестную энтропию в добавление к метрике Васерштейна. Для стабилизации процесса обучения используются веса для обучающей выборки. Экспериментальная оценка метода показывает, что условная порождающая сеть работает лучше, чем простая генерация дорожных знаков по иконке, однако не дотягивает до метода, в котором для каждого класса обучается отдельная порождающая нейросеть.",2018,,,,https://www.graphicon.ru/html/2018/papers/242-246.pdf
649fd37bb27da36dfa99e5bb27b9babaab454506,0,1,0,Metamorphic Testing for Object Detection Systems,"Recent advances in deep neural networks (DNNs) have led to object detectors that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based object detection as a standard computer vision service, object detection systems - similar to traditional software - may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users of these object detection systems. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, principled, systematic methods for testing object detection systems do not yet exist, despite their importance.  To fill this critical gap, we introduce the design and realization of MetaOD, the first metamorphic testing system for object detectors to effectively reveal erroneous detection results by commercial object detectors. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of object detection results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. Evaluated on four commercial object detection services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection defects in these object detectors. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is increased significantly, from an mAP score of 9.3 to an mAP score of 10.5.",2019,ArXiv,1912.12162,,https://arxiv.org/pdf/1912.12162.pdf
64a6d734c8b0977c0a9d1a38bf522dfca1d089c5,0,1,0,Unified Framework for Joint Attribute Classification and Person Re-identification,"Person re-identification (re-id) is an essential task in video surveillance. Existing approaches mainly concentrate on extracting useful appearance features from deep convolutional neural networks. However, they don’t utilize or only partially utilize semantic information such as attributes or person orientation. In this paper, we propose a novel deep neural network framework that greatly improves the accuracy of person re-id and also that of attribute classification. The proposed framework includes two branches, the identity one and the attribute one. The identity branch employs the refined triplet loss and exploits local cues from different regions of the pedestrian body. The attribute branch has an effective attribute predictor containing hierarchical attribute loss functions. After training the identification and attribute classifications, pedestrian representations are derived which contains hierarchical attribute information. The experimental results on DukeMTMC-reID and Matket-1501 datasets validate the effectiveness of the proposed framework in both person re-id and attribute classification. For person re-id, the Rank-1 accuracy is improved by 7.99% and 2.76%, and the mAP is improved by 14.72% and 5.45% on DukeMTMC-reID and Market-1501 datasets respectively. Specifically, it yields 90.95% in accuracy of attribute classification on DukeMTMC-reID, which outperforms the state-of-the-art attribute classification methods by 3.42%.",2018,ICANN,,10.1007/978-3-030-01418-6_63,
64aea3d4f1091921d59c9f5661074b420489d968,1,0,0,Dynamic prioritization of surveillance video data in real-time automated detection systems,"Abstract Automated object detection systems are a key component of modern surveillance applications. These systems rely on computationally expensive computer vision algorithms that perform object detection on visual data recorded by surveillance cameras. Due to the security and safety implications of these systems, this visual data st be processed accurately and in real-time. However, many of the frames that are created by the surveillance cameras may be of low importance, providing little or no useful information to the object detection system. Sub-sampling surveillance data by prioritizing important camera frames can greatly reduce unnecessary computation. Consequently, several works have explored dynamic visual data sub-sampling using various modalities of information (ie. spatial or temporal information) for prioritization. Few works, however, have combined and evaluated different modalities of information together for real-time prioritization of visual surveillance data. This work evaluates several individual and combined prioritization metrics derived from different modalities of information for use with a modern deep learning-based object detection algorithm. Both processing time and object detection rate are measured and used to rank the prioritization metrics. A novel approach that uses the historical detection confidences created by the object detection algorithm was demonstrated to be the best standalone prioritization metric. Additionally, a novel ensemble method that uses a KNN regressor to combine the best of the previously evaluated metrics to create a dynamic prioritization method is presented. This ensemble approach is shown to increase the object detection rate by up to 60% as compared to a static sub-sampling baseline as demonstrated using three publicly available datasets. The increased object detection rate was achieved while meeting the real-time constraints of the automated object detection system.",2020,Expert Syst. Appl.,,10.1016/j.eswa.2020.113672,
64cbf56ef8961decf4a39a4ee9769094ca61176f,1,0,0,Person re-identification based on Res2Net network,"Person re-identification (re-ID) has been gaining in popularity in the research community owing to its numerous applications and growing importance in the surveillance industry. Person re-ID remains challenging due to significant intra-class variations across different cameras. In this paper, we propose a multi-task network that simultaneously computes the identification loss and verification loss. Given a pair of input images, the network predicts the identities of the two input images and whether they belong to the same identity. In order to obtain deeper feature information of pedestrians, we propose to use the latest Res2Net network for feature extraction. Experiments on several large-scale person re-ID benchmark datasets demonstrate the accuracy of our approach. For example, rank-1 accuracies are 82.67% (+0.51) and 92.93% (+0.21) for the DukeMTMC and Market-1501 datasets, respectively. The proposed method shows encouraging improvements compared with state-of-the-art methods.",2019,ArXiv,,,
6570cb2eb1e951266241e105c8af85fc4b406d08,1,0,0,Details for Person Re-identification Baseline,,2020,,,10.1007/978-981-15-0187-6_54,
659c15db147539006b08f238bf3ef5be0a9634f3,0,1,0,"LEARNING QUALITY, AESTHETICS, AND FACIAL ATTRIBUTES FOR IMAGE ANNOTATION","Every day, a large number of digital images are produced by users of social networks, smartphone users, photography professionals, etc. This caused a problem in the management, organization, indexing, and recovery of digital images. In order to ease this problem, several methods have been introduced in the literature to catalog images automatically. These methods are designed to associate images with one or more keywords belonging to a predefined dictionary or to associate images with visual attributes such as, for example, quality, aesthetics, sentiment, memorability, interestingness, and complexity, etc. This thesis investigates the use of deep convolutional neural network for automatic estimation of image quality and image aesthetics. In the last few years, several methods for automatic image quality assessment have been proposed. Most of them have been designed to deal with synthetically distorted images, which by definition do not truly model distortions afflicting real-world images. In this thesis a method for the automatic quality assessment of authentically distorted images is investigated. It shows better performances than state-of-the-art methods both on synthetically and authentically distorted images datasets. Differently from the image quality, which characterizes the perceived quality of the image signal, aesthetics depicts perceived beauty. As first step, the problem of aesthetic quality assessment of real-life general content images has been investigated. The proposed solution outperformed state-of-the-art methods on the largest publicly available dataset. Given that one of the most popular visual contents is the face (e.g. on social networks for photo sharing), aesthetics assessment is, therefore, further investigated on the specific case of portrait images. To this end, in this thesis an algorithm involving the combination of the previously investigated visual attributes (i.e. quality and aesthetics of general content images) and the facial attributes (i.e. smiling, hair style, makeup) description is proposed. Facial attributes description is achieved thanks to two proposed methods. The first algorithm is a robust smile detector (it represents an important visual feature for portrait aesthetics), the second is a multiple-task model designed in order to simultaneously estimate soft biometrics and attributes such as hair colors and styles, types of beards. While the first algorithm outperforms state-of-the-art methods (also respect to highly distorted images), the multi-task model demonstrates comparable performance. Experimental results for the portrait image aesthetic assessment thanks to the use of the proposed algorithm show promising performance on three standard datasets.",2018,,,,https://pdfs.semanticscholar.org/659c/15db147539006b08f238bf3ef5be0a9634f3.pdf
65da5a05fa6b49e8aa11b4acce965a0f439c1716,1,1,0,Human Semantic Parsing for Person Re-identification,"Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that, by employing a simple yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 [48] by ~17% in mAP and ~6% in rank-1, CUHK03 [24] by ~4% in rank-1 and DukeMTMC-reID [50] by ~24% in mAP and ~10% in rank-1.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1804.00216,10.1109/CVPR.2018.00117,https://arxiv.org/pdf/1804.00216.pdf
662b23dab64a1203a114e1f0fa6e1616b29366da,0,1,0,Towards better Validity: Dispersion based Clustering for Unsupervised Person Re-identification,"Person re-identification aims to establish the correct identity correspondences of a person moving through a non-overlapping multi-camera installation. Recent advances based on deep learning models for this task mainly focus on supervised learning scenarios where accurate annotations are assumed to be available for each setup. Annotating large scale datasets for person re-identification is demanding and burdensome, which renders the deployment of such supervised approaches to real-world applications infeasible. Therefore, it is necessary to train models without explicit supervision in an autonomous manner. In this paper, we propose an elegant and practical clustering approach for unsupervised person re-identification based on the cluster validity consideration. Concretely, we explore a fundamental concept in statistics, namely \emph{dispersion}, to achieve a robust clustering criterion. Dispersion reflects the compactness of a cluster when employed at the intra-cluster level and reveals the separation when measured at the inter-cluster level. With this insight, we design a novel Dispersion-based Clustering (DBC) approach which can discover the underlying patterns in data. This approach considers a wider context of sample-level pairwise relationships to achieve a robust cluster affinity assessment which handles the complications may arise due to prevalent imbalanced data distributions. Additionally, our solution can automatically prioritize standalone data points and prevents inferior clustering. Our extensive experimental analysis on image and video re-identification benchmarks demonstrate that our method outperforms the state-of-the-art unsupervised methods by a significant margin. Code is available at this https URL.",2019,ArXiv,1906.01308,,https://arxiv.org/pdf/1906.01308.pdf
662edd90f6e820b42a7365e33bdffa4c82db7373,1,0,0,A No-Reference Image Quality Model for Object Detection on Embedded Cameras,"Automatic﻿ video﻿ analysis﻿ tools﻿ are﻿ an﻿ indispensable﻿ component﻿ in﻿ imaging﻿ applications.﻿ Object﻿ detection,﻿the﻿first﻿and﻿the﻿most﻿important﻿step﻿for﻿automatic﻿video﻿analysis,﻿is﻿implemented﻿in﻿many﻿ embedded﻿cameras.﻿The﻿accuracy﻿of﻿object﻿detection﻿relies﻿on﻿the﻿quality﻿of﻿images﻿that﻿are﻿processed.﻿ This﻿paper﻿proposes﻿a﻿new﻿image﻿quality﻿model﻿for﻿predicting﻿the﻿performance﻿of﻿object﻿detection﻿ on﻿embedded﻿cameras.﻿A﻿video﻿data﻿set﻿is﻿constructed﻿that﻿considers﻿different﻿factors﻿for﻿quality﻿ degradation﻿in﻿the﻿imaging﻿process,﻿such﻿as﻿reduced﻿resolution,﻿noise,﻿and﻿blur.﻿The﻿performances﻿ of﻿commonly﻿used﻿low-complexity﻿object﻿detection﻿algorithms﻿are﻿obtained﻿for﻿the﻿data﻿set.﻿A﻿noreference﻿regression﻿model﻿based﻿on﻿a﻿bagging﻿ensemble﻿of﻿regression﻿trees﻿is﻿built﻿to﻿predict﻿the﻿ accuracy﻿of﻿object﻿detection﻿using﻿observable﻿features﻿in﻿an﻿image.﻿Experimental﻿results﻿show﻿that﻿ the﻿proposed﻿model﻿provides﻿more﻿accurate﻿predictions﻿of﻿image﻿quality﻿for﻿object﻿detection﻿than﻿ commonly﻿known﻿image﻿quality﻿measures. KEywORDS Automatic Video Analysis, Image Distortion, Image Quality Assessment, No-Reference, Object Detection International Journal of Multimedia Data Engineering and Management Volume 10 • Issue 1 • January-March 2019",2019,Int. J. Multim. Data Eng. Manag.,,10.4018/IJMDEM.2019010102,
6647f243029773d7c7d64b84021b13024183b47d,0,1,0,Multi-View and Multi-Information Clustering for Semi-Supervised Person Re-Identification,"Deep learning based methods for person re-identification (re-id) have aroused extensive attention in recent years. However, most works adopt fully-supervised learning, which heavily rely on a large amount of labeled training data. And collecting labeled samples is quite time consuming. To address this problem, we present a semi-supervised framework for person re-id. The key point in this work is to estimate the label of unlabeled data, thus a multi-view and multi-information clustering (MVMIC) method is proposed. First, multi-view feature representation is obtained by two Convolutional Neural Networks, then KNN graphs can be constructed by the feature representation. Finally, multi-information is collected from the KNN graphs to select positive pairs and clustering will be achieving. Experimental results on two large-scale datasets demonstrate the superiority of the proposed method.",2019,2019 International Conference on Electronic Engineering and Informatics (EEI),,10.1109/EEI48997.2019.00051,
6648d1741914bf60cb81e23e63dea42f6fc74e7b,0,1,0,Data Augmentation with Improved Generative Adversarial Networks,"Data augmentation is always a routinely trick in neural network training to improve generalization of a model. However, traditional transformation based methods are domain-specific, the transformations are required to be carefully designed. Recently, Generative Adversarial Networks (GAN) has been proposed to generate new samples which match the real data distribution. But directly using GAN generated samples in data augmentation faces the problems of label absence and uncertain data quality. In this paper, we propose an efficient and robust data augmentation method using GAN generated samples. This method proposes a modified GAN to generate more diverse samples and label them with a soft distribution labeling method. With an improved stochastic gradient descent, all the data are used to train the final classifier. The experiments are conducted on the widely used datasets: MNIST, SVHN and CIFAR-10. Our method empirically obtains promising results, even with few original data.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545894,
66e4f5e354240a022789353798ce92e4ab68e109,1,1,0,Pose-Normalized Image Generation for Person Re-identification,"Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id feature free of the influence of pose variations. We show that this feature is strong on its own and complementary to features learned with the original images. Importantly, under the transfer learning setting, we show that our model generalizes well to any new re-id dataset without the need for collecting any training data for model fine-tuning. The model thus has the potential to make re-id model truly scalable.",2018,ECCV,1712.02225,10.1007/978-3-030-01240-3_40,https://arxiv.org/pdf/1712.02225.pdf
67289bd3b7c9406429c6012eb7292305e50dff0b,1,1,0,Integration Convolutional Neural Network for Person Re-Identification in Camera Networks,"In this paper, we propose a novel deep model named integration convolutional neural network (ICNN) for person re-identification in camera networks, which jointly learns global and local features in a unified framework. To this end, the proposed ICNN simultaneously applies two kinds of loss functions. Specifically, we propose the soft triplet loss to learn global features which automatically adjusts the margin threshold within one batch. The soft triplet loss could alleviate the difficult in tuning parameters and therefore learns discriminative global features. In order to avoid the part misalignment problem, we learn latent local features by conducting local horizontal average pooling on the convolutional maps. Afterward, we implement the identification task on each local feature. We concatenate global and local features using a weighted strategy to present the pedestrian images. We evaluate the proposed ICNN on three large-scale databases. Our method achieves rank-1 accuracy of 92.13% on Market 1501, 61.4% on CUHK03 and 85.3% on DukeMTMC-reID, and the results outperform the state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2852712,
67bfe638f6ba66aded51ab2f6ffe9662bce48d55,1,0,0,LONG WAVE INFRARED IMAGE COLORIZATION FOR PERSON RE-IDENTIFICATION,"Abstract. Person re-identification (ReID) in color and thermal images require matching of the object color and its temperature. While thermal cameras increase the performance of ReID systems during the night-time, identification of corresponding features in the visible and the long-wave infrared range is challenging. The biggest challenge arises from the multimodal relationship between an object’s color and its temperature. Modern ReID methods provide state-of-the-art results in person matching in the visible range. Hence, it is possible to perform multimodal matching by translation of a thermal probe image to the color domain. After that, the synthetic color probe image is matched with images from the real color gallery set. This paper is focused on the development of the ThermalReID multispectral person ReID framework. The framework performs matching in two steps. Firstly, it colorizes the input thermal probe image using a Generative Adversarial Network (GAN). Secondly, it matches images in the color domain using color histograms and MSCR features. We evaluate the ThermalReID framework using RegDB and ThermalWorld datasets. The results of the evaluation are twofold. Firstly, the developed GAN performs realistic colorization of thermal images. Secondly, the ThermalReID framework provides matching of persons in color and thermal images that compete with and surpass the state-of-the-art. The developed ThermalReID framework can be used in video surveillance systems for effective person ReID during the nighttime.",2019,,,10.5194/ISPRS-ARCHIVES-XLII-2-W12-111-2019,https://pdfs.semanticscholar.org/e195/f8d59960ff298c5f3d264b8e0dd16d852048.pdf
689ebdb9f5aae18c8e4d7af0e21e262e522958c3,1,1,0,Improved Res2Net Model for Person re-Identification,"Person re-identification has become a very popular research topic in the computer vision community owing to its numerous applications and growing importance in visual surveillance. Person re-identification remains challenging due to occlusion, illumination and significant intra-class variations across different cameras. In this paper, we propose a multi-task network base on an improved Res2Net model that simultaneously computes the identification loss and verification loss of two pedestrian images. Given a pair of pedestrian images, the system predicts the identities of the two input images and whether they belong to the same identity. In order to obtain deeper feature information of pedestrians, we propose to use the latest Res2Net model for feature extraction of each input image. Experiments on several large-scale person re-identification benchmark datasets demonstrate the accuracy of our approach. For example, rank-1 accuracies are 83.18% (+1.38) and 93.14% (+0.84) for the DukeMTMC and Market-1501 datasets, respectively. The proposed method shows encouraging improvements compared with state-of-the-art methods.",2019,2019 IEEE First International Conference on Cognitive Machine Intelligence (CogMI),1910.04061,10.1109/CogMI48466.2019.00041,https://arxiv.org/pdf/1910.04061.pdf
68acd09240b03f0462ae023cec5f34d03b9e38e9,0,1,0,Person Re-Identification Based on Two-Stream Network With Attention and Pose Features,"Due to posture, blurring, occlusion, and other problems, person re-identification(Re-ID) remains a challenging task at present. In this paper, we combine the advantages of pose estimation and attention mechanism to better solve these problems with better performance, which combines pose and attention with two-stream network. Our proposed method mainly consists of two parts. 1) Spatial Features with Fusion Multi-Layer Features and Attention: the same pedestrian presents different gestures under different camera angles, indicating that the simple spatial information is no longer reliable. Therefore, it becomes important to distinguish view invariant features from multiple semantic levels. As a consequence, we fusion the mid-level and high-level features, and then correlate global information through self-attention. Due to fusion the mid-level and high-level features, semantic information is more abundant, which enables the attention mechanism to better focus on the important areas of the picture; 2) Aggregation Attention Stream and Pose Estimation Stream Features: although self-attention mechanism can automatically pay attention to the important areas of the image, it may pay too much focus on the prominent parts of the body and ignore the edge information of the body. Hence, the guidance of pedestrian posture is needed to make self-attention better able to pay attention to all parts of the body. Finally, we use bilinear pooling aggregates the features of two-stream as the final features. We do not use any data enhancement and re-ranking methods to achieve the $rank=1$ accuracy of 93.3% and 85.5% in Market1501 and DukeMTMC-reID datasets, respectively, which indicates the effectiveness of our method.",2019,IEEE Access,,10.1109/ACCESS.2019.2935116,
6918a94c71c4a4989ce245d8d90f6355e61591a5,1,0,0,The MTA Dataset for Multi Target Multi Camera Pedestrian Tracking by Weighted Distance Aggregation,"Existing multi target multi camera tracking (MTMCT) datasets are small in terms of the number of identities and video length. The creation of new real world datasets is hard as privacy has to be guaranteed and the labeling is tedious. Therefore in the scope of this work a mod for GTA V to record a MTMCT dataset has been developed and used to record a simulated MTMCT dataset called Multi Camera Track Auto (MTA). The MTA dataset contains over 2,800 person identities, 6 cameras and a video length of over 100 minutes per camera. Additionally a MTMCT system has been implemented to provide a baseline for the created dataset. The system’s pipeline consists of stages for person detection, person re-identification, single camera multi target tracking, track distance calculation, and track association. The track distance calculation comprises a weighted aggregation of the following distances: a single camera time constraint, a multi camera time constraint using overlapping camera areas, an appearance feature distance, a homography matching with pairwise camera homographies, and a linear prediction based on the velocity and the time difference of tracks. When using all partial distances, we were able to surpass the results of state-of-the-art single camera trackers by +13% IDF1 score. The MTA dataset, code, and baselines are available at github.com/schuar-iosb/mta-dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00529,
69a7c8bca699ee4100fbe6a83b72459c132a6f10,1,1,0,Aware Person Re-identification across Multiple Resolutions,"Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about highand low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-ofthe-art results on all five datasets that we evaluate on. We then propose two new formulations of the person reID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints.",2018,,,,http://home.bharathh.info/pubs/pdfs/WangCVPR2018b.pdf
69bcbb9a769d3c3a3e02933f4ee6fe7584a481d6,1,0,0,Group Re-Identification via Transferred Representation and Adaptive Fusion,"Group re-identification (G-ReID) is a less-studied task. Its challenges include not only appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also group layout changes and group membership changes which are newly introduced by G-ReID. The key task of G-ReID is to learn representations robust to these changes. To address this issue, we design a Transferred Single and Couple Representation Learning Network (TSCN). The merits are two aspects: 1) Due to the lack of training samples, existing methods exploit unsatisfactory hand-crafted features. To obtain the superiority of deep learning models, we treat a group as multiple persons and transfer the labeled ReID dataset to the G-ReID dataset style to learn the single representation. 2) Taking into account neighborhood relationship in a group, we also propose the couple representation, which maintains more discriminative features in some cases. We also exploit an unsupervised weight learning method to adaptively fuse the results of different views together according to the result pattern. Extensive experimental results demonstrate the effectiveness of our approach, outperforming the state-of-the-art methods on two public datasets.",2019,2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM),,10.1109/BigMM.2019.00-34,
6a182bb830c480591bfbeb8663b054d0a100455f,1,0,0,CNN Backbone PEDAL Random image transformation IPFL Patch generation network Concatenate CNN 1 CNNM Θ LN Sampler Patch sampling grid Feature map Localization network Pull Push,"While discriminative local features have been shown effective in solving the person re-identification problem, they are limited to be trained on fully pairwise labelled data which is expensive to obtain. In this work, we overcome this problem by proposing a patch-based unsupervised learning framework in order to learn discriminative feature from patches instead of the whole images. The patch-based learning leverages similarity between patches to learn a discriminative model. Specifically, we develop a PatchNet to select patches from the feature map and learn discriminative features for these patches. To provide effective guidance for the PatchNet to learn discriminative patch feature on unlabeled datasets, we propose an unsupervised patch-based discriminative feature learning loss. In addition, we design an image-level feature learning loss to leverage all the patch features of the same image to serve as an image-level guidance for the PatchNet. Extensive experiments validate the superiority of our method for unsupervised person re-id. Our code is available at https: //github.com/QizeYang/PAUL.",2019,,,,https://pdfs.semanticscholar.org/6a18/2bb830c480591bfbeb8663b054d0a100455f.pdf
6a4e67cda339bf960c1e1ef4a1cf9b2d6f10c6bc,1,1,0,RGB-IR Person Re-identification by Cross-Modality Similarity Preservation,"Person re-identification (Re-ID) is an important problem in video surveillance for matching pedestrian images across non-overlapping camera views. Currently, most works focus on RGB-based Re-ID. However, RGB images are not well suited to a dark environment; consequently, infrared (IR) imaging becomes necessary for indoor scenes with low lighting and 24-h outdoor scene surveillance systems. In such scenarios, matching needs to be performed between RGB images and IR images, which exhibit different visual characteristics; this cross-modality matching problem is more challenging than RGB-based Re-ID due to the lack of visible colour information in IR images. To address this challenge, we study the RGB-IR cross-modality Re-ID (RGB-IR Re-ID) problem. Rather than applying existing cross-modality matching models that operate under the assumption of identical data distributions between training and testing sets to handle the discrepancy between RGB and IR modalities for Re-ID, we cast learning shared knowledge for cross-modality matching as the problem of cross-modality similarity preservation. We exploit same-modality similarity as the constraint to guide the learning of cross-modality similarity along with the alleviation of modality-specific information, and finally propose a Focal Modality-Aware Similarity-Preserving Loss. To further assist the feature extractor in extracting shared knowledge, we design a modality-gated node as a universal representation of both modality-specific and shared structures for constructing a structure-learnable feature extractor called Modality-Gated Extractor. For validation, we construct a new multi-modality Re-ID dataset, called SYSU-MM01, to enable wider study of this problem. Extensive experiments on this SYSU-MM01 dataset show the effectiveness of our method. Download link of dataset: https://github.com/wuancong/SYSU-MM01 .",2020,International Journal of Computer Vision,,10.1007/s11263-019-01290-1,
6a68e0b74ef3708a1090fde925c172359cf34830,0,1,0,End-to-end training of CNN ensembles for person re-identification,"Abstract We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively.",2020,Pattern Recognit.,2010.01342,10.1016/j.patcog.2020.107319,https://arxiv.org/pdf/2010.01342.pdf
6a76994a0321b33c1189938812ace01ee8c101f2,1,0,0,Retail Traffic-Flow Analysis Using a Fast Multi-object Detection and Tracking System,"Traffic-flow analysis allows to make critical decisions for retail operation management. Common approaches for traffic-flow analysis make use of hardware-based solutions, which have major drawbacks, such as high deployment and maintenance costs. In this work, we address this issue by proposing a Multiple-Object Tracking (MOT) system, following the tracking-by-detection paradigm, that leverages on an ensemble of detectors, each running every f frames. We further measured the performance of our model in the MOT16 Challenge and applied our algorithm to obtain heatmaps and paths for customers and shopping carts in a retail store from CCTV cameras.",2019,,,10.1007/978-3-030-36211-9_3,
6a82121c21c8d8ccffbe2cf4d7e4c1b7e15d617d,1,1,0,Deep Gabor convolution network for person re-identification,"Abstract Person re-identification is an import problem in computer vision fields and more and more deep neural network models have been developed for representation learning in this task due to their good performance. However, compared with hand-crafted feature representations, deep learned features cannot not be interpreted easily. To meet this demand, motivated by the Gabor filters’ good interpretability and the deep neural network models’ reliable learning ability, we propose a new convolution module for deep neural networks based on Gabor function (Gabor convolution). Compared with classical convolution module, every parameter in the proposed Gabor convolution kernel has a specific meaning while classical one has not. The Gabor convolution module has a good texture representation ability and is effective when it is embedded in the low layers of a network. Besides, in order to make the proposed Gabor module meaningful, a new loss function designed for this module is proposed as a regularizer of total loss function. By embedding the Gabor convolution module to the Resnet-50 network, we show that it has a good representation learning ability for person re-identification. And experiments on three widely used person re-identification datasets show favorable results compared with the state-of-the-arts.",2020,Neurocomputing,,10.1016/j.neucom.2019.10.083,http://crabwq.github.io/pdf/2020%20Deep%20Gabor%20Convolution%20Network%20for%20Person%20Re-identification.pdf
6ae6f25bf780e3f5603f131f7695de186c5a0d35,0,1,0,Progressive Transfer Learning for Person Re-identification,"Model fine-tuning is a widely used transfer learning approach in person Re-identification (ReID) applications, which fine-tuning a pre-trained feature extraction model into the target scenario instead of training a model from scratch. It is challenging due to the significant variations inside the target scenario, e.g., different camera viewpoint, illumination changes, and occlusion. These variations result in a gap between the distribution of each mini-batch and the distribution of the whole dataset when using mini-batch training. In this paper, we study model fine-tuning from the perspective of the aggregation and utilization of the global information of the dataset when using mini-batch training. Specifically, we introduce a novel network structure called Batch-related Convolutional Cell (BConv-Cell), which progressively collects the global information of the dataset into a latent state and uses this latent state to rectify the extracted feature. Based on BConv-Cells, we further proposed the Progressive Transfer Learning (PTL) method to facilitate the model fine-tuning process by joint training the BConv-Cells and the pretrained ReID model. Empirical experiments show that our proposal can improve the performance of the ReID model greatly on MSMT17, Market- 1501, CUHK03 and DukeMTMC-reID datasets. The code will be released later on at https://github.com/ZJULearning/PTL",2019,IJCAI,1908.02492,10.24963/ijcai.2019/586,https://www.ijcai.org/proceedings/2019/0586.pdf
6ae84b0adc5054db47814a5c475f732e6c1c9098,1,0,0,PathTrack: Fast Trajectory Annotation with Path Supervision,"Progress in Multiple Object Tracking (MOT) has been historically limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. In our novel path supervision the annotator loosely follows the object with the cursor while watching the video, providing a path annotation for each object in the sequence. Our approach is able to turn such weak annotations into dense box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art, in a fraction of the time. We further validate our approach by crowdsourcing the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences. Tracking approaches can benefit training on such large-scale datasets, as did object recognition. We prove this by re-training an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. On the latter, we improve the top-performing tracker (NOMT) dropping the number of ID Switches by 18% and fragments by 5%.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1703.02437,10.1109/ICCV.2017.40,https://www.research-collection.ethz.ch/bitstream/20.500.11850/247951/1/PathTrack.pdf
6b204ad5602e7d07c653e09fa1ea3622abadae84,0,1,0,Structure alignment of attributes and visual features for cross-dataset person re-identification,"Abstract In cross-dataset person re-identification, it is challenging to address the problem of domain shift between training and test data. Although unsupervised domain adaptation methods have been developed, the performance is still much weaker compared with that of supervised methods because these models cannot follow a supervised optimization in unlabeled target domains. To address this problem, a transductive structure alignment-based self-reconstruction dictionary learning approach is proposed in this paper for cross-dataset person re-identification (PRID). Specifically, visual-attribute embedding is first learned to achieve knowledge transfer from the source domain to the target domain. In this process, visual-attribute structures are aligned via class prototype dictionaries to promote the discrimination of predicted semantic attributes by exploiting structure information between the visual feature and class prototype. Moreover, to mitigate domain shift, domain-invariant visual-attribute self-reconstruction is integrated into our dictionary learning framework. An identifier is then constructed by integrating the discriminativeness of attribute and compatibility matrix shared both source domain and target domain. Finally, the pre-learned model is tuned by selecting samples from the target domain which are not labeled but assigned pseudo-labels. Extensive experimental results on benchmark datasets show that our approach outperforms several state-of-the-art approaches.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107414,
6b401f50952883c4fe0fb875cdde83b6ca6fe279,1,0,0,A Multi-level Equilibrium Clustering Approach for Unsupervised Person Re-identification,,2020,,,10.1007/978-3-030-60636-7_27,
6ba425056604beb9201d58e542fad3c9110feadd,1,0,0,Bilinear Attention Networks for Person Retrieval Supplementary Material,"Here, we show an additional ablation study to verify the effectiveness of Bi-attention with AiA on the DukeMTMCreID [3] and the MSMT17 [4] datasets in a single query setting. From this additional study, we can draw the same conclusions as in our main paper. Effect of Bilinear Attention. We evaluate the effect of Bi-attention on the feature extractors, and the results are shown in Table 1. The results on both datasets demonstrate that: Bi-attention improves the performance of both scenarios Fa and Fa + Fp, similar to the observations noticed on the Market-1501 and CUHK03 datasets.",2019,,,,https://pdfs.semanticscholar.org/6ba4/25056604beb9201d58e542fad3c9110feadd.pdf
6c6dfe2286708a11c2f1e92094e520a5661f0484,1,0,0,Re-identifying Pedestrians via Part Based Method,"Person re-identification (Re-id) has received more and more attention due to its wide applications and various approaches were proposed to solve the challenging task. In this paper, we adopt part-based model as the main method to perform person Re-id on the self-made dataset. In order to improve accuracy, transfer learning is applied in both a direct and indirect way by leveraging original public dataset Market-1501 and its transferred counterpart respectively. The transferred dataset is obtained by making the pedestrians in the original public dataset show similar styles to those of the self-made dataset while keeping their identities unchanged. Experimental results demonstrate the effectiveness of transfer learning and the superiority of applying transfer learning indirectly against applying transfer learning directly.",2020,2020 39th Chinese Control Conference (CCC),,10.23919/CCC50068.2020.9189140,
6c95bfbb5af83bfacff06596ef0cebcff9b80f67,1,0,0,Multiple Feature Subspaces Based Multi-Task Learning for Person Re-Identification,"Person re-identification(Re-ID) is an important task with the purpose of retrieving the person with the same identity across multiple cameras. Multi-task learning(MTL) is a useful learning paradigm in machine learning, and the aim is to leverage useful information included in multiple related tasks to help improve the generalization performance of all the tasks. So the multi-task learning may be an effective way to improve the performance of a model. Existing works in person Re-ID using multi-task learning often require the use of additional information or the design of complex multiple tasks. This paper presents a new simple multi-task learning method for Person Re-ID, which is based on multiple feature subspaces. The idea of this method is to regard the training of each feature subspace as a task in multi-task learning, and we can improve the performance by jointly training multiple feature subspaces. Our method is simple and effective compared to other methods because the feature subspace is easy to obtain. Experiments show that the proposed method can significantly improve performance for person Re-ID.",2019,2019 Chinese Automation Congress (CAC),,10.1109/CAC48633.2019.8996896,
6cc00ab2e72dc6d5e64ed80d9506f0b59ec73d5f,1,1,0,In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast Approximated Triplet Loss and Label Distillation,"The comparative losses (typically, triplet loss) are appealing choices for learning person re-identification (ReID) features. However, the triplet loss is computationally much more expensive than the (practically more popular) classification loss, limiting their wider usage in massive datasets. Moreover, the abundance of label noise and outliers in ReID datasets may also put the margin-based loss in jeopardy. This work addresses the above two shortcomings of triplet loss, extending its effectiveness to large-scale ReID datasets with potentially noisy labels. We propose a fast-approximated triplet (FAT) loss, which provably converts the point-wise triplet loss into its upper bound form, consisting of a point-to-set loss term plus cluster compactness regularization. It preserves the effectiveness of triplet loss, while leading to linear complexity to the training set size. A label distillation strategy is further designed to learn refined soft-labels in place of the potentially noisy labels, from only an identified subset of confident examples, through teacher-student networks. We conduct extensive experiments on three most popular ReID benchmarks (Market-1501, DukeMTMC-reID, and MSMT17), and demonstrate that FAT loss with distilled labels lead to ReID features with remarkable accuracy, efficiency, robustness, and direct transferability to unseen datasets.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1912.07863,10.1109/CVPRW50498.2020.00185,https://arxiv.org/pdf/1912.07863.pdf
6cdbe49de9f897530c34cd63e9ffa81d7f09b42a,1,0,0,HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking,"Multi-object tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, higher order tracking accuracy (HOTA), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.",2020,ArXiv,2009.07736,10.1007/s11263-020-01375-2,https://arxiv.org/pdf/2009.07736.pdf
6d05639926fdb48dee93a2dadd15d3ea70ad6f19,1,0,0,CAN: Composite Appearance Network for Person Tracking and How to Model Errors in a Tracking System,"Tracking multiple people across multiple cameras is an open problem. It is typically divided into two tasks: (i) single-camera tracking (SCT) - identify trajectories in the same scene, and (ii) inter-camera tracking (ICT) - identify trajectories across cameras for real surveillance scenes. Many methods cater to SCT, while ICT still remains a challenge. In this paper, we propose a tracking method which uses motion cues and a feature aggregation network for template-based person re-identification by incorporating metadata such as person bounding box and camera information. We present a feature aggregation architecture called Composite Appearance Network (CAN) to address the above problem. The key structure of this architecture is called EvalNet that pays attention to each feature vector and learns to weight them based on gradients it receives for the overall template for optimal re-identification performance. We demonstrate the efficiency of our approach with experiments on the challenging multi-camera tracking dataset, DukeMTMC. We also survey existing tracking measures and present an online error metric called ""Inference Error"" (IE) that provides a better estimate of tracking/re-identification error, by treating SCT and ICT errors uniformly.",2018,,,,https://pdfs.semanticscholar.org/e27c/86e51a4c821f6e0f8992363417349a995820.pdf
6d7c58885e2717420e82388876b16a5c4aa446b5,0,1,0,Illumination-Adaptive Person Re-identification,"Most person re-identification (ReID) approaches assume that person images are captured under relatively similar illumination conditions. In reality, long-term person retrieval is common and person images are captured under different illumination conditions at different times across a day. In this situation, the performances of existing ReID models often degrade dramatically. This paper addresses the ReID problem with illumination variations and names it as Illumination-Adaptive Person Re-identification (IAReID). We propose an Illumination-Identity Disentanglement (IID) network to separate different scales of illuminations apart, while preserving individuals’ identity information. To demonstrate the illumination issue and to evaluate our network, we construct two large-scale simulated datasets with a wide range of illumination variations. Experimental results on the simulated datasets and real-world images demonstrate the effectiveness of the proposed framework.",2019,IEEE Transactions on Multimedia,1905.04525,10.1109/TMM.2020.2969782,https://arxiv.org/pdf/1905.04525.pdf
6da06aa182965a373ae5f433ad1db4862983d33f,0,1,0,Deep Self-Paced Learning for Semi-Supervised Person Re-Identification Using Multi-View Self-Paced Clustering,"Semi-supervised person re-identification (Re-ID) is an extension of the existing popular Re-ID research, which only uses a small portion of labeled data, while the majority of the training samples are unlabeled. This paper approaches the problem by constructing a set of heterogeneous convolutional neural networks (CNNs) fine-tuned by utilizing the labeled training samples, and then propagating the labels to the unlabeled portion for further fine-tuning the overall system in a self-paced manner. In this work, a novel self-paced multi-view clustering is presented to generate pseudo labels for unlabeled training samples, which combines multiple heterogeneous CNNs features to cluster. In our clustering method, we introduce a self-paced regularizer to select reliable samples for fine-tuning each CNNs by minimizing ranking loss and identification loss. Specifically, we select a small portion of unlabeled training data when multiple CNNs are weak. With CNNs become stronger, more and more unlabeled samples are selected. Pseudo label estimation and CNNs training are improved simultaneously, which optimize alternatively until all the unlabeled training samples are selected. In our framework, both the optimization of multiple CNNs training and multi-view clustering on unlabeled training samples are self-paced optimizing procedure. Extensive experiments have been conducted on two large-scale Re-ID datasets to demonstrate the superiority of the proposed method.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803290,
6dc2c37a62ad509649bf20487114f0c805deb794,0,1,0,Rethinking the Distribution Gap of Person Re-identification with Camera-Based Batch Normalization,"The fundamental difficulty in person re-identification (ReID) lies in learning the correspondence among individual cameras. It strongly demands costly inter-camera annotations, yet the trained models are not guaranteed to transfer well to previously unseen cameras. These problems significantly limit the application of ReID. This paper rethinks the working mechanism of conventional ReID approaches and puts forward a new solution. With an effective operator named Camera-based Batch Normalization (CBN), we force the image data of all cameras to fall onto the same subspace, so that the distribution gap between any camera pair is largely shrunk. This alignment brings two benefits. First, the trained model enjoys better abilities to generalize across scenarios with unseen cameras as well as transfer across multiple training sets. Second, we can rely on intra-camera annotations, which have been undervalued before due to the lack of cross-camera information, to achieve competitive ReID performance. Experiments on a wide range of ReID tasks demonstrate the effectiveness of our approach. The code is available at this https URL.",2020,ECCV,2001.0868,10.1007/978-3-030-58610-2_9,https://arxiv.org/pdf/2001.08680.pdf
6dce5866ebc46355a35b8667c1e04a4790c2289b,1,0,0,Extensions of dominant sets and their applications in computer vision,"Many problems in computer vision can be formulated as a clustering problem, a problem that aims to organize a collection of data objects into groups or clusters, such that objects within a cluster are more “similar” to each other than they are to objects in the other groups. Assuming the feature-based representation, a computer vision problem can be formulated as follows: Given a set of data points in a ’d’ dimensional space, find the best partition of the space which gives us meaningful groups. The points in the representative metric space correspond to the feature vectors extracted from the object with their distances reflecting the dissimilarity relations. On the other hand, objects could also be described indirectly by their respective similarity relations, an approach which is more natural than feature-based technique as there are numerous application domains where it is not possible to find satisfactory features, but it is more natural to provide a measure of similarity. This work proposes similarity based data clustering framework, based on extensions of the dominant sets framework using theories and mathematical tools inherited from graph theory, optimization theory and game theory, that could be adapted “flexibly” in a wide range of vision applications, thereby combining the research domain of computer vision and that of machine learning. In our system, clusters are in one-to-one correspondence with Evolutionary Stable Strategies (ESS) a classic notion of equilibrium in evolutionary game theory field of a so-called “clustering game”. The clustering game is a non-cooperative game between two-players, where the objects to cluster form the set of strategies, while the affinity matrix provides the players’ payoffs. The dominant sets framework, a well-known graph-theoretic notion of a cluster which generalizes the concept of a maximal clique to edge-weighted graphs, has proven itself to be relevant in many computer vision problems such as action recognition, image segmentation, tracking, group detection and others. Its regularized counterpart, determining the global shape of the energy landscape as well as the location of its extrema, is able to organize the data to be clustered in a hierarchical manner. It generalizes the dominant sets framework in that putting the regularization parameter to zero results local solutions that are in one-to-one correspondence with dominant sets. In this thesis we propose constrained dominant sets, parameterized family of quadratic programs that generalizes both formulations, the dominant sets framework and its regularized counterpart, in that here, only a subset of elements in the main diagonal is allowed to take the parameter, the other ones being set to zero. In particular, we show that by properly controlling a regularization parameter which determines the structure and the scale of the underlying problem, we are in a position to extract groups of dominant sets clusters which are constrained to contain user-specified elements. We provide bounds that allow us to control this process, which are based on the spectral properties of certain submatrices of the original affinity matrix. Thanks to the many sensors, which generate a large amount of data every day, distributed in the society, there is a large amount of data for training and testing many computer vision systems. However, real data collected through those sensors is contaminated by outliers, and many computer vision tasks involve processing the available large amounts of data without any assumption on the existence of outliers. Recently, very few computer vision systems have shown that considering presence of outliers while solving computer vision problems help boosting the state-of-the-art results. However, most of the systems either try just to detect outliers from the computer vision datasets or solve their problems by detecting and rejecting outliers before applying the method on the dataset. Our proposed work is robust to outliers, and since we also believe that it is important for clustering methods to detect data consensuses and isolated outliers simultaneously in a clustering task, this thesis proposes a method for simultaneous clustering and outliers detection. Evolutionary game theory offers a whole class of simple dynamical systems to solve quadratic (constrained) optimization problems like ours. It envisages a scenario in which pairs of players are repeatedly drawn at random from a large population of individuals to play a symmetric two player game. One of the best known class of game dynamics to extract (constrained) dominant set from a graph is the so-called replicator dynamics whose computational complexity is quadratic per step which makes it handicapped for large-scale applications. In this thesis, we propose a fast algorithm, based on dynamics from evolutionary game theory, which is efficient and scalable to large-scale real-world applications. In general, the clustering algorithm proposed in this thesis has many interesting properties, suitable for solving many computer vision problems, such as: it does clustering while obliterating outliers in simultaneous fashion, it doesn’t need any a prior knowledge on the number of clusters, able to deal with compact clusters and with situations involving arbitrarily-shaped clusters in a context of heavy background noise, able to extract clusters which are constrained to contain user-specified elements, does not have any assumptions with the structure of the affinity matrix, it is fast and scalable to large scale problems, and others. This increased flexibility leads to an efficient method that we apply on different computer vision problems: interactive image segmentation, co-segmentation, multi-target multi-camera tracking in multiple non overlapping cameras, to extract dense neighbors which we have used to constrain the diffusion process locally, geo-localization and for person reidentification.",2018,,,,https://pdfs.semanticscholar.org/6dce/5866ebc46355a35b8667c1e04a4790c2289b.pdf
6dfd006a630c1b94941196918718bf0f53d11704,1,1,0,Deep Attention Aware Feature Learning for Person Re-Identification,"Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification. However, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the attention learning as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attentions have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) makes the feature maps obtained by backbone focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) makes the extracted features be decoupled into several groups and be separately responsible for different body parts (i.e., keypoints), thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet and Bag of Tricks) and observed significant performance improvement on five widely used datasets.",2020,ArXiv,2003.00517,,https://arxiv.org/pdf/2003.00517.pdf
6e3cc2cdfa44501a04bcea5afd246b63e9829d37,0,1,0,HMM-Based Person Re-identification in Large-Scale Open Scenario,"This paper aims to tackle person re-identification (person re-ID) in large-scale open scenario, which differs from the conventional person re-ID tasks but is significant for some real suspect investigation cases. In the large-scale open scenario, the image background and person appearance may change immensely. There are a large number of irrelevant pedestrians appearing in the urban surveillance systems, some of which may have very similar appearance with the target person. Existing methods utilize only surveillance video information, which can not solve the problem well due to above challenges. In this paper, we explore that pedestrians’ paths from multiple spaces (such as surveillance space and geospatial space) are matched due to temporal-spatial consistency. Moreover, people have their unique behavior path due to the differences of individual behavioral. Inspired by these two observations, we propose to use the association relationship of paths from surveillance space and geospatial space to solve the person re-ID in large-scale open scenario. A Hidden Markov Model based Path Association(HMM-PA) framework is presented to jointly analyze image path and geospatial path. In addition, according to our research scenario, we manually annotate path description on two large-scale public re-ID datasets, termed as Duke-PDD and Market-PDD. Comprehensive experiments on these two datasets show proposed HMM-PA outperforms the state-of-art methods.",2020,MMM,,10.1007/978-3-030-37731-1_66,
6e8ec0d79942014b1d93c11a13816c1dea194bf1,1,0,0,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline,"The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. At the same time, we propose the label smooth regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. In order to verify the effectiveness of the proposed method, we conducted experiments on three large datasets Market-1501, CUHK03 and DukeMTMC-reID. The experimental results show that the sample data generated by GAN effectively improves the discriminative ability of learning CNN embedding, which achieved improvements in the baseline CNN rank-1 accuracy rate of + 4.37%, + 1.6% and + 2.46%, respectively.",2019,ICCTA 2019,,10.1145/3323933.3324091,
6eef1856310619d03e65ac1753b039c44bf97bd0,0,1,0,Semantics-Aligned Representation Learning for Person Re-identification,"Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for re-ID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID. Code for our proposed method is available at: this https URL.",2020,AAAI,1905.13143,10.1609/AAAI.V34I07.6775,https://arxiv.org/pdf/1905.13143.pdf
6f3b8808e6e652ceef3b012a16a966c0df7d9bdd,0,1,0,Survey on person re-identification based on deep learning,"Person re-identification (Re-ID) is a fundamental subject in the field of the computer vision technologies. The traditional methods of person Re-ID have difficulty in solving the problems of person illumination, occlusion and attitude change under complex background. Meanwhile, the introduction of deep learning opens a new way of person Re-ID research and becomes a hot spot in this field. This study reviews the traditional methods of person Re-ID, then the authors focus on the related papers about different person Re-ID frameworks on the basis of deep learning, and discusses their advantages and disadvantages. Finally, they propose the direction of further research, especially the prospect of person Re-ID methods based on deep learning.",2018,CAAI Trans. Intell. Technol.,,10.1049/TRIT.2018.1001,
6f80a3d1301538c8f190bc1376e343e06627e3be,0,1,0,View Confusion Feature Learning for Person Re-Identification,"Person re-identification is an important task in video surveillance that aims to associate people across camera views at different locations and time. View variability is always a challenging problem seriously degrading person re-identification performance. Most of the existing methods either focus on how to learn view invariant feature or how to combine viewwise features. In this paper, we mainly focus on how to learn view-independent features by getting rid of view specific information through a view confusion learning mechanism. Specifically, we propose an end-to-end trainable framework, called View Confusion Feature Learning (VCFL), for person Re-ID across cameras. To the best of our knowledge, VCFL is originally proposed to learn view-independent identity-wise features, and it’s a kind of combination of view-generic and view-specific methods. Furthermore, we extract sift-guided features by using bag-of-words model to help supervise the training of deep networks and enhance the view invariance of features. In experiments, our approach is validated on three benchmark datasets including CUHK01, CUHK03, and MARKET1501, which show the superiority of the proposed method over several state-of-the-art approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1910.03849,10.1109/ICCV.2019.00674,https://arxiv.org/pdf/1910.03849.pdf
6f8e829996a5d9627a42db5c9cc6ac898def0882,1,0,0,Simulation Trust and the Internet of Things,"The urban environment is becoming increasingly more connected and complex. In the coming decades, we will be surrounded by billions of sensors, devices, and machines, the Internet of Things (IoT). As the world becomes more connected, we will become dependent on machines and simulation to make decisions on our behalf. When simulation systems use data from sensors, devices and machines (i.e., things) to make decisions, they need to learn how to trust that data, as well as the things they are interacting with. As embedded simulation becomes more commonplace in IoT and smart city applications, it is essential that decision makers are able to trust the simulation systems making decisions on their behalf. This paper looks at trust from an IoT perspective, describing a set of research projects conducted that span multiple dimensions of trust, and discusses whether these concepts of trust apply to simulation.",2019,2019 Winter Simulation Conference (WSC),,10.1109/WSC40007.2019.9004912,
6f920c33b222085a53ec43b30df66fb49fa9be21,1,0,0,Multi-object tracking with self-supervised associating network,"Multi-Object Tracking (MOT) is the task that has a lot of potential for development, and there are still many problems to be solved. In the traditional tracking by detection paradigm, There has been a lot of work on feature based object re-identification methods. However, this method has a lack of training data problem. For labeling multi-object tracking dataset, every detection in a video sequence need its location and IDs. Since assigning consecutive IDs to each detection in every sequence is a very labor-intensive task, current multi-object tracking dataset is not sufficient enough to train re-identification network. So in this paper, we propose a novel self-supervised learning method using a lot of short videos which has no human labeling, and improve the tracking performance through the re-identification network trained in the self-supervised manner to solve the lack of training data problem. Despite the re-identification network is trained in a self-supervised manner, it achieves the state-of-the-art performance of MOTA 62.0\% and IDF1 62.6\% on the MOT17 test benchmark. Furthermore, the performance is improved as much as learned with a large amount of data, it shows the potential of self-supervised method.",2020,ArXiv,2010.13424,,https://arxiv.org/pdf/2010.13424.pdf
6fcc4057de946c75c872460049f038ae53880e18,1,1,0,Deep Fusion Feature Presentations for Nonaligned Person Re-Identification,"Person re-identification aims to retrieve the pedestrian across different cameras. It is still a challenging task for the intelligent visual surveillance system because of similar appearances, camera shooting angles, scene illumination, and pedestrian pose. In this paper, we propose a novel two-stream network named spatial segmentation network that learns both the global and local features in a unified framework for nonaligned person re-identification. One stream focuses on spatial feature learning using global adaptive average pooling in deep convolutional neural networks. Another stream is utilized to learn the fine local features by adopting horizontal average pooling without division that depends on the pose predictor. To assess the importance ranking of all features, we also obtain the performance of every part feature and global features. Our evaluation of the proposed method on Market-1501 acquires 94.51% Rank-1 and 90.78% mAP, that on DukeMTMC-re-ID acquires 87.52% Rank-1 and 84.82% mAP, and that on CHUK03-detected acquires 69.71% Rank-1 and 71.67% mAP; these findings verify the state-of-the-art performance of the proposed method.",2019,IEEE Access,,10.1109/ACCESS.2019.2920426,
6fd4ea15000ce1ca36c5263ff23b28c57a9b73cd,0,1,0,FUsing Global and Semantic-Part Features with Multiple Granularities for Person Re-Identification,"A multiple granularities method for person re-identification (re-ID) is proposed in this paper, which fuses global and semantic-part representations. A prior guided human parsing method is employed to parse a human body into precise basic semantic parts from low-resolution images, and multiple granularities are generated by recombining the adjacent basic semantic parts. Then, convolutional neural networks that seam-lessly unify the Softmax and TriHard losses are proposed to learn and fuse the global-level and the part-level features in different granularities. The proposed method not only extracts precise part-level features, but also incorporates gradual cues between part-level and global-level features to boost a high performance of person re-ID. Extensive experimental results show our proposed SPMG model achieves state-of-the-art performance on three common datasets .",2019,"2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",,10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00206,
7002d8c61be9f1ea210f88059df6955c88db62b7,1,1,0,Person Transfer GAN to Bridge Domain Gap for Person Re-identification,"Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT171 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.08565,10.1109/CVPR.2018.00016,https://arxiv.org/pdf/1711.08565.pdf
70ce1a17f257320fc718d61964b21e7aeabd8cd5,1,1,0,Person re-identification with fusion of hand-crafted and deep pose-based body region features,"Person re-identification (re-ID) aims to accurately re- trieve a person from a large-scale database of images cap- tured across multiple cameras. Existing works learn deep representations using a large training subset of unique per- sons. However, identifying unseen persons is critical for a good re-ID algorithm. Moreover, the misalignment be- tween person crops to detection errors or pose variations leads to poor feature matching. In this work, we present a fusion of handcrafted features and deep feature representa- tion learned using multiple body parts to complement the global body features that achieves high performance on un- seen test images. Pose information is used to detect body regions that are passed through Convolutional Neural Net- works (CNN) to guide feature learning. Finally, a metric learning step enables robust distance matching on a dis- criminative subspace. Experimental results on 4 popular re-ID benchmark datasets namely VIPer, DukeMTMC-reID, Market-1501 and CUHK03 show that the proposed method achieves state-of-the-art performance in image-based per- son re-identification.",2018,ArXiv,1803.1063,,https://arxiv.org/pdf/1803.10630.pdf
71179332cef9a5d0b9cabfd5ea8bb4ac49383ee9,0,0,1,Gait recognition for person re-identification,,2020,,,10.1007/s11227-020-03409-5,https://link.springer.com/content/pdf/10.1007/s11227-020-03409-5.pdf
711cb6e6fd1ee0eca793842834547388565db634,1,1,0,Person re-identification based on multi-appearance model,"Person re-identification plays important roles in many practical applications. Due to various human poses, complex backgrounds and similarity of person clothes, person re-identification is still a challenging task. In this paper, we mainly focus on the robust and discriminative appearance feature representation and proposed a novel multi-appearance method for person re-identification. First, we proposed a deep feature fusion method and get the multi-appearance feature by combining two Convolutional Neural Networks. Then, in order to further enhance the representation of the appearance feature, the multi-part model was constructed by combining the whole body and the six body parts. Additionally, we optimized the feature extraction process by adding a pooling layer. Comprehensive and comparative experiments with the state-of-the-art methods over publicly available datasets demonstrated that the proposed method can get promising results.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-08927-1,
716e84b90ff1357810c0c2b04c0df9a1f8953e68,0,1,0,Joint deep learning of angular loss and hard sample mining for person re-identification,,2020,J. Intell. Fuzzy Syst.,,10.3233/jifs-179416,
718d5c5ac1146b9ecb6f3d1566bb83f173d2223e,0,1,0,Local Heterogeneous Features for Person Re-Identification in Harsh Environments,"Local features could learn semantic information for pedestrian images and they are very important for person re-identification (Re-ID) in harsh environments. However, most approaches only optimize one kind of local feature, which results in incomplete local features. In this paper, we propose Local Heterogeneous Features (LHF) to extract discriminative local features from three aspects. To this end, we utilize three kinds of losses to learn three kinds of local features, i.e., local discriminative features, local relative features, local compact features. As for local discriminative features, we split the attention maps into three horizontal sub-regions and perform the classification operation. Then, we divide the attention maps into two horizontal sub-regions, and we synchronously apply the triplet loss and center loss to learn local relative features and local compact features. Finally, we utilize local discriminative features to represent pedestrian. We evaluate LHF on public person Re-ID datasets and prove LHF is meaningful for local feature learning.",2020,IEEE Access,,10.1109/ACCESS.2020.2991838,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09084088.pdf
718e77bde7d3bd69f5d68ae0bf33eea069457b6d,1,0,0,Cross-Correlated Attention Networks for Person Re-Identification,"Abstract Deep neural networks need to make robust inference in the presence of occlusion, background clutter, pose and viewpoint variations -to name a few- when the task of person re-identification is considered. Attention mechanisms have recently proven to be successful in handling the aforementioned challenges to some degree. However previous designs fail to capture inherent inter-dependencies between the attended features; leading to restricted interactions between the attention blocks. In this paper, we propose a new attention module called Cross-Correlated Attention (CCA); which aims to overcome such limitations by maximizing the information gain between different attended regions. Moreover, we also propose a novel deep network that makes use of different attention mechanisms to learn robust and discriminative representations of person images. The resulting model is called the Cross-Correlated Attention Network (CCAN). Extensive experiments demonstrate that the CCAN comfortably outperforms current state-of-the-art algorithms by a tangible margin. Modeling the inherentspatial relations between different attended regions within the deep architecture. Joint end-to-end cross correlated attention and representational learning. State-of-the-art results in terms of mAP and Rank-1 accuracies across several challenging datasets.",2020,Image Vis. Comput.,2006.09597,10.1016/j.imavis.2020.103931,https://arxiv.org/pdf/2006.09597.pdf
7196aa4490a1d6fd5caac144acd4710cb17046b5,1,1,0,Unsupervised Person Re-Identification With Iterative Self-Supervised Domain Adaptation,"In real applications, person re-identification (re-id) is an inherently domain adaptive computer vision task which often requires the model trained on a group of people to perform well on an unlabeled dataset consisting of another group of pedestrians without supervised fine-tuning. Furthermore, there are typically a large number of classes (people) with small number of samples belonging to each class. Based on the characteristics of person re-id and general assumptions related to domain adaptation, we put forward a novel algorithm for cross-dataset person re-id. Our idea is simple yet effective: first, we preprocess the source dataset with style transfer GAN and train a baseline on it in a supervised learning manner, then we assign pseudo labels to unlabeled samples in target dataset based on the model trained on labeled source dataset; finally, we train on the target dataset with pseudo labels in traditional supervised learning manner. We adopt the idea of co-training in the training process to make the pseudo labels more reliable. We show the superiority of our model over all state-of-the-art methods through extensive experiments.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00195,http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Tang_Unsupervised_Person_Re-Identification_With_Iterative_Self-Supervised_Domain_Adaptation_CVPRW_2019_paper.pdf
71acac5a4d72f896ccb87fe021d0187ff0002231,0,1,0,Dual Attentive Features for Person Re-identification,"Person re-identification (re-ID) plays a vital role in intelligent video surveillance and is the task of retrieving particular persons across different cameras. Effective convolutional features is a key component in person re-ID but how to learn powerful features is still a challenging task due to pose variation, occlusion, and similar appearance among different persons. Considering this fact, we propose a novel dual attention module network which consists of spatial attention and channel-wise attention. Dual attention module add weights on image features in spatial and channel. Attentive features generated by the proposed network are able to better enhance the foreground objects while eliminate background clutter. Intensive experimental results have been prove that the proposed dual attention module network outperforms the state-of-the-art on Market-1501 and DukeMTMC-reID datasets.",2019,"2019 5th International Conference on Control, Automation and Robotics (ICCAR)",,10.1109/ICCAR.2019.8813500,
71b5c78982c3cfc9eb08ede5ea48e3155d6d4ced,1,0,0,Multiplex Labeling Graph for Near-Online Tracking in Crowded Scenes,"In recent years, the demand for intelligent devices related to the Internet of Things (IoT) is rapidly increasing. In the field of computer vision, many algorithms have been preinstalled in IoT devices to achieve higher efficiency, such as face recognition, area detection, target tracking, etc. Tracking is an important but complex task that needs high efficiency solutions in real applications. There is a common assumption that detection can only represent one pedestrian to describe nonoverlapping in physical space. In fact, the pixels of the image do not exactly correspond to the positions in the real world. In order to overcome the limitation of this assumption, we remove this unreasonable assumption and present a novel idea that each detector response can have multiple labels to describe different targets at the same time. Therefore, we propose a graph-based method for near-online tracking in this article. We introduce a detection multiplexing method for tracking in the monocular image and propose a multiplex labeling graph (MLG) model. Each node in MLG has the ability to represent multiple targets. In addition, we improve the shortage of graph-based trackers in using temporal features. We construct long short-term memory networks to model motion and appearance features for MLG optimization. On the public multiobject tracking challenge benchmark, our near-online method gains satisfactory efficiency and achieves state-of-the-art results without additional private detection as well.",2020,IEEE Internet of Things Journal,,10.1109/JIOT.2020.2996609,
71df94e8546baa72b84903a774a2fd685bd7e615,0,1,0,Camera-Aware Image-To-Image Translation Using Similarity Preserving StarGAN for Person Re-Identification,"Person re-identification is a crucial task in intelligent video surveillance systems. It can be defined as recognizing the same person from images of a person taken from different cameras at different times. In this paper, we present a camera-aware image-to-image translation using similarity preserving StarGAN (SP-StarGAN) as the data augmentation for person re-identification. We propose the addition of an identity mapping term and a multi-scale structural similarity term as additional losses for the generator. SP-StarGAN can learn the relationship among the multiple cameras with a single model and generate the camera-aware extra training samples for person re-identification. We evaluate our proposed method on public datasets (Market-1501 and DukeMTMC-reID) and demonstrate the efficacy of our method. We also report competitive performance with the state-of-the-art methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00193,http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Chung_Camera-Aware_Image-To-Image_Translation_Using_Similarity_Preserving_StarGAN_for_Person_Re-Identification_CVPRW_2019_paper.pdf
71fffc2c348afea03d310c396038b0b264afe866,1,1,0,AttKGCN: Attribute Knowledge Graph Convolutional Network for Person Re-identification,"Discriminative feature representation of person image is important for person re-identification (Re-ID) task. Recently, attributes have been demonstrated beneficially in guiding for learning more discriminative feature representations for Re-ID. As attributes normally co-occur in person images, it is desirable to model the attribute dependencies to improve the attribute prediction and thus Re-ID results. In this paper, we propose to model these attribute dependencies via a novel attribute knowledge graph (AttKG), and propose a novel Attribute Knowledge Graph Convolutional Network (AttKGCN) to solve Re-ID problem. AttKGCN integrates both attribute prediction and Re-ID learning together in a unified end-to-end framework which can boost their performances, respectively. AttKGCN first builds a directed attribute KG whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes. Then, AttKGCN learns a set of inter-dependent attribute classifiers which are combined with person visual descriptors for attribute prediction. Finally, AttKGCN integrates attribute description and deeply visual representation together to construct a more discriminative feature representation for Re-ID task. Extensive experiments on several benchmark datasets demonstrate the effectiveness of AttKGCN on attribute prediction and Re-ID tasks.",2019,ArXiv,1911.10544,,https://arxiv.org/pdf/1911.10544.pdf
720890c2532a95a53280a33b81aff788d87e6abf,0,1,0,Appearance and Pose-Conditioned Human Image Generation using Deformable GANs,"In this paper, we address the problem of generating person images conditioned on both pose and appearance information. Specifically, given an image xa of a person and a target pose P(xb), extracted from a different image xb, we synthesize a new image of that person in pose P(xb), while preserving the visual details in xa. In order to deal with pixel-to-pixel misalignments caused by the pose differences between P(xa) and P(xb), we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. Quantitative and qualitative results, using common datasets and protocols recently proposed for this task, show that our approach is competitive with respect to the state of the art. Moreover, we conduct an extensive evaluation using off-the-shell person re-identification (Re-ID) systems trained with person-generation based augmented data, which is one of the main important applications for this task. Our experiments show that our Deformable GANs can significantly boost the Re-ID accuracy and are even better than data-augmentation methods specifically trained using Re-ID losses.",2019,IEEE transactions on pattern analysis and machine intelligence,1905.00007,10.1109/tpami.2019.2947427,https://arxiv.org/pdf/1905.00007.pdf
722514cf193ea8b301475de9da5a0061f2e47bdd,0,1,0,A survey of image synthesis and editing with generative adversarial networks,"This paper presents a survey of image synthesis and editing with Generative Adversarial Networks (GANs). GANs consist of two deep networks, a generator and a discriminator, which are trained in a competitive way. Due to the power of deep networks and the competitive training manner, GANs are capable of producing reasonable and realistic images, and have shown great capability in many image synthesis and editing applications. This paper surveys recent GAN papers regarding topics including, but not limited to, texture synthesis, image inpainting, image-to-image translation, and image editing.",2017,,,10.23919/TST.2017.8195348,https://pdfs.semanticscholar.org/7225/14cf193ea8b301475de9da5a0061f2e47bdd.pdf
723c86df01597687c9b9c77d7786f73d4b9ba941,1,1,1,Unsupervised Person Re-Identification via Softened Similarity Learning,"Person re-identification (re-ID) is an important topic in computer vision. This paper studies the unsupervised setting of re-ID, which does not require any labeled information and thus is freely deployed to new scenarios. There are very few studies under this setting, and one of the best approach till now used iterative clustering and classification, so that unlabeled images are clustered into pseudo classes for a classifier to get trained, and the updated features are used for clustering and so on. This approach suffers two problems, namely, the difficulty of determining the number of clusters, and the hard quantization loss in clustering. In this paper, we follow the iterative training mechanism but discard clustering, since it incurs loss from hard quantization, yet its only product, image-level similarity, can be easily replaced by pairwise computation and a softened classification task. With these improvements, our approach becomes more elegant and is more robust to hyper-parameter changes. Experiments on two image-based and video-based datasets demonstrate state-of-the-art performance under the unsupervised re-ID setting.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.03547,10.1109/cvpr42600.2020.00345,https://arxiv.org/pdf/2004.03547.pdf
726523c505a9ff96610d09d31f1568cba5d08449,0,1,0,Improving Person Re-Identification With Iterative Impression Aggregation,"Our impression about one person often updates after we see more aspects of him/her and this process keeps iterating given more meetings. We formulate such an intuition into the problem of person re-identification (re-ID), where the representation of a query (probe) image is iteratively updated with new information from the candidates in the gallery. Specifically, we propose a simple attentional aggregation formulation to instantiate this idea and showcase that such a pipeline achieves competitive performance on standard benchmarks including CUHK03, Market-1501 and DukeMTMC. Not only does such a simple method improve the performance of the baseline models, it also achieves comparable performance with latest advanced re-ranking methods. Another advantage of this proposal is its flexibility to incorporate different representations and similarity metrics. By utilizing stronger representations and metrics, we further demonstrate state-of-the-art person re-ID performance, which also validates the general applicability of the proposed method.",2020,IEEE Transactions on Image Processing,2009.10066,10.1109/TIP.2020.3029415,https://arxiv.org/pdf/2009.10066.pdf
7266448bd3479f770800365e928f7d3548d59cea,1,0,0,Closed and Open World Multi-shot Person Re-identification,"In this thesis we tackle the open world person re-identification task in which the people we want to re-identify (probe) might not appear in the database of known identities (gallery). For a given probe person, the goal is to find out whether he is present in the gallery or not and if so, who he is. Our first contribution is based on a verification formulation of the problem. A linear transformation of the features is learnt so that the distance between features of the same person are below a threshold and that of distinct people are above that same threshold so that it is easy to determine whether two sets of images represent the same person or not. Our other contributions are based on collaborative sparse representations. A usual way to use collaborative sparse representation for re-identification is to approximate the feature of a probe image by a sparse linear combination of gallery elements, where all the known identities collaborate but only the most similar elements are selected. Gallery identities are then ranked according to how much they contributed to the approximation. We propose to enhance the collaborative aspect so that collaborative sparse representations can be used not only as a ranking tool but also as a detection tool which rejects wrong matches. A bidirectional variant gives even more robust results by taking into account the fact that a good match is a match where there is a reciprocal relation in which both the probe and the gallery identities consider the other one as a good match. COPReV shows average performances but bidirectional collaboration enhanced sparse representation method outperforms state-of-the-art methods for open world scenarios.",2017,,,,
729f2450ab52be354254b1924f73b54bdda80dd2,0,1,0,Person re-identification for 365-day video surveillance based on stride convolutional neural network,"Person re-identification (ReID) is an important task in video surveillance and can be applied in various practical applications. The traditional methods and deep learning model cannot satisfy the real-world challenges of environmental complexity and scene dynamics, especially under fixed scene. What’s more, most of the existing datasets are outdoor and has a single style, which is not good for indoor person re-identification. Focusing on these problems, the paper improves a Stride Convolutional Neural Network (S-CNN) to process indoor images based on multi-features fusion. The deep model is established in which the identity information, stride information and other information are learned to handle more challenging indoor images. Then a metric learning method (Joint Bayesian) is employed based on the deep model. Finally, the entire classifier is retrained with supervised learning. The experiment is tested on the OUC365 dataset created by us which is captured for 365 days including all seasons style. Compared with other state-of-the-art methods, the performance of the proposed method yields best results",2019,International Conference on Graphic and Image Processing,,10.1117/12.2524371,
72a6f4bb21104165064e6fced6476afba3dc11d2,0,1,0,Style Transfer with Adversarial Learning for Cross-Dataset Person Re-identification,"Person re-identification (ReID) has witnessed great progress in recent years. Existing approaches are able to achieve significant performance on the single dataset but fail to generalize well on another datasets. The emerging problem mainly comes from style difference between two datasets. To address this problem, we propose a novel style transfer framework based on Generative Adversarial Networks (GAN) to generate target-style images. Specifically, we get the style representation by calculating the Garm matrix of the three-channel original image, and then minimize the Euclidean distance of the style representation on different images transferred by the same generator while image generation. Finally, the labeled source dataset combined with the style-transferred images are all used to enhance the generalization ability of the ReID model. Experimental results suggest that the proposed strategy is very effective on the Market-1501 and DukeMTMC-reID.",2018,ACCV,,10.1007/978-3-030-20876-9_11,http://vipl.ict.ac.cn/uploadfile/upload/2018111615575467.pdf
72d6a77d3ff8caf218bf06a487a13b3342a80328,0,1,0,Online Joint Multi-Metric Adaptation From Frequent Sharing-Subset Mining for Person Re-Identification,"Person Re-IDentification (P-RID), as an instance-level recognition problem, still remains challenging in computer vision community. Many P-RID works aim to learn faithful and discriminative features/metrics from offline training data and directly use them for the unseen online testing data. However, their performance is largely limited due to the severe data shifting issue between training and testing data. Therefore, we propose an online joint multi-metric adaptation model to adapt the offline learned P-RID models for the online data by learning a series of metrics for all the sharing-subsets. Each sharing-subset is obtained from the proposed novel frequent sharing-subset mining module and contains a group of testing samples which share strong visual similarity relationships to each other. Unlike existing online P-RID methods, our model simultaneously takes both the sample-specific discriminant and the set-based visual similarity among testing samples into consideration so that the adapted multiple metrics can refine the discriminant of all the given testing samples jointly via a multi-kernel late fusion framework. Our proposed model is generally suitable to any offline learned P-RID baselines for online boosting, the performance improvement by our model is not only verified by extensive experiments on several widely-used P-RID benchmarks (CUHK03, Market1501, DukeMTMC-reID and MSMT17) and state-of-the-art P-RID baselines but also guaranteed by the provided in-depth theoretical analyses.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00298,https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Online_Joint_Multi-Metric_Adaptation_From_Frequent_Sharing-Subset_Mining_for_Person_CVPR_2020_paper.pdf
730c74190b2cfc1730088542680750e90540cca8,1,0,0,Multi-Camera Trajectory Forecasting: Pedestrian Trajectory Prediction in a Network of Cameras,"We introduce the task of multi-camera trajectory forecasting (MCTF), where the future trajectory of an object is predicted in a network of cameras. Prior works consider forecasting trajectories in a single camera view. Our work is the first to consider the challenging scenario of forecasting across multiple non-overlapping camera views. This has wide applicability in tasks such as re-identification and multi-target multi-camera tracking. To facilitate research in this new area, we release the Warwick-NTU Multi-camera Forecasting Database (WNMF), a unique dataset of multi-camera pedestrian trajectories from a network of 15 synchronized cameras. To accurately label this large dataset (600 hours of video footage), we also develop a semi-automated annotation method. An effective MCTF model should proactively anticipate where and when a person will reappear in the camera network. In this paper, we consider the task of predicting the next camera a pedestrian will reappear after leaving the view of another camera, and present several base-line approaches for this. The labeled database is available online: https://github.com/olly-styles/Multi-Camera-Trajectory-Forecasting.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2005.00282,10.1109/CVPRW50498.2020.00516,https://arxiv.org/pdf/2005.00282.pdf
7320649d7d0f495ae60ff8d7975b541c17057d48,0,1,0,Metric Embedding Autoencoders for Unsupervised Cross-Dataset Transfer Learning,"Cross-dataset transfer learning is an important problem in person re-identification (Re-ID). Unfortunately, not too many deep transfer Re-ID models exist for realistic settings of practical Re-ID systems. We propose a purely deep transfer Re-ID model consisting of a deep convolutional neural network and an autoencoder. The latent code is divided into metric embedding and nuisance variables. We then utilize an unsupervised training method that does not rely on co-training with non-deep models. Our experiments show improvements over both the baseline and competitors’ transfer learning models.",2018,ICANN,1807.10591,10.1007/978-3-030-01424-7_29,https://arxiv.org/pdf/1807.10591.pdf
73875b105ae702f073d29b8dacb528fd23c0c936,1,0,0,Instance-Aware Representation Learning and Association for Online Multi-Person Tracking,"Abstract Multi-Person Tracking (MPT) is often addressed within the detection-to-association paradigm. In such approaches, human detections are first extracted in every frame and person trajectories are then recovered by a procedure of data association (usually offline). However, their performances usually degenerate in presence of detection errors, mutual interactions and occlusions. In this paper, we present a deep learning based MPT approach that learns instance-aware representations of tracked persons and robustly online infers states of the tracked persons. Specifically, we design a multi-branch neural network (MBN), which predicts the classification confidences and locations of all targets by taking a batch of candidate regions as input. In our MBN architecture, each branch (instance-subnet) corresponds to an individual to be tracked and new branches can be dynamically created for handling newly appearing persons. Then based on the output of MBN, we construct a joint association matrix that represents meaningful states of tracked persons (e.g., being tracked or disappearing from the scene) and solve it by using the efficient Hungarian algorithm. Moreover, we allow the instance-subnets to be updated during tracking by online mining hard examples, accounting to person appearance variations over time. We comprehensively evaluate our framework on a popular MPT benchmark, demonstrating its excellent performance in comparison with recent online MPT methods.",2019,Pattern Recognit.,1905.12409,10.1016/j.patcog.2019.04.018,https://arxiv.org/pdf/1905.12409.pdf
739a07db5053620da32b8af4ab83279d40352f55,1,0,0,A Deep Clustering-Guide Learning for Unsupervised Person Re-identification,"Unsupervised person re-identification (RE-ID) has attracted increasing attentions due to its ability to overcome the scalability problem of supervised RE-ID methods. However, it is hard to learn discriminative features without pairwise labels and identity information in unlabeled target domains. To address this problem, we propose a deep clustering-guided model for unsupervised RE-ID that focuses on full mining of supervisions and a complete usage of the mined information. Specifically, we cluster person images from unlabeled target and labeled auxiliary datasets together. On the one hand, although the clustering IDs of unlabeled person images could be directly used as pseudo-labels to supervise the whole model, we further develop a non-parametric softmax variant for cluster-level supervision. On the other hand, since clustering badly suffers from intra-person appearance variation and inter-person appearance similarity in the unlabeled domain, we propose a reliable and hard mining in both intra-cluster and inter-cluster. Concretely, labeled persons (auxiliary domain) in each cluster are used as comparators to learn comparing vectors for each unlabeled persons. Following the consistency of the visual feature similarity and the corresponding comparing vector similarity, we mine reliable positive and hard negative pairs in the intra-cluster, and reliable negative and hard positive pairs in the inter-cluster for unlabeled persons. Moreover, a weighted point-to-set triplet loss is employed to adaptively assign higher (lower) weights to reliable (hard) pairs, which is more robust and effective compared with the conventional triplet loss in unsupervised RE-ID. We train our model with these two losses jointly to learn discriminative features for unlabeled persons. Extensive experiments validate the superiority of the proposed method for unsupervised RE-ID.",2019,ICONIP,,10.1007/978-3-030-36718-3_49,
73abb9a8575d24fd9a18f538f51503478c91ace4,1,1,0,"A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets","Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 11 feature extraction algorithms and 22 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR, DukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03, RAiD, iLIDSVID, HDA+, and Market1501. The evaluation codebase and results will be made publicly available for community use.",2019,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2018.2807450,
73ad12536fc3ebf617e7463779a1c8a30981b58e,1,0,1,One‐shot video‐based person re‐identification with variance subsampling algorithm,"Previous works propose the distance‐based sampling for unlabeled datapoints to address the few‐shot person re‐identification task, however, many selected samples may be assigned with wrong labels due to poor feature quality in these works, which negatively affects the learning procedure. In this article, we propose a novel sampling strategy to improve the quality of assigned pseudo‐labels, thus promoting the final performance. To illustrate, we first propose the concept of variance confidence to measure the credibility of pseudo‐labels, then we apply a novel variance subsampling algorithm to improve the accuracy of the selected sample labels. Our method combines distance confidence and variance confidence as a two‐round sampling criterion. Meanwhile, a variation decay strategy is used in our sampling process in combination with the actual distribution of features. We evaluate our approach on two publicly available datasets, MARS and DukeMTMC‐VideoReID, and achieve state‐of‐the‐art one‐shot performance.",2020,Comput. Animat. Virtual Worlds,,10.1002/cav.1964,
73c40c8aeb005ccb0dd22537b69e515b47fb416b,1,0,0,Deep Local Binary Coding for Person Re-Identification by Delving into the Details,"Person re-identification (ReID) has recently received extensive research interests due to its diverse applications in multimedia analysis and computer vision. However, the majority of existing works focus on improving matching accuracy, while ignoring matching efficiency. In this work, we present a novel binary representation learning framework for efficient person ReID, namely Deep Local Binary Coding (DLBC). Different from existing deep binary ReID approaches, DLBC attempts to learn discriminative binary codes by explicitly interacting with local visual details. Specifically, DLBC first extracts a set of local features from spatially salient regions of pedestrian images. Subsequently, DLBC formulates a new binary-local semantic mutual information (BSMI) maximization term, based on which a self-lifting (SL) block is built to further exploit the semantic importance of local features. The BSMI term together with the SL block simultaneously enhances the dependency of binary codes on selected local features as well as their robustness to cross-view visual inconsistency. In addition, an efficient optimizing method is developed to train the proposed deep models with orthogonal and binary constraints. Extensive experiments reveal that DLBC significantly minimizes the accuracy gap between binary ReID methods and the state-of-the-art real-valued ones, whilst remarkably reducing query time and memory cost.",2020,ACM Multimedia,,10.1145/3394171.3413979,
7405ba93a532a0c3439516cf6a0b0c6008f5cf75,1,1,0,A part-based attention network for person re-identification,"Person re-identification (re-id) is the task of recognizing images of the same pedestrian captured by different cameras with non-overlapping views. Person re-id is a challenging task due to the existence of large view variations, such as spatial misalignment, background clutter and human poses change. In this paper, we handle these challenges from the following two aspects: utilizing attention mechanism to alleviate misalignment problem and exploiting the complementary effects of global-local features for more stable pedestrian descriptors. Specifically, we first present a part-based attention model consisting of a channel attention block and a spatial attention block to sequentially refine the convolutional descriptors of person body parts. The channel and spatial attention blocks weight the channels and positions of body-part feature maps to spot the informative channels and regions, respectively. Then global full-body and local body-part of the refined feature maps are pooled into global and local representations, which are jointly trained using identity classification loss. We conduct extensive experiments on four standard benchmark datasets including Market1501, CUHK03, DukeMTMC-reID, and CUHK01, and the experimental results demonstrate the effectiveness of the presented method.",2020,Multimedia Tools and Applications,,10.1007/s11042-019-08395-2,
74b430ee6d416df0cc3a3c57bd880bb33f9b5e93,1,0,0,Improving Real-Time Pedestrian Detectors with RGB+Depth Fusion,"In this paper we investigate the benefit of using depth information on top of normal RGB for camera-based pedestrian detection. Indeed, depth sensing is easily acquired using depth cameras such as a Kinect or stereo setups. We investigate the best way to perform this sensor fusion with a special focus on lightweight single-pass CNN architectures, enabling real-time processing on limited hardware. We implement different network architectures, each fusing depth at different layers of our network. Our experiments show that midway fusion performs the best, outperforming a regular RGB detector substantially in accuracy. Moreover, we prove that our fusion network is better at detecting individuals in a crowd, by demonstrating that it has both a better localization of pedestrians and is better at handling occluded persons. The resulting network is computationally efficient and achieves real-time performance on both desktop and embedded GPUs.",2018,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2018.8639110,
74b9632e8c7bc7c96af5561a017b40b9613f196d,0,1,0,Deformable GANs for Pose-Based Human Image Generation,"In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1801.00055,10.1109/CVPR.2018.00359,https://arxiv.org/pdf/1801.00055.pdf
74bfaacd4e86a1304d2b5e7340591cffb38d84dd,1,1,0,SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification,"Abstract Many current successful Person Re-Identification (ReID) methods train a model with the softmax loss function to classify images of different persons and obtain the feature vectors at the same time. However, the underlying feature embedding space is ignored. In this paper, we use a modified softmax function, termed Sphere Softmax, to solve the classification problem and learn a hypersphere manifold embedding simultaneously. A balanced sampling strategy is also introduced. Finally, we propose a convolutional neural network called SphereReID adopting Sphere Softmax and training a single model end-to-end with a new warming-up learning rate schedule on four challenging datasets including Market-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results demonstrate that this single model outperforms the state-of-the-art methods on all four datasets without fine-tuning or re-ranking. For example, it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID. The code and trained weights of our model will be released.",2019,J. Vis. Commun. Image Represent.,1807.00537,10.1016/j.jvcir.2019.01.010,https://arxiv.org/pdf/1807.00537.pdf
74d3c6e6bd20d9ff76483f496e72ba3e05f5eac8,1,0,0,Pedestrian detection with unsupervised multispectral feature learning using deep neural networks,"Abstract Multispectral pedestrian detection is an important functionality in various computer vision applications such as robot sensing, security surveillance, and autonomous driving. In this paper, our motivation is to automatically adapt a generic pedestrian detector trained in a visible source domain to a new multispectral target domain without any manual annotation efforts. For this purpose, we present an auto-annotation framework to iteratively label pedestrian instances in visible and thermal channels by leveraging the complementary information of multispectral data. A distinct target is temporally tracked through image sequences to generate more confident labels. The predicted pedestrians in two individual channels are merged through a label fusion scheme to generate multispectral pedestrian annotations. The obtained annotations are then fed to a two-stream region proposal network (TS-RPN) to learn the multispectral features on both visible and thermal images for robust pedestrian detection. Experimental results on KAIST multispectral dataset show that our proposed unsupervised approach using auto-annotated training data can achieve performance comparable to state-of-the-art deep neural networks (DNNs) based pedestrian detectors trained using manual labels.",2019,Inf. Fusion,,10.1016/j.inffus.2018.06.005,http://e-motarjem.ir/storage/btn_uploaded/2018-08-27/1535365107_e-motarjem-EN11.pdf
74e38dfeb5abc7ddf077abc01de90f4d0a49c142,0,1,0,Omni-directional Feature Learning for Person Re-identification,"Person re-identification (PReID) has received increasing attention due to it is an important part in intelligent surveillance. Recently, many state-of-the-art methods on PReID are part-based deep models. Most of them focus on learning the part feature representation of person body in horizontal direction. However, the feature representation of body in vertical direction is usually ignored. Besides, the spatial information between these part features and the different feature channels is not considered. In this study, we introduce a multi-branches deep model for PReID. Specifically, the model consists of five branches. Among the five branches, two of them learn the local feature with spatial information from horizontal or vertical orientations, respectively. The other one aims to learn interdependencies knowledge between different feature channels generated by the last convolution layer. The remains of two other branches are identification and triplet sub-networks, in which the discriminative global feature and a corresponding measurement can be learned simultaneously. All the five branches can improve the representation learning. We conduct extensive comparative experiments on three PReID benchmarks including CUHK03, Market-1501 and DukeMTMC-reID. The proposed deep framework outperforms many state-of-the-art in most cases.",2018,ArXiv,1812.05319,,https://arxiv.org/pdf/1812.05319.pdf
753d2a35c9edf5dfcac4ef3a6adc993b657b01f0,1,1,0,Beyond Part Models: Person Retrieval with Refined Part Pooling,"Employing part-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in very recent literature. A prerequisite of part discovery is that each part should be well located. Instead of using external cues, e.g., pose estimation, to directly locate parts, this paper lays emphasis on the content consistency within each part.  Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval.  (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin.",2018,ECCV,1711.09349,10.1007/978-3-030-01225-0_30,https://arxiv.org/pdf/1711.09349.pdf
75aaf4b189b9c8cfabc42c2fcee9e0a26dfb2239,1,0,0,Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification,"Person re-identification (ReID) remains a challenging task in many real-word video analytics and surveillance applications, even though state-of-the-art accuracy has improved considerably with the advent of deep learning (DL) models trained on large image datasets. Given the shift in distributions that typically occurs between video data captured from the source and target domains, and absence of labeled data from the target domain, it is difficult to adapt a DL model for accurate recognition of target data. We argue that for pair-wise matchers that rely on metric learning, e.g., Siamese networks for person ReID, the unsupervised domain adaptation (UDA) objective should consist in aligning pair-wise dissimilarity between domains, rather than aligning feature representations. Moreover, dissimilarity representations are more suitable for designing open-set ReID systems, where identities differ in the source and target domains. In this paper, we propose a novel Dissimilarity-based Maximum Mean Discrepancy (D-MMD) loss for aligning pair-wise distances that can be optimized via gradient descent. From a person ReID perspective, the evaluation of D-MMD loss is straightforward since the tracklet information allows to label a distance vector as being either within-class or between-class. This allows approximating the underlying distribution of target pair-wise distances for D-MMD loss optimization, and accordingly align source and target distance distributions. Empirical results with three challenging benchmark datasets show that the proposed D-MMD loss decreases as source and domain distributions become more similar. Extensive experimental evaluation also indicates that UDA methods that rely on the D-MMD loss can significantly outperform baseline and state-of-the-art UDA methods for person ReID without the common requirement for data augmentation and/or complex networks.",2020,ECCV,2007.1389,10.1007/978-3-030-58583-9_10,https://arxiv.org/pdf/2007.13890.pdf
75cb7f0541db6a2257fc09f306d4b97427375f02,0,1,0,Novelty Detection for Person Re-identification in an Open World,"A fundamental assumption in most contemporary person re-identification research, is that all query persons that need to be re-identified belong to a closed gallery of known persons, i.e., they have been observed and a representation of their appearance is available. For several real-world applications, this closed-world assumption does not hold, as image queries may contain people that the re-identification system has never observed before. In this work, we remove this constraining assumption. To do so, we introduce a novelty detection mechanism that decides whether a person in a query image exists in the gallery. The re-identification of persons existing in the gallery is easily achieved based on the persons representation employed by the novelty detection mechanism. The proposed method operates on a hybrid person descriptor that consists of both supervised (learnt) and unsupervised (hand-crafted) components. A series of experiments on public, state of the art datasets and in comparison with state of the art methods shows that the proposed approach is very accurate in identifying persons that have not been observed before and that this has a positive impact on re-identification accuracy.",2019,VISIGRAPP,,10.5220/0007368304010411,
767ac8398e845779be111eb4ce8cb60a8b69a511,0,1,0,AlignedReID: Surpassing Human-Level Performance in Person Re-Identification,"In this paper, we propose a novel method called AlignedReID that extracts a global feature which is jointly learned with local features. Global feature learning benefits greatly from local feature learning, which performs an alignment/matching by calculating the shortest path between two sets of local features, without requiring extra supervision. After the joint learning, we only keep the global feature to compute the similarities between images. Our method achieves rank-1 accuracy of 94.4% on Market1501 and 97.8% on CUHK03, outperforming state-of-the-art methods by a large margin. We also evaluate human-level performance and demonstrate that our method is the first to surpass human-level performance on Market1501 and CUHK03, two widely used Person ReID datasets.",2017,ArXiv,1711.08184,,https://arxiv.org/pdf/1711.08184.pdf
76f1ba3ab24f7eb045b823c1b95b5c1a3199b157,0,1,0,Multi-Branch Context-Aware Network for Person Re-Identification,"Most existing methods on person re-identification pay redundant attention to global features or local features which ignore contextual dependencies which are equally important in representing pedestrian images. In this paper, we propose a Multi-Branch Context-Aware Network (MBCAN) for person re-identification to exploit rich context information. MBCAN learns global features and local-part features in two separate branches to take full advantages of both coarse-grained and fine-grained features. Additionally, two types of attention modules are introduced to capture contextual dependencies in spatial dimension and channel dimension, respectively. A module called feature vector extraction block is designed to find an efficient way to integrate features from coarse to fine. Extensive experiments with ablation analysis show the effectiveness of our method, and state-of-the-art results are achieved on Market-1501, DukeMTMC-reID and CUHK03 datasets.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME.2019.00128,
76fb9e2963928bf8e940944d45c13d52db947702,0,1,0,Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification,"Person re-identification (ReID) is an important task in computer vision. Recently, deep learning with a metric learning loss has become a common framework for ReID. In this paper, we also propose a new metric learning loss with hard sample mining called margin smaple mining loss (MSML) which can achieve better accuracy compared with other metric learning losses, such as triplet loss. In experi- ments, our proposed methods outperforms most of the state-of-the-art algorithms on Market1501, MARS, CUHK03 and CUHK-SYSU.",2017,ArXiv,1710.00478,,https://arxiv.org/pdf/1710.00478.pdf
77075dbc03c753935b1ddbbf77a926f7f80032c8,0,1,0,Unsupervised Person Re-Identification by Camera-Aware Similarity Consistency Learning,"For matching pedestrians across disjoint camera views in surveillance, person re-identification (Re-ID) has made great progress in supervised learning. However, it is infeasible to label data in a number of new scenes when extending a Re-ID system. Thus, studying unsupervised learning for Re-ID is important for saving labelling cost. Yet, cross-camera scene variation is a key challenge for unsupervised Re-ID, such as illumination, background and viewpoint variations, which cause domain shift in the feature space and result in inconsistent pairwise similarity distributions that degrade matching performance. To alleviate the effect of cross-camera scene variation, we propose a Camera-Aware Similarity Consistency Loss to learn consistent pairwise similarity distributions for intra-camera matching and cross-camera matching. To avoid learning ineffective knowledge in consistency learning, we preserve the prior common knowledge of intra-camera matching in the pretrained model as reliable guiding information, which does not suffer from cross-camera scene variation as cross-camera matching. To learn similarity consistency more effectively, we further develop a coarse-to-fine consistency learning scheme to learn consistency globally and locally in two steps. Experiments show that our method outperformed the state-of-the-art unsupervised Re-ID methods.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00702,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2019_paper.pdf
770f563987e72a504e1095a640137096f59633d6,0,1,0,Unsupervised Domain-Adaptive Person Re-Identification Based on Attributes,"Pedestrian attributes, e. g., hair length, clothes type and color, locally describe the semantic appearance of a person. Training person re-identification (ReID) algorithms under the supervision of such attributes have proven to be effective in extracting local features which are important for ReID. Unlike person identity, attributes are consistent across different domains (or datasets). However, most of ReID datasets lack attribute annotations. On the other hand, there are several datasets labeled with sufficient attributes for the case of pedestrian attribute recognition. Exploiting such data for ReID purpose can be a way to alleviate the shortage of attribute annotations in ReID case. In this work, an unsupervised domain adaptive ReID feature learning framework is proposed to make full use of attribute annotations. We propose to transfer attribute-related features from their original domain to the ReID one: to this end, we introduce an adversarial discriminative domain adaptation method in order to learn domain invariant features for encoding semantic attributes. Experiments on three large-scale datasets validate the effectiveness of the proposed ReID framework.",2019,2019 IEEE International Conference on Image Processing (ICIP),1908.10359,10.1109/ICIP.2019.8803465,https://arxiv.org/pdf/1908.10359.pdf
773eaa5f9d2a6c8557fff9c4ed7c78e0247961d6,1,0,0,Adaptive deep metric embeddings for person re-identification under occlusions,"Abstract Person re-identification (ReID) under occlusions is a challenging problem in video surveillance. Most of existing person ReID methods take advantage of local features to deal with occlusions. However, these methods usually independently extract features from the local regions of an image without considering the relationship among different local regions. In this paper, we propose a novel person ReID method, which extracts the discriminative feature representation of the pedestrian image based on Long Short-Term Memory (LSTM), dealing with the problem of occlusions. In particular, the multi-directional spatial encoded local features are developed to learn the spatial dependencies between the local regions by taking advantage of LSTM. Furthermore, we propose a novel loss (termed the adaptive nearest neighbor loss) based on the classification uncertainty to effectively reduce intra-class variations while enlarging inter-class differences within the adaptive neighborhood of the sample. The proposed loss enables the deep neural network to adaptively learn discriminative metric embeddings, which significantly improve the generalization capability of recognizing unseen person identities. Extensive comparative evaluations on challenging person ReID datasets demonstrate the significantly improved performance of the proposed method compared with several state-of-the-art methods.",2019,Neurocomputing,2002.02603,10.1016/j.neucom.2019.02.042,https://arxiv.org/pdf/2002.02603.pdf
774b649c75078e10759b3b6c8ea581e68fc45a40,0,1,1,Robust Anchor Embedding for Unsupervised Video Person re-IDentification in the Wild,"This paper addresses the scalability and robustness issues of estimating labels from imbalanced unlabeled data for unsupervised video-based person re-identification (re-ID). To achieve it, we propose a novel Robust AnChor Embedding (RACE) framework via deep feature representation learning for large-scale unsupervised video re-ID. Within this framework, anchor sequences representing different persons are firstly selected to formulate an anchor graph which also initializes the CNN model to get discriminative feature representations for later label estimation. To accurately estimate labels from unlabeled sequences with noisy frames, robust anchor embedding is introduced based on the regularized affine hull. Efficiency is ensured with kNN anchors embedding instead of the whole anchor set under manifold assumptions. After that, a robust and efficient top-k counts label prediction strategy is proposed to predict the labels of unlabeled image sequences. With the newly estimated labeled sequences, the unified anchor embedding framework enables the feature learning process to be further facilitated. Extensive experimental results on the large-scale dataset show that the proposed method outperforms existing unsupervised video re-ID methods.",2018,ECCV,,10.1007/978-3-030-01234-2_11,http://www.comp.hkbu.edu.hk/~mangye/files/RACE.pdf
77875d6e4d8c7ed3baeb259fd5696e921f59d7ad,0,0,1,Style Aggregated Network for Facial Landmark Detection,"Recent advances in facial landmark detection achieve success by learning discriminative features from rich deformation of face shapes and poses. Besides the variance of faces themselves, the intrinsic variance of image styles, e.g., grayscale vs. color images, light vs. dark, intense vs. dull, and so on, has constantly been overlooked. This issue becomes inevitable as increasing web images are collected from various sources for training neural networks. In this work, we propose a style-aggregated approach to deal with the large intrinsic variance of image styles for facial landmark detection. Our method transforms original face images to style-aggregated images by a generative adversarial module. The proposed scheme uses the style-aggregated image to maintain face images that are more robust to environmental changes. Then the original face images accompanying with style-aggregated ones play a duet to train a landmark detector which is complementary to each other. In this way, for each face, our method takes two images as input, i.e., one in its original style and the other in the aggregated style. In experiments, we observe that the large variance of image styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image styles by comparing to a variant of our approach, in which the generative adversarial module is removed, and no style-aggregated images are used. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on benchmark datasets AFLW and 300-W. Code is publicly available on GitHub: https://github.com/D-X-Y/SAN",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.04108,10.1109/CVPR.2018.00047,https://arxiv.org/pdf/1803.04108.pdf
77b5526c488f824aa2a4e898f22a674a375b1aa7,1,0,0,Integrating the University of São Paulo Security Mobile App to the Electronic Monitoring System,"The University of São Paulo is the largest public university in Brasil, with 11 campuses where the campus in the city of São Paulo alone has an area of more than 3 million square meters. Security in the university is an issue that is being prioritized in the past years. A mobile app was introduced two years ago with which users in any campus can report security and maintenance events. Security events are monitored by the Campus Security Guard that has the capacity to dispatch agents immediately upon report of an event by the app. Last year an Electronic Monitoring System (EMS) composed of more than 300 cameras was deployed in the campus of the city of São Paulo together with analytics software that greatly enhances its functionality. Integrating the mobile app and the EMS is a great challenge that is being addressed in this paper. Here we present a description of both systems: the mobile application and the EMS. Then we present the Artificial Intelligence Surveillance system (AISUSP), which integrates both other systems using machine learning for automated scene analysis, an impossible task for human operators in an environment with more than 300 cameras. The ongoing result will lead to the prediction of emergency situations using a historical data base and making intelligent decisions related to the university’s users safety.",2018,2018 IEEE International Conference on Big Data (Big Data),,10.1109/BigData.2018.8622069,
78e7d35d92dda56e4889154a130b0373e66195f6,1,0,0,Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs,"Crowd counting in single-view images has achieved outstanding performance on existing counting datasets. However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. Therefore, to solve the wide-area counting task requires multiple cameras with overlapping fields-of-view. In this paper, we propose a deep neural network framework for multi-view crowd counting, which fuses information from multiple camera views to predict a scene-level density map on the ground-plane of the 3D world. We consider 3 versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model favors that features aligned to the same ground-plane point have consistent scales. We test our 3 fusion models on 3 multi-view counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view counting dataset containing a crowded street intersection. Our methods achieve state-of-the-art results compared to other multi-view counting baselines.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00849,https://www.cs.cityu.edu.hk/news/seminars/sem2018-2019_no61.pdf
78fde57462fb68530a49f913c89343da5727580d,1,1,0,DukeMTMC4ReID: A Large-Scale Multi-camera Person Re-identification Dataset,"In the past decade, research in person re-identification (re-id) has exploded due to its broad use in security and surveillance applications. Issues such as inter-camera viewpoint, illumination and pose variations make it an extremely difficult problem. Consequently, many algorithms have been proposed to tackle these issues. To validate the efficacy of re-id algorithms, numerous benchmarking datasets have been constructed. While early datasets contained relatively few identities and images, several large-scale datasets have recently been proposed, motivated by data-driven machine learning. In this paper, we introduce a new large-scale real-world re-id dataset, DukeMTMC4ReID, using 8 disjoint surveillance camera views covering parts of the Duke University campus. The dataset was created from the recently proposed fully annotated multi-target multi-camera tracking dataset DukeMTMC[36]. A benchmark summarizing extensive experiments with many combinations of existing re-id algorithms on this dataset is also provided for an up-to-date performance analysis.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.185,https://www.ecse.rpi.edu/Homepages/rjradke/papers/dukemtmc4reid.pdf
7a22ecd4a245e8b16abed36d20af43385defafc6,0,1,0,Multi-Source Transfer Network for Cross Domain Person Re-Identification,"Unsupervised person re-identification has been improved significantly by the development of cross domain person re-identification models, which apply useful knowledge in source data to completely unlabeled target data. However, existing cross domain re-identification models still remain a major limitation that they are all based on single-source and single-target setting. The only one source domain may remain a tremendous gap between target, generating negative effect for the model training in target domain. To overcome this drawback, this paper proposes a Multi-Source Transfer Network to learn a shared target-biased feature space between multi-source and target domains, which achieves transfer learning in feature-level, pixel-level, and task-level by the proposed target-biased multi-source transfer learning module, relativistic adversarial learning module, and task-gap bridging module, respectively. Through leveraging the domain gaps in feature-level, pixel-level, and task-level, this network can synthetically learn a discriminative model from multiple source domains to effectively conduct re-identification in target domain. Furthermore, this paper conducts extensive experiments on three widely-recognized person re-identification datasets, and the proposed network achieves rank-1 accuracies of 80.9% and 74.6% on DukeMTMC-reID and Market-1501 datasets, respectively. The results demonstrate the contribution of the proposed method, compared with state-of-the-art methods, including hand-crated feature, clustering and transfer learning based methods.",2020,IEEE Access,,10.1109/ACCESS.2020.2991440,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09082604.pdf
7a609736cb7d92a668dbafde515b7ef9be19bf08,1,0,0,Trajectory-Based Surveillance Analysis: A Survey,"Due to the advancement of camera hardware and machine learning techniques, video object tracking for surveillance has received noticeable attention from the computer vision research community. Object tracking and trajectory modeling have important applications in surveillance video analysis. For example, trajectory clustering, summarization or synopsis generation, and detection of anomalous or abnormal events in videos are mainly being exploited by the research community. However, barring one research work (which is almost a decade old), there is no recent review that emphasizes the use of video object trajectories, particularly in the perspective of visual surveillance. This paper presents a survey of trajectory-based surveillance applications with a focus on clustering, anomaly detection, summarization, and synopsis generation. The methods reviewed in this paper broadly summarize the abovementioned applications. The main purpose of this survey is to summarize the state-of-the-art video object trajectory analysis techniques used in the indoor and outdoor surveillance.",2019,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2018.2857489,
7a77239da35958e134a1f783a06712a37e2ca2db,0,1,0,Generating Pedestrian Images for Person Re-identification,"Person re-identification (re-ID) is mainly used to search the target pedestrian in different cameras. In this paper, we employ generative adversarial network (GAN) to expand training samples and evaluate the performance of two different label assignment strategies for the generated samples. We also investigate how the number of generated samples influences the re-ID performance. We do several experiments on the Market1501 database, and the experimental results are of essential reference value to this research field.",2018,CSPS,,10.1007/978-981-13-6504-1_5,
7a9589949aa1b78e514e26bdbf6a5197ae8e4ba9,0,1,0,SliceNet: Mask Guided Efficient Feature Augmentation for Attention-Aware Person Re-Identification,"Person re-identification (re-ID) is a challenging task since the same person captured by different cameras can appear very differently, due to the uncontrolled factors such as occlusion, illumination, viewpoint and pose variation etc. Attention-based person re-ID methods have been extensively studied to focus on discriminative regions of the last convolutional layer, which, however, ignore the low-level fine-grained information. In this paper, we propose a novel SliceNet with efficient feature augmentation modules for open-world person re-identification. Specifically, with the philosophy of divide and conquer, we divide the baseline network into three sub-networks from low, middle and high levels, which are called slice networks, followed by a Self-Alignment Attention Module respectively to learn multi-level discriminative parts. In contrast with existing works that uniformly partition the images into multiple patches, our attention module aims to learn self-alignment masks for discovering and exploiting the align-attention regions. Further, SliceNet is combined with the attention free baseline network to characterize global features. Extensive experiments on the benchmark datasets including Market-1501, CUHK03, and DukeMTMC-reID show that our proposed SliceNet achieves favorable performance compared with the state-of-the art methods.",2019,IScIDE,,10.1007/978-3-030-36189-1_8,
7b5ab0033996cf5eedce1eb95d1116864139ed96,0,1,0,Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification,"Part-level representations are important for robust person re-identification (ReID), but in practice feature quality suffers due to the body part misalignment problem. In this paper, we present a robust, compact, and easy-to-use method called the Multi-task Part-aware Network (MPN), which is designed to extract semantically aligned part-level features from pedestrian images. MPN solves the body part misalignment problem via multi-task learning (MTL) in the training stage. More specifically, it builds one main task (MT) and one auxiliary task (AT) for each body part on the top of the same backbone model. The ATs are equipped with a coarse prior of the body part locations for training images. ATs then transfer the concept of the body parts to the MTs via optimizing the MT parameters to identify part-relevant channels from the backbone model. Concept transfer is accomplished by means of two novel alignment strategies: namely, parameter space alignment via hard parameter sharing and feature space alignment in a class-wise manner. With the aid of the learned high-quality parameters, MTs can independently extract semantically aligned part-level features from relevant channels in the testing stage. Systematic experiments on four large-scale ReID databases demonstrate that MPN consistently outperforms state-of-the-art approaches by significant margins.",2020,IEEE transactions on pattern analysis and machine intelligence,2003.08069,10.1109/TPAMI.2020.3024900,https://arxiv.org/pdf/2003.08069.pdf
7b6b49adf60d56d1b33b428fdf66aff7426fca6e,1,0,0,Survey on Deep Learning Techniques for Person Re-Identification Task,"Intelligent video-surveillance is currently an active research field in computer vision and machine learning techniques. It provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one among these tools. It consists of recognizing whether an individual has already been observed over a camera in a network or not. This tool can also be employed in various possible applications such as off-line retrieval of all the video-sequences showing an individual of interest whose image is given a query, and online pedestrian tracking over multiple camera views. To this aim, many techniques have been proposed to increase the performance of PReID. Among the systems, many researchers utilized deep neural networks (DNNs) because of their better performance and fast execution at test time. Our objective is to provide for future researchers the work being done on PReID to date. Therefore, we summarized state-of-the-art DNN models being used for this task. A brief description of each model along with their evaluation on a set of benchmark datasets is given. Finally, a detailed comparison is provided among these models followed by some limitations that can work as guidelines for future research.",2018,ArXiv,1807.05284,,https://arxiv.org/pdf/1807.05284.pdf
7b9b4f4b29af7d2dd74247815c81ea9915d1259a,1,1,0,Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification,"Person re-identification (re-ID), is a challenging task due to the high variance within identity samples and imaging conditions. Although recent advances in deep learning have achieved remarkable accuracy in settled scenes, i.e., source domain, few works can generalize well on the unseen target domain. One popular solution is assigning unlabeled target images with pseudo labels by clustering, and then retraining the model. However, clustering methods tend to introduce noisy labels and discard low confidence samples as outliers, which may hinder the retraining process and thus limit the generalization ability. In this study, we argue that by explicitly adding a sample filtering procedure after the clustering, the mined examples can be much more efficiently used. To this end, we design an asymmetric co-teaching framework, which resists noisy labels by cooperating two models to select data with possibly clean labels for each other. Meanwhile, one of the models receives samples as pure as possible, while the other takes in samples as diverse as possible. This procedure encourages that the selected training samples can be both clean and miscellaneous, and that the two models can promote each other iteratively. Extensive experiments show that the proposed framework can consistently benefit most clustering based methods, and boost the state-of-the-art adaptation accuracy. Our code is available at https://github.com/FlyingRoastDuck/ACT_AAAI20.",2020,AAAI,1912.01349,10.1609/AAAI.V34I07.6950,https://arxiv.org/pdf/1912.01349.pdf
7bfc5bbad852f9e6bea3b86c25179d81e2e7fff6,1,1,0,Online Inter-Camera Trajectory Association Exploiting Person Re-Identification and Camera Topology,"Online inter-camera trajectory association is a promising topic in intelligent video surveillance, which concentrates on associating trajectories belong to the same individual across different cameras according to time. It remains challenging due to the inconsistent appearance of a person in different cameras and the lack of spatio-temporal constraints between cameras. Besides, the orientation variations and the partial occlusions significantly increase the difficulty of inter-camera trajectory association. Targeting to solve these problems, this work proposes an orientation-driven person re-identification (ODPR) and an effective camera topology estimation based on appearance features for online inter-camera trajectory association. ODPR explicitly leverages the orientation cues and stable torso features to learn discriminative feature representations for identifying trajectories across cameras, which alleviates the pedestrian orientation variations by the designed orientation-driven loss function and orientation aware weights. The effective camera topology estimation introduces appearance features to generate the correct spatio-temporal constraints for narrowing the retrieval range, which improves the time efficiency and provides the possibility for intelligent inter-camera trajectory association in large-scale surveillance environments. Extensive experimental results demonstrate that our proposed approach significantly outperforms most state-of-the-art methods on the popular person re-identification datasets and the public multi-target, multi-camera tracking benchmark.",2018,ACM Multimedia,,10.1145/3240508.3240663,
7c25f32e78d0a9ae75825445daf51b064c3f58e9,0,1,0,Diversity Regularized Spatiotemporal Attention for Video-Based Person Re-identification,"Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts. This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments. The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part. Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.09882,10.1109/CVPR.2018.00046,https://arxiv.org/pdf/1803.09882.pdf
7c266a09cf567ea912e1951cad6f819b8e2a13b8,1,1,0,Push for Center Learning via Orthogonalization and Subspace Masking for Person Re-Identification.,"Person re-identification aims to identify whether pairs of images belong to the same person or not. This problem is challenging due to large differences in camera views, lighting and background. One of the mainstream in learning CNN features is to design loss functions which reinforce both the class separation and intra-class compactness. In this paper, we propose a novel Orthogonal Center Learning method with Subspace Masking for person re-identification. We make the following contributions: (i) we develop a center learning module to learn the class centers by simultaneously reducing the intra-class differences and inter-class correlations by orthogonalization; (ii) we introduce a subspace masking mechanism to enhance the generalization of the learned class centers; and (iii) we propose to integrate the average pooling and max pooling in a regularizing manner that fully exploits their powers. Extensive experiments show that our proposed method consistently outperforms the state-of-the-art methods on large-scale ReID datasets including Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17.",2020,IEEE transactions on image processing : a publication of the IEEE Signal Processing Society,1908.10535,10.1109/tip.2020.3036720,https://arxiv.org/pdf/1908.10535.pdf
7c306b73511e8a5a422c1706de56ee5316cc6d81,1,0,0,Domain Adaptive Person Re-Identification via Camera Style Generation and Label Propagation,"Unsupervised domain adaptation in person re-identification resorts to labeled source data to promote the model training on target domain, facing the dilemmas caused by large domain shift and large camera variations. The non-overlapping labels challenge that the source domain and the target domain have entirely different persons further increases the re-identification difficulty. In this paper, we propose a novel algorithm to narrow such domain gaps. We derive a camera style adaptation framework to learn the style-based mappings between different camera views, from the target domain to the source domain, and then we can transfer the identity-based distribution from the source domain to the target domain on the camera level. Target camera variations can be captured by the style adaptation method, thus, the re-identification model trained on the target domain can learn target camera-invariant features better. It indicates that the style translator approximates an appropriate metric space for improving feature matching. To overcome the non-overlapping labels challenge and guide the person re-identification model to narrow the gap further, an efficient and effective soft-labeling method is proposed to mine the intrinsic local structure of the target domain through building the connection between GAN-translated source domain and the target domain. Experiment results conducted on real benchmark datasets indicate that our method gets state-of-the-art results.",2020,IEEE Transactions on Information Forensics and Security,1905.05382,10.1109/TIFS.2019.2939750,https://arxiv.org/pdf/1905.05382.pdf
7c58c7b455839272a5ac395bcd0529a527dd5667,0,1,0,Unity Style Transfer for Person Re-Identification,"Style variation has been a major challenge for person re-identification, which aims to match the same pedestrians across different cameras. Existing works attempted to address this problem with camera-invariant descriptor subspace learning. However, there will be more image artifacts when the difference between the images taken by different cameras is larger. To solve this problem, we propose a UnityStyle adaption method, which can smooth the style disparities within the same camera and across different cameras. Specifically, we firstly create UnityGAN to learn the style changes between cameras, producing shape-stable style-unity images for each camera, which is called UnityStyle images. Meanwhile, we use UnityStyle images to eliminate style differences between different images, which makes a better match between query and gallery. Then, we apply the proposed method to Re-ID models, expecting to obtain more style-robust depth features for querying. We conduct extensive experiments on widely used benchmark datasets to evaluate the performance of the proposed framework, the results of which confirm the superiority of the proposed model.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.02068,10.1109/cvpr42600.2020.00692,https://arxiv.org/pdf/2003.02068.pdf
7c75af24b77833a38d0a9bbb4c473a011f3856f7,1,1,0,Locality Aware Appearance Metric for Multi-Target Multi-Camera Tracking,"Multi-target multi-camera tracking (MTMCT) systems track targets across cameras. Due to the continuity of target trajectories, tracking systems usually restrict their data association within a local neighborhood. In single camera tracking, local neighborhood refers to consecutive frames; in multi-camera tracking, it refers to neighboring cameras that the target may appear successively. For similarity estimation, tracking systems often adopt appearance features learned from the re-identification (re-ID) perspective. Different from tracking, re-ID usually does not have access to the trajectory cues that can limit the search space to a local neighborhood. Due to its global matching property, the re-ID perspective requires to learn global appearance features. We argue that the mismatch between the local matching procedure in tracking and the global nature of re-ID appearance features may compromise MTMCT performance.  To fit the local matching procedure in MTMCT, in this work, we introduce locality aware appearance metric (LAAM). Specifically, we design an intra-camera metric for single camera tracking, and an inter-camera metric for multi-camera tracking. Both metrics are trained with data pairs sampled from their corresponding local neighborhoods, as opposed to global sampling in the re-ID perspective. We show that the locally learned metrics can be successfully applied on top of several globally learned re-ID features. With the proposed method, we report new state-of-the-art performance on the DukeMTMC dataset, and a substantial improvement on the CityFlow dataset.",2019,ArXiv,1911.12037,,https://arxiv.org/pdf/1911.12037.pdf
7c92a03fd728aeab020fafd9b256b903a35b367c,1,0,0,A Comprehensive Study on Large-Scale Person Retrieval in Real Surveillance Scenarios,"Person retrieval is a hot research topic due to its important application potential for public security. Though existing algorithms have achieved impressive progresses on current public datasets, it is still a challenging task in the real surveillance scenarios due to the various viewpoints, pose variations and occlusions. Moreover, few of the existing works study the problem of person retrieval on large-scale gallery set, where lots of distractions may deteriorate the retrieval results heavily. To have a deep understanding on the above challenges, we perform a comprehensive study on current state-of-the-art person retrieval algorithms with a large-scale benchmark in real surveillance scenarios. In the study, two kinds of techniques, i.e., attribute recognition and person re-identification, including eight algorithms, are evaluated at both algorithm level and system level. Here, the system-level evaluations investigate the effects of the combinations of the above algorithms with the module of person detection, where lots of distractions in person detection results pose a big challenge for person retrieval in real scenes. Extensive evaluations with large gallery sizes (up to 243k) and comprehensive analyses are presented in the study, which will guide researchers to develop more advanced algorithms in future.",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909851,
7c9bfdfcdb36a32710a19b9f7dadabc32a9cca1b,0,1,0,Person re-identification with expanded neighborhoods distance re-ranking,"Abstract In the person re-identification (re-ID) community, pedestrians often have great changes in appearance, and there are many similar persons, which incurs will degrades the accuracy. Re-ranking is an effective method to solve these problems, this paper proposes an expanded neighborhoods distance (END) to re-rank the re-ID results. We assume that if the two persons in different image are same, their initial ranking lists and two-level neighborhoods will be very similar when they are taken as the query. Our method follows the principle of similarity, and selects expanded neighborhoods in initial ranking list to calculate the END distance. Final distance is calculated as the combination of the END distance and Jaccard distance. Experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets confirm the effectiveness of the novel re-ranking method in this article. Compare with re-ID baseline, the proposed method in this paper increases mAP by 14.2% on Market-1501 and Rank1 by 12.9% on DukeMTMC-reID.",2020,Image Vis. Comput.,,10.1016/j.imavis.2020.103875,
7c9d8593cdf2f8ba9f27906b2b5827b145631a0b,0,1,0,MsCGAN: Multi-scale Conditional Generative Adversarial Networks for Person Image Generation,"To synthesize high-quality person images with arbitrary poses is challenging. In this paper, we propose a novel Multi-scale Conditional Generative Adversarial Networks (MsCGAN), aiming to convert the input conditional person image to a synthetic image of any given target pose, whose appearance and the texture are consistent with the input image. MsCGAN is a multi-scale adversarial network consisting of two generators and two discriminators. One generator transforms the conditional person image into a coarse image of the target pose globally, and the other is to enhance the detailed quality of the synthetic person image through a local reinforcement network. The outputs of the two generators are then merged into a synthetic, discriminant and high-resolution image. On the other hand, the synthetic image is downsampled to multiple resolutions as the input to multi-scale discriminator networks. The proposed multi-scale generators and discriminators handling different levels of visual features can benefit to synthesizing high-resolution person images with realistic appearance and texture. Experiments are conducted on the Market-1501 and DeepFashion datasets to evaluate the proposed model, and both qualitative and quantitative results demonstrate the superior performance of the proposed MsCGAN.",2020,2020 Chinese Control And Decision Conference (CCDC),1810.08534,10.1109/ccdc49329.2020.9164755,https://arxiv.org/pdf/1810.08534.pdf
7ced52cf176a96dee39f774bb225cb7e0e2d4f92,1,0,1,Concentrated Multi-Grained Multi-Attention Network for Video Based Person Re-Identification,"Occlusion is still a severe problem in the video-based Re-IDentification (Re-ID) task, which has a great impact on the success rate. The attention mechanism has been proved to be helpful in solving the occlusion problem by a large number of existing methods. However, their attention mechanisms still lack the capability to extract sufficient discriminative information into the final representations from the videos. The single attention module scheme employed by existing methods cannot exploit multi-scale spatial cues, and the attention of the single module will be dispersed by multiple salient parts of the person. In this paper, we propose a Concentrated Multi-grained Multi-Attention Network (CMMANet) where two multi-attention modules are designed to extract multi-grained information through processing multi-scale intermediate features. Furthermore, multiple attention submodules in each multi-attention module can automatically discover multiple discriminative regions of the video frames. To achieve this goal, we introduce a diversity loss to diversify the submodules in each multi-attention module, and a concentration loss to integrate their attention responses so that each submodule can strongly focus on a specific meaningful part. The experimental results show that the proposed approach outperforms the state-of-the-art methods by large margins on multiple public datasets.",2020,ArXiv,2009.13019,,https://arxiv.org/pdf/2009.13019.pdf
7d4916d528e093374fc1e7fd6219c8711d5efaaa,1,0,0,Weakly Supervised Open-Set Domain Adaptation by Dual-Domain Collaboration,"In conventional domain adaptation, a critical assumption is that there exists a fully labeled domain (source) that contains the same label space as another unlabeled or scarcely labeled domain (target). However, in the real world, there often exist application scenarios in which both domains are partially labeled and not all classes are shared between these two domains. Thus, it is meaningful to let partially labeled domains learn from each other to classify all the unlabeled samples in each domain under an open-set setting. We consider this problem as weakly supervised open-set domain adaptation. To address this practical setting, we propose the Collaborative Distribution Alignment (CDA) method, which performs knowledge transfer bilaterally and works collaboratively to classify unlabeled data and identify outlier samples. Extensive experiments on the Office benchmark and an application on person reidentification show that our method achieves state-of-the-art performance.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.13179,10.1109/CVPR.2019.00554,https://arxiv.org/pdf/1904.13179.pdf
7d7be3b939c5063f41e55ee63207b31f62050921,0,1,0,Multi-level attention model for person re-identification,"Abstract Person re-identification (re-ID) is an important research topic in computer vision. Due to its important applications in video surveillance, it has been receiving increasing attention. The key challenge of this task is how to capture appearance variations under different camera views. Current state-of-the-art methods employ deep CNN feature and part-level representations to generate robust representation for pedestrians. However, when human parts are not well located and aligned, discriminative information is difficult to be captured. To address this issue, we propose a feature attention block for person re-ID task. The proposed block learns part-level attention on different local regions, and the weighted part-level features are pooled into a global representation. The proposed attention block can be extended to multi-level situation and generates more robust representation. The proposed feature attention block can be seamlessly integrated into existing CNN structures (e.g., ResNet and DenseNet), and is trained only with identity loss. We conduct extensive experiments on three popular person re-ID benchmarks including Market-1501, DukeMTMC-reID, and CUHK03. The proposed framework achieves promising results compared with current state-of-the-arts.",2019,Pattern Recognit. Lett.,,10.1016/J.PATREC.2018.08.024,
7d8d0b47db6078ad30ee09bd18ea4efe15cd1214,0,1,0,Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-identification,"Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identity-discriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of-the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.09786,10.1109/CVPR.2018.00242,https://arxiv.org/pdf/1803.09786.pdf
7da74354fbe91185e68ca5cf19bd95426b46ee70,0,1,0,Grafted network for person re-identification,"Abstract Convolutional neural networks have shown outstanding effectiveness in person re-identification (re-ID). However, the models always have large number of parameters and much computation for mobile application. In order to relieve this problem, we propose a novel grafted network (GraftedNet), which is designed by grafting a high-accuracy rootstock and a light-weighted scion. The rootstock is based on the former parts of ResNet-50 to provide a strong baseline, while the scion is a new designed module, composed of the latter parts of SqueezeNet, to compress the parameters. To extract more discriminative feature representation, a joint multi-level and part-based feature is proposed. In addition, to train GraftedNet efficiently, we propose an accompanying learning method, by adding an accompanying branch to train the model in training and removing it in testing for saving parameters and computation. On three public person re-ID benchmarks (Market1501, DukeMTMC-reID and CUHK03), the effectiveness of GraftedNet is evaluated and its components are analyzed. Experimental results show that the proposed GraftedNet achieves 93.02%, 85.3% and 76.2% in Rank-1 and 81.6%, 74.7% and 71.6% in mAP, with only 4.6M parameters.",2020,Signal Process. Image Commun.,2006.01967,10.1016/j.image.2019.115674,https://arxiv.org/pdf/2006.01967.pdf
7daa2c0f76fd3bfc7feadf313d6ac7504d4ecd20,1,1,0,Dual Attention Matching Network for Context-Aware Feature Sequence Based Person Re-identification,"Typical person re-identification (ReID) methods usually describe each pedestrian with a single feature vector and match them in a task-specific metric space. However, the methods based on a single feature vector are not sufficient enough to overcome visual ambiguity, which frequently occurs in real scenario. In this paper, we propose a novel end-to-end trainable framework, called Dual ATtention Matching network (DuATM), to learn context-aware feature sequences and perform attentive sequence comparison simultaneously. The core component of our DuATM framework is a dual attention mechanism, in which both intrasequence and inter-sequence attention strategies are used for feature refinement and feature-pair alignment, respectively. Thus, detailed visual cues contained in the intermediate feature sequences can be automatically exploited and properly compared. We train the proposed DuATM network as a siamese network via a triplet loss assisted with a decorrelation loss and a cross-entropy loss. We conduct extensive experiments on both image and video based ReID benchmark datasets. Experimental results demonstrate the significant advantages of our approach compared to the state-of-the-art methods.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.09937,10.1109/CVPR.2018.00562,https://arxiv.org/pdf/1803.09937.pdf
7dac9cc7e0b4ad6e63db59cdefd3a805bd1db279,1,1,0,Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification,"Unsupervised domain adaptive person Re-IDentification (ReID) is challenging because of the large domain gap between source and target domains, as well as the lackage of labeled data on the target domain. This paper tackles this challenge through jointly enforcing visual and temporal consistency in the combination of a local one-hot classification and a global multi-class classification. The local one-hot classification assigns images in a training batch with different person IDs, then adopts a Self-Adaptive Classification (SAC) model to classify them. The global multi-class classification is achieved by predicting labels on the entire unlabeled training set with the Memory-based Temporal-guided Cluster (MTC). MTC predicts multi-class labels by considering both visual similarity and temporal consistency to ensure the quality of label prediction. The two classification models are combined in a unified framework, which effectively leverages the unlabeled data for discriminative feature learning. Experimental results on three large-scale ReID datasets demonstrate the superiority of proposed method in both unsupervised and unsupervised domain adaptive ReID tasks. For example, under unsupervised setting, our method outperforms recent unsupervised domain adaptive methods, which leverage more labels for training.",2020,ECCV,2007.10854,10.1007/978-3-030-58586-0_29,https://arxiv.org/pdf/2007.10854.pdf
7dbb212555aa099673530b9298384bd562db7b6d,1,1,0,End-to-End Deep Kronecker-Product Matching for Person Re-identification,"Person re-identification aims to robustly measure similarities between person images. The significant variation of person poses and viewing angles challenges for accurate person re-identification. The spatial layout and correspondences between query person images are vital information for tackling this problem but are ignored by most state-of-the-art methods. In this paper, we propose a novel Kronecker Product Matching module to match feature maps of different persons in an end-to-end trainable deep neural network. A novel feature soft warping scheme is designed for aligning the feature maps based on matching results, which is shown to be crucial for achieving superior accuracy. The multi-scale features based on hourglass-like networks and self residual attention are also exploited to further boost the re-identification performance. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1807.11182,10.1109/CVPR.2018.00720,https://arxiv.org/pdf/1807.11182.pdf
7dc8762c415ca37ea32f110b5863c49a739bb5fb,1,1,0,The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching,"Many vision problems require matching images of object instances across different domains. These include fine-grained sketch-based image retrieval (FG-SBIR) and Person Re-identification (person ReID). Existing approaches attempt to learn a joint embedding space where images from different domains can be directly compared. In most cases, this space is defined by the output of the final layer of a deep neural network (DNN), which primarily contains features of a high semantic level. In this paper, we argue that both high and mid-level features are relevant for cross-domain instance matching (CDIM). Importantly, mid-level features already exist in earlier layers of the DNN. They just need to be extracted, represented, and fused properly with the final layer. Based on this simple but powerful idea, we propose a unified framework for CDIM. Instantiating our framework for FG-SBIR and ReID, we show that our simple models can easily beat the state-of-the-art models, which are often equipped with much more elaborate architectures.",2017,ArXiv,1711.08106,,https://arxiv.org/pdf/1711.08106.pdf
7e1b1eb0e6ce537dbff4f48df2fe1d8a8b171a4a,1,1,0,Learning Multi-Scale Features and Batch-Normalized Global Features for Person Re-Identification,"In recent years, person re-identification based on deep learning approaches has made great progress and achieved good results. However, many of the latest network design methods, which usually deploy ResNet or SENet as the backbone network, were originally designed for classification tasks. Since the person re-identification task is essentially different from the classification task, the structure of the backbone network should be modified accordingly. In this paper, we propose a retrieval network based on a multi-scale backbone architecture, which is specifically suitable for the person re-identification task. By constructing hierarchical residual-like connections within a single residual block, the model learns multi-scale discriminative features of pedestrian images. Unlike many state-of-the-art methods that use complex network structures and concatenate multi-branch features, our proposed retrieval network is implemented using only global features, simple triplet loss, and softmax with cross-entropy loss. The results of extensive experiments show that the proposed network has stronger fine-grained pedestrian representation ability, leading to performance gains for person re-identification tasks. Our proposed network achieves a rank-1 accuracy of 96.03% on the Market-1501 and 92.11% on DukeMTMC-reID datasets while only using global features.",2020,IEEE Access,,10.1109/ACCESS.2020.3029594,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09217451.pdf
7e22e678c070932f6ddd907ac8ea1ab6a7714808,1,0,0,Discriminative Feature Learning With Consistent Attention Regularization for Person Re-Identification,"Person re-identification (Re-ID) has undergone a rapid development with the blooming of deep neural network. Most methods are very easily affected by target misalignment and background clutter in the training process. In this paper, we propose a simple yet effective feedforward attention network to address the two mentioned problems, in which a novel consistent attention regularizer and an improved triplet loss are designed to learn foreground attentive features for person Re-ID. Specifically, the consistent attention regularizer aims to keep the deduced foreground masks similar from the low-level, mid-level and high-level feature maps. As a result, the network will focus on the foreground regions at the lower layers, which is benefit to learn discriminative features from the foreground regions at the higher layers. Last but not least, the improved triplet loss is introduced to enhance the feature learning capability, which can jointly minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Experimental results on the Market1501, DukeMTMC-reID and CUHK03 datasets have shown that our method outperforms most of the state-of-the-art approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00813,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.pdf
7e500d4cee87ef6b06cf9a22dd546d34ad05a66f,1,0,1,Spatio-Temporal Associative Representation for Video Person Re-Identification,"Learning discriminative spatio-temporal representation is the key for solving video re-identification (re-id) challenges. Most existing methods focus on learning appearance features and/or selecting image frames, but ignore optimising the compatibility and interaction of appearance and motion attentive information. To address this limitation, we propose a novel model to learning Spatio-Temporal Associative Representation (STAR). We design local frame-level spatio-temporal association to learn discriminative attentive appearance and short-term motion features, and global video-level spatio-temporal association to form compact and discriminative holistic video representation. We further introduce a pyramid ranking regulariser for facilitating end-to-end model optimisation. Extensive experiments demonstrate the superiority of STAR against state-of-the-art methods on four video re-id benchmarks, including MARS, DukeMTMC-VideoReID, iLIDS-VID and PRID-2011.",2019,,,,http://www.eecs.qmul.ac.uk/~sgg/papers/WuEtAl_BMVC2019.pdf
7e50fd37e278f11df307fc0a3e084b061c78bbd3,1,0,0,Structured Domain Adaptation with Online Relation Regularization for Unsupervised Person Re-ID,"Unsupervised domain adaptation (UDA) aims at adapting the model trained on a labeled source-domain dataset to an unlabeled target-domain dataset. The task of UDA on open-set person re-identification (re-ID) is even more challenging as the identities (classes) do not overlap between the two domains. One major research direction was based on domain translation, which, however, has fallen out of favor in recent years due to inferior performance compared to pseudo-label-based methods. We argue that translation-based methods have great potential on exploiting the valuable source-domain data but they did not provide proper regularization on the translation process. Specifically, these methods only focus on maintaining the identities of the translated images while ignoring the inter-sample relation during translation. To tackle the challenge, we propose an end-to-end structured domain adaptation framework with an online relation-consistency regularization term. During training, the person feature encoder is optimized to model inter-sample relations on-the-fly for supervising relation-consistency domain translation, which in turn, improves the encoder with informative translated images. An improved pseudo-label-based encoder can therefore be obtained by jointly training the source-to-target translated images with ground-truth identities and target-domain images with pseudo identities. In the experiments, our proposed framework is shown to outperform state-of-the-art methods on multiple UDA tasks of person re-ID. Code is available at this https URL.",2020,,2003.0665,,https://arxiv.org/pdf/2003.06650.pdf
7eac8af0344d6e078246bbd13d76842a70d1b148,1,0,0,Relation Network for Person Re-identification,"Person re-identification (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model outperforms the state of the art on the Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations.",2020,AAAI,1911.09318,10.1609/AAAI.V34I07.6857,https://arxiv.org/pdf/1911.09318.pdf
7ee8898ad50fcaf7c38a0d9375339e87565346b3,1,1,0,Selective transfer cycle GAN for unsupervised person re-identification,"Recently, many researchers pay more attention to unsupervised person re-identification (UPRID) due to its scalability and flexibility without any labeled data. The popular methods for UPRID are divided into clustering based and transfer learning based approaches. However, both of them either lack pre-knowledge or some useless knowledge transfered, which always achieves poor matching performance. Specifically, transfer learning based methods can utilize existing labeled data to boost the training of unlabeled data in target domain. Nevertheless, they generally offer weaker re-identification performance because there are a lot of negative images in source domain bringing negative effect on accuracy for the target domain, which is very challenging to address this negative source data removing problem. In this paper, we propose a Selective Transfer Cycle Generative Adversarial Network (STCGAN) by selecting “valuable” source domain knowledge to boost the training efficiency in unlabeled target domain. Our STCGAN approach is developed from a Cycle GAN attached a selector for source data, a part based feature extractor for target data and reconstructed source images following target distribution. Such that it can simultaneously learn “valuable” source images through the selector and exploit transferable discriminative information from these selected source images into target domain. Then, we introduce a joint optimization method and conduct extensively experiments on two widely used person re-identification datasets. The results show the superiority of the proposed STCGAN model over a range of the-state-of-the-arts.",2020,Multimedia Tools and Applications,,10.1007/s11042-019-08604-y,
7eeb12e7f709113d8626b1c7a7ec235367640aa9,1,1,0,Front-End Smart Visual Sensing and Back-End Intelligent Analysis: A Unified Infrastructure for Economizing the Visual System of City Brain,"The visual data, which are acquired from the ubiquitous visual sensors deployed in metropolitans, are of great value and paramount significance to enhance the effectiveness and pursue the future development of smart cities. In this paper, the essential building blocks of the unified visual data management and analysis infrastructure that serve as the foundation for the economical visual system in the city brain, are introduced to facilitate the utilization of the visual signal in the artificial intelligence era. In particular, we start by the discussion of the front-end smart visual sensing in the context of economical communication and service with the heterogeneous network, and the functionalities and necessities of compact visual feature and deep learning model representations are detailed. Subsequently, the utilities of the infrastructure are demonstrated through two intelligent applications at the back-end, including vehicle re-identification and person re-identification. The standardizations regarding compact feature and deep neural network representations, which are regarded as the key ingredients in this infrastructure and greatly facilitate the construction of the visual system in the city brain, are also discussed. Finally, we envision how the potential issues regarding the economical visual communications for future smart cities might be pragmatically approached within this unified infrastructure.",2019,IEEE Journal on Selected Areas in Communications,,10.1109/JSAC.2019.2916488,
7f23a4bb0c777dd72cca7665a5f370ac7980217e,1,1,1,Improving Person Re-identification by Attribute and Identity Learning,"Abstract Person re-identification (re-ID) and attribute recognition share a common target at learning pedestrian descriptions. Their difference consists in the granularity. Most existing re-ID methods only take identity labels of pedestrians into consideration. However, we find the attributes, containing detailed local descriptions, are beneficial in allowing the re-ID model to learn more discriminative feature representations. In this paper, based on the complementarity of attribute labels and ID labels, we propose an attribute-person recognition (APR) network, a multi-task network which learns a re-ID embedding and at the same time predicts pedestrian attributes. We manually annotate attribute labels for two large-scale re-ID datasets, and systematically investigate how person re-ID and attribute recognition benefit from each other. In addition, we re-weight the attribute predictions considering the dependencies and correlations among the attributes. The experimental results on two large-scale re-ID benchmarks demonstrate that by learning a more discriminative representation, APR achieves competitive re-ID performance compared with the state-of-the-art methods. We use APR to speed up the retrieval process by ten times with a minor accuracy drop of 2.92% on Market-1501. Besides, we also apply APR on the attribute recognition task and demonstrate improvement over the baselines.",2019,Pattern Recognit.,1703.0722,10.1016/j.patcog.2019.06.006,https://arxiv.org/pdf/1703.07220.pdf
7f261b047f4b5d182af129a12935a512c0a65385,1,0,1,Vision Meets Wireless Positioning: Effective Person Re-identification with Recurrent Context Propagation,"Existing person re-identification methods rely on the visual sensor to capture the pedestrians. The image or video data from visual sensor inevitably suffers the occlusion and dramatic variations of pedestrian postures, which degrades the re-identification performance and further limits its application to the open environment. On the other hand, for most people, one of the most important carry-on items is the mobile phone, which can be sensed by WiFi and cellular networks in the form of a wireless positioning signal. Such signal is robust to the pedestrian occlusion and visual appearance change, but suffers some positioning error. In this work, we approach person re-identification with the sensing data from both vision and wireless positioning. To take advantage of such cross-modality cues, we propose a novel recurrent context propagation module that enables information to propagate between visual data and wireless positioning data and finally improves the matching accuracy. To evaluate our approach, we contribute a new Wireless Positioning Person Re-identification (WP-ReID) dataset. Extensive experiments are conducted and demonstrate the effectiveness of the proposed algorithm. Code will be released at https://github.com/yolomax/WP-ReID.",2020,ACM Multimedia,2008.04146,10.1145/3394171.3413984,https://arxiv.org/pdf/2008.04146.pdf
7f2fca65dea8365d9013eac84f285db1581e869e,0,0,1,"The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices","Over the last decades, the world has been witnessing growing threats to the security in urban spaces, which has augmented the relevance given to visual surveillance solutions able to detect, track and identify persons of interest in crowds. In particular, unmanned aerial vehicles (UAVs) are a potential tool for this kind of analysis, as they provide a cheap way for data collection, cover large and difficult-to-reach areas, while reducing human staff demands. In this context, all the available datasets are exclusively suitable for the pedestrian re-identification problem, in which the multi-camera views per ID are taken on a single day, and allows the use of clothing appearance features for identification purposes. Accordingly, the main contributions of this paper are two-fold: 1) we announce the UAV-based P-DESTRE dataset, which is the first of its kind to provide consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions; and 2) we compare the results attained by state-of-the-art pedestrian detection, tracking, reidentification and search techniques in well-known surveillance datasets, to the effectiveness obtained by the same techniques in the P-DESTRE data. Such comparison enables to identify the most problematic data degradation factors of UAV-based data for each task, and can be used as baselines for subsequent advances in this kind of technology. The dataset and the full details of the empirical evaluation carried out are freely available at this http URL.",2020,ArXiv,2004.02782,,https://arxiv.org/pdf/2004.02782.pdf
7f467eadab1a1b8aa3159e2bae2bd54acbe529a4,1,0,0,Cross-domain person re-identification using Dual Generation Learning in camera sensor networks,,2020,Ad Hoc Networks,,10.1016/j.adhoc.2019.102019,
7f68f265cdfc31db4e93e4899f0f115e2952889e,0,1,0,Multitask Person Re-Identification using Homoscedastic Uncertainty Learning,"In this paper, we propose a new multitask neural network called Part Attribute Loss Net (PALNet) for person re-identification (re-id) with homoscedastic uncertainty learning. Currently, many person re-id algorithms use person identity as the main ground truth information to train and to perform prediction task. This single task approach is simple to setup but usually provides poor generalization performance. Some other person re-id works [1] [2] incorporate additional cues such as body parts to improve the learned representations. However, this requires additional body part annotations. Person attributes, which are present in some of the large person re-id datasets like Market1501 [3] and DukeMTMC-reID [4], are not used. Furthermore, the multitask networks found in some of the recent works use heuristic approach in weighing their task losses. Our PALNet seeks to address these issues by leveraging on person identity classification, body part detection and person attribute prediction, all built into a unified network. We also incorporate the homoscedastic uncertainty learning to automatically determine the weighing of the loss function of the different tasks. The PALNet consists of three tasks, with person identity classification as the main task. The body part detection and person attributes are two sub tasks to share different learning experience with the main task. The uncertainty learning provide us good indication of the observable task noises and allows us to optimize the accuracy performance without the time consuming grid search method. When benchmarking on DukeMTMC-reID dataset [4], our approach outperforms other state-of-the-art methods. On Market1501 dataset [3], we are on par with the best state-of-the-art method but outperforms the rest by at least 6.4% mAP and 3.5% Rank-1 accuracy. These results are reported without re-ranking.",2019,2019 IEEE International Symposium on Circuits and Systems (ISCAS),,10.1109/ISCAS.2019.8702566,
7fabb871deb4da96c30cedaa227559bb459885e6,1,1,1,Person Re-Identification Based on Multi-Parts of Local Feature Network,"Person re-identification has become increasing popular because of its widely application in computer vision. In this paper, we propose a novel, simple and efficient person re-id network called MPLFN. The network combines two tasks: the classification task and the metric learning task. In the classification task, we uniformly partition N feature parts from an image, and compute the person classification loss in each part separately. Computing the part loss separately guides the network to focus on every body part and learn discriminative representations for each of them. And then in the metric learning task, we recalculate the distance of two images by the shortest path between two sets of feature parts. Then the distances are put into a triplet loss to perform a dynamic part alignment during the training. With the joint learning of these two tasks, the performance of the network is significantly enhanced. Compared with existing person re-id works, MPLFN achieves a better performance on three mainstream person re-identification datasets. Extensive experiments have been conducted to validate our proposed method.",2019,IEEE Access,,10.1109/ACCESS.2019.2941002,
7fdd1b95b7a41abde5efa4d0cec93d75d152b925,0,1,0,Multi-Scale Body-Part Mask Guided Attention for Person Re-Identification,"Person re-identification becomes a more and more important task due to its wide applications. In practice, person re-identification still remains challenging due to the variation of person pose, different lighting, occlusion, misalignment, background clutter, etc. In this paper, we propose a multi-scale body-part mask guided attention network (MMGA), which jointly learns whole-body and part-body attention to help extract global and local features simultaneously. In MMGA, body-part masks are used to guide the training of corresponding attention. Experiments show that our proposed method can reduce the negative influence of variation of person pose, misalignment and background clutter. Our method achieves rank-1/mAP of 95.0%/87.2% on the Market1501 dataset, 89.5%/78.1% on the DukeMTMC-reID dataset, outperforming current state-of-the-art methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1904.11041,10.1109/CVPRW.2019.00197,https://arxiv.org/pdf/1904.11041.pdf
8016c973136e75129a6fe78503aed6d8608084f3,1,1,0,Imbalance Robust Softmax for Deep Embeeding Learning,"Deep embedding learning is expected to learn a metric space in which features have smaller maximal intra-class distance than minimal inter-class distance. In recent years, one research focus is to solve the open-set problem by discriminative deep embedding learning in the field of face recognition (FR) and person re-identification (re-ID). Apart from open-set problem, we find that imbalanced training data is another main factor causing the performance degradation of FR and re-ID, and data imbalance widely exists in the real applications. However, very little research explores why and how data imbalance influences the performance of FR and re-ID with softmax or its variants. In this work, we deeply investigate data imbalance in the perspective of neural network optimisation and feature distribution about softmax. We find one main reason of performance degradation caused by data imbalance is that the weights (from the penultimate fully-connected layer) are far from their class centers in feature space. Based on this investigation, we propose a unified framework, Imbalance-Robust Softmax (IR-Softmax), which can simultaneously solve the open-set problem and reduce the influence of data imbalance. IR-Softmax can generalise to any softmax and its variants (which are discriminative for open-set problem) by directly setting the weights as their class centers, naturally solving the data imbalance problem. In this work, we explicitly re-formulate two discriminative softmax (A-Softmax and AM-Softmax) under the framework of IR-Softmax. We conduct extensive experiments on FR databases (LFW, MegaFace) and re-ID database (Market-1501, Duke), and IR-Softmax outperforms many state-of-the-art methods.",2020,ArXiv,2011.11155,,https://arxiv.org/pdf/2011.11155.pdf
80359f64343722f2fa6b3fb2013ffd643c54f3a8,1,1,0,Long-Term Cloth-Changing Person Re-identification,"Person re-identification (Re-ID) aims to match a target person across camera views at different locations and times. Existing Re-ID studies focus on the short-term cloth-consistent setting, under which a person re-appears in different camera views with the same outfit. A discriminative feature representation learned by existing deep Re-ID models is thus dominated by the visual appearance of clothing. In this work, we focus on a much more difficult yet practical setting where person matching is conducted over long-duration, e.g., over days and months and therefore inevitably under the new challenge of changing clothes. This problem, termed Long-Term Cloth-Changing (LTCC) Re-ID is much understudied due to the lack of large scale datasets. The first contribution of this work is a new LTCC dataset containing people captured over a long period of time with frequent clothing changes. As a second contribution, we propose a novel Re-ID method specifically designed to address the cloth-changing challenge. Specifically, we consider that under cloth-changes, soft-biometrics such as body shape would be more reliable. We, therefore, introduce a shape embedding module as well as a cloth-elimination shape-distillation module aiming to eliminate the now unreliable clothing appearance features and focus on the body shape information. Extensive experiments show that superior performance is achieved by the proposed model on the new LTCC dataset. The code and dataset will be available at this https URL.",2020,ArXiv,2005.12633,,https://arxiv.org/pdf/2005.12633.pdf
807f7aadba3f960ed8c09acf10daaee384a4944e,0,1,0,Visible-Infrared Person Re-Identification via Homogeneous Augmented Tri-Modal Learning,"Matching person images between the daytime visible modality and night-time infrared modality (VI-ReID) is a challenging cross-modality pedestrian retrieval problem. Existing methods usually learn the multi-modality features in raw image, ignoring the image-level discrepancy. Some methods apply GAN technique to generate the cross-modality images, but it destroys the local structure and introduces unavoidable noise. In this paper, we propose a Homogeneous Augmented Tri-Modal (HAT) learning method for VI-ReID, where an auxiliary grayscale modality is generated from their homogeneous visible images, without additional training process. It preserves the structure information of visible images and approximates the image style of infrared modality. Learning with the grayscale visible images enforces the network to mine structure relations across multiple modalities, making it robust to color variations. Specifically, we solve the tri-modal feature learning from both multi-modal classification and multi-view retrieval perspectives. For multi-modal classification, we learn a multi-modality sharing identity classifier with a parameter-sharing network, trained with a homogeneous and heterogeneous identification loss. For multi-view retrieval, we develop a weighted tri-directional ranking loss to optimize the relative distance across multiple modalities. Incorporated with two invariant regularizers, HAT simultaneously minimizes multiple modality variations. In-depth analysis demonstrates the homogeneous grayscale augmentation significantly outperforms the current state-of-the-art by a large margin.",2021,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.3001665,https://www.comp.hkbu.edu.hk/~mangye/files/TIFS_HAT.pdf
80c40aa765597d9a577a9ebce13ecc85a0aa3666,1,0,0,Devil's in the Detail: Graph-based Key-point Alignment and Embedding for Person Re-ID,"Although Person Re-Identification has made impressive progress, difficult cases like occlusion, change of view-point and similar clothing still bring great challenges. Besides overall visual features, matching and comparing detailed local information is also essential for tackling these challenges. This paper proposes two key recognition patterns to better utilize the local information of pedestrian images. From the spatial perspective, the model should be able to select and align key-points from the image pairs for comparison (i.e. key-points alignment). From the perspective of feature channels, the feature of a query image should be dynamically adjusted based on the gallery image it needs to match (i.e. conditional feature embedding). Most of the existing methods are unable to satisfy both key-point alignment and conditional feature embedding. By introducing novel techniques including correspondence attention module and discrepancy-based GCN, we propose an end-to-end ReID method that integrates both patterns into a unified framework, called Siamese-GCN. The experiments show that Siamese-GCN achieves state-of-the-art performance on three public datasets.",2020,ArXiv,2009.0525,,https://arxiv.org/pdf/2009.05250.pdf
816a5789f6d8061573b5b6373676da4719fb4a3e,1,0,0,A light tracker for online multiple pedestrian tracking,"We propose a novel real-time multiple pedestrian tracker for videos acquired from both static and moving cameras in unconstrained real-world environment. In such scenes, trackers always suffer from noisy detections and frequent occlusions. Existing methods usually use complex learning approaches and a large number of training samples to get discriminative appearance features. However, this leads to high computational cost and hardly works in occlusions (missing detections) and undistinguishable appearance. Addressing this, we design a light two-stage tracker. Firstly, a shallow net with two layers of full convolution is proposed to encode appearance. Compared with other deep architectures and sophisticated learning approaches, our shallow net is efficient and robust enough without any online updating. Secondly, we design a motion model to deal with noisy detections and missing objects caused by motion blur or occlusion. By mining the motion pattern, our tracker can reliably predict the object location under challenging scenes. Furthermore, we propose a speedup version to verify our robustness and the possibility of using in online applications. Extensive experiments are implemented on multiple object tracking benchmarks, MOT15 and MOT17. The performance is competitive over a number of state-of-the-art trackers and demonstrates that our tracker is very promising for real-time applications.",2020,Journal of Real-Time Image Processing,,10.1007/s11554-020-00962-3,
81a2614003fd8b241bc18581c4a0777f9e50d84e,1,1,0,Attention-Aware Compositional Network for Person Re-identification,"Person re-identification (ReID) is to identify pedestrians observed from different camera views based on visual appearance. It is a challenging task due to large pose variations, complex background clutters and severe occlusions. Recently, human pose estimation by predicting joint locations was largely improved in accuracy. It is reasonable to use pose estimation results for handling pose variations and background clutters, and such attempts have obtained great improvement in ReID performance. However, we argue that the pose information was not well utilized and hasn't yet been fully exploited for person ReID. In this work, we introduce a novel framework called Attention-Aware Compositional Network (AACN) for person ReID. AACN consists of two main components: Pose-guided Part Attention (PPA) and Attention-aware Feature Composition (AFC). PPA is learned and applied to mask out undesirable background features in pedestrian feature maps. Furthermore, pose-guided visibility scores are estimated for body parts to deal with part occlusion in the proposed AFC module. Extensive experiments with ablation analysis show the effectiveness of our method, and state-of-the-art results are achieved on several public datasets, including Market-1501, CUHK03, CUHK01, SenseReID, CUHK03-NP and DukeMTMC-reID.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1805.03344,10.1109/CVPR.2018.00226,https://arxiv.org/pdf/1805.03344.pdf
81b3cfd55ca84802cdcc971410e633ed40e04980,0,0,1,Fast Parameter Adaptation for Few-shot Image Captioning and Visual Question Answering,"Given only a few image-text pairs, humans can learn to detect semantic concepts and describe the content. For machine learning algorithms, they usually require a lot of data to train a deep neural network to solve the problem. However, it is challenging for the existing systems to generalize well to the few-shot multi-modal scenario, because the learner should understand not only images and texts but also their relationships from only a few examples. In this paper, we tackle two multi-modal problems, i.e., image captioning and visual question answering (VQA), in the few-shot setting. We propose Fast Parameter Adaptation for Image-Text Modeling (FPAIT) that learns to learn jointly understanding image and text data by a few examples. In practice, FPAIT has two benefits. (1) Fast learning ability. FPAIT learns proper initial parameters for the joint image-text learner from a large number of different tasks. When a new task comes, FPAIT can use a small number of gradient steps to achieve a good performance. (2) Robust to few examples. In few-shot tasks, the small training data will introduce large biases in Convolutional Neural Networks (CNN) and damage the learner's performance. FPAIT leverages dynamic linear transformations to alleviate the side effects of the small training set. In this way, FPAIT flexibly normalizes the features and thus reduces the biases during training. Quantitatively, FPAIT achieves superior performance on both few-shot image captioning and VQA benchmarks.",2018,ACM Multimedia,,10.1145/3240508.3240527,https://xuanyidong.com/pdf/FPAIT-MM-18.pdf
81ed4e4eb2142af223c87912e014cd4b443c2982,0,1,0,Attribute Memory Transfer Network for Unsupervised Cross-Domain Person Re-Identification,"Person re-identification (re-ID) presents various applications in surveillance system, but most existing models are proposed under supervised framework. These methods require large amounts of annotated pedestrian data, which limits their scalability and flexibility in a new application scenario. Aiming to relax this limitation, this article exploits the attribute-invariant characteristics and domain correlations into cross-domain person re-ID, while most unsupervised methods only consider the identity features and ignore the different importance of each source image to the target domain. Specifically, this article proposes an Attribute Memory Transfer Network (AMTNet) with two major contributions of domain-balanced memory and attribute-invariant memory modules. The first domain balanced-memory integrates a domain correlation learning method to evaluate the importance of each source image to the target domain, which is involved into the transfer learning; The second attribute-invariant memory can transfer the source attribute knowledge into the target domain with preserving the identity information to conduct the re-ID process. Extensive evaluated experiments elaborate the superiority of AMTNet on two large datasets of Market-1501 and DukeMTMC-reID, compared with hand-crafted and deep learning feature-based methods.",2020,IEEE Access,,10.1109/ACCESS.2020.3029216,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09214814.pdf
820e95b257d8291621e792545a25f6e1ab62498a,0,1,0,Augmentation Data Synthesis Via Gans: Boosting Latent Fingerprint Reconstruction,"Latent fingerprint reconstruction is a vital preprocessing step for its identification. This task is very challenging due to not only existing complicated degradation patterns but also its scarcity of paired training data. To address these challenges, we propose a novel generative adversarial network (GAN) based data augmentation scheme to improve such reconstruction. It translates the abundant clean fingerprints to their corresponding latent ones, only exploiting a small-scale latent dataset and an unpaired large-scale clean dataset, from which a large-scale paired clean-latent augmentation set is built for the reconstruction task. Specifically, our method models the distribution of the latent degradation patterns into a Gaussian one and generates latent fingerprints based on the sampled degradation patterns and clean fingerprints. Besides, we develop an auxiliary training procedure to stabilize training and further disentangle ridge structures and degradation patterns by regressing a latent fingerprint from its latent representation and its corresponding binarized fingerprint. Boosted by the proposed data augmentation, our reconstruction shows significant improvements in visual evaluation and fingerprint identification performance.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053801,
82297c21eb134f1d684628e89c1e426ad67faca6,1,0,0,Spatio-Temporal Correlation Graph for Association Enhancement in Multi-object Tracking,"Due to the frequent interaction between targets in real-world scenarios, various data association problems, such as association ambiguities and association failure, are caused by potential correlation between interactive tracklets, especially during crowded and cluttered scenes. To overcome the non-intuitionistic of tracklet interaction, spatio-temporal correlation graph (STCG) is proposed to model the potential correlation between pairwise tracklets. Three primitive interactions (aggregation, abruption, stability) are defined to model the completed period of the tracklet interaction. Furthermore, STCG model is applied into network flow tracking to exploit the potential correlation between tracklets and enhance the association of the interactive tracklets, especially when overlapping or occlusion is happened. Our method is effective on MOT challenge benchmarks and achieves considerable competitive results with current state-of-the-art trackers.",2019,KSEM,,10.1007/978-3-030-29551-6_35,
823c0abb5a7a0b2503c3bc0d5629950cc78e1a4f,1,0,0,A Locality Aware City-Scale Multi-Camera Vehicle Tracking System,"Vehicle tracking across multiple cameras can be difficult for modern tracking systems. Given unlikely candidates and faulty similarity estimation, data association struggles at city-scale tracking. In order to avoid difficulties in a large scenario, we keep the tracking procedure within a minimal range. The benefit of this smaller scenario idea is two-fold. On the one hand, ruling out most unlikely candidates decrease the possibility of mis-assignment. On the other hand, the system can devote all its discriminative power on the remaining local candidate pool. In fact, our tracking system features two parts to keep the data association within a small range, while at the same time increase the locality awareness for smaller scenarios. First, multiple cues including spatial-temporal information and camera topology are leveraged to restrict the candidate selection. Second, the appearance similarity estimation module is carefully tuned so that it focuses on the smaller local candidate pool. Based on a minimal view for the large scenario, the proposed system finished 5 place in the 2019 AI-City challenge for city-scale multi-camera vehicle tracking.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/823c/0abb5a7a0b2503c3bc0d5629950cc78e1a4f.pdf
823db9675e2ccfa057aa02764da064e653452ce6,0,1,0,Person Re-Identification With Reinforced Attribute Attention Selection,"Person re-identification (Re-ID) aims to match pedestrian images across various scenes in video surveillance. There are a few works using attribute information to boost Re-ID performance. Specifically, those methods leverage attribute information to boost Re-ID performance by introducing auxiliary tasks like verifying the image level attribute information of two pedestrian images or recognizing identity level attributes. Identity level attribute annotations cost less manpower and are well-fitted for person re-identification task compared with image-level attribute annotations. However, the identity attribute information may be very noisy due to incorrect attribute annotation or lack of discriminativeness to distinguish different persons, which is probably unhelpful for the Re-ID task. In this paper, we propose a novel Attribute Attentional Block (AAB), which can be integrated into any backbone network or framework. Our AAB adopts reinforcement learning to drop noisy attributes based on our designed reward and then utilizes aggregated attribute attention of the remaining attributes to facilitate the Re-ID task. Experimental results demonstrate that our proposed method achieves state-of-the-art results on three benchmark datasets.",2021,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3036762,
824cc046954c6a5b2f1aa1b313e8cdede80aac31,1,0,0,Real-Time Online Multi-Object Tracking: A Joint Detection and Tracking Framework,"In recent years, object detection technology has been continuously developed, and the tracking-by-detection strategy has gradually become the main method of multi-object tracking. Based on detection, the accuracy of the multi-object tracking depends on the detection results to a large extent. However, in many practical applications, especially the case of complex scenes and crowded objects, the detection results are usually inaccurate. In this paper, a joint detection and tracking framework is proposed with a unified confidence scoring function to evaluate tracks confidence and complement low confidence detections with high confidence tracks. In this way, detections and tracks can be combined organically and achieved complementarity. High confidence detection results can prevent long-term tracking drift, while high confidence tracking prediction can deal with false detection and missed detection caused by occlusion during object interaction. Moreover, we trained the ReID appearance feature with higher identification capabilities on the large-scale person re-identification datasets, which has higher identification capability. Extensive experiments are conducted on MOT17 benchmarks to demonstrate the real-time and advanced performance of our tracker.",2019,CSAI,,10.1145/3374587.3374628,
82a84705d3f56c4982807dc10a141a51fa11a9e0,0,1,0,SAT: Single-Shot Adversarial Tracker,"Deep learning-based tracking methods have shown favorable performance on multiple benchmarks. However, most of these methods are not designed for real-time video surveillance systems due to the complex online optimization process. In this article, we propose a single-shot adversarial tracker (SAT) to efficiently locate objects of interest in surveillance videos. Specifically, we propose a lightweight convolutional neural network-based generator, which fuses multilayer feature maps to accurately generate the target probability map (TPM) for tracking. To more effectively train the generator, an adversarial learning framework is presented. During the online tracking stage, the learned TPM generator can be directly employed to generate the target probability map corresponding to the searching region in a single shot. The proposed SAT can lead to the average tracking speed of 212 FPS on a single GPU, while still achieving the favorable performance on several popular benchmarks. Furthermore, we also present a variant of SAT by considering both scale estimation and online updating in SAT, which achieves better accuracy than SAT while still maintaining very fast tracking speed (i.e., exceeding 100 FPS).",2020,IEEE Transactions on Industrial Electronics,,10.1109/TIE.2019.2955411,
82e2ec85b5309f885fabc6a6a49fbaa4f054c082,1,0,0,Hierarchical-Matching-Based Online and Real-Time Multi-Object Tracking with Deep Appearance Features,"Based on tracking-by-detection, we propose a hierarchical-matching-based online and real-time multi-object tracking approach with deep appearance features, which can effectively reduce the false positives (FP) in tracking. For the purpose of increasing the accuracy rate of data association, we define the trajectory confidence using its position information, appearance information, and the information of historical relevant detections, after which we can classify the trajectories into different levels. In order to obtain discriminative appearance features, we developed a deep convolutional neural network to extract the appearance features of objects and trained it on a large-scale pedestrian re-identification dataset. Last but not least, we used the proposed diverse and hierarchical matching strategy to associate detection and trajectory sets. Experimental results on the MOT benchmark dataset show that our proposed approach performs well against other online methods, especially for the metrics of FP and frames per second (FPS).",2020,Algorithms,,10.3390/a13040080,
834ff8e06ed3f01c10958a276f1526fce7ffd387,1,0,0,Self-supervised Multi-view Person Association and Its Applications.,"Reliable markerless motion tracking of people participating in a complex group activity from multiple moving cameras is challenging due to frequent occlusions, strong viewpoint and appearance variations, and asynchronous video streams. To solve this problem, reliable association of the same person across distant viewpoints and temporal instances is essential. We present a self-supervised framework to adapt a generic person appearance descriptor to the unlabeled videos by exploiting motion tracking, mutual exclusion constraints, and multi-view geometry. The adapted discriminative descriptor is used in a tracking-by-clustering formulation. We validate the effectiveness of our descriptor learning on WILDTRACK [14] and three new complex social scenes captured by multiple cameras with up to 60 people ""in the wild"". We report significant improvement in association accuracy (up to 18%) and stable and coherent 3D human skeleton tracking (5 to 10 times) over the baseline. Using the reconstructed 3D skeletons, we cut the input videos into a multi-angle video where the image of a specified person is shown from the best visible front-facing camera. Our algorithm detects inter-human occlusion to determine the camera switching moment while still maintaining the flow of the action well.",2020,IEEE transactions on pattern analysis and machine intelligence,1805.08717,10.1109/TPAMI.2020.2974726,https://arxiv.org/pdf/1805.08717.pdf
837c7e88634ba7b0ed93d8501c19f8f6a13f0f21,1,1,0,Real-Time Person Re-identification at the Edge: A Mixed Precision Approach,"A critical part of multi-person multi-camera tracking is person re-identification (re-ID) algorithm, which recognizes and retains identities of all detected unknown people throughout the video stream. Many re-ID algorithms today exemplify state of the art results, but not much work has been done to explore the deployment of such algorithms for computation and power constrained real-time scenarios. In this paper, we study the effect of using a light-weight model, MobileNet-v2 for re-ID and investigate the impact of single (FP32) precision versus half (FP16) precision for training on the server and inference on the edge nodes. We further compare the results with the baseline model which uses ResNet-50 on state of the art benchmarks including CUHK03, Market-1501, and Duke-MTMC. The MobileNet-V2 mixed precision training method can improve both inference throughput on the edge node, and training time on server 3.25\(\times \) reaching to 27.77 fps and 1.75\(\times \), respectively and decreases power consumption on the edge node by 1.45\(\times \), while it deteriorates accuracy only 5.6% in respect to ResNet-50 single precision on the average for three different datasets. The code and pre-trained networks are publicly available. (https://github.com/TeCSAR-UNCC/person-reid)",2019,ICIAR,1908.07842,10.1007/978-3-030-27272-2_3,https://arxiv.org/pdf/1908.07842.pdf
838e7b46185e72c43dda1ceb2d5f1a0154542c16,1,0,0,An Ethical Highlighter for People-Centric Dataset Creation,"Important ethical concerns arising from computer vision datasets of people have been receiving significant attention, and a number of datasets have been withdrawn as a result. To meet the academic need for people-centric datasets, we propose an analytical framework to guide ethical evaluation of existing datasets and to serve future dataset creators in avoiding missteps. Our work is informed by a review and analysis of prior works and highlights where such ethical challenges arise.",2020,ArXiv,2011.13583,,https://arxiv.org/pdf/2011.13583.pdf
8390dc31b8a4ebd8bfbe07188b147a11741d81d6,1,0,0,Online Multiple Pedestrian Tracking using Deep Temporal Appearance Matching Association,"In online multiple pedestrian tracking, it is of great importance to model appearance and geometric similarity between existing tracks and targets appeared in a new frame. The appearance model contains discriminative information with higher dimension compared to the geometric model. Thanks to the recent success of deep learning based methods, handling of high dimensional appearance information becomes possible. Among many deep networks, the Siamese network with triplet loss is popularly adopted as an appearance feature extractor. Since the Siamese network can extract features of each input independently, it is possible to update and maintain target-specific features. However, it is not suitable for multi-object settings that require comparison with other inputs. In this paper we propose a novel track appearance model based on joint-inference network to address this issue. The proposed method enables comparison of two inputs to be used for adaptive appearance modeling. It contributes to disambiguating the process of target-observation matching and consolidating the identity consistency. Diverse experimental results support effectiveness of our method. Our work has been awarded as a 3rd-highest tracker on MOTChallenge19, held in CVPR2019.",2019,ArXiv,1907.00831,10.1016/j.ins.2020.10.002,https://arxiv.org/pdf/1907.00831.pdf
83d85570e2c7dc6c343b7869b34cc65d13ae5716,1,0,0,Deep Unsupervised Progressive Learning for Distant Domain Adaptation,"The superiority of deeply learned representation has been reported in very recent literature of re-identification (Re-ID) task. In this paper, we study a novel transfer learning problem termed Distant Domain Transfer Learning (DDTL) for Re-ID task. Different from existing transfer learning problems which assume that there is a close relation between source domain and target domain, in the DDTL problem, target domain can be totally different from source domain. For example, the source domain classifies pedestrian images but the target domain distinguishes vehicle images. In this work, our goal is to execute an unseen and unrelated task based on a labeled dataset training previously without any samples from intermediate domains. Particularly, we consider the more pragmatic issue of learning a deep feature with no labels, and propose a Deep Unsupervised Progressive Learning (DUPL) method to transfer pretrained deep representations to unseen domains. Specifically, our work performs clustering and fine-tuning of the CNN to improve the performance of original model trained on the irrelevant labeled dataset. Empirical studies on distant domain adaptation task (pedestrian -> vehicle) demonstrate the effectiveness of the proposed method, and the improvement in terms of the mAP accuracy is up to 15% over ""non-transfer"" methods.",2019,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),,10.1109/ICTAI.2019.00128,
83f803f455c3fb321ebf0046b1c94c4a69fe56d5,0,1,0,Cross-modality paired-images generation and augmentation for RGB-infrared person re-identification,"RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. Considering no correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing marginal distribution divergence between the entire RGB and IR sets. However, this set-level alignment strategy may lead to misalignment of some instances, which limit the performance for RGB-IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross-modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged features. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Third, our method learns a latent manifold space. In the space, we can random sample and generate lots of images of unseen classes. Training with those images, the learned identity feature space is more smooth can generalize better when test. Finally, extensive experimental results on two standard benchmarks demonstrate that the proposed model favorably against state-of-the-art methods.",2020,Neural Networks,,10.1016/j.neunet.2020.05.008,
840c8edc30f1131d1cd40ae35685f3c83e29d0f9,1,0,0,Evaluation of Multiple Object Tracking in Surveillance Video,"Multiple object tracking is the process of assigning unique and consistent identities to objects throughout a video sequence. A popular approach to multiple object tracking, and object tracking in ...",2019,,,,http://liu.diva-portal.org/smash/get/diva2:1326842/FULLTEXT01.pdf
84364190d6859067d0d26e4c8eb680627648c058,1,0,0,Real-time Implementation of YOLO+JPDA for Small Scale UAV Multiple Object Tracking,"This paper describes the development of a real-time multiple object detection and tracking system for a small scale UAV. The YOLO deep learning visual object detection algorithm and JPDA multiple target detection algorithm, were selected and implemented. The theory and implementation details of these algorithms are presented. The performance analysis of the system is done on both public dataset and aerial videos taken by UAV.",2018,2018 International Conference on Unmanned Aircraft Systems (ICUAS),,10.1109/ICUAS.2018.8453398,http://dspace.lib.cranfield.ac.uk/bitstream/1826/13970/1/Real-time_implementation_%20of_YOLO%2bJPD_for_small_scale_UAV-2019.pdf
845fbe3e7d750147717ee6e2411eba5b88c57ecc,0,1,0,Fast and Accurate Person Re-identification with Xception Conv-Net and C2F,"Person re-identification (re-id) is the task of identifying a person of interest across disjoint camera views in a multi-camera system. This is a challenging problem due to the different poses, viewpoints and lighting conditions. Deeply learned systems have become prevalent in the person re-identification field as they are capable to deal with the these obstacles. Conv-Net using a coarse-to-fine search framework (Conv-Net+C2F) is such a deeply learned system, which has been developed with both a high-retrieval accuracy as a fast query time in mind. We propose three contributions to improve Conv-Net+C2F: (1) training with an improved optimizer, (2) constructing Conv-Net using a different Convolutional Neural Network (CNN) not yet used for person re-id and (3) coarse descriptors having fewer dimensions for improved speed as well as increased accuracy. With these adaptations Xception Conv-Net+C2F achieves state-of-the-art results on Market-1501 (single-query, 72.4% mAP) and the new, challenging data split of CUHK03 (detected, 42.6% mAP).",2018,CIARP,,10.1007/978-3-030-13469-3_71,
846f351cdad98897a75f41777e093b80366dbbea,1,1,0,Calibrated Domain-Invariant Learning for Highly Generalizable Large Scale Re-Identification,"Many real-world applications, such as city scale traffic monitoring and control, requires large scale re-identification. However, previous ReID methods often failed to address two limitations in existing ReID benchmarks, i.e., low spatiotem-poral coverage and sample imbalance. Notwithstanding their demonstrated success in every single benchmark, they have difficulties in generalizing to unseen environments. As a result, these methods are less applicable in a large scale setting due to poor generalization. In seek for a highly generalizable large-scale ReID method, we present an adversarial domain-invariant feature learning framework (ADIN) that explicitly learns to separate identity-related features from challenging variations, where for the first time ""free"" annotations in ReID data such as video timestamp and camera index are utilized. Furthermore, we find that the imbalance of nuisance classes jeopardizes the adversarial training, and for mitigation we propose a calibrated adversarial loss that is attentive to nuisance distribution. Experiments on existing large-scale person/vehicle ReID datasets demonstrate that ADIN learns more robust and generalizable representations, as evidenced by its outstanding direct transfer performance across datasets, which is a criterion that can better measure the generalizability of large scale Re-ID methods.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1911.11314,10.1109/WACV45572.2020.9093521,https://arxiv.org/pdf/1911.11314.pdf
84e2077e3dc4d6c40c40f8900cc253a1e33a98fe,1,1,0,LWA: A lightweight and accurate method for unsupervised Person Re-identification,"To improve the accuracy of person re-identification and reduce the size of models, a lightweight and accurate (LWA) unsupervised method is proposed. Firstly, a lightweight image classifier (LWC) is proposed. It combines DenseNet's convolutional layer and batch normalization layer to simplify its model structure. Then the spatio-temporal information of datasets is added to form a powerful classifier. Secondly, an optimization learning algorithm is proposed. A feeble classifier (LWB) is implemented by constructing a binary network. For the training of the network, the ranking information of a powerful classifier is used to enhance the recognition capability of the LWB. Furthermore, spatio-temporal information of datasets is added again to construct LWA classifier. The results show that the model size of LWB is about 80% smaller than that of the Siamese network model on the CUHK01 and VIPeR datasets. At the same time, the recognition accuracy of the LWA classifier is slightly improved on the Market-1501 and DukeMTMC target datasets.",2019,2019 6th International Conference on Systems and Informatics (ICSAI),,10.1109/ICSAI48974.2019.9010093,
850d2697fec9c2bb434907db1c0a11e200f32dbe,0,1,0,Improving Person Re-identification by Body Parts Segmentation Generated by GAN,"Person re-identification(ReID) is a task of associating persons that cross the non-overlapping camera views at different locations and times. It is a challenging task due to the large variations in person pose, background, luminance, occlusion, low resolution, etc. How to extracting a powerful features representation is the prime problem in ReID and is still unsolved. In this paper, we propose a cascade network architecture combined with a generative adversarial networks(GANs) and a convolutional neural network(CNN) to improve the performance of person re-identification. The GANs first generates the person body parts segmentation from the person image, and then inputs the segmentation label into the connected CNN together with the original person image. Finally obtain a discriminative and robust feature representation for ReID task. The body parts segmentation partitioning the person image into multiple segments, such as background, head, face, arms, lags, etc. The body parts segmentation information contains accurate borders and category attributes for body parts, which makes the our model more accurate compared to other predefined rigid parts alignment models. Experiments are conduced on the CUHK03, Market1501, DukeMTMC-ReID datasets and the results demonstrate that this approach outperforms several existing state-of-the-art methods.",2018,2018 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2018.8489450,
852c34f152dbddafb75ca0a45ca7f61efd66ad7d,1,0,0,TCTS: A Task-Consistent Two-Stage Framework for Person Search,"The state of the art person search methods separate person search into detection and re-ID stages, but ignore the consistency between these two stages. The general person detector has no special attention on the query target; The re-ID model is trained on hand-drawn bounding boxes which are not available in person search. To address the consistency problem, we introduce a Task-Consist Two-Stage (TCTS) person search framework, includes an identity-guided query (IDGQ) detector and a Detection Results Adapted (DRA) re-ID model. In the detection stage, the IDGQ detector learns an auxiliary identity branch to compute query similarity scores for proposals. With consideration of the query similarity scores and foreground score, IDGQ produces query-like bounding boxes for the re-ID stage. In the re-ID stage, we predict identity labels of detected bounding boxes, and use these examples to construct a more practical mixed train set for the DRA model. Training on the mixed train set improves the robustness of the re-ID stage to inaccurate detection. We evaluate our method on two benchmark datasets, CUHK-SYSU and PRW. Our framework achieves 93.9% of mAP and 95.1% of rank1 accuracy on CUHK-SYSU, outperforming the previous state of the art methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.01197,https://pdfs.semanticscholar.org/56fc/3349d9f61e61b736336f5c9a722014c2ed20.pdf
8594e007463fd3437e9550ab2b7e9ba732a0cacb,1,1,0,Adaptive Feature Fusion via Graph Neural Network for Person Re-identification,"Person Re-identification (ReID) targets to identify a probe person appeared under multiple camera views. Existing methods focus on proposing a robust model to capture the discriminative information. However, they all generate a representation by mining useful clues from a given single image, and ignore the intercommunication with other images. To address this issue, we propose a novel network named Feature-Fusing Graph Neural Network (FFGNN), which fully utilizes the relationships among the nearest neighbors of the given image, and allows message propagation to update the feature of the node during representation learning. Given an anchor image, the FFGNN firstly obtains its Top-K nearest images based on the feature generated by the trained Feature-Extracting Network(FEN). We then construct a graph G based on the obtained K+1 images, in which each node represents the feature of an image. The edge of the graph G is obtained by combing the visual similarity and Jaccard similarity between nodes. Within the constructed graph G, FFGNN conducts message propagation and adaptive feature fusion between nodes by iteratively performing graph convolutional operation on the input features. Finally, the FFGNN outputs a robust and discriminative representation which contains the information from its similar images. Extensive experiments on three public person ReID datasets including Market-1501, DukeMTMC-ReID, and CUHK03 demonstrate that the proposed model can achieve significant improvement against state-of-the-art methods.",2019,ACM Multimedia,,10.1145/3343031.3350982,
85a2ba580f6faa73f38672b0517cbcdeb84cf6c5,0,1,0,VERI-Wild: A Large Dataset and a New Method for Vehicle Re-Identification in the Wild,"Vehicle Re-identification (ReID) is of great significance to the intelligent transportation and public security. However, many challenging issues of Vehicle ReID in real-world scenarios have not been fully investigated, e.g., the high viewpoint variations, extreme illumination conditions, complex backgrounds, and different camera sources. To promote the research of vehicle ReID in the wild, we collect a new dataset called VERI-Wild with the following distinct features: 1) The vehicle images are captured by a large surveillance system containing 174 cameras covering a large urban district (more than 200km^2) The camera network continuously captures vehicles for 24 hours in each day and lasts for 1 month. 3) It is the first vehicle ReID dataset that is collected from unconstrained conditionsns. It is also a large dataset containing more than 400 thousand images of 40 thousand vehicle IDs. In this paper, we also propose a new method for vehicle ReID, in which, the ReID model is coupled into a Feature Distance Adversarial Network (FDA-Net), and a novel feature distance adversary scheme is designed to generate hard negative samples in feature space to facilitate ReID model training. The comprehensive results show the effectiveness of our method on the proposed dataset and the other two existing datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00335,http://openaccess.thecvf.com/content_CVPR_2019/papers/Lou_VERI-Wild_A_Large_Dataset_and_a_New_Method_for_Vehicle_CVPR_2019_paper.pdf
85aefb7a0c0e0f7a60b7c453d1767c9dd6b7964a,0,1,0,Defect Image Sample Generation With GAN for Improving Defect Recognition,"This article aims to improve deep-learning-based surface defect recognition. Owing to the insufficiency of the defect images in practical production lines and the high cost of labeling, it is difficult to obtain a sufficient defect data set in terms of diversity and quantity. A new generation method called surface defect-generation adversarial network (SDGAN), which employs generative adversarial networks (GANs), is proposed to generate defect images using a large number of defect-free images from industrial sites. Experiments show that the defect images generated by the SDGAN have better image quality and diversity than those generated by the state-of-the-art methods. The SDGAN is applied to expand the commutator cylinder surface defect image data sets with and without labels (referred to as the CCSD-L and CCSD-NL data sets, respectively). Regarding anomaly recognition, a 1.77% error rate and a 49.43% relative improvement (IMP) for the CCSD-NL defect data set are obtained. Regarding defect classification, a 0.74% error rate and a 57.47% IMP for the CCSD-L defect data set are achieved. Moreover, defect classification trained on the images augmented by the SDGAN is robust to uneven and poor lighting conditions. Note to Practitioners—This article proposes a method of defect image generation to address the lack of industrial defect images. Traditional defect recognition methods have two disadvantages: different types of defects require different algorithms and handcrafted features are deficient. Defect recognition using deep learning can solve the above problems. However, deep learning requires a plethora of images, and the number of industrial defect images cannot meet this requirement. We propose a new defect image-generation method called SDGAN to generate a defect image data set that balances diversity and authenticity. In practice, we employ a large number of defect-free images to generate a large number of defect images using our method to expand the industry defect-free image data set. Then, the augmented defect data set is used to build a deep-learning defect recognition model. Experiments show that the accuracy of defect recognition can be significantly improved by building a deep-learning defect recognition model using the augmented data set. Therefore, deep learning can achieve excellent performance in defect recognition with a limited number of defect images.",2020,IEEE Transactions on Automation Science and Engineering,,10.1109/TASE.2020.2967415,
85fca9771eb00e55c190885a580b0fef078f9deb,0,1,0,Tasks Integrated Networks: Joint Detection and Retrieval for Image Search,"The traditional object (person) retrieval (re-identification) task aims to learn a discriminative feature representation or metric on the cropped objects. However, in many real-world scenarios, the objects are seldom accurately annotated. Therefore, object-level retrieval becomes intractable without annotation, which leads to a new but challenging topic, i.e. image search with joint detection and retrieval. To address the image search issue, we introduce an end-to-end Integrated Net, which has four merits: 1) A Siamese architecture and an on-line pairing strategy for similar and dissimilar objects in the given images are designed. 2) A novel on-line pairing (OLP) loss is introduced with a dynamic feature dictionary, which alleviates the multi-task training stagnation problem, by automatically generating a number of negative pairs to restrict the positives. 3) Two modules are tailored to handle different tasks separately in the integrated framework, such that the task specification is guaranteed. 4) A class-center guided HEP loss (C2HEP) by exploiting the stored class centers is proposed, such that the intra-similarity and inter-dissimilarity can be captured. Extensive experiments on the CUHK-SYSU and PRW datasets for person search and the large-scale WebTattoo dataset for tattoo search, demonstrate that the proposed model outperforms the state-of-the-art image search models.",2020,IEEE transactions on pattern analysis and machine intelligence,,10.1109/tpami.2020.3009758,
863de3191fe66fed09cef82b3ce03f122e8ae1b4,1,0,0,Collaborative Attention Network for Person Re-identification,"Jointly utilizing global and local features to improve model accuracy is becoming a popular approach for the person re-identification (ReID) problem, because previous works using global features alone have very limited capacity at extracting discriminative local patterns in the obtained feature representation. Existing works that attempt to collect local patterns either explicitly slice the global feature into several local pieces in a handcrafted way, or apply the attention mechanism to implicitly infer the importance of different local regions. In this paper, we show that by explicitly learning the importance of small local parts and part combinations, we can further improve the final feature representation for Re-ID. Specifically, we first separate the global feature into multiple local slices at different scale with a proposed multi-branch structure. Then we introduce the Collaborative Attention Network (CAN) to automatically learn the combination of features from adjacent slices. In this way, the combination keeps the intrinsic relation between adjacent features across local regions and scales, without losing information by partitioning the global features. Experiment results on several widely-used public datasets including Market-1501, DukeMTMC-ReID and CUHK03 prove that the proposed method outperforms many existing state-of-the-art methods.",2019,ArXiv,1911.13008,,https://arxiv.org/pdf/1911.13008.pdf
864b1ad255f479e1f61c26e4a1666fe0ca4cbd9b,1,0,0,Autonomous Aerial Cinematography In Unstructured Environments With Learned Artistic Decision-Making,"Aerial cinematography is revolutionizing industries that require live and dynamic camera viewpoints such as entertainment, sports, and security. However, safely piloting a drone while filming a moving target in the presence of obstacles is immensely taxing, often requiring multiple expert human operators. Hence, there is demand for an autonomous cinematographer that can reason about both geometry and scene context in real-time. Existing approaches do not address all aspects of this problem; they either require high-precision motion-capture systems or GPS tags to localize targets, rely on prior maps of the environment, plan for short time horizons, or only follow artistic guidelines specified before flight.  In this work, we address the problem in its entirety and propose a complete system for real-time aerial cinematography that for the first time combines: (1) vision-based target estimation; (2) 3D signed-distance mapping for occlusion estimation; (3) efficient trajectory optimization for long time-horizon camera motion; and (4) learning-based artistic shot selection. We extensively evaluate our system both in simulation and in field experiments by filming dynamic targets moving through unstructured environments. Our results indicate that our system can operate reliably in the real world without restrictive assumptions. We also provide in-depth analysis and discussions for each module, with the hope that our design tradeoffs can generalize to other related applications. Videos of the complete system can be found at: this https URL.",2020,J. Field Robotics,1910.06988,10.1002/rob.21931,https://arxiv.org/pdf/1910.06988.pdf
865113a6d44623aeda656d97f0d24f6cad3a186e,0,1,0,Mining Hard Negative Samples for SAR-Optical Image Matching Using Generative Adversarial Networks,"In this paper, we propose a generative framework to produce similar yet novel samples for a specified image. We then propose the use of these images as hard-negatives samples, within the framework of hard-negative mining, in order to improve the performance of classification networks in applications which suffer from sparse labelled training data. Our approach makes use of a variational autoencoder (VAE) which is trained in an adversarial manner in order to learn a latent distribution of the training data, as well as to be able to generate realistic, high quality image patches. We evaluate our proposed generative approach to hard-negative mining on a synthetic aperture radar (SAR) and optical image matching task. Using an existing SAR-optical matching network as the basis for our investigation, we compare the performance of the matching network trained using our approach to the baseline method, as well as to two other hard-negative mining methods. Our proposed generative architecture is able to generate realistic, very high resolution (VHR) SAR image patches which are almost indistinguishable from real imagery. Furthermore, using the patches as hard-negative samples, we are able to improve the overall accuracy, and significantly decrease the false positive rate of the SAR-optical matching task—thus validating our generative hard-negative mining approaches’ applicability to improve training in data sparse applications.",2018,Remote. Sens.,,10.3390/rs10101552,https://pdfs.semanticscholar.org/8651/13a6d44623aeda656d97f0d24f6cad3a186e.pdf
86672b817f2ea3b51ca88104c4c670b86e00e490,1,0,0,Towards Visually Explaining Similarity Models,,2020,ArXiv,2008.06035,,https://arxiv.org/pdf/2008.06035.pdf
86692a98f10b9a9949d0624e8a1fb6bb7bc6e426,1,1,0,REVAMP2T: Real-Time Edge Video Analytics for Multicamera Privacy-Aware Pedestrian Tracking,"This article presents real-time edge video analytics for multicamera privacy-aware pedestrian tracking (REVAMP2T), as an integrated end-to-end Internet of Things (IoT) system for privacy built-in decentralized situational awareness. REVAMP2T presents novel algorithmic and system constructs to push deep learning and video analytics next to IoT devices (i.e., video cameras). On the algorithm side, REVAMP2T proposes a unified integrated computer vision pipeline for detection, reidentification, and tracking across multiple cameras without the need for storing the streaming data. At the same time, it avoids facial recognition and tracks and reidentifies the pedestrians based on their key features at runtime. On the IoT system side, REVAMP2T provides an infrastructure to maximize the hardware utilization on the edge, orchestrates global communications, and provides system-wide reidentification, without the use of personally identifiable information, for a distributed IoT network. For the results and evaluation, this article also proposes a new metric, accuracy ${\cdot}$ efficiency (Æ), for holistic evaluation of IoT systems for real-time video analytics based on accuracy, performance, and power efficiency. REVAMP2T outperforms the current state of the art by as much as 13-fold Æ improvement.",2020,IEEE Internet of Things Journal,1911.09217,10.1109/JIOT.2019.2954804,https://arxiv.org/pdf/1911.09217.pdf
86ead1d2ff0153f148cafa2e36419efcd15552e6,0,1,0,Global Based Deep Refineing Model For Person Retrieval,"The performance of traditional part model in person retrieval is greatly affected by the quality of parts. The recent work[1] consider refining the hard partitioned part when training the network itself and got state-of-the-art performance, but during our experimentation, we found the masks which it generated contains the problems that misguide the networks with additional constraints, Targeting to solve above problem, we proposed a new networks called Global Refine Net. The backbone network focus on learning the local information which improve the ability of extract feature of details, Global Refine block introduce global information to adjust the hard-shaped part generated by the backbone network in an end-to-end manner. Also we modified the self-adversarial training mechanism in [1]. We employ an special loss function to prevent the incorrect convergence and adjust the degree of self-adversarial training, the new regularization term we added in the loss benefit both in stabilizing and speeding the training process. The performance of our model beat most previous soft partitioned works, improved about 2.3% rank-1 accuracy and 5.1% mAP to the PCB baseline on market-1501 dataset.",2018,"2018 10th International Conference on Communications, Circuits and Systems (ICCCAS)",,10.1109/ICCCAS.2018.8769188,
8719e1c89279d2cdafbcc90f2b2ab3bba2fced09,0,1,0,Deep features for person re-identification on metric learning,"Abstract Person re-identification, a branch of image retrieval, is an increasingly important public safety application. When monitoring larger areas, it is crucial to correctly match the same person in different camera views. With the emergence of deep learning and large-scale data, metric learning has significantly improved person re-identification performance, but the extent to which deep features affect metric learning performance is unknown. However, given the large number of approaches, datasets, evaluation indices, and experimental environments, comparing metric learning methods directly is difficult. To obtain a more comprehensive empirical evaluation of the person re-identification, here we summarize the different types of features and metric learning approaches from a label attributes perspective. Then, by combining advanced approaches to data enhancement and feature extraction, we conduct comprehensive experiments on metric learning methods with two datasets. For fairness, all methods use a unified code library that includes two data enhancement schemes, eight feature extraction algorithms, and eight metric learning methods. Our results show that, the relations of loss function with deep feature space and metric learning.",2021,Pattern Recognit.,,10.1016/j.patcog.2020.107424,
87268a5407d4b6b42e7cc66f4b676187d5f502ac,0,1,0,View-specific subspace learning and re-ranking for semi-supervised person re-identification,"Abstract Person re-identification (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace Learning (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each other’s ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasets (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107568,
8741e8eea8025682cbb85748b5c38678f1e5de3b,1,0,0,Multi-target Tracking in Multiple Non-overlapping Cameras Using Fast-Constrained Dominant Sets,"In this paper, a unified three-layer hierarchical approach for solving tracking problem in a multiple non-overlapping cameras setting is proposed. Given a video and a set of detections (obtained by any person detector), we first solve within-camera tracking employing the first two layers of our framework and then, in the third layer, we solve across-camera tracking by associating tracks of the same person in all cameras simultaneously. To best serve our purpose, we propose fast-constrained dominant set clustering (FCDSC), a novel method which is several orders of magnitude faster (close to real time) than existing methods. FCDSC is a parameterized family of quadratic programs that generalizes the standard quadratic optimization problem. In our method, we first build a graph where nodes of the graph represent short-tracklets, tracklets and tracks in the first, second and third layer of the framework, respectively. The edge weights reflect the similarity between nodes. FCDSC takes as input a constrained set, a subset of nodes from the graph which need to be included in the extracted cluster. Given a constrained set, FCDSC generates compact clusters by selecting nodes from the graph which are highly similar to each other and with elements in the constrained set. We have tested this approach on a very large and challenging dataset (namely, MOTchallenge DukeMTMC) and show that the proposed framework outperforms the state-of-the-art approaches. Even though the main focus of this paper is on multi-target tracking in non-overlapping cameras, the proposed approach can also be applied to solve video-based person re-identification problem. We show that when the re-identification problem is formulated as a clustering problem, FCDSC can be used in conjunction with state-of-the-art video-based re-identification algorithms, to increase their already good performances. Our experiments demonstrate the general applicability of the proposed framework for multi-target multi-camera tracking and person re-identification tasks.",2019,International Journal of Computer Vision,1706.06196,10.1007/s11263-019-01180-6,https://iris.unive.it/bitstream/10278/3717301/1/1706.06196.pdf
875498c719d9566e0531813c5e47024ece0633ac,0,1,0,Object Reidentification via Joint Quadruple Decorrelation Directional Deep Networks in Smart Transportation,"Object reidentification with the goal of matching pedestrian or vehicle images captured from different camera viewpoints is of considerable significance to public security. Quadruple directional deep learning features (QD-DLFs) can comprehensively describe object images. However, the correlation among QD-DLFs is an unavoidable problem, since QD-DLFs are learned with quadruple independent directional deep networks (QIDDNs) driven with the same training data, and each network holds the same basic deep feature learning architecture (BDFLA). The correlation among QD-DLFs is harmful to the complementarity of QD-DLFs, restricting the object reidentification performance. For that, we propose joint quadruple decorrelation directional deep networks (JQD3Ns) to reduce the correlation among the learned QD-DLFs. In order to jointly train JQD3Ns, besides the softmax loss functions, a parameter correlation cost function is proposed to indirectly reduce the correlation among QD-DLFs by enlarging the dissimilarity among the parameters of JQD3Ns. Extensive experiments on three publicly available large-scale data sets demonstrate that the proposed JQD3Ns approach is superior to multiple state-of-the-art object reidentification methods.",2020,IEEE Internet of Things Journal,,10.1109/JIOT.2020.2963996,
87f00419be743b70e89ac925289e63d0bec7e4fa,1,0,0,Deep Learning based Person Re-identification,"Automated person re-identification in a multi-camera surveillance setup is very important for effective tracking and monitoring crowd movement. In the recent years, few deep learning based re-identification approaches have been developed which are quite accurate but time-intensive, and hence not very suitable for practical purposes. In this paper, we propose an efficient hierarchical re-identification approach in which color histogram based comparison is first employed to find the closest matches in the gallery set, and next deep feature based comparison is carried out using Siamese network. Reduction in search space after the first level of matching helps in achieving a fast response time as well as improving the accuracy of prediction by the Siamese network by eliminating vastly dissimilar elements. A silhouette part-based feature extraction scheme is adopted in each level of hierarchy to preserve the relative locations of the different body structures and make the appearance descriptors more discriminating in nature. The proposed approach has been evaluated on five public data sets and also a new data set captured by our team in our laboratory. Results reveal that it outperforms most state-of-the-art approaches in terms of overall accuracy.",2020,ArXiv,2005.03293,,https://arxiv.org/pdf/2005.03293.pdf
87f0ef3215594a00b0542efedab63c15f60d5ee4,0,1,0,Camera Style Guided Feature Generation for Person Re-identification,"Camera variance has always been a troublesome matter in person re-identification (re-ID). Recently, more and more interests have grown in alleviating the camera variance problem by data augmentation through generative models. However, these methods, mostly based on image-level generative adversarial networks (GANs), require huge computational power during the training process of generative models. In this paper, we propose to solve the person re-ID problem by adopting a feature level camera-style guided GAN, which can serve as an intra-class augmentation method to enhance the model robustness against camera variance. Specifically, the proposed method makes camera-style transfer on input features while preserving the corresponding identity information. Moreover, the training process can be directly injected into the re-ID task in an end-to-end manner, which means we can deploy our methods with much less time and space costs. Experiments show the validity of the generative model and its benefits towards re-ID performance on Market-1501 and DukeMTMC-reID datasets.",2020,WASA,,10.1007/978-3-030-59016-1_14,
87f85e0cbbf194d416d57b098cf79b06a688b041,1,0,0,Construction and Stability Simulation of a UAV Automatic Navigation System using Fuzzy Control and Nonlinear Filtering,"In this paper we study the design of the automatic navigation system for UAV (unmanned aerial vehicles) and discuss simulation to determine the stability of the system. We outline the dynamic model of the UAV control framework and use fuzzy controllers and non-linear filtering to achieve an adaptive self-governing landing system.. A prime need is to continuously update the display in order to get the correct self-governing flight control data. The computation and elevation estimation of the UAV relative to the helipad position are the main data utilized to control the longitudinal, sideway and slip velocities of the vehicle. The control framework approach comprises three fuzzy controllers (FC) to deal with the movement along the three axes of the UAV framework. The paper shows a simulation based self-governing landing control approach for the UAV.",2017,,,,https://pdfs.semanticscholar.org/87f8/5e0cbbf194d416d57b098cf79b06a688b041.pdf
880af1685645c7938fadacbcd5a9090da54b0f54,1,0,0,Multi-Drone based Single Object Tracking with Agent Sharing Network,"Drone equipped with cameras can dynamically track the target in the air from a broader view compared with static cameras or moving sensors over the ground. However, it is still challenging to accurately track the target using a single drone due to several factors such as appearance variations and severe occlusions. In this paper, we collect a new Multi-Drone single Object Tracking (MDOT) dataset that consists of 92 groups of video clips with 113,918 high resolution frames taken by two drones and 63 groups of video clips with 145,875 high resolution frames taken by three drones. Besides, two evaluation metrics are specially designed for multi-drone single object tracking, i.e. automatic fusion score (AFS) and ideal fusion score (IFS). Moreover, an agent sharing network (ASNet) is proposed by self-supervised template sharing and view-aware fusion of the target from multiple drones, which can improve the tracking accuracy significantly compared with single drone tracking. Extensive experiments on MDOT show that our ASNet significantly outperforms recent state-of-the-art trackers.",2020,ArXiv,2003.06994,,https://arxiv.org/pdf/2003.06994.pdf
881dfa0f00945dc2d75992f13582f42250c3e693,1,0,0,Aggregating Deep Pyramidal Representations for Person Re-Identification,"Learning discriminative, view-invariant and multi-scale representations of person appearance with different semantic levels is of paramount importance for person Re-Identification (Re-ID). A surge of effort has been spent by the community to learn deep Re-ID models capturing a holistic single semantic level feature representation. To improve the achieved results, additional visual attributes and body part-driven models have been considered. However, these require extensive human annotation labor or demand additional computational efforts. We argue that a pyramid-inspired method capturing multi-scale information may overcome such requirements. Precisely, multi-scale stripes that represent visual information of a person can be used by a novel architecture factorizing them into latent discriminative factors at multiple semantic levels. A multi-task loss is combined with a curriculum learning strategy to learn a discriminative and invariant person representation which is exploited for triplet-similarity learning. Results on three benchmark Re-ID datasets demonstrate that better performance than existing methods are achieved (e.g., more than 90% accuracy on the Duke-MTMC dataset).",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00196,
883b9edbe8f3cbbdf992319cd3b9c947518f8909,1,1,0,Learning Discriminative and Generalizable Representations by Spatial-Channel Partition for Person Re-Identification,"In Person Re-Identification (Re-ID) task, combining local and global features is a common strategy to overcome missing key parts and misalignment on models based only on global features. Using this combination, neural networks yield impressive performance in Re-ID task. Previous part-based models mainly focus on spatial partition strategies. Recently, operations on channel information, such as Group Normalization and Channel Attention, have brought significant progress to various visual tasks. However, channel partition has not drawn much attention in Person Re-ID. In this paper, we conduct a study to exploit the potential of channel partition in Re-ID task. Based on this study, we propose an end-to-end Spatial and Channel partition Representation network (SCR) in order to better exploit both spatial and channel information. Experiments conducted on three mainstream image-based evaluation protocols including Market-1501, DukeMTMC-ReID and CUHK03 and one video-based evaluation protocol MARS validate the performance of our model, which outperforms previous state-of- the-art in both single and cross domain Re-ID tasks.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093541,https://hal.archives-ouvertes.fr/hal-02374246/file/Learning_Discriminative_and_Generalizable_Representations_by_Spatial_ChannelPartition_for_Person_Re_Identification%20%282%29.pdf
887ee1e832c8d2d93a46de053ac579f9b08655b4,0,1,0,Semi-supervised feature learning for improving writer identification,"Data augmentation is typically used by supervised feature learning approaches for offline writer identification, but such approaches require a mass of additional training data and potentially lead to overfitting errors. In this study, a semi-supervised feature learning pipeline is proposed to improve the performance of writer identification by training with extra unlabeled data and the original labeled data simultaneously. Specifically, we propose a weighted label smoothing regularization (WLSR) method for data augmentation, which assigns a weighted uniform label distribution to the extra unlabeled data. The WLSR method regularizes the convolutional neural network (CNN) baseline to allow more discriminative features to be learned to represent the properties of different writing styles. The experimental results on well-known benchmark datasets (ICDAR2013 and CVL) showed that our proposed semi-supervised feature learning approach significantly improves the baseline measurement and perform competitively with existing writer identification approaches. Our findings provide new insights into offline writer identification.",2019,Inf. Sci.,1807.0549,10.1016/j.ins.2019.01.024,https://arxiv.org/pdf/1807.05490.pdf
88a266199cbe69c45741ef7e3bbf5c36e90286b7,1,0,0,Attribute analysis with synthetic dataset for person re-identification,,2020,ArXiv,2006.07139,,https://arxiv.org/pdf/2006.07139.pdf
8928371206f313d409eeb5242d646a8e71061d90,1,0,0,"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects","We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for image sequences. It can reliably discover and track objects through the sequence; it can also conditionally generate future frames, thereby simulating expected motion of objects. This is achieved by explicitly encoding object numbers, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al. 2016), including unsupervised learning, made possible by inductive biases present in the model structure. We use a moving multi-\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how \textsc{sqair} overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.",2018,NeurIPS,1806.01794,,https://arxiv.org/pdf/1806.01794.pdf
898e4299322ff42a7295af91073e327f42403a53,0,1,0,Relation-Aware Global Attention for Person Re-Identification,"For person re-identification (re-id), attention mechanisms have become attractive as they aim at strengthening discriminative features and suppressing irrelevant ones, which matches well the key of re-id, i.e., discriminative feature learning. Previous approaches typically learn attention using local convolutions, ignoring the mining of knowledge from global structure patterns. Intuitively, the affinities among spatial positions/nodes in the feature map provide clustering-like information and are helpful for inferring semantics and thus attention, especially for person images where the feasible human poses are constrained. In this work, we propose an effective Relation-Aware Global Attention (RGA) module which captures the global structural information for better attention learning. Specifically, for each feature position, in order to compactly grasp the structural information of global scope and local appearance information, we propose to stack the relations, i.e., its pairwise correlations/affinities with all the feature positions (e.g., in raster scan order), and the feature itself together to learn the attention with a shallow convolutional model. Extensive ablation studies demonstrate that our RGA can significantly enhance the feature representation power and help achieve the state-of-the-art performance on several popular benchmarks. The source code is available at https://github.com/microsoft/Relation-Aware-Global-Attention-Networks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.00325,
8992a5459fe8c682b6023e3324e3c621c0699720,1,1,0,Learning Deep Representations by Mutual Information for Person Re-identification,"Most existing person re-identification (ReID) methods have good feature representations to distinguish pedestrians with deep convolutional neural network (CNN) and metric learning methods. However, these works concentrate on the similarity between encoder output and ground-truth, ignoring the correlation between input and encoder output, which affects the performance of identifying different pedestrians. To address this limitation, We design a Deep InfoMax (DIM) network to maximize the mutual information (MI) between the input image and encoder output, which doesn't need any auxiliary labels. To evaluate the effectiveness of the DIM network, we propose end-to-end Global-DIM and Local-DIM models. Additionally, the DIM network provides a new solution for cross-dataset unsupervised ReID issue as it needs no extra labels. The experiments prove the superiority of MI theory on the ReID issue, which achieves the state-of-the-art results.",2019,ArXiv,1908.0586,,https://arxiv.org/pdf/1908.05860.pdf
89ab8eacedebdcfef0ba7ceef8602c2245f17004,1,0,0,Multi-camera Multi-Object Tracking,"In this paper, we propose a pipeline for multi-target visual tracking under multi-camera system. For multi-camera system tracking problem, efficient data association across cameras, and at the same time, across frames becomes more important than single-camera system tracking. However, most of the multi-camera tracking algorithms emphasis on single camera across frame data association. Thus in our work, we model our tracking problem as a global graph, and adopt Generalized Maximum Multi Clique optimization problem as our core algorithm to take both across frame and across camera data correlation into account all together. Furthermore, in order to compute good similarity scores as the input of our graph model, we extract both appearance and dynamic motion similarities. For appearance feature, Local Maximal Occurrence Representation(LOMO) feature extraction algorithm for ReID is conducted. When it comes to capturing the dynamic information, we build Hankel matrix for each tracklet of target and apply rank estimation with Iterative Hankel Total Least Squares(IHTLS) algorithm to it. We evaluate our tracker on the challenging Terrace Sequences from EPFL CVLAB as well as recently published Duke MTMC dataset.",2017,ArXiv,1709.07065,,https://arxiv.org/pdf/1709.07065.pdf
89d351bd20fb496ff9ca31acb3c3eb90494f65dd,1,1,0,Improved Hierarchical Clustering with Non-locally Enhanced Features for Unsupervised Person Re-identification,"Due to the high cost of data annotation in supervised person re-identification (re-ID) methods, unsupervised person re-ID methods have attracted more and more attention. The unsupervised person re-ID methods based on deep clustering have achieved good performance. However, the distance metrics used in existing unsupervised clustering methods ignore intra-cluster distance and are likely to cause some wrong merging situations and uneven distribution within clusters. Besides, these models based on deep clustering usually ignore the importance of global features for person re-ID. In this paper, we address the above problems by proposing an improved hierarchical clustering approach with non-locally enhanced features. To improve the clustering performance, we design a new metric which consists of intermediate distance as inter-cluster distance and compactness degree as intra-cluster distance. The former one can prevent some wrong merging situations and the latter one can promote the uniform distribution within clusters. In addition, we develop a non-locally enhanced feature network to take advantage of global features of images. Extensive experiments on Market-1501, DukeMTMC-reID, MARS and DukeMTMC-VideoReID demonstrate that our method obtains significant improvement over the state-of-the-art unsupervised methods.",2020,2020 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN48605.2020.9206722,http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20330.pdf
8a054a002ccd421a6ae8404e9236d01cea964bee,0,1,0,SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline In Vitro,"Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although research studies have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning-based generative adversarial networks (GANs) brings us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this letter proposes a SiftingGAN approach to generate more numerous, more diverse, and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known aerial image data set demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations.",2019,IEEE Geoscience and Remote Sensing Letters,1809.04985,10.1109/LGRS.2018.2890413,https://arxiv.org/pdf/1809.04985.pdf
8a0e2cc67b5567d3baf381b076adad6fbb8c7946,1,0,0,Customer Loyalty in Go-Food: The Antecedent of Satisfaction,"This study aims to analyze factors that affect customer satisfaction which will influence customer loyalty in Go-Food, an Online Delivery-Sourcing in Indonesia. Customer loyalty model was based on previous research which includes perceived value and satisfaction aspect. The antecedents of satisfaction were adopted from mobile service quality (M-S-QUAL). To validate the factors, 852 respondents' data were collected. The data were analyzed using Covariance-Based Structural Equation Model (CB-SEM) and processed in AMOS 22.0 tools. Based on the analysis, this study found six antecedents of satisfaction which are efficiency, content, fulfilment, responsiveness, contact, and billing. All the factors positively affect satisfaction. Among those factors, fulfilment factor has the strongest impact. Meanwhile, privacy and compensation were found not affecting customer satisfaction. These rmdings could be used to maintain customer loyalty of Go- Food by improving factors influencing satisfaction.",2018,2018 International Conference on Advanced Computer Science and Information Systems (ICACSIS),,10.1109/ICACSIS.2018.8618250,
8a193e0ee60efb931054c35866491ed0f08c55bd,1,0,0,MOTChallenge: A Benchmark for Single-camera Multiple Target Tracking,"Standardized benchmarks have been crucial in pushing the performance of computer vision algorithms, especially since the advent of deep learning. Although leaderboards should not be over-claimed, they often provide the most objective measure of performance and are therefore important guides for research. We present MOTChallenge, a benchmark for single-camera Multiple Object Tracking (MOT) launched in late 2014, to collect existing and new data, and create a framework for the standardized evaluation of multiple object tracking methods. The benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community, with applications ranging from robot navigation to self-driving cars. This paper collects the first three releases of the benchmark: (i) MOT15, along with numerous state-of-the-art results that were submitted in the last years, (ii) MOT16, which contains new challenging videos, and (iii) MOT17, that extends MOT16 sequences with more precise labels and evaluates tracking performance on three different object detectors. The second and third release not only offers a significant increase in the number of labeled boxes but also provide labels for multiple object classes beside pedestrians, as well as the level of visibility for every single object of interest. We finally provide a categorization of state-of-the-art trackers and a broad error analysis. This will help newcomers understand the related work and research trends in the MOT community, and hopefully shred some light into potential future research directions.",2020,ArXiv,2010.07548,,https://arxiv.org/pdf/2010.07548.pdf
8a77025bde5479a1366bb93c6f2366b5a6293720,1,1,0,Sharp Attention Network via Adaptive Sampling for Person Re-Identification,"In this paper, we present novel sharp attention networks by adaptively sampling feature maps from convolutional neural networks for person re-identification (re-ID) problems. Due to the introduction of sampling-based attention models, the proposed approach can adaptively generate sharper attention-aware feature masks. This greatly differs from the gating-based attention mechanism that relies on soft gating functions to select the relevant features for person re-ID. In contrast, the proposed sampling-based attention mechanism allows us to effectively trim irrelevant features by enforcing the resultant feature masks to focus on the most discriminative features. It can produce sharper attentions that is more assertive in localizing subtle features relevant to re-identifying people across cameras. For this purpose, a differentiable Gumbel-Softmax sampler is employed to approximate the Bernoulli sampling to train the sharp attention networks. Extensive experimental evaluations demonstrate the superiority of this new sharp attention model for person re-ID over other related existing, published state-of-the-art works on three challenging benchmarks, including CUHK03, Market-1501, and DukeMTMC-reID.",2019,IEEE Transactions on Circuits and Systems for Video Technology,1805.02336,10.1109/TCSVT.2018.2872503,https://arxiv.org/pdf/1805.02336.pdf
8aad3cd389462e1d9b1771d074fa39e293861ea6,0,1,0,Deep manifold clustering based optimal pseudo pose representation (DMC-OPPR) for unsupervised person re-identification,"Abstract Person re-identification (re-ID) is highly complex in a diverse surveillance environment. The existing person re-ID methods are evaluated as a closed set problem with limited environmental variation. It is highly challenging to estimate the diverse poses of a dynamically crowded environment using the traditional unsupervised person re-ID methods. To resolve this issue of handling complex diverse poses and camera angles, a contextual incremental multi-clustering based unsupervised person re-ID method have been proposed. Cam-pose based optimal similarity distance threshold is determined to label the unlabeled person re-ID images efficiently. Frequent intra and inter-camera pseudo pose sequences are represented with optimal distance threshold. This resolves the over-fitting issue created by the dominant samples of an identity and reduces the source-target domain gap. The experimental results show the supremacy of our proposed method over the existing unsupervised person re-ID methods in handling complex poses and camera angles in an incremental self-learning diverse surveillance environment.",2020,Image Vis. Comput.,,10.1016/j.imavis.2020.103956,
8ae35fdd525cb8f3845ad95dfa985e0b59ef906e,1,0,0,Consistent Online Multi-object Tracking with Part-Based Deep Network,"Multi-object tracking is still a challenge problem in complex and crowded scenarios. Mismatches will always happen when objects have similar appearance or are occluded with each other. In this paper, we appeal for more attention to the consistency of the trajectories and propose a part-based deep network which employs ROI pooling method to extract full and part-based features for the objects. An occlusion detector is proposed to predict the occlusion degree and guide the procedure of part-based feature fusion and appearance model update. In this way, the feature extraction speed of our tracker is faster, and the objects can be associated correctly even if they are partly occluded. Besides, we train the network based on siamese architecture to learn a dissimilarity metric between pairs of identities. Extensive experiments with multiple evaluation metrics show that our tracker can associate the objects consistently and gain a significant improvement in tracking accuracy.",2018,PRCV,,10.1007/978-3-030-03335-4_16,
8b2b76465566590f8db0b68141adc030a0daba34,0,1,0,Computation offloading for algorithms in absence of the Cloud,"Mobile cloud computing is a way of delegating complex algorithms from a mobile device to the cloud to complete the tasks quickly and save energy on the mobile device. However, the cloud may not be available or suitable for helping all the time. For example, in a battlefield scenario, the cloud may not be reachable. This work considers neighbouring devices as alternatives to the cloud for offloading computation and presents three key contributions, namely a comprehensive investigation of the trade-off between computation and communication, Multi-Objective Optimisation based approach to offloading, and Queuing Theory based algorithms that present the benefits of offloading to neighbours. Initially, the states of neighbouring devices are considered to be known and the decision of computation offloading is proposed as a multi-objective optimisation problem. Novel Pareto optimal solutions are proposed. The results on a simulated dataset show up to 30% increment in performance even when cloud computing is not available. However, information about the environment is seldom known completely. In Chapter 5, a realistic environment is considered such as delayed node state information and partially connected sensors. The network of sensors is modelled as a network of queues (Open Jackson network). The offloading problem is posed as minimum cost problem and solved using Linear solvers. In addition to the simulated dataset, the proposed solution is tested on a real computer vision dataset. The experiments on the random waypoint dataset showed up to 33% boost on performance whereas in the real dataset, exploiting the temporal",2018,,,,
8b35b6a80b3abcb7734166a2eb8e46218ab02e26,0,0,1,Loss Architecture Search for Few-Shot Object Recognition,"Few-shot object recognition, which exploits a set of well-labeled data to build a classifier for new classes that have only several samples per class, has received extensive attention from the machine learning community. In this paper, we investigate the problem of designing an optimal loss function for few-shot object recognition and propose a novel few-shot object recognition system that includes the following three steps: (1) generate a loss function architecture using a recurrent neural network (generator); (2) train a base embedding network with the generated loss function on a training set; (3) fine-tune the base embedding network using the few-shot instances from a validation set to obtain the accuracy and use it as a reward signal to update the generator. This procedure is repeated and implemented in the reinforcement learning framework for finding the best loss architecture such that the embedding network yields the highest validation accuracy. Our key insight is to create a search space of the loss function architectures and evaluate the quality of a particular loss function on the dataset of interest. We conduct experiments on three popular datasets for few-shot learning. The results show that the proposed approach achieves better performance than state-of-the-art methods.",2020,Complex.,,10.1155/2020/1041962,https://pdfs.semanticscholar.org/6928/77b6225c48a8940df0460ff37cb1eb1634d6.pdf
8b88c357917eb24b39f46be7cf7fe7c56834d18b,0,1,0,Generalizable Person Re-Identification by Domain-Invariant Mapping Network,"We aim to learn a domain generalizable person re-identification (ReID) model. When such a model is trained on a set of source domains (ReID datasets collected from different camera networks), it can be directly applied to any new unseen dataset for effective ReID without any model updating. Despite its practical value in real-world deployments, generalizable ReID has seldom been studied. In this work, a novel deep ReID model termed Domain-Invariant Mapping Network (DIMN) is proposed. DIMN is designed to learn a mapping between a person image and its identity classifier, i.e., it produces a classifier using a single shot. To make the model domain-invariant, we follow a meta-learning pipeline and sample a subset of source domain training tasks during each training episode. However, the model is significantly different from conventional meta-learning methods in that: (1) no model updating is required for the target domain, (2) different training tasks share a memory bank for maintaining both scalability and discrimination ability, and (3) it can be used to match an arbitrary number of identities in a target domain. Extensive experiments on a newly proposed large-scale ReID domain generalization benchmark show that our DIMN significantly outperforms alternative domain generalization or meta-learning methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00081,https://www.pure.ed.ac.uk/ws/files/82435461/Generalizable_Person_Re_identification_SONG_DoA110319_AFV.pdf
8ba606d7667c50054d74083867230abbed755574,1,0,0,"ReXCam: Resource-Efficient, Cross-Camera Video Analytics at Enterprise Scale","The deployment of large camera networks for video analytics is an established and accelerating trend. Many real video inference applications entail a common problem template: searching for an object or activity of interest (e.g. a person, a speeding vehicle) through a large camera network in live video. This capability, called cross-camera analytics, is compute and data intensive -- requiring automated search across cameras and across frames, at the throughput of the live video stream. To address the cost challenge of processing every raw video frame from a large deployment, we present ReXCam, a new system for efficient cross-camera video analytics. ReXCam exploits spatial and temporal locality in the dynamics of real camera networks to guide its inference-time search for a query identity. In an offline profiling phase, ReXCam builds a cross-camera correlation model that encodes the locality observed in historical traffic patterns. At inference time, ReXCam applies this model to filter frames that are not spatially and temporally correlated with the query identity's current position. In the cases of occasional missed detections, ReXCam performs a fast-replay search on recently filtered video frames, enabling gracefully recovery. Together, these techniques allow ReXCam to reduce compute workload by 4.6x and improve inference precision by 27% on a well-known video dataset with footage from eight cameras, while maintaining within 1-2% of baseline recall.",2018,ArXiv,1811.01268,,https://arxiv.org/pdf/1811.01268.pdf
8ba92b2ecda02923c5445e212b6defab3e2ccd4d,1,1,0,Large-Scale Person Re-Identification Based on Deep Hash Learning,"Person re-identification in the image processing domain has been a challenging research topic due to the influence of pedestrian posture, background, lighting, and other factors. In this paper, the method of harsh learning is applied in person re-identification, and we propose a person re-identification method based on deep hash learning. By improving the conventional method, the method proposed in this paper uses an easy-to-optimize shallow convolutional neural network to learn the inherent implicit relationship of the image and then extracts the deep features of the image. Then, a hash layer with three-step calculation is incorporated in the fully connected layer of the network. The hash function is learned and mapped into a hash code through the connection between the network layers. The generation of the hash code satisfies the requirements that minimize the error of the sum of quantization loss and Softmax regression cross-entropy loss, which achieve the end-to-end generation of hash code in the network. After obtaining the hash code through the network, the distance between the pedestrian image hash code to be retrieved and the pedestrian image hash code library is calculated to implement the person re-identification. Experiments conducted on multiple standard datasets show that our deep hashing network achieves the comparable performances and outperforms other hashing methods with large margins on Rank-1 and mAP value identification rates in pedestrian re-identification. Besides, our method is predominant in the efficiency of training and retrieval in contrast to other pedestrian re-identification algorithms.",2019,Entropy,,10.3390/e21050449,https://pdfs.semanticscholar.org/8ba9/2b2ecda02923c5445e212b6defab3e2ccd4d.pdf
8bc4d395f685140c0d7583effdfa84571e8050d8,0,1,0,Person Attribute Recognition by Sequence Contextual Relation Learning,"Person attribute recognition aims to identify the attribute labels from the pedestrian images. Extracting contextual relation from the images and attributes, including the spatial-semantic relations, the spatial context and the semantic correlation, is beneficial to enhance the discrimination of the features for recognizing the attributes. Thus, this work proposes a sequence contextual relation learning (SCRL) method to capture these relations. It first embeds the images and attributes into sequences in two branches. Then SCRL flexibly learns the contextual relation from the sequences with the parallel attention model structure, which integrates the inter-attention and intra-attention models. The inter-attention module is utilized to extract the spatial-semantic relations, while the intra-attention is designed to gain the spatial context and the semantic correlation. Both attention modules are comprised of several parallel attention units and each unit can obtain the pairwise relations in one subspace. Therefore, they obtain the relations in multiple subspaces, which can improve the comprehensiveness of the relation learning. Additionally, for the sake of better extraction of spatial-semantic relations, this paper employs connectionist temporal classification (CTC) loss which is capable of driving the network to enforce monotonic alignment between the image and attribute. It can also accelerate the convergence of the network by the algorithm in it. Extensive experiments on five public datasets, i.e., Market-1501 attribute, Duke attribute, PETA, RAP and PA-100K datasets, demonstrate the effectiveness of the proposed method.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2020.2982962,
8bca75107f6bf1e329feb6643c0fe3abf557732d,0,1,0,FastReID: A Pytorch Toolbox for General Instance Re-identification,"General Instance Re-identification is a very important task in the computer vision, which can be widely used in many practical applications, such as person/vehicle re-identification, face recognition, wildlife protection, commodity tracing, and snapshop, etc.. To meet the increasing application demand for general instance re-identification, we present FastReID as a widely used software system in JD AI Research. In FastReID, highly modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art projects, including person re-id, partial re-id, cross-domain re-id and vehicle re-id, and plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most general and high-performance toolbox that supports single and multiple GPU servers, you can reproduce our project results very easily and are very welcome to use it, the code and models are available at this https URL.",2020,ArXiv,2006.02631,,https://arxiv.org/pdf/2006.02631.pdf
8c31527844d90c2c877e9816ce86541a9258ae0a,0,1,0,Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-Based Person Re-Identification,"Video-based person re-identification (reID) aims at matching the same person across video clips. It is a challenging task due to the existence of redundancy among frames, newly revealed appearance, occlusion, and motion blurs. In this paper, we propose an attentive feature aggregation module, namely Multi-Granularity Reference-aided Attentive Feature Aggregation (MG-RAFA), to delicately aggregate spatio-temporal features into a discriminative video-level feature representation. In order to determine the contribution/importance of a spatial-temporal feature node, we propose to learn the attention from a global view with convolutional operations. Specifically, we stack its relations, \ieno, pairwise correlations with respect to a representative set of reference feature nodes (S-RFNs) that represents global video information, together with the feature itself to infer the attention. Moreover, to exploit the semantics of different levels, we propose to learn multi-granularity attentions based on the relations captured at different granularities. Extensive ablation studies demonstrate the effectiveness of our attentive feature aggregation module MG-RAFA. Our framework achieves the state-of-the-art performance on three benchmark datasets.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.12224,10.1109/cvpr42600.2020.01042,https://arxiv.org/pdf/2003.12224.pdf
8c777d6b2caf6a3ac5152787574372eb65ce92e3,1,0,0,Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in Large Scenes,"Crowd counting in single-view images has achieved outstanding performance on existing counting datasets. However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. Therefore, to solve the wide-area counting task requires multiple cameras with overlapping fields-of-view. In this paper, we propose a deep neural network framework for multi-view crowd counting, which fuses information from multiple camera views to predict a scene-level density map on the ground-plane of the 3D world. We consider three versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model ensures that features aligned to the same ground-plane point have consistent scales. A rotation selection module further ensures consistent rotation alignment of the features. We test our 3 fusion models on 3 multi-view counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view counting dataset containing a crowded street intersection. Our methods achieve state-of-the-art results compared to other multi-view counting baselines.",2020,ArXiv,2012.00946,,https://arxiv.org/pdf/2012.00946.pdf
8d04ad17be9ac41c458f13fa281d1b3b3530e8d0,1,0,0,Multiple People Tracking Using Body and Joint Detections,"Most multiple people tracking systems compute trajectories based on the tracking-by-detection paradigm. Consequently, the performance depends to a large extent on the quality of the employed input detections. However, despite an enormous progress in recent years, partially occluded people are still often not recognized. Also, many correct detections are mistakenly discarded when the non-maximum suppression is performed. Improving the tracking performance thus requires to augment the coarse input. Well-suited for this task are fine-graded body joint detections, as they allow to locate even strongly occluded persons. Thus in this work, we analyze the suitability of including joint detections for multiple people tracking. We introduce different affinities between the two detection types and evaluate their performances. Tracking is then performed within a near-online framework based on a min cost graph labeling formulation. As a result, our framework can recover heavily occluded persons and solve the data association efficiently. We evaluate our framework on the MOT16/17 benchmark. Experimental results demonstrate that our framework achieves state-of-the-art results.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00105,http://openaccess.thecvf.com/content_CVPRW_2019/papers/BMTT/Henschel_Multiple_People_Tracking_Using_Body_and_Joint_Detections_CVPRW_2019_paper.pdf
8d3d62d9d85d5735b24d7bccc1d763ad3e661e2f,0,1,0,Skin Lesion Segmentation Ensemble with Diverse Training Strategies,"This paper presents a novel strategy to perform skin lesion segmentation from dermoscopic images. We design an effective segmentation pipeline, and explore several pre-training methods to initialize the features extractor, highlighting how different procedures lead the Convolutional Neural Network (CNN) to focus on different features. An encoder-decoder segmentation CNN is employed to take advantage of each pre-trained features extractor. Experimental results reveal how multiple initialization strategies can be exploited, by means of an ensemble method, to obtain state-of-the-art skin lesion segmentation accuracy.",2019,CAIP,,10.1007/978-3-030-29888-3_8,https://iris.unimore.it/bitstream/11380/1178303/7/2019_CAIP_REDUCED_Skin_Lesion_Segmentation_Ensemble_with_Diverse_Training_Strategies.pdf
8d71bfb0925dee6a1330d68de3838975efd6f13e,0,1,1,Spatial-Aware GAN for Unsupervised Person Re-identification,"The recent person re-identification research has achieved great success by learning from a large number of labeled person images. On the other hand, the learned models often experience significant performance drops when applied to images collected in a different environment. Unsupervised domain adaptation (UDA) has been investigated to mitigate this constraint, but most existing systems adapt images at pixel level only and ignore obvious discrepancies at spatial level. This paper presents an innovative UDA-based person re-identification network that is capable of adapting images at both spatial and pixel levels simultaneously. A novel disentangled cycle-consistency loss is designed which guides the learning of spatial-level and pixel-level adaptation in a collaborative manner. In addition, a novel multi-modal mechanism is incorporated which is capable of generating images of different geometry views and augmenting training images effectively. Extensive experiments over a number of public datasets show that the proposed UDA network achieves superior person re-identification performance as compared with the state-of-the-art.",2019,ArXiv,1911.11312,,https://arxiv.org/pdf/1911.11312.pdf
8dc450c64ae1084e99f0764e2b2264684664281b,0,1,0,Evaluation of Local Features Using Convolutional Neural Networks for Person Re-Identification,"In this paper, we mainly evaluate the influence of local features extracted by convolutional neural networks for person re-identification. Considering the variant body parts with different structural information, we divide the holistic person images into several parts and extract their features. Two kinds of aggregation methods are used to aggregate local features. Experiments on the challenging person re-identification database, Market-1501 database, show that the max aggregation is more effective for extracting the discriminative local features than the sum aggregation.",2018,CSPS,,10.1007/978-981-13-6504-1_107,
8dedbb2476d42f9d02d535bdaa86222b7f523380,1,0,0,SIF: Self-Inspirited Feature Learning for Person Re-Identification,"The re-identification (ReID) task has received increasing studies in recent years and its performance has gained significant improvement. The progress mainly comes from searching for new network structures to learn person representations. However, limited efforts have been made to explore the potential performance of existing ReID networks directly by better training scheme, which leaves a large space for ReID research. In this paper, we propose a Self-Inspirited Feature Learning (SIF) method to enhance the performance of given ReID networks from the viewpoint of optimization. We design a simple adversarial learning scheme to encourage a network to learn more discriminative person representation. In our method, an auxiliary branch is added into the network only in the training stage, while the structure of the original network stays unchanged during the testing stage. In summary, SIF has three aspects of advantages: 1) it is designed under general setting; 2) it is compatible with many existing feature learning networks on the ReID task; 3) it is easy to implement and has steady performance. We evaluate the performance of SIF on three public ReID datasets: Market1501, DuckMTMC-reID, and CUHK03(both labeled and detected). The results demonstrate significant improvement in performance brought by SIF. We also apply SIF to obtain state-of-the-art results on all the three datasets. Specifically, mAP / Rank-1 accuracy are: 87.6%/95.2% (without re-rank) on Market1501, 79.4%/89.8% on DuckMTMC-reID, 77.0%/79.5% on CUHK03 (labeled) and 73.9%/76.6% on CUHK03 (detected), respectively.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2975712,
8e1327a72e6d60857e8c690bfb15e9538f2bb874,0,1,0,Dual Pattern Learning Networks by Empirical Dual Prediction Risk Minimization,"Motivated by the observation that humans can learn patterns from two given images at one time, we propose a dual pattern learning network architecture in this paper. Unlike conventional networks, the proposed architecture has two input branches and two loss functions. Instead of minimizing the empirical risk of a given dataset, dual pattern learning networks is trained by minimizing the empirical dual prediction loss. We show that this can improve the performance for single image classification. This architecture forces the network to learn discriminative class-specific features by analyzing and comparing two input images. In addition, the dual input structure allows the network to have a considerably large number of image pairs, which can help address the overfitting issue due to limited training data. Moreover, we propose to associate each input branch with a random interest value for learning corresponding image during training. This method can be seen as a stochastic regularization technique, and can further lead to generalization performance improvement. State-of-the-art deep networks can be adapted to dual pattern learning networks without increasing the same number of parameters. Extensive experiments on CIFAR-10, CIFAR- 100, FI-8, Google commands dataset, and MNIST demonstrate that our DPLNets exhibit better performance than original networks. The experimental results on subsets of CIFAR- 10, CIFAR-100, and MNIST demonstrate that dual pattern learning networks have good generalization performance on small datasets.",2018,ArXiv,1806.03902,,https://arxiv.org/pdf/1806.03902.pdf
8e15c4d8ec12d4c7b6c4c37a037e6e033989fb6f,0,1,0,Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification,"Unsupervised domain adaptation (UDA) for person re-identification is challenging because of the huge gap between the source and target domain. A typical self-training method is to use pseudo-labels generated by clustering algorithms to iteratively optimize the model on the target domain. However, a drawback to this is that noisy pseudo-labels generally cause troubles in learning. To address this problem, a mutual learning method by dual networks has been developed to produce reliable soft labels. However, as the two neural networks gradually converge, their complementarity is weakened and they likely become biased towards the same kind of noise. In this paper, we propose a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity and further depress noise in the pseudo-labels. Specifically, we first introduce a parameter-free module, the WaveBlock, which creates a difference between two networks by waving blocks of feature maps differently. Then, an attention mechanism is leveraged to enlarge the difference created and discover more complementary features. Furthermore, two kinds of combination strategies, i.e. pre-attention and post-attention, are explored. Experiments demonstrate that the proposed method achieves state-of-the-art performance with significant improvements of 9.4%, 5.9%, 7.4%, and 7.7% in mAP on Duke-to-Market, Market-to-Duke, Duke-to-MSMT, and Market-to-MSMT UDA tasks, respectively.",2020,ArXiv,2006.06525,,https://arxiv.org/pdf/2006.06525.pdf
8e42568c2b3feaafd1e442e1e861ec50a4ac144f,0,1,0,An Evaluation of Deep CNN Baselines for Scene-Independent Person Re-identification,"In recent years, a variety of proposed methods based on deep convolutional neural networks (CNNs) have improved the state of the art for large-scale person re-identification (ReID). While a large number of optimizations and network improvements have been proposed, there has been relatively little evaluation of the influence of training data and baseline network architecture. In particular, it is usually assumed either that networks are trained on labeled data from the deployment location (scene-dependent), or else adapted with unlabeled data, both of which complicate system deployment. In this paper, we investigate the feasibility of achieving scene-independent person ReID by forming a large composite dataset for training. We present an in-depth comparison of several CNN baseline architectures for both scene-dependent and scene-independent ReID, across a range of training dataset sizes. We show that scene-independent ReID can produce leading-edge results, competitive with unsupervised domain adaption techniques. Finally, we introduce a new dataset for comparing within-camera and across-camera person ReID.",2018,2018 15th Conference on Computer and Robot Vision (CRV),1805.06086,10.1109/CRV.2018.00049,https://arxiv.org/pdf/1805.06086.pdf
8e6cf7f47e5e74a2d5b60d122f1295a9ac225d17,1,0,0,A Survey of Open-World Person Re-Identification,"Person re-identification (re-ID) has been a popular topic in computer vision and pattern recognition communities for a decade. Several important milestones such as metric-based and deeply-learned re-ID in recent years have promoted this topic. However, most existing re-ID works are designed for closed-world scenarios rather than realistic open-world settings, which limits the practical application of the re-ID technique. On one hand, the performance of the latest re-ID methods has surpassed the human-level performance on several commonly used benchmarks (e.g., Market1501 and CUHK03), which are collected from closed-world scenarios. On the other hand, open-world tasks that are less developed and more challenging have received increasing attention in the re-ID community. Therefore, this paper starts the first attempt to analyze the trends of open-world re-ID and summarizes them from both narrow and generalized perspectives. In the narrow perspective, open-world re-ID is regarded as person verification (i.e., open-set re-ID) instead of person identification, that is, the query person may not occur in the gallery set. In the generalized perspective, application-driven methods that are designed for specific applications are defined as generalized open-world re-ID. Their settings are usually close to realistic application requirements. Specifically, this survey mainly includes the following four points for open-world re-ID: 1) analyzing the discrepancies between closed- and open-world scenarios; 2) describing the developments of existing open-set re-ID works and their limitations; 3) introducing specific application-driven works from three aspects, namely, raw data, practical procedure, and efficiency; and 4) summarizing the state-of-the-art methods and future directions for open-world re-ID. This survey on open-world re-ID provides a guidance for improving the usability of re-ID technique in practical applications.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2898940,https://www.comp.hkbu.edu.hk/~mangye/files/tcsvt19_survey.pdf
8e921e30c849c53f6eca82e3d34e4a91991818c7,1,0,0,Estimating 3D Camera Pose from 2D Pedestrian Trajectories,"We consider the task of re-calibrating the 3D pose of a static surveillance camera, whose pose may change due to external forces, such as birds, wind, falling objects or earthquakes. Conventionally, camera pose estimation can be solved with a PnP (Perspective-n-Point) method using 2Dto-3D feature correspondences, when 3D points are known. However, 3D point annotations are not always available or practical to obtain in real-world applications. We propose an alternative strategy for extracting 3D information to solve for camera pose by using pedestrian trajectories. We observe that 2D pedestrian trajectories indirectly contain useful 3D information that can be used for inferring camera pose. To leverage this information, we propose a data-driven approach by training a neural network (NN) regressor to model a direct mapping from 2D pedestrian trajectories projected on the image plane to 3D camera pose. We demonstrate that our regressor trained only on synthetic data can be directly applied to real data, thus eliminating the need to label any real data. We evaluate our method across six different scenes from the Town Centre Street and DUKEMTMC datasets. Our method achieves an improvement of ∼ 50% on both position and orientation prediction accuracy when compared to other SOTA methods.",,,,,https://pdfs.semanticscholar.org/8e92/1e30c849c53f6eca82e3d34e4a91991818c7.pdf
8efce72999dd3225a1a7f65d0d3544c9847625a4,1,1,0,Second-Order Non-Local Attention Networks for Person Re-Identification,"Recent efforts have shown promising results for person re-identification by designing part-based architectures to allow a neural network to learn discriminative representations from semantically coherent parts. Some efforts use soft attention to reallocate distant outliers to their most similar parts, while others adjust part granularity to incorporate more distant positions for learning the relationships. Others seek to generalize part-based methods by introducing a dropout mechanism on consecutive regions of the feature map to enhance distant region relationships. However, only few prior efforts model the distant or non-local positions of the feature map directly for the person re-ID task. In this paper, we propose a novel attention mechanism to directly model long-range relationships via second-order feature statistics. When combined with a generalized DropBlock module, our method performs equally to or better than state-of-the-art results for mainstream person re-identification datasets, including Market1501, CUHK03, and DukeMTMC-reID.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1909.00295,10.1109/ICCV.2019.00386,https://arxiv.org/pdf/1909.00295.pdf
8f0a3a6997b3f31719f56c499c5024414a5dd8bf,0,1,0,Person Re-identification in the Wild,"This paper presents a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification (re-ID) accuracy and assessing the effectiveness of different detectors for re-ID. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-ID through two simple yet effective improvements: a cascaded fine-tuning strategy that trains a detection model first and then the classification model, and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-ID.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),1604.02531,10.1109/CVPR.2017.357,https://arxiv.org/pdf/1604.02531.pdf
8f2ec21937e5dd1e3ecec4481ddadeb0183e7fbc,0,1,0,Using the VQ-VAE to improve the recognition of abnormalities in short-duration 12-lead electrocardiogram records,"BACKGROUND AND OBJECTIVE Morphological diagnosis is a basic clinical task of the short-duration 12-lead electrocardiogram (ECG). Due to the scarcity of positive samples and other factors, there is currently no algorithm that is comparable to human experts in ECG morphological recognition. Our objective is to develop an ECG specialist-level deep learning method that can accurately identify ten ECG morphological abnormalities in real scene data.   METHODS We established a short-duration 12-lead ECG image dataset that consists of approximately 200,000 samples. To address the problems with small positive samples, a data augmentation method was proposed. We solved it by interpolating in the latent space of the vector quantized variational autoencoder (VQ-VAE) and generating new samples via sampling. The trained final classifier, general doctors, and ECG specialists evaluated the diagnostic performance on a test set that consisted of 1000 samples.   RESULTS Relative to that of unaugmented data, the F1 score was improved by 0-6%. Compared with ECG specialists, the deep neural network achieved higher F1 scores and sensitivity in most categories.   CONCLUSIONS Our method can improve the classification performance of ECG data with insufficient positive samples and reach the level of ECG specialists. This approach can provide specialized reference opinions for ordinary clinicians and reduce the errors of ECG specialists.",2020,Comput. Methods Programs Biomed.,,10.1016/j.cmpb.2020.105639,
8f8987b74a066a70c05bd4f702acf1bae2265910,1,1,0,Person Re-identification in the 3D Space,"People live in a 3D world. However, existing works on person re-identification (re-id) mostly consider the representation learning in a 2D space, intrinsically limiting the understanding of people. In this work, we address this limitation by exploring the prior knowledge of the 3D body structure. Specifically, we project 2D images to a 3D space and introduce a novel Omni-scale Graph Network (OG-Net) to learn the representation from sparse 3D points. With the help of 3D geometry information, we can learn a new type of deep re-id feature free from noisy variants, such as scale and viewpoint. To our knowledge, we are among the first attempts to conduct person re-identification in the 3D space. Extensive experiments show that the proposed method achieves competitive results on three popular large-scale person re-id datasets, and has good scalability to unseen datasets.",2020,ArXiv,,,https://arxiv.org/pdf/2006.04569.pdf
8fbb73bc6fb74e119b5fdf02482fa90afb7e443e,0,1,0,Parts Semantic Segmentation Aware Representation Learning for Person Re-Identification,"Person re-identification is a typical computer vision problem which aims at matching pedestrians across disjoint camera views. It is challenging due to the misalignment of body parts caused by pose variations, background clutter, detection errors, camera point of view variation, different accessories and occlusion. In this paper, we propose a person re-identification network which fuses global and local features, to deal with part misalignment problem. The network is a four-branch convolutional neural network (CNN) which learns global person appearance and local features of three human body parts respectively. Local patches, including the head, torso and lower body, are segmented by using a U_Net semantic segmentation CNN architecture. All four feature maps are then concatenated and fused to represent a person image. We propose a DropParts method to solve the parts missing problem, with which the local features are weighed according to the number of parts found by semantic segmentation. Since three body parts are well aligned, the approach significantly improves person re-identification. Experiments on the standard benchmark datasets, such as Market1501, CUHK03 and DukeMTMC-reID datasets, show the effectiveness of our proposed pipeline.",2019,,,10.3390/app9061239,https://pdfs.semanticscholar.org/8fbb/73bc6fb74e119b5fdf02482fa90afb7e443e.pdf
8fcd7301470e352b550d6b097f379356c9c1a89c,1,1,0,Towards Rich Feature Discovery With Class Activation Maps Augmentation for Person Re-Identification,"The fundamental challenge of small inter-person variation requires Person Re-Identification (Re-ID) models to capture sufficient fine-grained information. This paper proposes to discover diverse discriminative visual cues without extra assistance, e.g., pose estimation, human parsing. Specifically, a Class Activation Maps (CAM) augmentation model is proposed to expand the activation scope of baseline Re-ID model to explore rich visual cues, where the backbone network is extended by a series of ordered branches which share the same input but output complementary CAM. A novel Overlapped Activation Penalty is proposed to force the new branch to pay more attention to the image regions less activated by the old ones, such that spatial diverse visual features can be discovered. The proposed model achieves state-of-the-art results on three person Re-ID benchmarks. Moreover, a visualization approach termed ranking activation map (RAM) is proposed to explicitly interpret the ranking results in the test stage, which gives qualitative validations of the proposed method.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00148,http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Towards_Rich_Feature_Discovery_With_Class_Activation_Maps_Augmentation_for_CVPR_2019_paper.pdf
8ff1ef41aed45ed20419cf52aa528850988fef27,0,1,0,Deep learning methods in real-time image super-resolution: a survey,"Super-resolution is generally defined as a process to obtain high-resolution images form inputs of low-resolution observations, which has attracted quantity of attention from researchers of image-processing community. In this paper, we aim to analyze, compare, and contrast technical problems, methods, and the performance of super-resolution research, especially real-time super-resolution methods based on deep learning structures. Specifically, we first summarize fundamental problems, perform algorithm categorization, and analyze possible application scenarios that should be considered. Since increasing attention has been drawn in utilizing convolutional neural networks (CNN) or generative adversarial networks (GAN) to predict high-frequency details lost in low- resolution images, we provide a general overview on background technologies and pay special attention to super-resolution methods built on deep learning architectures for real-time super-resolution, which not only produce desirable reconstruction results, but also enlarge possible application scenarios of super resolution to systems like cell phones, drones, and embedding systems. Afterwards, benchmark datasets with descriptions are enumerated, and performance of most representative super-resolution approaches is provided to offer a fair and comparative view on performance of current approaches. Finally, we conclude the paper and suggest ways to improve usage of deep learning methods on real-time image super-resolution.",2019,Journal of Real-Time Image Processing,,10.1007/s11554-019-00925-3,
8ff879b3fcee252ee836f22bba3749c09abe1a8f,0,1,0,"Towards Digital Retina in Smart Cities: A Model Generation, Utilization and Communication Paradigm","The digital retina in smart cities is to select what the City Eye tells the City Brain, and convert the acquired visual data from front-end visual sensors to features in an intelligent sensing manner. By deploying deep learning and/or handcrafted models in front-end devices, the compact features can be extracted and subsequently delivered to back-end cloud for search and advanced analytics. In this context, we propose a model generation, utilization, and communication paradigm, aiming to address a set of unique challenges for better artificial intelligence services in smart cities. In particular, we present an integrated multiple deep learning models reuse and prediction strategy, which greatly increases the feasibility of the digital retina in processing and analyzing the large-scale visual data in smart cities. The promise of the proposed paradigm is demonstrated through a set of experiments.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),1907.13368,10.1109/ICME.2019.00012,https://arxiv.org/pdf/1907.13368.pdf
8ffc49aead99fdacb0b180468a36984759f2fc1e,1,1,0,Sparse Label Smoothing for Semi-supervised Person Re-Identification,"Person re-identification (re-id) is a cross-camera retrieval task which establishes a correspondence between images of a person from multiple cameras. Deep Learning methods have been successfully applied to the problem and achieved impressive results. However, these methods require large amounts of labeled training data. Current labeled datasets in person re-id are limited in scale and manually acquiring such large-scale dataset in surveillance camera is a tedious and labor-intensive task. In this paper, we propose a semi-supervised framework that performs Sparse Label Smoothing Regularization (SLSR) by considering similarities between unlabeled sample and training sample in the same feature space. Our approach first exploits the clustering property of existing person re-id datasets to create groups of similar objects that model the correlation among view. We make use of the training set to create cluster of similar objects using the intermediate feature representation of a CNN model. Each cluster is used to generate synthetic data samples using a generative adversarial model. We finally defined a sparse smoothing regularization term and train the network with join supervision of cross-entropy loss. The proposed approach tackles two problems (1) how to efficiently use the generated data and (2) how to address the over-smoothness problem found in current regularization. We solve these two problems by using a generative model for data augmentation and by maintaining and propagating similarities across the network through the concatenation of training images and generated images into one homogeneous feature space. Extensive experiments on four large-scale datasets show that our regularization method significantly improves the Re-ID accuracy compared to existing semi-supervised methods.",2018,ArXiv,1809.04976,,https://arxiv.org/pdf/1809.04976.pdf
903d9fabec4ce5d1626ce0dedb26f401ecd8ca82,1,1,1,GAN-Based Pose-Aware Regulation for Video-Based Person Re-Identification,"Video-based person re-identification deals with the inherent difficulty of matching sequences with different length, unregulated, and incomplete target pose/viewpoint structure. Common approaches operate either by reducing the problem to the still images case, facing a significant information loss, or by exploiting inter-sequence temporal dependencies as in Siamese Recurrent Neural Networks or in gait analysis. However, in all cases, the inter-sequences pose/viewpoint misalignment is considered, and the existing spatial approaches are mostly limited to the still images context. To this end, we propose a novel approach that can exploit more effectively the rich video information, by accounting for the role that the changing pose/viewpoint factor plays in the sequences matching process. In particular, our approach consists of two components. The first one attempts to complement the original pose-incomplete information carried by the sequences with synthetic GAN-generated images, and fuse their features vectors into a more discriminative viewpoint-insensitive embedding, namely Weighted Fusion (WF). Another one performs an explicit pose-based alignment of sequence pairs to promote coherent feature matching, namely Weighted-Pose Regulation (WPR). Extensive experiments on two large video-based benchmark datasets show that our approach outperforms considerably existing methods.",2019,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),1903.11552,10.1109/WACV.2019.00130,https://pureadmin.qub.ac.uk/ws/files/161094826/618.pdf
9042e204d9460d47c629f289e013440f8b55b600,1,0,0,ArTIST: Autoregressive Trajectory Inpainting and Scoring for Tracking,"One of the core components in online multiple object tracking (MOT) frameworks is associating new detections with existing tracklets, typically done via a scoring function. Despite the great advances in MOT, designing a reliable scoring function remains a challenge. In this paper, we introduce a probabilistic autoregressive generative model to score tracklet proposals by directly measuring the likelihood that a tracklet represents natural motion. One key property of our model is its ability to generate multiple likely futures of a tracklet given partial observations. This allows us to not only score tracklets but also effectively maintain existing tracklets when the detector fails to detect some objects even for a long time, e.g., due to occlusion, by sampling trajectories so as to inpaint the gaps caused by misdetection. Our experiments demonstrate the effectiveness of our approach to scoring and inpainting tracklets on several MOT benchmark datasets. We additionally show the generality of our generative model by using it to produce future representations in the challenging task of human motion prediction.",2020,ArXiv,2004.07482,,https://arxiv.org/pdf/2004.07482.pdf
90c18409b7a3be2cd6da599d02accba4c769e94e,1,1,0,Person Re-identification with Cascaded Pairwise Convolutions,"In this paper, a novel deep architecture named BraidNet is proposed for person re-identification. BraidNet has a specially designed WConv layer, and the cascaded WConv structure learns to extract the comparison features of two images, which are robust to misalignments and color differences across cameras. Furthermore, a Channel Scaling layer is designed to optimize the scaling factor of each input channel, which helps mitigate the zero gradient problem in the training phase. To solve the problem of imbalanced volume of negative and positive training samples, a Sample Rate Learning strategy is proposed to adaptively update the ratio between positive and negative samples in each batch. Experiments conducted on CUHK03-Detected, CUHK03-Labeled, CUHK01, Market-1501 and DukeMTMC-reID datasets demonstrate that our method achieves competitive performance when compared to state-of-the-art methods.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00159,http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1860.pdf
912c5e57dec2ff38c570b9ba35ad768cd7a2a902,0,1,0,Domain Generalized Person Re-Identification via Cross-Domain Episodic Learning,"Aiming at recognizing images of the same person across distinct camera views, person re-identification (re-ID) has been among active research topics in computer vision. Most existing re-ID works require collection of a large amount of labeled image data from the scenes of interest. When the data to be recognized are different from the source-domain training ones, a number of domain adaptation approaches have been proposed. Nevertheless, one still needs to collect labeled or unlabelled target-domain data during training. In this paper, we tackle an even more challenging and practical setting, domain generalized (DG) person re-ID. That is, while a number of labeled source-domain datasets are available, we do not have access to any target-domain training data. In order to learn domain-invariant features without knowing the target domain of interest, we present an episodic learning scheme which advances meta learning strategies to exploit the observed source-domain labeled data. The learned features would exhibit sufficient domain-invariant properties while not overfitting the source-domain data or ID labels. Our experiments on four benchmark datasets confirm the superiority of our method over the state-of-the-arts.",2020,ArXiv,2010.09561,,https://arxiv.org/pdf/2010.09561.pdf
9147cce0c48a1248f116792c51d2ceee7f59a056,1,1,0,Person Re-Identification via Group Symmetry Theory,"In recent years, deep learning represented by convolutional neural networks (CNNs) has developed rapidly. The development of deep learning has also led to rapid progress in the field of person re-identification. Many related researchers have begun to use deep learning to solve the problem of person re-identification. The existing deep learning methods for person re-identification mainly use the convolutional neural networks to extract features. The middle layers of the convolutional neural networks contain a wealth of structural information but the previous methods have not fully exploited them. This paper proposes a network named ResGroupNet that uses group symmetry theory to constrain the middle structure of ResNet-50. In detail, we added a branch at the fourth layer of the backbone, the branch was implemented based on theory, at each tail to the backbone and branch, we use sphere loss and triplet loss, respectively. According to our survey, we are the first to introduce the group theory into the ReID task. The experiments show that the proposed method is effective and have achieved good results on the Market-1501, DukeMTMC-reID, and CUHK03-NP datasets.",2019,IEEE Access,,10.1109/ACCESS.2019.2913559,
91725d7ae49b08dbb585823ed7a7e76c82e31d9e,1,0,0,Re-identification for Online Person Tracking using Discriminative Spatio-temporal Features,"The goal of this dissertation is to develop an effective online multi-person multicamera tracking system without unrealistic assumptions. The key technical elements necessary for this include: (i) transformation of detection of each person into a feature space to identify individuals and handle changes in position and posture, (ii) automated procedure to extract features and form trajectories without the need of prior information of individuals, and (iii) learning the evolution of the frame-by-frame spatial representation with the temporal dependencies. The thesis presents a novel model named Continuous Entity Association that combines the two acts of tracking within and across cameras and reformulates it as a single problem of continuous re-identification. The approach unifies the two separate tasks and presents a much clearer and simpler online solution which has the advantage of not requiring temporally contiguous sequences of video frames for tracking. This is accomplished by extracting appearance and facial features, and modeling location constraints across cameras. The approach is validated by using a simple and efficient inference algorithm. Next, a discriminative spatio-temporal learning approach for online tracking using LSTM networks is proposed. The idea is to exploit LSTM’s temporal step-by-step functionality to identify detections as belonging to the same individual and recovering from past errors in associating different individuals to a particular trajectory. State-of-the-art tracking results are obtained on two large publicly available datasets, CamNeT and DukeMTMC.",2018,,,,https://pdfs.semanticscholar.org/9172/5d7ae49b08dbb585823ed7a7e76c82e31d9e.pdf
9177a797701fbe2961f13f8981e825f1b24790fe,1,0,0,Deep Learning in Video Multi-Object Tracking: A Survey,"Abstract The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions.",2020,Neurocomputing,1907.1274,10.1016/j.neucom.2019.11.023,https://arxiv.org/pdf/1907.12740.pdf
91ddf662748051390bb3356380568fc9447a48ed,0,1,0,Joint Semi-supervised Learning and Re-ranking for Vehicle Re-identification,"Vehicle re-identification (re-ID) remains an unproblematic problem due to the complicated variations in vehicle appearances from multiple camera views. Most existing algorithms for solving this problem are developed in the fully-supervised setting, requiring access to a large number of labeled training data. However, it is impractical to expect large quantities of labeled data because the high cost of data annotation. Besides, re-ranking is a significant way to improve its performance when considering vehicle re-ID as a retrieval process. Yet limited effort has been devoted to the research of re-ranking in the vehicle re-ID. To address these problems, in this paper, we propose a semi-supervised learning system based on the Convolutional Neural Network (CNN) and re-ranking strategy for Vehicle re-ID. Specifically, we adopt the structure of Generative Adversarial Network (GAN) to obtain more vehicle images and enrich the training set, then a uniform label distribution will be assigned to the unlabeled samples according to the Label Smoothing Regularization for Outliers (LSRO), which regularizes the supervised learning model and improves the performance of re-ID. To optimize the re-ID results, an improved re-ranking method is exploited to optimize the initial rank list. Experimental results on publically available datasets, VeRi-776 and VehicleID, demonstrate that the method significantly outperforms the state-of-the-art.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545584,https://livrepository.liverpool.ac.uk/3033098/1/icpr_fangyu.pdf
921a2dcb2298a434d301189d69237e7e94c3ceb4,0,1,0,Adversarial Generation of Training Examples for Vehicle License Plate Recognition,"Generative Adversarial Networks (GAN) have attracted much research attention recently, leading to impressive results for natural image generation. However, to date little success was observed in using GAN generated images for improving classification tasks. Here we attempt to explore, in the context of car license plate recognition, whether it is possible to generate synthetic training data using GAN to improve recognition accuracy. With a carefully-designed pipeline, we show that the answer is affirmative. First, a large-scale image set is generated using the generator of GAN, without manual annotation. Then, these images are fed to a deep convolutional neural network (DCNN) followed by a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM), which performs the feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experimental results on a few data sets demonstrate the effectiveness of using GAN images: an improvement of 7.5% over a strong baseline with moderate-sized real data being available. We show that the proposed framework achieves competitive recognition accuracy on challenging test datasets. We also leverage the depthwise separate convolution to construct a lightweight convolutional RNN, which is about half size and 2x faster on CPU. Combining this framework and the proposed pipeline, we make progress in performing accurate recognition on mobile and embedded devices.",2017,ArXiv,1707.03124,,https://arxiv.org/pdf/1707.03124.pdf
921ba5497aa426ee5b37c9c7ac17443b71e1ab6c,0,1,0,Attributes Guided Feature Learning for Vehicle Re-identification,"Vehicle Re-ID has recently attracted enthusiastic attention due to its potential applications in smart city and urban surveillance. However, it suffers from large intra-class variation caused by view variations and illumination changes, and inter-class similarity especially for different identities with the similar appearance. To handle these issues, in this paper, we propose a novel deep network architecture, which guided by meaningful attributes including camera views, vehicle types and colors for vehicle Re-ID. In particular, our network is end-to-end trained and contains three subnetworks of deep features embedded by the corresponding attributes (i.e., camera view, vehicle type and vehicle color). Moreover, to overcome the shortcomings of limited vehicle images of different views, we design a view-specified generative adversarial network to generate the multi-view vehicle images. For network training, we annotate the view labels on the VeRi-776 dataset. Note that one can directly adopt the pre-trained view (as well as type and color) subnetwork on the other datasets with only ID information, which demonstrates the generalization of our model. Extensive experiments on the benchmark datasets VeRi-776 and VehicleID suggest that the proposed approach achieves the promising performance and yields to a new state-of-the-art for vehicle Re-ID.",2019,ArXiv,1905.08997,,https://arxiv.org/pdf/1905.08997.pdf
92459ffb2f5ec02190da7f76744a88a6277fad1a,0,1,0,When Person Re-identification Meets Changing Clothes,"Person re-identification (ReID) is now an active research topic for AI-based video surveillance applications such as specific person search, but the practical issue that the target person(s) may change clothes (clothes inconsistency problem) has been overlooked for long. For the first time, this paper systematically studies this problem. We first overcome the difficulty of lack of suitable dataset, by collecting a small yet representative real dataset for testing whilst building a large realistic synthetic dataset for training and deeper studies. Facilitated by our new datasets, we are able to conduct various interesting new experiments for studying the influence of clothes inconsistency. We find that changing clothes makes ReID a much harder problem in the sense of bringing difficulties to learning effective representations and also challenges the generalization ability of previous ReID models to identify persons with unseen (new) clothes. Representative existing ReID models are adopted to show informative results on such a challenging setting, and we also provide some preliminary efforts on improving the robustness of existing models on handling the clothes inconsistency issue in the data. We believe that this study can be inspiring and helpful for encouraging more researches in this direction. The dataset is available on the project website: https://wanfb.github.io/dataset.html.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2003.0407,10.1109/CVPRW50498.2020.00423,https://arxiv.org/pdf/2003.04070.pdf
927ec8dde9eb0e3bc5bf0b1a0ae57f9cf745fd9c,1,1,0,Learning Discriminative Features with Multiple Granularities for Person Re-Identification,"The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification (Re-ID) tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances. In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities. We carefully design the Multiple Granularity Network (MGN), a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feature representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03 indicate that our method robustly achieves state-of-the-art performances and outperforms any existing approaches by a large margin. For example, on Market-1501 dataset in single query mode, we obtain a top result of Rank-1/mAP=96.6%/94.2% with this method after re-ranking.",2018,ACM Multimedia,1804.01438,10.1145/3240508.3240552,https://arxiv.org/pdf/1804.01438.pdf
92a1105d68bf1edee44fe1ea78b47f9e2b692a3c,1,1,0,Multi-level feature learning with attention for person re-identification,"Person re-identification (re-ID) aims to match a specific person in a large gallery with different cameras and locations. Previous part-based methods mainly focus on part-level features with uniform partition, which increases learning ability for discriminative feature but not efficient or robust to scenarios with large variances. To address this problem, in this paper, we propose a novel feature fusion strategy based on traditional convolutional neural network. Then, a multi-branch deeper feature fusion network architecture is designed to perform discriminative learning for three semantically aligned region. Based on it, a novel self-attention mechanism is employed to softly assign corresponding weights to the semantic aligned feature during back-propagation. Comprehensive experiments have been conducted on several large-scale benchmark datasets, which demonstrates that proposed approach yields consistent and competitive re-ID accuracy compared with current single-domain re-ID methods.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09569-z,
9339168bbb1debe977fa85af7b143e87e01c6a47,1,0,1,Spatio-Temporal Associative Representation for Video Person Re-Identification,"Learning discriminative spatio-temporal representation is the key for solving video re-identification (re-id) challenges. Most existing methods focus on learning appearance features and/or selecting image frames, but ignore optimising the compatibility and interaction of appearance and motion attentive information. To address this limitation, we propose a novel model to learning Spatio-Temporal Associative Representation (STAR). We design local frame-level spatio-temporal association to learn discriminative attentive appearance and short-term motion features, and global video-level spatio-temporal association to form compact and discriminative holistic video representation. We further introduce a pyramid ranking regulariser for facilitating end-to-end model optimisation. Extensive experiments demonstrate the superiority of STAR against state-of-the-art methods on four video re-id benchmarks, including MARS, DukeMTMC-VideoReID, iLIDS-VID and PRID-2011.",2019,BMVC,,,https://bmvc2019.org/wp-content/uploads/papers/0332-paper.pdf
93582236c8a3ac0cb86e9a9a8f1b1249f6d87a54,1,0,0,DLGAN: Disentangling Label-Specific Fine-Grained Features for Image Manipulation,"Several recent studies have shown how disentangling images into content and feature spaces can provide controllable image translation/manipulation. In this paper, we propose a framework to enable utilizing discrete multi-labels to control which features to be disentangled,i.e., disentangling label-specific fine-grained features for image manipulation (dubbed DLGAN). By mapping the discrete label-specific attribute features into a continuous prior distribution, we enable leveraging the advantages of both discrete labels and reference images to achieve image manipulation in a hybrid fashion. For example, given a face image dataset (e.g., CelebA) with multiple discrete fine-grained labels, we can learn to smoothly interpolate a face image between black hair and blond hair through reference images while immediately control the gender and age through discrete input labels. To the best of our knowledge, this is the first work to realize such a hybrid manipulation within a single model. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method",2019,ArXiv,1911.09943,,https://arxiv.org/pdf/1911.09943.pdf
93817c245aaeb78dc8ebf1b24450942a3e24ffe7,0,1,0,Cross-dataset person re-identification using deep convolutional neural networks: effects of context and domain adaptation,"Over the past years, the impact of surveillance systems on public safety increases dramatically. One significant challenge in this domain is person re-identification, which aims to detect whether a person has already been captured by another camera in the surveillance network or not. Most of the work that has been conducted on person re-identification problem uses a single dataset, in which the training and test data are coming from the same source. However, as we have shown in this work, there is a strong bias among the person re-identification datasets, therefore, a method that has been trained and optimized on a specific person re-identification dataset may not generalize well and perform successfully on the other datasets. This is a problem for many real-world applications, since it is not feasible to collect and annotate sufficient amount of data from the target application to train or fine-tune a deep convolutional neural network model. Taking this issue into account, in this work, we have focused on cross-dataset person re-identification problem and first explored and analyzed in detail the use of the state-of-the-art deep convolutional neural network architectures, namely AlexNet, VGGNet, GoogLeNet, ResNet, and DenseNet that have been developed for generic image classification task. These deep CNN models have been adapted to the person re-identification domain by fine-tuning them for each human body part separately, as well as on the entire body, with the two relatively large person re-identification datasets: CUHK03 and Market-1501. Then, the performance of each adapted model has been evaluated on two different publicly available datasets: VIPeR and PRID2011. We have shown that, even just a domain adaptation leads comparable results to the state-of-the-art cross-dataset approaches. Another point that we have addressed in this paper is context adaptation. It has been known that person re-identification approaches implicitly utilizes background as context information. Therefore, to have a consistent background across different camera views, we have employed the cycle-consistent generative adversarial network. We have shown that this further improves the performance.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-6409-3,
939b2ff8ce2e4965d6a18c86d73f999040358ed0,1,0,0,Progress and Outlook of Visual Tracking: Bibliographic Analysis and Perspective,"Benefitting from continuous progress in computer architecture and computer vision algorithms, the visual tracking field has earned its rapid development in recent years. This paper surveys this interesting field through bibliographic analysis on the Web-of-Science literature from 1990 to 2019. Specifically, statistical analysis methods are used to obtain the most productive authors and countries/regions, the most cited papers, and so on. In order to realize an in-depth analysis, the co-authors, co-keywords and keyword-author co-occurrence networks are built to intuitively exhibit the evolution of research hotspots and the collaboration patterns among world-wide researchers. Brief introductions of the topics that occur frequently in co-keywords networks are provided as well. Furthermore, existing challenges and future research directions within the visual tracking field are discussed, revealing that tracking-by-detection and deep learning will continue receiving much attention. In addition, the parallel vision approach should be adopted for training and evaluating visual tracking models in a virtual-real interaction manner.",2019,IEEE Access,,10.1109/ACCESS.2019.2959942,
947b868aa1c38940df280ebeb8077d4e729fb988,1,0,0,"The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose","The availability of a large labeled dataset is a key requirement for applying deep learning methods to solve various computer vision tasks. In the context of understanding human activities, existing public datasets, while large in size, are often limited to a single RGB camera and provide only per-frame or per-clip action annotations. To enable richer analysis and understanding of human activities, we introduce IKEA ASM---a three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. Additionally, we benchmark prominent methods for video action recognition, object segmentation and human pose estimation tasks on this challenging dataset. The dataset enables the development of holistic methods, which integrate multi-modal and multi-view data to better perform on these tasks.",2020,ArXiv,2007.00394,,https://arxiv.org/pdf/2007.00394.pdf
94a0271b0ccc6312765891242c4b9332132468e2,0,1,0,Learning End-to-End Action Interaction by Paired-Embedding Data Augmentation,"In recognition-based action interaction, robots' responses to human actions are often pre-designed according to recognized categories and thus stiff. In this paper, we specify a new Interactive Action Translation (IAT) task which aims to learn end-to-end action interaction from unlabeled interactive pairs, removing explicit action recognition. To enable learning on small-scale data, we propose a Paired-Embedding (PE) method for effective and reliable data augmentation. Specifically, our method first utilizes paired relationships to cluster individual actions in an embedding space. Then two actions originally paired can be replaced with other actions in their respective neighborhood, assembling into new pairs. An Act2Act network based on conditional GAN follows to learn from augmented data. Besides, IAT-test and IAT-train scores are specifically proposed for evaluating methods on our task. Experimental results on two datasets show impressive effects and broad application prospects of our method.",2020,ArXiv,2007.08071,,https://arxiv.org/pdf/2007.08071.pdf
94e10216bf51f925f24baced640c19999882af24,0,1,0,A Dynamic Part-Attention Model for Person Re-Identification,"Person re-identification (ReID) is gaining more attention due to its important applications in pedestrian tracking and security prevention. Recently developed part-based methods have proven beneficial for stronger and explicit feature descriptions, but how to find real significant parts and reduce miscorrelation between images to improve accuracy of ReID still leaves much room to improve. In this paper, we propose a dynamic part-attention (DPA) method based on masks, which aims to improve the use of variable attention parts. Particularly, a two-branch network with a dynamic loss function is designed to extract features of the global image and the parts of the body separately. With the comprehensive but targeting learning strategy, the proposed method can capture discriminative features based, but not depending on, masks, which guides the whole network to focus on body features more consciously and achieves more robust performance. Our method achieves rank-1 accuracy of 91.68% on public dataset Market1501, and experimental results on three public datasets indicate that the proposed method is effective and achieves favorable accuracy when compared with the state-of-the-art methods.",2019,Sensors,,10.3390/s19092080,https://pdfs.semanticscholar.org/94e1/0216bf51f925f24baced640c19999882af24.pdf
9521aaa9f7dcd996773fc699f2a92553bd3b8bdb,1,0,1,VRSTC: Occlusion-Free Video Person Re-Identification,"Video person re-identification (re-ID) plays an important role in surveillance video analysis. However, the performance of video re-ID degenerates severely under partial occlusion. In this paper, we propose a novel network, called Spatio-Temporal Completion network (STCnet), to explicitly handle partial occlusion problem. Different from most previous works that discard the occluded frames, STCnet can recover the appearance of the occluded parts. For one thing, the spatial structure of a pedestrian frame can be used to predict the occluded body parts from the unoccluded body parts of this frame. For another, the temporal patterns of pedestrian sequence provide important clues to generate the contents of occluded parts. With the spatio-temporal information, STCnet can recover the appearance for the occluded parts, which could be leveraged with those unoccluded parts for more accurate video re-ID. By combining a re-ID network with STCnet, a video re-ID framework robust to partial occlusion (VRSTC) is proposed. Experiments on three challenging video re-ID databases demonstrate that the proposed approach outperforms the state-of-the-arts.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1907.08427,10.1109/CVPR.2019.00735,https://arxiv.org/pdf/1907.08427.pdf
952a8ef56f35376a52e4540d3df9e48f4077b09e,0,1,0,Deep feature embedding learning for person re-identification based on lifted structured loss,"Person re-identification (re-id) aims at matching the same individual in videos captured by multiple cameras, and much progress has been made in recent years due to large scale pedestrian data sets and deep learning-based techniques. In this paper, we propose deep feature embedding learning for person re-id based on lifted structured loss. Triplet loss is commonly used in deep neural networks for person re-id. However, the triplet loss-based framework is not able to make full use of the batch information, and thus needs to choose hard negative samples manually that is time-consuming. To address this problem, we adopt lifted structured loss for deep neural networks that makes the network learn better feature embedding by minimizing intra-class variation and maximizing inter-class variation. Extensive experiments on Market-1501, CUHK03, CUHK01 and VIPeR data sets demonstrate the superior performance of the proposed method over state-of-the-arts in terms of the cumulative match curve (CMC) metric.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-6408-4,
95549a8692f734b978f1177c76242e074d52e67a,0,1,0,Matching Adversarial Networks,"Generative Adversarial Nets (GANs) and Conditonal GANs (CGANs) show that using a trained network as loss function (discriminator) enables to synthesize highly structured outputs (e.g. natural images). However, applying a discriminator network as a universal loss function for common supervised tasks (e.g. semantic segmentation, line detection, depth estimation) is considerably less successful. We argue that the main difficulty of applying CGANs to supervised tasks is that the generator training consists of optimizing a loss function that does not depend directly on the ground truth labels. To overcome this, we propose to replace the discriminator with a matching network taking into account both the ground truth outputs as well as the generated examples. As a consequence, the generator loss function also depends on the targets of the training examples, thus facilitating learning. We demonstrate on three computer vision tasks that this approach can significantly outperform CGANs achieving comparable or superior results to task-specific solutions and results in stable training. Importantly, this is a general approach that does not require the use of task-specific loss functions.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00837,http://openaccess.thecvf.com/content_cvpr_2018/papers/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.pdf
957d185f5f7f810734adff507a623d5bca5a0c1c,1,0,0,Conditional GAN based individual and global motion fusion for multiple object tracking in UAV videos,"Abstract Multiple Object Tracking (MOT) meets great challenges in videos captured by Unmanned Aerial Vehicles (UAVs). Different from traditional videos, due to high altitude and abrupt motion changes of UAVs, the sizes of target objects in UAVs videos are usually very small and the appearance information of target objects is unreliable. The motion analysis is meaningful to associate multiple objects in UAV videos. However, the traditional motion analysis models inevitably suffer from the autonomous motion of UAVs. In this paper, we proposed a Conditional Generative Adversarial Networks (GAN) based model to predict complex motions in UAV videos. We regard the objects motions and the UAV movement as the individual motions and global motions respectively. They are complementary with each other and are employed jointly to facilitate accurate motion prediction. Specifically, a social Long Short Term Memory network is exploited to estimate the individual motion of objects, and a Siamese network is constructed to generate the global motion to reflect the view changes from UAVs, and a conditional GAN is developed to generate the final motion affinity. Extensive experimental results are conducted on public UAV datasets contained various types of objects and 4 different kinds of object detection inputs. Robust motion prediction and improved MOT performance are achieved compared with state-of-the-art methods.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2019.12.018,
95830c54bdf8aafeedd9baba126259920bbd1375,1,0,1,Iterative Local-Global Collaboration Learning Towards One-Shot Video Person Re-Identification,"Video person re-identification (video Re-ID) plays an important role in surveillance video analysis and has gained increasing attention recently. However, existing supervised methods require vast labeled identities across cameras. Although some unsupervised approaches have been exploited for video Re-ID, they are still in their infancy due to the complex nature of learning discriminative features on unlabelled data. In this article, we focus on one-shot video Re-ID and present an iterative local-global collaboration learning approach to learn robust and discriminative person representations. Specifically, it jointly considers the global video information and local frame sequence information to better capture the diverse appearance of the person for feature learning and pseudo-label estimation. Moreover, as the cross-entropy loss may induce the model to focus on identity-irrelevant factors, we introduce the variational information bottleneck as a regularization term to train the model together. It can help filter undesirable information and characterize subtle differences among persons. Since accuracy cannot always be guaranteed for pseudo-labels, we adopt a dynamic selection strategy to select part of pseudo-labeled data with higher confidence to update the training set and re-train the learning model. During training, our method iteratively executes the feature learning, pseudo-label estimation, and dynamic sample selection until all the unlabeled data have been seen. Extensive experiments on two public datasets, i.e., DukeMTMC-VideoReID and MARS, have verified the superiority of our model to several cutting-edge competitors.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3026625,
96001a90a5f76d5ef7f65935b82ed74f7cb05da3,1,0,0,Enhanced Association With Supervoxels in Multiple Hypothesis Tracking,"Remarkable progress has been made in the field of multi-object tracking. Although tracking-by-detection has recently became one of the most popular frameworks, it still has one main drawback: this approach relies heavily on the quality of detection. Thus, the missing detections caused by partial occlusion usually lead to fragment problem. To address this problem, this paper introduces supervoxels to represent objects with partial occlusion, even for missing detections. We first extract superpixels of the foreground, and then our proposed supervoxel consists of spatial-temporal sequences of superpixels. The supervoxels represent tracklets at the image level, so it is robust for initial detection. Then, we incorporate supervoxels into multiple hypotheses tracking by considering the enhanced association with supervoxels (EAS). Moreover, we propose a detection refinement method based on EAS. As our approach allows us to handle partial occlusion problems, we achieve remarkable results in crowded scenes. Finally, our experiments on both MOT15 and MOT16 benchmarks show that our EAS is competitive with the state-of-the-art trackers.",2019,IEEE Access,,10.1109/ACCESS.2018.2881019,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08532351.pdf
9643dabbf1771d2d82ded2fde3baaa15a67f6e56,1,0,0,Unsupervised Joint Subspace and Dictionary Learning for Enhanced Cross-Domain Person Re-Identification,"Person re-identification (Re-ID) has drawn increasing attention from both academia and industry due to its great potentials in surveillance applications. Most existing research efforts have attempted to tackle cross-view variation in single-domain person Re-ID. However, there is still a lack of effective approaches to cross-domain person Re-ID problem. In this paper, an unsupervised joint subspace and dictionary learning (UJSDL) framework is proposed to address the cross-domain person Re-ID problem, where both cross-view (i.e., across different cameras in the same network of cameras) and cross-domain (i.e., across different network of cameras) variation are jointly addressed. In particular, to reduce the impact of cross-view distribution variation, the graph Laplacian approach is used to project the images from different camera views in each domain into a shared subspace. To alleviate the impact of cross-domain distribution variation, a shared dictionary is learned from all the projection subspaces such that the discriminative information from both the labeled source datasets and the unlabeled target dataset are well encoded. To efficiently solve the joint subspace and dictionary learning task, an alternating optimization algorithm is presented. We used multiple different feature sets and conducted experiments on multiple benchmark datasets as the target domain. The results demonstrate that UJSDL outperforms the state-of-the-art approaches.",2018,IEEE Journal of Selected Topics in Signal Processing,,10.1109/JSTSP.2018.2877475,
969e9d7c4b65fac87e4b81c64c9f1befc972d662,0,1,0,Pixel and Channel Attention Network for Person Re-identification,"The combination of global and partial features has been an effective method to improve the precision for Person Re-identification. However, illumination, camera angle and pedestrian pose, etc. still have adverse effects on the retrieval results. In particular, a lot of background and other redundant information is contained in the boundingbox. Meanwhile, the part-based solutions are imprecise on account of unbalanced partitioning. In order to minimize the impact of these factors on the retrieval results, we introduced the pixel, channel attention modules and middle layer supervision into the ReID system to aggregate person features. In this paper, we propose a novel architecture for Person Re-Identification, with the pixel and channel attention modules that are beneficial for feature extraction. Comprehensive experiments results on the mainstream datasets including Market-1501, DukeMTMC-ReId, CUHK03-labeled and CUHK03-detected show that our method achieves better results.",2019,IGTA,,10.1007/978-981-13-9917-6_10,
96b10af78ab05dc9a99d7de953e246caa9e0e16e,1,1,0,Multi-Level Feature Network With Multi-Loss for Person Re-Identification,"Person re-identification has become a challenging task due to various factors. One key to effective person re-identification is the extraction of the discriminative features of a person’s appearance. Most previous works based on deep learning extract pedestrian characteristics from neural networks but only from the top feature layer. However, the low-layer feature could be more discriminative in certain circumstances. Hence, we propose a method, named the multi-level feature network with multiple losses (MFML), which has a multi-branch network architecture that consists of multiple middle layers and one top layer for feature representations. To extract the discriminative middle-layer features and have a good effect on deeper layers, we utilize the triplet loss function to train the middle-layer features. For the top layer, we focus on learning more discriminative feature representations, so we utilize the hybrid loss (HL) function to train the top-layer feature. Instead of concatenating multilayer features directly, we concatenate the weighted middle-layer features and the weighted top-layer feature as the discriminative features in the testing phase. The extensive evaluations conducted on three datasets show that our method achieves a competitive accuracy level compared with the state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2927052,
96e77135e745385e87fdd0f7ced951bf1fe9a756,1,1,1,People Tracking and Re-Identification from Multiple Cameras,"In many surveillance or monitoring applications, one or more cameras view several people that move in an environment. Multi-person tracking amounts to using the videos from these cameras to determine who is where at all times. The problem is very challenging both computationally and conceptually. On one hand the amount of video to process is enormous while near real-time performance is desired. On the other hand people's varying appearance due to lighting, occlusions, viewpoint changes, and unpredictable motion in blind spots make person re-identification challenging. This dissertation makes several contributions to person re-identification and multi-person tracking from multiple cameras. We present a weighted triplet loss for learning appearance descriptors which addresses both problems uniformly, doesn't suffer from the imbalance between positive and negative examples, and remains robust to outliers. We introduce the largest tracking benchmark to date, DukeMTMC, and adequate performance measures that emphasize correct person identification. A correlation clustering formulation for associating person observations is then introduced which maximizes agreements on the evidence graph. We assemble a tracker called DeepCC that combines an existing person detector, hierarchical and online reasoning, our appearance features and correlation clustering association. DeepCC achieves increased performance on two challenging sequences from the DukeMTMC benchmark, and ablation experiments demonstrate the merits of individual components.",2018,,,,http://vision.cs.duke.edu/DukeMTMC/data/misc/Ristani_dissertation.pdf
96fece067ba203eb32c4884ce66031bf7c4a8f78,0,1,0,Pedestrian Attribute Recognition: A Survey,"Recognizing pedestrian attributes is an important task in computer vision community due to it plays an important role in video surveillance. Many algorithms has been proposed to handle this task. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attributes recognition (PAR, for short), including the fundamental concepts of pedestrian attributes and corresponding challenges. Secondly, we introduce existing benchmarks, including popular datasets and evaluation criterion. Thirdly, we analyse the concept of multi-task learning and multi-label learning, and also explain the relations between these two learning algorithms and pedestrian attribute recognition. We also review some popular network architectures which have widely applied in the deep learning community. Fourthly, we analyse popular solutions for this task, such as attributes group, part-based, \emph{etc}. Fifthly, we shown some applications which takes pedestrian attributes into consideration and achieve better performance. Finally, we summarized this paper and give several possible research directions for pedestrian attributes recognition. The project page of this paper can be found from the following website: \url{this https URL}.",2019,ArXiv,1901.07474,,https://arxiv.org/pdf/1901.07474.pdf
970d00225441346b8bba916ca11edadc645e3706,1,0,0,Learning Flat Latent Manifolds with VAEs,"Measuring the similarity between data points often requires domain knowledge, which can in parts be compensated by relying on unsupervised methods such as latent-variable models, where similarity/distance is estimated in a more compact latent space. Prevalent is the use of the Euclidean metric, which has the drawback of ignoring information about similarity of data stored in the decoder, as captured by the framework of Riemannian geometry. We propose an extension to the framework of variational auto-encoders allows learning flat latent manifolds, where the Euclidean metric is a proxy for the similarity between data points. This is achieved by defining the latent space as a Riemannian manifold and by regularising the metric tensor to be a scaled identity matrix. Additionally, we replace the compact prior typically used in variational auto-encoders with a recently presented, more expressive hierarchical one---and formulate the learning problem as a constrained optimisation problem. We evaluate our method on a range of data-sets, including a video-tracking benchmark, where the performance of our unsupervised approach nears that of state-of-the-art supervised approaches, while retaining the computational efficiency of straight-line-based approaches.",2020,ICML 2020,2002.04881,,https://arxiv.org/pdf/2002.04881.pdf
971325d1eb1dd60abca2d304c5d6cb8324ac2c46,1,1,1,A Comprehensive Overview of Person Re-Identification Approaches,"Person re-identification, identifying and tracking pedestrians in cross-domain monitoring systems, is an important technology in the computer vision field and of real significance for the construction of smart cities. With the development of deep learning techniques, especially convolutional neural networks, this technology has received more extensive attention and improvement in recent years and a large number of noteworthy achievements have emerged. This paper provides a comprehensive overview of person re-identification approaches to assist researchers in quickly understand this field with preference as well as to provide a more structured framework. By reviewing more than 300 re-identification related papers, the focus of these studies is summarized as information extraction, metric learning, post-processing, efficiency improvement, labeling cost reduction, and data type expansion. This classification is then organized based on different technologies, and on this basis, the pros and cons of each technology are analyzed. Moreover, this overview summarizes the difficulties and challenges of re-identification and discusses the possible research directions for reference.",2020,IEEE Access,,10.1109/ACCESS.2020.2978344,
972c5e77a3352b28ac877467937c8a7f5fdd0910,0,1,0,Multi-View Vehicle Re-Identification using Temporal Attention Model and Metadata Re-ranking,"Object re-identification (ReID) is an arduous task which requires matching an object across different nonoverlapping camera views. Recently, many researchers are working on person ReID by taking advantages of appearance, human pose, temporal constraints, etc. However, vehicle ReID is even more challenging because vehicles have fewer discriminant features than human due to viewpoint orientation, changes in lighting condition and inter-class similarity. In this paper, we propose a viewpoint-aware temporal attention model for vehicle ReID utilizing deep learning features extracted from consecutive frames with vehicle orientation and metadata attributes (i.e., type, brand, color) being taken into consideration. In addition, re-ranking with soft decision boundary is applied as post-processing for result refinement. The proposed method is evaluated on CVPR AI City Challenge 2019 dataset, achieving mAP of 79.17% with the second place ranking in the competition.",2019,CVPR Workshops,,,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Huang_Multi-View_Vehicle_Re-Identification_using_Temporal_Attention_Model_and_Metadata_Re-ranking_CVPRW_2019_paper.pdf
974fea3530307da6d22ef91c6765f5404514b3c5,1,1,0,Cross-Domain Adversarial Feature Learning for Sketch Re-identification,"Under person re-identification (Re-ID), a query photo of the target person is often required for retrieval. However, one is not always guaranteed to have such a photo readily available under a practical forensic setting. In this paper, we define the problem of Sketch Re-ID, which instead of using a photo as input, it initiates the query process using a professional sketch of the target person. This is akin to the traditional problem of forensic facial sketch recognition, yet with the major difference that our sketches are whole-body other than just the face. This problem is challenging because sketches and photos are in two distinct domains. Specifically, a sketch is the abstract description of a person. Besides, person appearance in photos is variational due to camera viewpoint, human pose and occlusion. We address the Sketch Re-ID problem by proposing a cross-domain adversarial feature learning approach to jointly learn the identity features and domain-invariant features. We employ adversarial feature learning to filter low-level interfering features and remain high-level semantic information. We also contribute to the community the first Sketch Re-ID dataset with 200 persons, where each person has one sketch and two photos from different cameras associated. Extensive experiments have been performed on the proposed dataset and other common sketch datasets including CUFSF and QUML-shoe. Results show that the proposed method outperforms the state-of-the-arts.",2018,ACM Multimedia,,10.1145/3240508.3240606,
9757f250e9c3b1974d2ffa9ba3546d87a1e64f6f,1,1,0,Deep Residual Network with Self Attention Improves Person Re-Identification Accuracy,"In this paper, we present an attention mechanism scheme to improve the person re-identification task. Inspired by biology, we propose Self Attention Grid (SAG) to discover the most informative parts from a high-resolution image using its internal representation. In particular, given an input image, the proposed model is fed with two copies of the same image and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learns a filtering attention grid. We apply a max filter operation to non-overlapping sub-regions on the high feature representation before element-wise multiplied with the output of the second branch. The feature maps of the second branch are subsequently weighted to reflect the importance of each patch of the grid using a softmax operation. Our attention module helps the network to learn the most discriminative visual features of multiple image regions and is specifically optimized to attend feature representation at different levels. Extensive experiments on three large-scale datasets show that our self-attention mechanism significantly improves the baseline model and outperforms various state-of-art models by a large margin.",2019,ICMLC '19,,10.1145/3318299.3318324,
975baa030808206f0ee5de4ade1d3dfef80beacf,0,1,0,Enhancing Person Re-identification in a Self-Trained Subspace,"Despite the promising progress made in recent years, person re-identification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. For this challenging problem, a large variety of algorithms have been developed in the fully supervised setting, requiring access to a large amount of labeled training data. However, the main bottleneck for fully supervised re-ID is the limited availability of labeled training samples. To address this problem, we propose a self-trained subspace learning paradigm for person re-ID that effectively utilizes both labeled and unlabeled data to learn a discriminative subspace where person images across disjoint camera views can be easily matched. The proposed approach first constructs pseudo-pairwise relationships among unlabeled persons using the k-nearest neighbors algorithm. Then, with the pseudo-pairwise relationships, the unlabeled samples can be easily combined with the labeled samples to learn a discriminative projection by solving an eigenvalue problem. In addition, we refine the pseudo-pairwise relationships iteratively, which further improves learning performance. A multi-kernel embedding strategy is also incorporated into the proposed approach to cope with the non-linearity in a person’s appearance and explore the complementation of multiple kernels. In this way, the performance of person re-ID can be greatly enhanced when training data are insufficient. Experimental results on six widely used datasets demonstrate the effectiveness of our approach, and its performance can be comparable to the reported results of most state-of-the-art fully supervised methods while using much fewer labeled data.",2017,ACM Trans. Multim. Comput. Commun. Appl.,1704.0602,10.1145/3089249,https://arxiv.org/pdf/1704.06020.pdf
97834ad2a0556fedbea0407a84ca8f520f0fc11f,0,1,0,Face-Specific Data Augmentation for Unconstrained Face Recognition,"We identify two issues as key to developing effective face recognition systems: maximizing the appearance variations of training images and minimizing appearance variations in test images. The former is required to train the system for whatever appearance variations it will ultimately encounter and is often addressed by collecting massive training sets with millions of face images. The latter involves various forms of appearance normalization for removing distracting nuisance factors at test time and making test faces easier to compare. We describe novel, efficient face-specific data augmentation techniques and show them to be ideally suited for both purposes. By using knowledge of faces, their 3D shapes, and appearances, we show the following: (a) We can artificially enrich training data for face recognition with face-specific appearance variations. (b) This synthetic training data can be efficiently produced online, thereby reducing the massive storage requirements of large-scale training sets and simplifying training for many appearance variations. Finally, (c) The same, fast data augmentation techniques can be applied at test time to reduce appearance variations and improve face representations. Together, with additional technical novelties, we describe a highly effective face recognition pipeline which, at the time of submission, obtains state-of-the-art results across multiple benchmarks. Portions of this paper were previously published by Masi et al. (European conference on computer vision, Springer, pp 579–596, 2016b, International conference on automatic face and gesture recognition, 2017).",2019,International Journal of Computer Vision,,10.1007/s11263-019-01178-0,https://talhassner.github.io/home/projects/augmented_faces/Masietal_IJCV2019.pdf
979bcd527f2a1c0ffb5f08c57959f48a0bb65f84,0,0,1,Progressive deep feature learning for manga character recognition via unlabeled training data,"The recognition of manga (Japanese comics) characters is an essential step in industrial applications, such as manga character retrieval, content analysis and copyright protection. However, conventional methods for manga character recognition are mainly based on handcrafted features which are not robust enough for manga of various style. The emergence of deep learning based methods provides representational features, which has a huge demand for labeled data. In this paper, we propose a framework to exploit unlabeled manga data to facilitate the discriminative capability of deep feature representations for manga character recognition (i.e., unsupervised learning on manga images), which does not rely on any manual annotation. Specifically, we first train an initial feature model using an anime character dataset. Then, we adopt a Progressive Main Characters Mining (PMCM) strategy which iterates between two steps: 1) produce selected data with estimated labels from unlabeled data, 2) update the feature model by the selected data. These two steps are mutually promoted in essence. Experimental results on Manga109 dataset, to which we introduce new head annotations, demonstrate the effectiveness of the proposed framework and the usefulness in manga character verification and retrieval.",2019,ACM TUR-C,,10.1145/3321408.3322624,
97bb52ba3493067cfa0a31c38632508c99b5710a,1,0,0,Improved Generalization of Heading Direction Estimation for Aerial Filming Using Semi-Supervised Regression,"In the task of Autonomous aerial filming of a moving actor (e.g. a person or a vehicle), it is crucial to have a good heading direction estimation for the actor from the visual input. However, the models obtained in other similar tasks, such as pedestrian collision risk analysis and human-robot interaction, are very difficult to generalize to the aerial filming task, because of the difference in data distributions. Towards improving generalization with less amount of labeled data, this paper presents a semi-supervised algorithm for heading direction estimation problem. We utilize temporal continuity as the unsupervised signal to regularize the model and achieve better generalization ability. This semi-supervised algorithm is applied to both training and testing phases, which increases the testing performance by a large margin. We show that by leveraging unlabeled sequences, the amount of labeled data required can be significantly reduced. We also discuss several important details on improving the performance by balancing labeled and unlabeled loss, and making good combinations. Experimental results show that our approach robustly outputs the heading direction for different types of actor. The aesthetic value of the video is also improved in the aerial filming task.",2019,2019 International Conference on Robotics and Automation (ICRA),1903.11174,10.1109/ICRA.2019.8793994,https://arxiv.org/pdf/1903.11174.pdf
97c296b7fbd12fac073779fac321c5812b830cea,0,1,0,Deep Association Learning for Unsupervised Video Person Re-identification,"Deep learning methods have started to dominate the research progress of video-based person re-identification (re-id). However, existing methods mostly consider supervised learning, which requires exhaustive manual efforts for labelling cross-view pairwise data. Therefore, they severely lack scalability and practicality in real-world video surveillance applications. In this work, to address the video person re-id task, we formulate a novel Deep Association Learning (DAL) scheme, the first end-to-end deep learning method using none of the identity labels in model initialisation and training. DAL learns a deep re-id matching model by jointly optimising two margin-based association losses in an end-to-end manner, which effectively constrains the association of each frame to the best-matched intra-camera representation and cross-camera representation. Existing standard CNNs can be readily employed within our DAL scheme. Experiment results demonstrate that our proposed DAL significantly outperforms current state-of-the-art unsupervised video person re-id methods on three benchmarks: PRID 2011, iLIDS-VID and MARS.",2018,BMVC,1808.07301,,https://arxiv.org/pdf/1808.07301.pdf
97e0a9a9662ce640753149df4637ad56b6132146,0,1,0,Hierarchical Feature Embedding for Attribute Recognition,"Attribute recognition is a crucial but challenging task due to viewpoint changes, illumination variations and appearance diversities, etc. Most of previous work only consider the attribute-level feature embedding, which might perform poorly in complicated heterogeneous conditions. To address this problem, we propose a hierarchical feature embedding (HFE) framework, which learns a fine-grained feature embedding by combining attribute and ID information. In HFE, we maintain the inter-class and intra-class feature embedding simultaneously. Not only samples with the same attribute but also samples with the same ID are gathered more closely, which could restrict the feature embedding of visually hard samples with regard to attributes and improve the robustness to variant conditions. We establish this hierarchical structure by utilizing HFE loss consisted of attribute-level and ID-level constraints. We also introduce an absolute boundary regularization and a dynamic loss weight as supplementary components to help build up the feature embedding. Experiments show that our method achieves the state-of-the-art results on two pedestrian attribute datasets and a facial attribute dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2005.11576,10.1109/cvpr42600.2020.01307,https://arxiv.org/pdf/2005.11576.pdf
9812542cae5a470ea601e7c3a871331694105093,1,1,0,Person Re-identification by Deep Learning Attribute-Complementary Information,"Automatic person re-identification (re-id) across camera boundaries is a challenging problem. Approaches have to be robust against many factors which influence the visual appearance of a person but are not relevant to the person's identity. Examples for such factors are pose, camera angles, and lighting conditions. Person attributes are a semantic high level information which is invariant across many such influences and contain information which is often highly relevant to a person's identity. In this work we develop a re-id approach which leverages the information contained in automatically detected attributes. We train an attribute classifier on separate data and include its responses into the training process of our person re-id model which is based on convolutional neural networks (CNNs). This allows us to learn a person representation which contains information complementary to that contained within the attributes. Our approach is able to identify attributes which perform most reliably for re-id and focus on them accordingly. We demonstrate the performance improvement gained through use of the attribute information on multiple large-scale datasets and report insights into which attributes are most relevant for person re-id.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.186,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w17/papers/Schumann_Person_Re-Identification_by_CVPR_2017_paper.pdf
982cb4421cedce057ae2fc864efac8e43d9c0a5a,1,0,0,TAO: A Large-Scale Benchmark for Tracking Any Object,"For many years, multi-object tracking benchmarks have focused on a handful of categories. Motivated primarily by surveillance and self-driving applications, these datasets provide tracks for people, vehicles, and animals, ignoring the vast majority of objects in the world. By contrast, in the related field of object detection, the introduction of large-scale, diverse datasets (e.g., COCO) have fostered significant progress in developing highly robust solutions. To bridge this gap, we introduce a similarly diverse dataset for Tracking Any Object (TAO). It consists of 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. Importantly, we adopt a bottom-up approach for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. To this end, we ask annotators to label objects that move at any point in the video, and give names to them post factum. Our vocabulary is both significantly larger and qualitatively different from existing tracking datasets. To ensure scalability of annotation, we employ a federated approach that focuses manual effort on labeling tracks for those relevant objects in a video (e.g., those that move). We perform an extensive evaluation of state-of-the-art trackers and make a number of important discoveries regarding large-vocabulary tracking in an open-world. In particular, we show that existing single- and multi-object trackers struggle when applied to this scenario in the wild, and that detection-based, multi-object trackers are in fact competitive with user-initialized ones. We hope that our dataset and analysis will boost further progress in the tracking community.",2020,ECCV,2005.10356,10.1007/978-3-030-58558-7_26,https://arxiv.org/pdf/2005.10356.pdf
987092484f6a93eb3f0dc9f2fca33c74f0cdaa03,1,1,0,Attention-Aware Adversarial Network for Person Re-Identification,"Person re-identification (re-ID) is a fundamental problem in the field of computer vision. The performance of deep learning-based person re-ID models suffers from a lack of training data. In this work, we introduce a novel image-specific data augmentation method on the feature map level to enforce feature diversity in the network. Furthermore, an attention assignment mechanism is proposed to enforce that the person re-ID classifier focuses on nearly all important regions of the input person image. To achieve this, a three-stage framework is proposed. First, a baseline classification network is trained for person re-ID. Second, an attention assignment network is proposed based on the baseline network, in which the attention module learns to suppress the response of the current detected regions and re-assign attentions to other important locations. By this means, multiple important regions for classification are highlighted by the attention map. Finally, the attention map is integrated in the attention-aware adversarial network (AAA-Net), which generates high-performance classification results with an adversarial training strategy. We evaluate the proposed method on two large-scale benchmark datasets, including Market1501 and DukeMTMC-reID. Experimental results show that our algorithm performs favorably against the state-of-the-art methods.",2019,,,10.3390/APP9081550,https://pdfs.semanticscholar.org/9870/92484f6a93eb3f0dc9f2fca33c74f0cdaa03.pdf
9882c3918240c491164d296e567ae17fa68ee5a2,1,1,1,Unsupervised Tracklet Person Re-Identification,"Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in a practical re-id deployment, due to the lack of exhaustive identity labelling of positive and negative image pairs for every camera-pair. In this work, we present an unsupervised re-id deep learning approach. It is capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data end-to-end. We formulate an Unsupervised Tracklet Association Learning (UTAL) framework. This is by jointly learning within-camera tracklet discrimination and cross-camera tracklet association in order to maximise the discovery of tracklet identity matching both within and across camera views. Extensive experiments demonstrate the superiority of the proposed model over the state-of-the-art unsupervised learning and domain adaptation person re-id methods on eight benchmarking datasets.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,1903.00535,10.1109/TPAMI.2019.2903058,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/61627/2/Gong%20Unsupervised%20Tracklet%20Person%202019%20Accepted.pdf
98936da38b3459a71fb2de370c059cc24914133b,1,0,0,"ReXCam: Resource-Efficient, Cross-Camera Video Analytics at Scale","Enterprises are increasingly deploying large camera networks for video analytics. Many target applications entail a common problem template: searching for and tracking an object or activity of interest (e.g. a speeding vehicle, a break-in) through a large camera network in live video. Such cross-camera analytics is compute and data intensive, with cost growing with the number of cameras and time. To address this cost challenge, we present ReXCam, a new system for efficient cross-camera video analytics. ReXCam exploits spatial and temporal locality in the dynamics of real camera networks to guide its inference-time search for a query identity. In an offline profiling phase, ReXCam builds a cross-camera correlation model that encodes the locality observed in historical traffic patterns. At inference time, ReXCam applies this model to filter frames that are not spatially and temporally correlated with the query identity's current position. In the cases of occasional missed detections, ReXCam performs a fast-replay search on recently filtered video frames, enabling gracefully recovery. Together, these techniques allow ReXCam to reduce compute workload by 8.3x on an 8-camera dataset, and by 23x - 38x on a simulated 130-camera dataset. ReXCam has been implemented and deployed on a testbed of 5 AWS DeepLens cameras.",2018,,,,https://pdfs.semanticscholar.org/9893/6da38b3459a71fb2de370c059cc24914133b.pdf
98b354ddfa46ccf9fc604c751eb9deebf4c909f8,1,0,0,Textual Dependency Embedding for Person Search by Language,"Person search by language aims to associate the pedestrian images with free-form natural language descriptions. Although great efforts have been made to align images with sentences, most researchers neglect the difficulty of long-distance dependency modeling in textual encoding, which is very important for solving this problem because the description sentences are always long and have complex structures for distinguishing different pedestrians. In this work, we focus on the long-distance dependencies in a sentence for better textual encoding, and accordingly propose the Textual Dependency Embedding (TDE) method. We first employ the sentence analysis tools to figure out the long-distance syntactic dependencies from a dependent to its governor in a sentence. Then we embed the dependent representations to their governor adaptively in our Governor-guided Dependent Attention Module (GDAM) to model these long-distance relations. After that, we further consider the dependency types, which also tell the importance of different dependents semantically, and embed them together with the dependents' features to clarify their inequivalent contributions to their governor. Extensive experiments and analysis on person search by language and image-text matching have validated the effectiveness of our method, and we have obtained the state-of-the-art performance on the CUHK-PEDES and Flickr30K datasets.",2020,ACM Multimedia,,10.1145/3394171.3413895,
98bda8768fd4a384695ecc736876a87f51c4ca0e,0,1,0,Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond,"State-of-the-art pedestrian detection models have achieved great success in many benchmarks. However, these models require lots of annotation information and the labeling process usually takes much time and efforts. In this paper, we propose a method to generate labeled pedestrian data and adapt them to support the training of pedestrian detectors. The proposed framework is built on the Generative Adversarial Network (GAN) with multiple discriminators, trying to synthesize realistic pedestrians and learn the background context simultaneously. To handle the pedestrians of different sizes, we adopt the Spatial Pyramid Pooling (SPP) layer in the discriminator. We conduct experiments on two benchmarks. The results show that our framework can smoothly synthesize pedestrians on background images of variations and different levels of details. To quantitatively evaluate our approach, we add the generated samples into training data of the baseline pedestrian detectors and show the synthetic images are able to improve the detectors' performance.",2018,ArXiv,1804.02047,,https://arxiv.org/pdf/1804.02047.pdf
98bf42055160845e6f8f3c022298e3b8e4e55f80,1,0,0,Vision Meets Drones: A Challenge,"In this paper we present a large-scale visual object detection and tracking benchmark, named VisDrone2018, aiming at advancing visual understanding tasks on the drone platform. The images and video sequences in the benchmark were captured over various urban/suburban areas of 14 different cities across China from north to south. Specifically, VisDrone2018 consists of 263 video clips and 10,209 images (no overlap with video clips) with rich annotations, including object bounding boxes, object categories, occlusion, truncation ratios, etc. With intensive amount of effort, our benchmark has more than 2.5 million annotated instances in 179,264 images/video frames. Being the largest such dataset ever published, the benchmark enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. In particular, we design four popular tasks with the benchmark, including object detection in images, object detection in videos, single object tracking, and multi-object tracking. All these tasks are extremely challenging in the proposed dataset due to factors such as occlusion, large scale and pose variation, and fast motion. We hope the benchmark largely boost the research and development in visual analysis on drone platforms.",2018,ArXiv,1804.07437,,https://arxiv.org/pdf/1804.07437.pdf
991649f2903d05a27243a7d26016c4df735a6fbb,0,1,0,High-Quality Video Generation from Static Structural Annotations,"This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released: https://github.com/junting/seg2vid .",2020,International Journal of Computer Vision,,10.1007/s11263-020-01334-x,
998f42a3b771de38b1d4b68c978dc496f7e95dd2,0,0,1,Pedestrian Re-Recognition Algorithm Based on Optimization Deep Learning-Sequence Memory Model,"Pedestrian re-recognition is an important research because it affects applications such as intelligent monitoring, content-based video retrieval, and human-computer interaction. It can help relay tracking and criminal suspect detection in large-scale video surveillance systems. Although the existing traditional pedestrian re-recognition methods have been widely applied to address practical problems, they have deficiencies such as low recognition accuracy, inefficient computation, and difficulty to adapt to specific applications. In recent years, the pedestrian re-recognition algorithms based on deep learning have been widely used in the pedestrian re-recognition field because of their strong adaptive ability and high recognition accuracy. The deep learning models provide a technical approach for pedestrian re-recognition tasks with their powerful learning ability. However, the pedestrian re-recognition method based on deep learning also has the following problems: First, the existing deep learning pedestrian re-recognition methods lack memory and prediction mechanisms, and the deep learning methods offer only limited improvement to pedestrian re-recognition accuracy. Second, they exhibit overfitting problems. Finally, initializing the existing LSTM parameters is problematic. In view of this, this paper introduces a revertive connection into the pedestrian re-recognition detector, making it more similar to the human cognitive process by converting a single image into an image sequence; then, the memory image sequence pattern reidentifies the pedestrian image. This approach endows deep learning-based pedestrian re-recognition algorithms with the ability to memorize image sequence patterns and allows them to reidentify pedestrians in images. At the same time, this paper proposes a selective dropout method for shallow learning. Selective dropout uses the classifier obtained through shallow learning to modify the probability that a node weight in the hidden layer is set to 0, thereby eliminating the overfitting phenomenon of the deep learning model. Therefore, this paper also proposes a greedy layer-by-layer pretraining algorithm for initializing LSTM and obtains better generalization performance. Based on the above explanation, this paper proposes a pedestrian re-recognition algorithm based on an optimized LSTM deep learning-sequence memory learning model. Experiments show that the pedestrian re-recognition method proposed in this paper not only has strong self-adaptive ability but also identifies the average accuracy. The proposed method also demonstrates a significant improvement compared with other mainstream methods because it can better memorize and learn the continuous motion of pedestrians and effectively avoid overfitting and parameter initialization in the deep learning model. This proposal provides a technical method and approach for adaptive pedestrian re-recognition algorithms.",2019,Complex.,,10.1155/2019/5069026,https://pdfs.semanticscholar.org/6cde/0ff1661a165dc67510482fd9a97619577992.pdf
9a6541dfc22e9cce7376c168631018258fe82da3,1,0,0,Deep Reinforced Attention Learning for Quality-Aware Visual Recognition,"In this paper, we build upon the weakly-supervised generation mechanism of intermediate attention maps in any convolutional neural networks and disclose the effectiveness of attention modules more straightforwardly to fully exploit their potential. Given an existing neural network equipped with arbitrary attention modules, we introduce a meta critic network to evaluate the quality of attention maps in the main network. Due to the discreteness of our designed reward, the proposed learning method is arranged in a reinforcement learning setting, where the attention actors and recurrent critics are alternately optimized to provide instant critique and revision for the temporary attention representation, hence coined as Deep REinforced Attention Learning (DREAL). It could be applied universally to network architectures with different types of attention modules and promotes their expressive ability by maximizing the relative gain of the final recognition performance arising from each individual attention module, as demonstrated by extensive experiments on both category and instance recognition benchmarks.",2020,ECCV,2007.06156,10.1007/978-3-030-58517-4_29,https://arxiv.org/pdf/2007.06156.pdf
9a65d009b54726a48db9b9ff5071dfedfe11479e,1,1,0,Learning deep features for online person tracking using non-overlapping cameras: A survey,"Abstract Target-agnostic person tracking and re-identification across multiple non-overlapping cameras is an open vision problem. It is the task of maintaining the correct identity of people at different time instances and possibly different cameras. This study focuses on existing algorithms that facilitate online person tracking by using discriminative spatio-temporal features from video data, and presents the open issues and future research directions. The initial take on the problem introduces person tracking as a pure association problem, where the influence of human appearance, biometric and location information on re-identification are addressed explicitly. These constraints are modeled and used to understand and associate detections in real world environments. Next, a spatio-temporal model using LSTM networks for propagating associations and recovering from errors by taking advantage of the spatial and temporal information in videos is described. The spatio-temporal context indicates a way for discriminative appearance learning. The novelty of the mentioned approaches is that they do not require to learn target-specific appearance models and collect samples to distinguish different people from each other. The methods are evaluated on large-scale tracking datasets. State-of-the-art performance is achieved using motion metadata such as person bounding box and camera number, and shows better associations for the challenging exit-entry cases.",2019,Image Vis. Comput.,,10.1016/J.IMAVIS.2019.07.007,
9a781204aa07acc27bbfacb0b181986ad7745caf,1,0,0,Estimating 3D Camera Pose from 2D Pedestrian Trajectories,"We consider the task of re-calibrating the 3D pose of a static surveillance camera, whose pose may change due to external forces, such as birds, wind, falling objects or earthquakes. Conventionally, camera pose estimation can be solved with a PnP (Perspective-n-Point) method using 2Dto-3D feature correspondences, when 3D points are known. However, 3D point annotations are not always available or practical to obtain in real-world applications. We propose an alternative strategy for extracting 3D information to solve for camera pose by using pedestrian trajectories. We observe that 2D pedestrian trajectories indirectly contain useful 3D information that can be used for inferring camera pose. To leverage this information, we propose a data-driven approach by training a neural network (NN) regressor to model a direct mapping from 2D pedestrian trajectories projected on the image plane to 3D camera pose. We demonstrate that our regressor trained only on synthetic data can be directly applied to real data, thus eliminating the need to label any real data. We evaluate our method across six different scenes from the Town Centre Street and DUKEMTMC datasets. Our method achieves an average location error of 0.22m and orientation error of 1.97◦.",2019,,,,https://pdfs.semanticscholar.org/9a78/1204aa07acc27bbfacb0b181986ad7745caf.pdf
9a81f46fcf8c6c0efbe34649552b5056ce419a3d,0,1,0,Deep person re-identification with improved embedding and efficient training,"Person re-identification task has been greatly boosted by deep convolutional neural networks (CNNs) in recent years. The core of which is to enlarge the inter-class distinction as well as reduce the intra-class variance. However, to achieve this, existing deep models prefer to adopt image pairs or triplets to form verification loss, which is inefficient and unstable since the number of training pairs or triplets grows rapidly as the number of training data grows. Moreover, their performance is limited since they ignore the fact that different dimension of embedding may play different importance. In this paper, we propose to employ identification loss with center loss to train a deep model for person re-identification. The training process is efficient since it does not require image pairs or triplets for training while the inter-class distinction and intra-class variance are well handled. To boost the performance, a new feature reweighting (FRW) layer is designed to explicitly emphasize the importance of each embedding dimension, thus leading to an improved embedding. Experiments 1 on several benchmark datasets have shown the superiority of our method over the state-of-the-art alternatives on both accuracy and speed.",2017,2017 IEEE International Joint Conference on Biometrics (IJCB),1705.03332,10.1109/BTAS.2017.8272706,https://arxiv.org/pdf/1705.03332.pdf
9ad5f32b98c8b8d25813998d1ba7dcae2fde2b62,0,1,0,An Efficient Person Re-Identification Model Based on New Regularization Technique,"The aim of person re-identification (ReID) is to recognize the same persons across different scenes. Due to the many demanding applications that utilize large-scale data, more and more attention has been devoted to matching efficiency and accuracy. Many methods that are based on binary coding have been presented to reach efficient ReID. Those methods learn projections to map the high-dimensional features into deep neural networks or compact binary codes through simple insertion of an extra fully connected layer with tanh-like activation. Nevertheless, the former approach needs hand-crafted feature extraction that also wastes a lot of time and complex (discrete) optimizations. In contrast, the latter approach lacks the essential discriminative information to a large extent because of the straightforward activation functions. A ReID framework is proposed in the current work, and it is inspired by the adversarial framework depending on the new regularization approach (ABC-NReg). We embedded the discriminative network into adversarial binary coding (ABC) with our new regularization, which improved the discriminative power combined with the triplet network. ABC-NReg and triplet networks were optimized, and three large-scale benchmark datasets, namely CUHK03, Market-1501, and DukeMTMC-reID datasets, were utilized to test the performance of our proposed model. We further compared the simulation results with the present hashing and non-hashing algorithms. Our model provided better results than other present models using the Market-1501 and DukeMTMC-reID datasets when considering Rank-1. For CUHK03 dataset, the proposed model exceeded the performance of other works when considering Rank 5 and Rank 20.",2020,IEEE Access,,10.1109/ACCESS.2020.3024120,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195809.pdf
9bc62e06f414e239f461ee0ee9318e5bbfe2e33a,0,1,0,From person to group re-identification via unsupervised transfer of sparse features,"Abstract The visual association of a person appearing in the field of view of different cameras is today well known as Person Re-Identification. Current approaches find a solution to such a problem by considering persons as individuals, hence avoiding the fact that frequently they form groups or move in crowds. In such cases, the information acquired by neighboring individuals can provide relevant visual context to boost the performance in re-identifying persons within the group. In light of enriched information, groups re-identification encompasses additional problems to the common person re-identification ones, such as severe occlusions and changes in the relative position of people within the group. In this paper, the single person re-identification knowledge is transferred by means of a sparse dictionary learning to group re-identification. First, patches extracted from single person images are used to learn a dictionary of sparse atoms. This is used to obtain a sparsity-driven residual group representation that is exploited to perform group re-identification. To evaluate the performance of the proposed approach, we considered the i-LIDS groups dataset that is the only group re-identification publicly available dataset. The benchmark datasets for single person re-identification evaluation do not include group information, hence we collected two additional datasets under challenging scenarios and used them to validate our solution.",2019,Image Vis. Comput.,,10.1016/J.IMAVIS.2019.02.009,
9bcb9461da31910ad19218fab2e0786c16ea8054,0,1,0,Improving Fine-Grained Object Classification Using Adversarial Generated Unlabelled Samples,"Recently Convolutional Neural Networks(CNNs) models have achieved remarkable results for fine-grained image classification. However, CNNs require a large amount of training data during supervised learning and labeling so much data is expensive in many cases. To address this issue, this paper innovatively presents a semi-supervised pipeline to improve fine-grained classification tasks without any extra data. We carefully combine CNNs with Generative Adversarial Nets(GANs) for classification, which shows that the result is affirmative. In addition, we propose a multi dimension label regularization(MDLR) method to train labeled images and unlabeled images simultaneously. First we use a pre-trained Yolo v2 object detection model to detect coarse-grained object on the original dataset. Second we feed cropped images to the generator of GAN to produce more generated data and assign a uniform label distribution to the generated images. Third we mix these origin real images and generated images. Then these mixed images are fed to a baseline CNN classifier and a feature-fused CNN classifier. We obtain competitive or state-of-the-art results: using feature-fused CNN model on Stanford Dogs dataset we set a new state-of-the-art result of 90.7%; on Oxford 102 Flowers dataset, we show consistent improvements over baseline.",2018,2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM),,10.1109/BigMM.2018.8499075,
9bd0e9736478c15585fbc7eab6c597a917627a98,0,1,0,Dynamic Graph Co-Matching for Unsupervised Video-Based Person Re-Identification,"Cross-camera label estimation from a set of unlabeled training data is an extremely important component in the unsupervised person re-identification (re-ID) systems. With the estimated labels, the existing advanced supervised learning methods can be leveraged to learn discriminative re-ID models. In this paper, we utilize the graph matching technique for accurate label estimation due to its advantages in optimal global matching and intra-camera relationship mining. However, the graph structure constructed with non-learned similarity measurement cannot handle the large cross-camera variations, which leads to noisy and inaccurate label outputs. This paper designs a dynamic graph matching (DGM) framework, which improves the label estimation process by iteratively refining the graph structure with better similarity measurement learned from the intermediate estimated labels. In addition, we design a positive re-weighting strategy to refine the intermediate labels, which enhances the robustness against inaccurate matching output and noisy initial training data. To fully utilize the abundant video information and reduce false matchings, a co-matching strategy is further incorporated into the framework. Comprehensive experiments conducted on three video benchmarks demonstrate that DGM outperforms the state-of-the-art unsupervised re-ID methods and yields the competitive performance to fully supervised upper bounds.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2893066,
9be285ae8994b68868b54c16ab355e93adce41ad,1,1,1,CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification,"Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.09254,10.1109/CVPR.2019.00900,https://arxiv.org/pdf/1903.09254.pdf
9c2a70f27b661a87f6aed1db698c47d6b99e6f52,0,1,0,Prediction and Recovery for Adaptive Low-Resolution Person Re-Identification,"Low-resolution person re-identification (LR re-id) is a challenging task with low-resolution probes and high-resolution gallery images. To address the resolution mismatch, existing methods typically recover missing details for low-resolution probes by super-resolution. However, they usually pre-specify fixed scale factors for all images, and ignore the fact that choosing a preferable scale factor for certain image content probably greatly benefits the identification. In this paper, we propose a novel Prediction, Recovery and Identification (PRI) model for LR re-id, which adaptively recovers missing details by predicting a preferable scale factor based on the image content. To deal with the lack of ground-truth optimal scale factors, our model contains a self-supervised scale factor metric that automatically generates dynamic soft labels. The generated labels indicate probabilities that each scale factor is optimal, which are used as guidance to enhance the content-aware scale factor prediction. Consequently, our model can more accurately predict and recover the content-aware details, and achieve state-of-the-art performances on four LR re-id datasets.",2020,ECCV,,10.1007/978-3-030-58574-7_12,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710188.pdf
9c364f90dbd09e5366c2d492cd12f989bf8588e6,0,1,0,Style Normalization and Restitution for Generalizable Person Re-Identification,"Existing fully-supervised person re-identification (ReID) methods usually suffer from poor generalization capability caused by domain gaps. The key to solving this problem lies in filtering out identity-irrelevant interference and learning domain-invariant person representations. In this paper, we aim to design a generalizable person ReID framework which trains a model on source domains yet is able to generalize/perform well on target domains. To achieve this goal, we propose a simple yet effective Style Normalization and Restitution (SNR) module. Specifically, we filter out style variations (e.g., illumination, color contrast) by Instance Normalization (IN). However, such a process inevitably removes discriminative information. We propose to distill identity-relevant feature from the removed information and restitute it to the network to ensure high discrimination. For better disentanglement, we enforce a dual causal loss constraint in SNR to encourage the separation of identity-relevant features and identity-irrelevant features. Extensive experiments demonstrate the strong generalization capability of our framework. Our models empowered by the SNR modules significantly outperform the state-of-the-art domain generalization approaches on multiple widely-used person ReID benchmarks, and also show superiority on unsupervised domain adaptation.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2005.11037,10.1109/cvpr42600.2020.00321,https://arxiv.org/pdf/2005.11037.pdf
9c5856b90650454a51de763b53abfc7fdbdacffc,0,1,0,Fairest of Them All: Establishing a Strong Baseline for Cross-Domain Person ReID,"Person re-identification (ReID) remains a very difficult challenge in computer vision, and critical for large-scale video surveillance scenarios where an individual could appear in different camera views at different times. There has been recent interest in tackling this challenge using cross-domain approaches, which leverages data from source domains that are different than the target domain. Such approaches are more practical for real-world widespread deployment given that they don't require on-site training (as with unsupervised or domain transfer approaches) or on-site manual annotation and training (as with supervised approaches). In this study, we take a systematic approach to establishing a large baseline source domain and target domain for cross-domain person ReID. We accomplish this by conducting a comprehensive analysis to study the similarities between source domains proposed in literature, and studying the effects of incrementally increasing the size of the source domain. This allows us to establish a balanced source domain and target domain split that promotes variety in both source and target domains. Furthermore, using lessons learned from the state-of-the-art supervised person re-identification methods, we establish a strong baseline method for cross-domain person ReID. Experiments show that a source domain composed of two of the largest person ReID domains (SYSU and MSMT) performs well across six commonly-used target domains. Furthermore, we show that, surprisingly, two of the recent commonly-used domains (PRID and GRID) have too few query images to provide meaningful insights. As such, based on our findings, we propose the following balanced baseline for cross-domain person ReID consisting of: i) a fixed multi-source domain consisting of SYSU, MSMT, Airport and 3DPeS, and ii) a multi-target domain consisting of Market-1501, DukeMTMC-reID, CUHK03, PRID, GRID and VIPeR.",2019,ArXiv,1907.12016,,https://arxiv.org/pdf/1907.12016.pdf
9ce12c9f1d1661f56908edc8ef3848e91b24d557,1,1,0,Query Adaptive Late Fusion for Image Retrieval,"Feature fusion is a commonly used strategy in image retrieval tasks, which aggregates the matching responses of multiple visual features. Feasible sets of features can be either descriptors (SIFT, HSV) for an entire image or the same descriptor for different local parts (face, body). Ideally, the to-be-fused heterogeneous features are pre-assumed to be discriminative and complementary to each other. However, the effectiveness of different features varies dramatically according to different queries. That is to say, for some queries, a feature may be neither discriminative nor complementary to existing ones, while for other queries, the feature suffices. As a result, it is important to estimate the effectiveness of features in a query-adaptive manner. To this end, this article proposes a new late fusion scheme at the score level. We base our method on the observation that the sorted score curves contain patterns that describe their effectiveness. For example, an ""L""-shaped curve indicates that the feature is discriminative while a gradually descending curve suggests a bad feature. As such, this paper introduces a query-adaptive late fusion pipeline. In the hand-crafted version, it can be an unsupervised approach to tasks like particular object retrieval. In the learning version, it can also be applied to supervised tasks like person recognition and pedestrian retrieval, based on a trainable neural module. Extensive experiments are conducted on two object retrieval datasets and one person recognition dataset. We show that our method is able to highlight the good features and suppress the bad ones, is resilient to distractor features, and achieves very competitive retrieval accuracy compared with the state of the art. In an additional person re-identification dataset, the application scope and limitation of the proposed method are studied.",2018,ArXiv,1810.13103,,https://arxiv.org/pdf/1810.13103.pdf
9cf60f1d6a544339a30e9fbbdad185826e01b2bd,1,0,1,Person Re-identification in Videos by Analyzing Spatio-temporal Tubes,"Typical person re-identification frameworks search for k best matches in a gallery of images that are often collected in varying conditions. The gallery usually contains image sequences for video re-identification applications. However, such a process is time consuming as video re-identification involves carrying out the matching process multiple times. In this paper, we propose a new method that extracts spatio-temporal frame sequences or tubes of moving persons and performs the re-identification in quick time. Initially, we apply a binary classifier to remove noisy images from the input query tube. In the next step, we use a key-pose detection-based query minimization technique. Finally, a hierarchical re-identification framework is proposed and used to rank the output tubes. Experiments with publicly available video re-identification datasets reveal that our framework is better than existing methods. It ranks the tubes with an average increase in the CMC accuracy of 6-8% across multiple datasets. Also, our method significantly reduces the number of false positives. A new video re-identification dataset, named Tube-based Re-identification Video Dataset (TRiViD), has been prepared with an aim to help the re-identification research community.",2020,Multimedia Tools and Applications,1902.04856,10.1007/s11042-020-09096-x,https://link.springer.com/content/pdf/10.1007/s11042-020-09096-x.pdf
9cff8318a1d3816e559ab286d09b4a49efb8e1c8,1,1,1,AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-Identification,"Domain adaptive person re-identification (re-ID) is a challenging task, especially when person identities in target domains are unknown. Existing methods attempt to address this challenge by transferring image styles or aligning feature distributions across domains, whereas the rich unlabeled samples in target domains are not sufficiently exploited. This paper presents a novel augmented discriminative clustering (AD-Cluster) technique that estimates and augments person clusters in target domains and enforces the discrimination ability of re-ID models with the augmented clusters. AD-Cluster is trained by iterative density-based clustering, adaptive sample augmentation, and discriminative feature learning. It learns an image generator and a feature encoder which aim to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space in an adversarial min-max manner. Finally, AD-Cluster increases the diversity of sample clusters and improves the discrimination capability of re-ID models greatly. Extensive experiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms the state-of-the-art with large margins.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.08787,10.1109/cvpr42600.2020.00904,https://arxiv.org/pdf/2004.08787.pdf
9d117f1f1c3a787a61667573bcc94671a767d1c4,0,1,0,Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification,"Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.06052,10.1109/ICCV.2019.00818,https://arxiv.org/pdf/1908.06052.pdf
9d14eedd992081c5011d43747b80535629f68372,1,0,0,Prime-Aware Adaptive Distillation,"Knowledge distillation(KD) aims to improve the performance of a student network by mimicing the knowledge from a powerful teacher network. Existing methods focus on studying what knowledge should be transferred and treat all samples equally during training. This paper introduces the adaptive sample weighting to KD. We discover that previous effective hard mining methods are not appropriate for distillation. Furthermore, we propose Prime-Aware Adaptive Distillation (PAD) by the incorporation of uncertainty learning. PAD perceives the prime samples in distillation and then emphasizes their effect adaptively. PAD is fundamentally different from and would refine existing methods with the innovative view of unequal training. For this reason, PAD is versatile and has been applied in various tasks including classification, metric learning, and object detection. With ten teacher-student combinations on six datasets, PAD promotes the performance of existing distillation methods and outperforms recent state-of-the-art methods.",2020,ArXiv,2008.01458,,https://arxiv.org/pdf/2008.01458.pdf
9d1f591122320f46a037deb30b1285503fe03325,1,1,0,Hierarchical Clustering-guided re-ID with Triplet loss,"For clustering-guided fully unsupervised person reidentification (re-ID) methods, the quality of pseudo labels generated by clustering directly decides the model performance. In order to improve the quality of pseudo labels in existing methods, we propose the HCT method which combines hierarchical clustering with hard-batch triplet loss. The key idea of HCT is to make full use of the similarity among samples in the target dataset through hierarchical clustering, reduce the influence of hard examples through hard-batch triplet loss, so as to generate high quality pseudo labels and improve model performance. Specifically, (1) we use hierarchical clustering to generate pseudo labels, (2) we use PK sampling in each iteration to generate a new dataset for training, (3) we conduct training with hard-batch triplet loss and evaluate model performance in each iteration. We evaluate our model on Market-1501 and DukeMTMC-reID. Results show that HCT achieves 56.4% mAP on Market-1501 and 50.7% mAP on DukeMTMC-reID which surpasses state-of-the-arts a lot in fully unsupervised re-ID and even better than most unsupervised domain adaptation (UDA) methods which use the labeled source dataset. Code will be released soon on https://github.com/zengkaiwei/HCT",2019,ArXiv,,,
9d5a5517650d5f9a7d9818bcc1eb59ba65d316e1,0,1,0,Conditional Generative Adversarial Network for Structured Domain Adaptation,"In recent years, deep neural nets have triumphed over many computer vision problems, including semantic segmentation, which is a critical task in emerging autonomous driving and medical image diagnostics applications. In general, training deep neural nets requires a humongous amount of labeled data, which is laborious and costly to collect and annotate. Recent advances in computer graphics shed light on utilizing photo-realistic synthetic data with computer generated annotations to train neural nets. Nevertheless, the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e., integrating GAN into the FCN framework to mitigate the gap between source and target domains. Specifically, we learn a conditional generator to transform features of synthetic images to real-image like features, and a discriminator to distinguish them. For each training batch, the conditional generator and the discriminator compete against each other so that the generator learns to produce real-image like features to fool the discriminator; afterwards, the FCN parameters are updated to accommodate the changes of GAN. In experiments, without using labels of real image data, our method significantly outperforms the baselines as well as state-of-the-art methods by 12% ~ 20% mean IoU on the Cityscapes dataset.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00145,http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf
9d7b2b4cdb2e312a2989a5a13e64fd15dade8c5e,1,0,0,Lifted Disjoint Paths with Application in Multiple Object Tracking,"We present an extension to the disjoint paths problem in which additional \emph{lifted} edges are introduced to provide path connectivity priors. We call the resulting optimization problem the lifted disjoint paths problem. We show that this problem is NP-hard by reduction from integer multicommodity flow and 3-SAT. To enable practical global optimization, we propose several classes of linear inequalities that produce a high-quality LP-relaxation. Additionally, we propose efficient cutting plane algorithms for separating the proposed linear inequalities. The lifted disjoint path problem is a natural model for multiple object tracking and allows an elegant mathematical formulation for long range temporal interactions. Lifted edges help to prevent id switches and to re-identify persons. Our lifted disjoint paths tracker achieves nearly optimal assignments with respect to input detections. As a consequence, it leads on all three main benchmarks of the MOT challenge, improving significantly over state-of-the-art.",2020,ICML 2020,2006.1455,,https://arxiv.org/pdf/2006.14550.pdf
9d995edfdc9f95ec9c732c3305192ab34f0ba825,1,0,0,Removing nuisance in tracklet data,"In this article, the problem of the lack of robustness and reliability of surveillance systems through disturbing security irrelevant events such as tree shaking, birds flying, etc. is tackled. A novel scene analysis approach based on hypergraph-based trajectories is introduced for reducing the rate of false positives. The conception of hypergraph-based trajectories relaxes the notion of point-based trajectories by allowing multiple incidences between subsequent points in time. This allows a principled approach for the extraction of robust features based on bounding boxes resulting from existing 3rd party detection methods. The experimental part is based on data collected from single-view camera systems over a two-year non-stop recording in the frame of the Austrian KIRAS project SKIN1 on protecting critical infrastructure. The results show substantial reduction of irrelevant false alarms, hence improving the overall system’s performance.",2018,Security + Defence,,10.1117/12.2325636,http://www.flll.uni-linz.ac.at/sites/default/files/AdvKBT-W1718-2.pdf
9daae6a28b2cd86f006d3cc87609779d541b23c2,1,0,0,AI City Challenge 2019 - City-Scale Video Analytics for Smart Transportation,"Understanding large-scale video traffic big data is the new frontier of today’s AI smart transportation advancement. The AI City Challenge 2019 is the third sequel of a yearly event that draws significantly growing attention and participation. This paper presents works contributed to the three Challenges Tracks. In Track 1 CityScale Multi-Camera Vehicle Tracking, we developed a new multi-camera fusion method by extending the state-of-theart single-camera tracking-by-detection with site calibrations. Our approach jointly optimizes the matching of vehicle image features and geometrical factors including trajectory continuity, vehicle moving directions and travel duration across views, to effectively fuse tracks and identify vehicles across 40+ cameras in a city-wide scale. In Track 2 City-Scale Multi-Camera Vehicle Re-Identification, we propose a Pyramid Granularity Attentive Model (PGAM) for ReID by improving the recent Region-Aware deep Model (RAM) with a pyramid design and training strategy improvements. In Track 3 Traffic Anomaly Detection, we improved the 2nd-best method from AIC2018 with refined event recognizers of stalled vehicles with back-tracking to accurately locate event occurrence. The proposed methods achieve compelling performance in the leaderboard among 80+ world-wide participant teams.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/9daa/e6a28b2cd86f006d3cc87609779d541b23c2.pdf
9db472ccc6367940bfca9eb68a9e0c6beb93fb15,1,1,0,Discriminant Feature Learning with Self-attention for Person Re-identification,"Person re-identification (re-ID) across cameras is a crucial task, especially when cameras’ fields of views are non-overlapping. Feature extraction is challenging due to changing illumination conditions, complex background clutters, various camera viewing angles, and occlusions in this case. Moreover, the space mis-alignment of human corresponding regions caused by detectors is a big issue for feature matching across views. In this paper, we propose a strategy of merging attention models with the resnet-50 network for robust feature learning. The efficient self-attention model is used directly on the feature map to solve the space mis-alignment and local feature dependency problems. Furthermore, the loss function which jointly considers the cross-entropy loss and the triplet loss in training enables the network to capture both invariant features within the same individual and distinctive features between different people. Extensive experiments show that our proposed mechanism outperforms the state-of-the-art approaches on the large-scale datasets Market-1501 and DukeMTMC-reID.",2019,ICONIP,,10.1007/978-3-030-36802-9_2,
9e44cfd8832acc2c1d7753733bb92b716bcdce2d,1,1,0,Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification,"Person Re-Identification is a challenging task that aims to retrieve all instances of a query image across a system of non-overlapping cameras. Due to the various extreme changes of view, it is common that local regions that could be used to match people are suppressed, which leads to a scenario where approaches have to evaluate the similarity of images based on less informative regions. In this work, we introduce the Top-DB-Net, a method based on Top DropBlock that pushes the network to learn to focus on the scene foreground, with special emphasis on the most task-relevant regions and, at the same time, encodes low informative regions to provide high discriminability. The Top-DB-Net is composed of three streams: (i) a global stream encodes rich image information from a backbone, (ii) the Top DropBlock stream encourages the backbone to encode low informative regions with high discriminative features, and (iii) a regularization stream helps to deal with the noise created by the dropping process of the second stream, when testing the first two streams are used. Vast experiments on three challenging datasets show the capabilities of our approach against state-of-the-art methods. Qualitative results demonstrate that our method exhibits better activation maps focusing on reliable parts of the input images.",2020,ArXiv,2010.05435,,https://arxiv.org/pdf/2010.05435.pdf
9e99f02d153728a8bcad2dbe8f60dad79a457154,0,1,0,Surpassing Real-World Source Training Data: Random 3D Characters for Generalizable Person Re-Identification,"Person re-identification has seen significant advancement in recent years. However, the ability of learned models to generalize to unknown target domains still remains limited. One possible reason for this is the lack of large-scale and diverse source training data, since manually labeling such a dataset is very expensive and privacy sensitive. To address this, we propose to automatically synthesize a large-scale person re-identification dataset following a set-up similar to real surveillance but with virtual environments, and then use the synthesized person images to train a generalizable person re-identification model. Specifically, we design a method to generate a large number of random UV texture maps and use them to create different 3D clothing models. Then, an automatic code is developed to randomly generate various different 3D characters with diverse clothes, races and attributes. Next, we simulate a number of different virtual environments using Unity3D, with customized camera networks similar to real surveillance systems, and import multiple 3D characters at the same time, with various movements and interactions along different paths through the camera networks. As a result, we obtain a virtual dataset, called RandPerson, with 1,801,816 person images of 8,000 identities. By training person re-identification models on these synthesized person images, we demonstrate, for the first time, that models trained on virtual data can generalize well to unseen target images, surpassing the models trained on various real-world datasets, including CUHK03, Market-1501, DukeMTMC-reID, and almost MSMT17. The RandPerson dataset is available at https://github.com/VideoObjectSearch/RandPerson.",2020,ACM Multimedia,2006.12774,10.1145/3394171.3413815,https://arxiv.org/pdf/2006.12774.pdf
9ea66b4fba63176e52247f8729d81bdc5718c6b8,1,0,0,An Attention-Driven Two-Stage Clustering Method for Unsupervised Person Re-identification,"The progressive clustering method and its variants, which iteratively generate pseudo labels for unlabeled data and per form feature learning, have shown great process in unsupervised person re-identification (re-id). However, they have an intrinsic problem of modeling the incamera variability of images successfully, that is, pedestrian features extracted from the same camera tend to be clustered into the same class. This often results in a non-convergent model in the real world application of clustering based re-id models, leading to degenerated performance. In the present study, we propose an attention-driven two-stage clustering (ADTC) method to solve this problem. Specifically, our method consists of two strategies. Firstly, we use an unsupervised attention kernel to shift the learned features from the image background to the pedestrian foreground, which results in more informative clusters. Secondly, to aid the learning of the attention driven clustering model, we separate the clustering process into two stages. We first use kmeans to generate the centroids of clusters (stage 1) and then apply the k-reciprocal Jaccard distance (KRJD) metric to re-assign data points to each cluster (stage 2). By iteratively learning with the two strategies, the attentive regions are gradually shifted from the background to the foreground and the features become more discriminative. Using two benchmark datasets Market1501 and DukeMTMC, we demonstrate that our model outperforms other state-of-the-art unsupervised approaches for person re-id.",2020,ECCV,,10.1007/978-3-030-58604-1_2,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730018.pdf
9f0251cc11c2be6216560646d9155bed6134e7e8,0,1,0,Metric Attack and Defense for Person Re-identification,"Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance.  Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Meanwhile, we also present an early attempt of training a metric-preserving network, thereby defending the metric against adversarial attacks. At last, by benchmarking various adversarial settings, we expect that our work can facilitate the development of adversarial attack and defense in metric-based applications.",2019,,1901.1065,,https://arxiv.org/pdf/1901.10650.pdf
9f69382b0112b5467b8118eafaade5d30720c3b2,1,0,0,Deep Affinity Network for Multiple Object Tracking,"Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis and computer vision. Most MOT methods employ two steps: Object Detection and Data Association. The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks. Object detection has made tremendous progress in the last few years due to deep learning. However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames. In this paper, we harness the power of deep learning for data association in tracking by jointly modeling object appearances and their affinities between different frames in an end-to-end fashion. The proposed Deep Affinity Network (DAN) learns compact, yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities. DAN also accounts for multiple objects appearing and disappearing between video frames. We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking. Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges. The open source implementation of our work is available at https://github.com/shijieS/SST.git.",2019,IEEE transactions on pattern analysis and machine intelligence,1810.1178,10.1109/TPAMI.2019.2929520,https://arxiv.org/pdf/1810.11780.pdf
9f8c337087bfb7498ec3d6990b9df44ce7ff876d,0,0,1,READ: Reciprocal Attention Discriminator for Image-to-Video Re-identification,"Person re-identification (re-ID) is the problem of visually identifying a person given a database of identities. In this work, we focus on image-to-video re-ID which compares a single query image to videos in the gallery. The main challenge is the asymmetry association of an image and a video, and overcoming the difference caused by the additional temporal dimension. To this end, we propose an attention-aware discriminator architecture. The attention occurs across different modalities, and even different identities to aggregate useful spatio-temporal information for comparison. The information is effectively fused into a united feature, followed by the final prediction of a similarity score. The performance of the method is shown with image-to-video person re-identification benchmarks (DukeMTMC-VideoReID, and MARS).",2020,ECCV,,10.1007/978-3-030-58568-6_20,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590324.pdf
9fb7ed106077615e4d9e86f7be4943c353b879ed,0,1,0,Person Re-Identification Based on Data Prior Distribution,"In order to solve the problem of insufficient accuracy of the existing person re-identification methods. We propose a neural network model for identifying pedestrian properties and pedestrian ID. Compared with the existing methods, the model mainly has the following three advantages. First, our network adds extra full connection layer, ensure model migration ability. Second, based on the number of samples in each attribute, the loss function of each attribute has been normalized, avoid number unbalanced among the attributes to effect the identification accuracy. Third, we use the distribution of the attribute data in the prior knowledge, through the number to adjust the weight of each attribute in the loss layer, avoid the number of data sets for each attribute of positive and negative samples uneven impact on recognition. Experimental results show that the algorithm proposed in this paper has high recognition rate, and the rank-1 accuracy rate on DukeMTMC dataset is 72.83%, especially on Market1501 dataset. The rank-1 accuracy rate is up to 86.90%. Keywordsperson re-identification; data prior distribution; weight adjustment; deep learning; neural network",2018,,,10.2991/ACAAI-18.2018.21,https://download.atlantis-press.com/article/25892468.pdf
9fc5c067559b705c681bf5e344a0b780b4d87412,0,1,0,Unsupervised Domain Adaptation for Facial Expression Recognition Using Generative Adversarial Networks,"In the facial expression recognition task, a good-performing convolutional neural network (CNN) model trained on one dataset (source dataset) usually performs poorly on another dataset (target dataset). This is because the feature distribution of the same emotion varies in different datasets. To improve the cross-dataset accuracy of the CNN model, we introduce an unsupervised domain adaptation method, which is especially suitable for unlabelled small target dataset. In order to solve the problem of lack of samples from the target dataset, we train a generative adversarial network (GAN) on the target dataset and use the GAN generated samples to fine-tune the model pretrained on the source dataset. In the process of fine-tuning, we give the unlabelled GAN generated samples distributed pseudolabels dynamically according to the current prediction probabilities. Our method can be easily applied to any existing convolutional neural networks (CNN). We demonstrate the effectiveness of our method on four facial expression recognition datasets with two CNN structures and obtain inspiring results.",2018,Comput. Intell. Neurosci.,,10.1155/2018/7208794,https://pdfs.semanticscholar.org/9fc5/c067559b705c681bf5e344a0b780b4d87412.pdf
9fdf7df1e95c3ce345d39afd8858ea2fce94c339,1,0,0,Multiple Uses of Global and Local Features for Person Re-identification,"Person re-identification has been extensively studied in recent years and has made great progress. Many papers propose a lot of effective methods to improve the accuracy of the person re-identification. However, there are still many problems that remain unsolved. For example, persons are often occluded by obstacles or other persons, leading to loss of the complete person information, and changes in person behaviors or postures make it difficult to identify. In this paper, we propose a person re-identification algorithm that repeatedly uses global feature information and local feature information for mutual supervised learning. The algorithm consists of two parts, person alignment branch and spatial channel feature branch. First, for person alignment branch, we use global feature information and local feature information to correct misaligned person pictures, and calculate the shortest distance to match the right part of the images. For the spatial channel feature branch, the spatial features are segmented to obtain the local feature information of the person image. At the same time, the spatial feature information is extended using the convolution layer and divided to obtain global feature information of the person image. The global feature information and local feature information are used to calculate the spatial channel feature loss. So that the network can learn better discriminative features through the global information and local information repeatedly. The experimental results show that, on the market-1501 and Duke datasets, the algorithm in this paper obtains good experimental results, has strong robustness, and has greatly improved the rate compared with the existing person re-identification algorithms.",2020,MM 2020,,10.1145/3404716.3404724,
a012b45811eea5f7362d02e4d56cef103b47f45e,1,0,0,WiseNET: An indoor multi-camera multi-space dataset with contextual information and annotations for people detection and tracking,"Nowadays, camera networks are part of our every-day life environments, consequently, they represent a massive source of information for monitoring human activities and to propose new services to the building users. To perform human activity monitoring, people must be detected and the analysis has to be done according to the information relative to the environment and the context. Available multi-camera datasets furnish videos with few (or none) information of the environment where the network was deployed. The proposed dataset provides multi-camera multi-space video sets along with the complete contextual information of the environment. The dataset regroups 11 video sets (composed of 62 single videos) recorded using 6 indoor cameras deployed on multiple spaces. The video sets represent more than 1 h of video footage, include 77 people tracks and captured different human actions such as walking around, standing/sitting, motionless, entering/leaving a space and group merging/splitting. Moreover, each video has been manually and automatically annotated to include people detection and tracking meta-information. The automatic people detection annotations were obtained by using different complexity and robustness detectors, from machine learning to state-of-art deep Convolutional Neural Network (CNN) models. Concerning the contextual information, the Industry Foundation Classes (IFC) file that represents the environment's Building Information Modeling (BIM) data is also provided. The BIM/IFC file describes the complete structure of the environment, it's topology and the elements contained in it. To our knowledge, the WiseNET dataset is the first to provide a set of videos along with the complete information of the environment. The WiseNET dataset is publicly available at https://doi.org/10.4121/uuid:c1fb5962-e939-4c51-bfd5-eac6f2935d44, as well as at the project's website http://wisenet.checksem.fr/#/dataset.",2019,Data in brief,,10.1016/j.dib.2019.104654,
a0367f3773cf5eee8605907b538c684854a62a51,1,0,0,Aggregate Tracklet Appearance Features for Multi-Object Tracking,"Multi-object tracking (MOT) has wide applications in the fields of video analysis and signal processing. A major challenge in MOT is how to associate the noisy detections into long and continuous trajectories. In this letter, we address the association problem at the tracklet-level, and mainly focus on the appearance representation designed for tracklets. A multitask convolutional neural network is proposed to learn the discriminative features and spatial-temporal attentions jointly. In particular, we decompose an object in a static image with spatial attentions, and then aggregate multiple features in a tracklet based on the temporal attentions. Appearance misalignment that caused by occlusion and inaccurate bounding is then mitigated by multi-feature aggregation. Experimental results on two challenging MOT benchmarks have demonstrated the effectiveness of the proposed method and shown significant improvement on the quality of tracking identities.",2019,IEEE Signal Processing Letters,,10.1109/LSP.2019.2940922,
a0a58bdf11c3f07d6faf5a1a33b7ff186d07db96,1,1,1,Progressive Sample Mining and Representation Learning for One-Shot Person Re-identification with Adversarial Samples,"In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabeled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabeled samples to pseudo labelled samples based on the distance matrix, and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). Code will be available soon.",2021,Pattern Recognit.,1911.00666,10.1016/j.patcog.2020.107614,https://arxiv.org/pdf/1911.00666.pdf
a0dfc588cd1bc35a06734a31fca81e7adc94b940,1,1,0,Weighted Bilinear Coding over Salient Body Parts for Person Re-identification,"Deep convolutional neural networks (CNNs) have demonstrated dominant performance in person re-identification (Re-ID). Existing CNN based methods utilize global average pooling (GAP) to aggregate intermediate convolutional features for Re-ID. However, this strategy only considers the first-order statistics of local features and treats local features at different locations equally important, leading to sub-optimal feature representation. To deal with these issues, we propose a novel weighted bilinear coding (WBC) framework for local feature aggregation in CNN networks to pursue more representative and discriminative feature representations, which can adapt to other state-of-the-art methods and improve their performance. In specific, bilinear coding is used to encode the channel-wise feature correlations to capture richer feature interactions. Meanwhile, a weighting scheme is applied on the bilinear coding to adaptively adjust the weights of local features at different locations based on their importance in recognition, further improving the discriminability of feature aggregation. To handle the spatial misalignment issue, we use a salient part net (spatial attention module) to derive salient body parts, and apply the WBC model on each part. The final representation, formed by concatenating the WBC encoded features of each part, is both discriminative and resistant to spatial misalignment. Experiments on three benchmarks including Market-1501, DukeMTMC-reID and CUHK03 evidence the favorable performance of our method against other outstanding methods.",2020,Neurocomputing,1803.0858,10.1016/j.neucom.2020.05.009,https://arxiv.org/pdf/1803.08580.pdf
a0e447a465a8b4729c976214368c172d2b272959,0,1,0,ExGAN: Adversarial Generation of Extreme Samples,"Mitigating the risk arising from extreme events is a fundamental goal with many applications, such as the modelling of natural disasters, financial crashes, epidemics, and many others. To manage this risk, a vital step is to be able to understand or generate a wide range of extreme scenarios. Existing approaches based on Generative Adversarial Networks (GANs) excel at generating realistic samples, but seek to generate typical samples, rather than extreme samples. Hence, in this work, we propose ExGAN, a GAN-based approach to generate realistic and extreme samples. To model the extremes of the training distribution in a principled way, our work draws from Extreme Value Theory (EVT), a probabilistic approach for modelling the extreme tails of distributions. For practical utility, our framework allows the user to specify both the desired extremeness measure, as well as the desired extremeness probability they wish to sample at. Experiments on real US Precipitation data show that our method generates realistic samples, based on visual inspection and quantitative measures, in an efficient manner. Moreover, generating increasingly extreme examples using ExGAN can be done in constant time (with respect to the extremeness probability), as opposed to the exponential time required by the baseline approach.",2020,ArXiv,2009.08454,,https://arxiv.org/pdf/2009.08454.pdf
a1b4c56bdeab97bd9ebd58f35ce2e8e891a77436,1,0,0,Deep Multi-Shot Network for modelling Appearance Similarity in Multi-Person Tracking applications,"The automatization of Multi-Object Tracking becomes a demanding task in real unconstrained scenarios, where the algorithms have to deal with crowds, crossing people, occlusions, disappearances and the presence of visually similar individuals. In those circumstances, the data association between the incoming detections and their corresponding identities could miss some tracks or produce identity switches. In order to reduce these tracking errors, and even their propagation in further frames, this article presents a Deep Multi-Shot neural model for measuring the Degree of Appearance Similarity (MS-DoAS) between person observations. This model provides temporal consistency to the individuals' appearance representation, and provides an affinity metric to perform frame-by-frame data association, allowing online tracking. The model has been deliberately trained to be able to manage the presence of previous identity switches and missed observations in the handled tracks. With that purpose, a novel data generation tool has been designed to create training tracklets that simulate such situations. The model has demonstrated a high capacity to discern when a new observation corresponds to a certain track, achieving a classification accuracy of 97\% in a hard test that simulates tracks with previous mistakes. Moreover, the tracking efficiency of the model in a Surveillance application has been demonstrated by integrating that into the frame-by-frame association of a Tracking-by-Detection algorithm.",2020,ArXiv,2004.03531,,https://arxiv.org/pdf/2004.03531.pdf
a1bbe29ebe5e42211b752c3a2b959cf6020556e6,1,0,0,DeepMOT: A Differentiable Framework for Training Multiple Object Trackers,"Multiple Object Tracking accuracy and precision (MOTA and MOTP) are two standard and widely-used metrics to assess the quality of multiple object trackers. They are specifically designed to encode the challenges and difficulties of tracking multiple objects. To directly optimize a tracker based on MOTA and MOTP is difficult, since both the metrics are strongly rely on the Hungarian algorithm, which are non-differentiable. We propose a differentiable proxy for the MOTA and MOTP, thus allowing to train a deep multiple-object tracker by directly optimizing (a proxy of) the standard MOT metrics. The proposed approximation is based on a bidirectional recurrent network that inputs the object-to-hypothesis distance matrix and outputs the optimal hypothesis-to-object association, thus emulating the Hungarian algorithm. Followed by a differentiable module, the estimated association is used to compute the MOTA and MOTP. The experimental study demonstrates the benefits of this differentiable framework on two recent deep trackers over the MOT17 dataset. Moreover, the code is publicly available from this https URL.",2019,ArXiv,,,
a1e97c4043d5cc9896dc60ae7ca135782d89e5fc,1,0,0,"Re-identification of Humans in Crowds using Personal, Social and Environmental Constraints","This paper addresses the problem of human re-identification across non-overlapping cameras in crowds.Re-identification in crowded scenes is a challenging problem due to large number of people and frequent occlusions, coupled with changes in their appearance due to different properties and exposure of cameras. To solve this problem, we model multiple Personal, Social and Environmental (PSE) constraints on human motion across cameras. The personal constraints include appearance and preferred speed of each individual assumed to be similar across the non-overlapping cameras. The social influences (constraints) are quadratic in nature, i.e. occur between pairs of individuals, and modeled through grouping and collision avoidance. Finally, the environmental constraints capture the transition probabilities between gates (entrances / exits) in different cameras, defined as multi-modal distributions of transition time and destination between all pairs of gates. We incorporate these constraints into an energy minimization framework for solving human re-identification. Assigning $1-1$ correspondence while modeling PSE constraints is NP-hard. We present a stochastic local search algorithm to restrict the search space of hypotheses, and obtain $1-1$ solution in the presence of linear and quadratic PSE constraints. Moreover, we present an alternate optimization using Frank-Wolfe algorithm that solves the convex approximation of the objective function with linear relaxation on binary variables, and yields an order of magnitude speed up over stochastic local search with minor drop in performance. We evaluate our approach using Cumulative Matching Curves as well $1-1$ assignment on several thousand frames of Grand Central, PRID and DukeMTMC datasets, and obtain significantly better results compared to existing re-identification methods.",2016,ArXiv,1612.02155,,https://arxiv.org/pdf/1612.02155.pdf
a208a2accca19efeecd9957a4951c6c592518fc4,1,0,0,Estimating 3D Camera Pose from 2D Pedestrian Trajectories,"We consider the task of re-calibrating the 3D pose of a static surveillance camera, whose pose may change due to external forces, such as birds, wind, falling objects or earthquakes. Conventionally, camera pose estimation can be solved with a PnP (Perspective-n-Point) method using 2D-to-3D feature correspondences, when 3D points are known. However, 3D point annotations are not always available or practical to obtain in real-world applications. We propose an alternative strategy for extracting 3D information to solve for camera pose by using pedestrian trajectories. We observe that 2D pedestrian trajectories indirectly contain useful 3D information that can be used for inferring camera pose. To leverage this information, we propose a data-driven approach by training a neural network (NN) regressor to model a direct mapping from 2D pedestrian trajectories projected on the image plane to 3D camera pose. We demonstrate that our regressor trained only on synthetic data can be directly applied to real data, thus eliminating the need to label any real data. We evaluate our method across six different scenes from the Town Centre Street and DUKEMTMC datasets. Our method achieves an improvement of ~ 50% on both position and orientation prediction accuracy when compared to other SOTA methods.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1912.05758,10.1109/WACV45572.2020.9093539,https://arxiv.org/pdf/1912.05758.pdf
a20f132a30e99541aa7ba6dddac86e6a393778e8,1,1,0,Self Attention Grid for Person Re-Identification,"In this paper, we present an attention mechanism scheme to improve person re-identification task. Inspired by biology, we propose Self Attention Grid (SAG) to discover the most informative parts from a high-resolution image using its internal representation. In particular, given an input image, the proposed model is fed with two copies of the same image and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learn a filtering attention grid. We apply a max filter operation to non-overlapping sub-regions on the high feature representation before element-wise multiplied with the output of the second branch. The feature maps of the second branch are subsequently weighted to reflect the importance of each patch of the grid using a softmax operation. Our attention module helps the network learn the most discriminative visual features of multiple image regions and is specifically optimized to attend feature representation at different levels. Extensive experiments on three large-scale datasets show that our self-attention mechanism significantly improves the baseline model and outperforms various state-of-art models by a large margin.",2018,ArXiv,1809.08556,,https://arxiv.org/pdf/1809.08556.pdf
a213bbf9740854a276fdf71dad8f30cfbe3ea4d4,1,0,0,The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking,"With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision, delivering more efficiency and convenience than surveillance cameras with fixed camera angle, scale and view. However, very limited UAV datasets are proposed, and they focus only on a specific task such as visual tracking or object detection in relatively constrained scenarios. Consequently, it is of great importance to develop an unconstrained UAV benchmark to boost related researches. In this paper, we construct a new UAV benchmark focusing on complex scenarios with new level challenges. Selected from 10 hours raw videos, about 80,000 representative frames are fully annotated with bounding boxes as well as up to 14 kinds of attributes (e.g., weather condition, flying altitude, camera view, vehicle category, and occlusion) for three fundamental computer vision tasks: object detection, single object tracking, and multiple object tracking. Then, a detailed quantitative study is performed using most recent state-of-the-art algorithms for each task. Experimental results show that the current state-of-the-art methods perform relative worse on our dataset, due to the new challenges appeared in UAV based real scenes, e.g., high density, small object, and camera motion. To our knowledge, our work is the first time to explore such issues in unconstrained scenes comprehensively.",2018,ECCV,1804.00518,10.1007/978-3-030-01249-6_23,https://arxiv.org/pdf/1804.00518.pdf
a2d631e169205c7e0059d2cb8f65657850878a0f,1,0,0,Global Deep Feature Representation for Person Re-Identification,"Person re-identification (re-ID) has attracted tremendous attention in the field of computer vision, especially in intelligent visual surveillance (IVS). The propose of re-ID is to retrieval the interest person across different cameras. There are still lots of challenges and difficulties that are the same appearance such as clothes, the lens distance, various poses and different shooting angles, all of which influence the performance of re-ID. In this paper, we propose a novel architecture, called global deep convolutional network (GDCN), which applies classical convolutional network as the backbone network and calculates the similarity between query and gallery. We evaluate the proposed GDCN on three large-scale public datasets: Market-1501 by 92.72% in Rank-1 and 88.86% in mAP, CHUK03 by 60.78% in Rank-1 and 62.47% in mAP, DukeMTMC-re-ID by 82.22% in Rank-1 and 77.99% in mAP, respectively. Besides, we compare the experimental results with previous work to verify the state-of-art performance of the proposed method that is implemented by NVIDIA Ge-Force GTX 1080Ti.",2019,CSPS,,10.1007/978-981-13-9409-6_22,
a308063fa36856cbbc0e97e9c1397f265dce81dc,0,1,0,Embedding Adversarial Learning for Vehicle Re-Identification,"The high similarities of different real-world vehicles and great diversities of the acquisition views pose grand challenges to vehicle re-identification (ReID), which traditionally maps the vehicle images into a high-dimensional embedding space for distance optimization, vehicle discrimination, and identification. To improve the discriminative capability and robustness of the ReID algorithm, we propose a novel end-to-end embedding adversarial learning network (EALN) that is capable of generating samples localized in the embedding space. Instead of selecting abundant hard negatives from the training set, which is extremely difficult if not impossible, with our embedding adversarial learning scheme, the automatically generated hard negative samples in the specified embedding space can greatly improve the capability of the network for discriminating similar vehicles. Moreover, the more challenging cross-view vehicle ReID problem, which requires the ReID algorithm to be robust with different query views, can also benefit from such a scheme based on the artificially generated cross-view samples. We demonstrate the promise of EALN through extensive experiments and show the effectiveness of hard negative and cross-view generation in facilitating vehicle ReID based on the comparisons with the state-of-the-art schemes.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2902112,
a325700e1318e22b2ed169660412283a50b05b07,1,0,0,Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification,"Person re-identification (Re-ID) aims at retrieving an input person image from a set of images captured by multiple cameras. Although recent Re-ID methods have made great success, most of them extract features in terms of the attributes of clothing (e.g., color, texture). However, it is common for people to wear black clothes or be captured by surveillance systems in low light illumination, in which cases the attributes of the clothing are severely missing. We call this problem the Black Re-ID problem. To solve this problem, rather than relying on the clothing information, we propose to exploit head-shoulder features to assist person Re-ID. The head-shoulder adaptive attention network (HAA) is proposed to learn the head-shoulder feature and an innovative ensemble method is designed to enhance the generalization of our model. Given the input person image, the ensemble method would focus on the head-shoulder feature by assigning a larger weight if the individual insides the image is in black clothing. Due to the lack of a suitable benchmark dataset for studying the Black Re-ID problem, we also contribute the first Black-reID dataset, which contains 1274 identities in training set. Extensive evaluations on the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model achieves the best result compared with the state-of-the-art Re-ID methods on both Black and conventional Re-ID problems. Furthermore, our method is also proved to be effective in dealing with person Re-ID in similar clothing. Our code and dataset are avaliable on https://github.com/xbq1994/.",2020,ACM Multimedia,2008.08528,10.1145/3394171.3414056,https://arxiv.org/pdf/2008.08528.pdf
a337c0b962fafff56001bb2945607430b39addab,1,1,0,Rethinking Classification Loss Designs for Person Re-identification with a Unified View,"Person Re-identification (ReID) aims at matching a person of interest across images. In convolutional neural networks (CNNs) based approaches, loss design plays a role of metric learning which guides the feature learning process to pull closer features of the same identity and to push far apart features of different identities. In recent years, the combination of classification loss and triplet loss achieves superior performance and is predominant in ReID. In this paper, we rethink these loss functions within a generalized formulation and argue that triplet-based optimization can be viewed as a two-class subsampling classification, which performs classification over two sampled categories based on instance similarities. Furthermore, we present a case study which demonstrates that increasing the number of simultaneously considered instance classes significantly improves the ReID performance, since it is aligned better with the ReID test/inference process. With the multi-class subsampling classification incorporated, we provide a strong baseline which achieves the state-of-the-art performance on the benchmark person ReID datasets. Finally, we propose a new meta prototypical N-tuple loss for more efficient multi-class subsampling classification. We aim to inspire more new loss designs in the person ReID field.",2020,ArXiv,2006.04991,,https://arxiv.org/pdf/2006.04991.pdf
a33eeae54b6dcc6c25ea444d0dfed9a1654ef23c,1,1,0,Non-full multi-layer feature representations for person re-identification,"Person re-identification(Re-ID) has attracted increasing attention in the field of computer vision due to its great significance for the potential real-world applications. Profited from the success of convolutional neural networks(CNNs), existing multi-layer approaches leverage different scales of convolutional layers to learn more discriminative features, improving the Re-ID performance to some extent. However, these methods do not further explore whether all the scales of convolutional layers are positive for person re-identification. In this work, we propose a novel non-full multi-layer(NFML) network, which can jointly learn discriminative feature embeddings from positive multiple layers with the manner of combining global and local cues. Moreover, considering few works focus on how to effectively handle the feature maps, a simple yet effective feature progressing module named Pooling Batch Normalization(PBN), consisting of pooling, reduction and batch normalization operations, is introduced to optimize the model structure and further improve the Re-ID performance. Results on three mainstream benchmark datasets Market-1501, DukeMTMC-reID and CUHK03 demonstrate that our method can significantly boost the performances, outperforming the state-of-the-art methods.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09410-7,
a34f8768b10d928aa4f4105afb971819c26a2219,1,1,0,Multi-Pseudo Regularized Label for Generated Data in Person Re-Identification,"Sufficient training data normally is required to train deeply learned models. However, due to the expensive manual process for a labeling large number of images (i.e., annotation), the amount of available training data (i.e., real data) is always limited. To produce more data for training a deep network, generative adversarial network can be used to generate artificial sample data (i.e., generated data). However, the generated data usually does not have annotation labels. To solve this problem, in this paper, we propose a virtual label called Multi-pseudo Regularized Label (MpRL) and assign it to the generated data. With MpRL, the generated data will be used as the supplementary of real training data to train a deep neural network in a semi-supervised learning fashion. To build the corresponding relationship between the real data and generated data, MpRL assigns each generated data a proper virtual label which reflects the likelihood of the affiliation of the generated data to pre-defined training classes in the real data domain. Unlike the traditional label which usually is a single integral number, the virtual label proposed in this paper is a set of weight-based values each individual of which is a number in (0,1] called multi-pseudo label and reflects the degree of relation between each generated data to every pre-defined class of real data. A comprehensive evaluation is carried out by adopting two state-of-the-art convolutional neural networks (CNNs) in our experiments to verify the effectiveness of MpRL. Experiments demonstrate that by assigning MpRL to generated data, we can further improve the person re-ID performance on five re-ID datasets, i.e., Market-1501, DukeMTMC-reID, CUHK03, VIPeR, and CUHK01. The proposed method obtains +6.29%, +6.30%, +5.58%, +5.84%, and +3.48% improvements in rank-1 accuracy over a strong CNN baseline on the five datasets, respectively, and outperforms state-of-the-art methods.",2019,IEEE Transactions on Image Processing,1801.06742,10.1109/TIP.2018.2874715,https://arxiv.org/pdf/1801.06742.pdf
a377b7e40ce65f13f3443bfa0b00b4b4e3b5f99f,0,1,0,Co-Attentive Lifting for Infrared-Visible Person Re-Identification,"Infrared-visible cross-modality person re-identification (IV-ReID) has attracted much attention with the popularity of dual-mode video surveillance systems, where the RGB mode works in the daytime and automatically switches to the infrared mode at night. Despite its significant application value, IV-ReID remains a difficult problem mainly due to two great challenges. First, it is difficult to identify persons in the infrared image, which lacks color and texture clues. Second, there is a significant gap between the infrared and visible modalities where appearances of the same person vary considerably. This paper proposes a novel attention-based approach to handle the two difficulties in a unified framework. 1) We propose an attention lifting mechanism to learn discriminative features in each modality. 2) We propose a co-attentive learning mechanism to bridge the gap between the two modalities. Our method only makes slight modifications of a given backbone network and requires small computation overhead while improving the performance significantly. We conduct extensive experiments to demonstrate the superiority of our proposed method.",2020,ACM Multimedia,,10.1145/3394171.3413933,
a380f9394801a0def6d6ede428a85c7ba72b2577,0,1,0,A deep person re-identification model with multi visual-semantic information embedding,"The local features of different body parts have been widely used to learn more discriminative representation for person re-identification, which act as either extra visual semantic information or auxiliary means to deal with the issue of misalignment and background bias. However, the existing person re-identification works mainly focuses on the common impact of multiple body parts while failing to explicitly explore the influence of body edge contour. As the edge contour is one of the most significant visual-semantic clues for object detection and person identification in the blurred scene, this paper intentionally explores the effect of edge contour clues on person re-identification and proposes a deep learning framework with multi visual-semantic information embedding, including body parts and edge contour. Meanwhile, we conceive a practical strategy which can effectively fuse the different body part features and reduce the dimensionality of features. Extensive experimental results on four benchmark data sets show that our model has achieved competitive accuracy compared to the state-of-the-art models.",2020,,,10.1007/s11042-020-09957-5,
a3d64cecedb5b85b5bde3504cb5501c92c5eaa3f,1,0,0,Quasi-Dense Instance Similarity Learning,"Similarity metrics for instances have drawn much attention, due to their importance for computer vision problems such as object tracking. However, existing methods regard object similarity learning as a post-hoc stage after object detection and only use sparse ground truth matching as the training objective. This process ignores the majority of the regions on the images. In this paper, we present a simple yet effective quasi-dense matching method to learn instance similarity from hundreds of region proposals in a pair of images. In the resulting feature space, a simple nearest neighbor search can distinguish different instances without bells and whistles. When applied to joint object detection and tracking, our method can outperform existing methods without using location or motion heuristics, yielding almost 10 points higher MOTA on BDD100K and Waymo tracking datasets. Our method is also competitive on one-shot object detection, which further shows the effectiveness of quasi-dense matching for category-level metric learning. The code will be available at this https URL.",2020,ArXiv,2006.06664,,https://arxiv.org/pdf/2006.06664.pdf
a4189f837906cbc8acc84aa5f740222196034ab0,0,1,0,Salience-Guided Cascaded Suppression Network for Person Re-Identification,"Employing attention mechanisms to model both global and local features as a final pedestrian representation has become a trend for person re-identification (Re-ID) algorithms. A potential limitation of these methods is that they focus on the most salient features, but the re-identification of a person may rely on diverse clues masked by the most salient features in different situations, e.g., body, clothes or even shoes. To handle this limitation, we propose a novel Salience-guided Cascaded Suppression Network (SCSN) which enables the model to mine diverse salient features and integrate these features into the final representation by a cascaded manner. Our work makes the following contributions: (i) We observe that the previously learned salient features may hinder the network from learning other important information. To tackle this limitation, we introduce a cascaded suppression strategy, which enables the network to mine diverse potential useful features that be masked by the other salient features stage-by-stage and each stage integrates different feature embedding for the last discriminative pedestrian representation. (ii) We propose a Salient Feature Extraction (SFE) unit, which can suppress the salient features learned in the previous cascaded stage and then adaptively extracts other potential salient feature to obtain different clues of pedestrians. (iii) We develop an efficient feature aggregation strategy that fully increases the network’s capacity for all potential salience features. Finally, experimental results demonstrate that our proposed method outperforms the state-of-the-art methods on four large-scale datasets. Especially, our approach exceeds the current best method by over 7% on the CUHK03 dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00336,https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Salience-Guided_Cascaded_Suppression_Network_for_Person_Re-Identification_CVPR_2020_paper.pdf
a42758b4943c7599925e8c8415ee9b8078ff57ad,0,1,0,In Defense of the Triplet Loss for Person Re-Identification,"In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.",2017,ArXiv,1703.07737,,https://arxiv.org/pdf/1703.07737.pdf
a440f939cc7cb430a4bb0b2d84f8f59793bf3df0,1,0,0,In-depth exploration of attribute information for person re-identification,"Pedestrian’s attribute information plays an important role in person re-identification (re-ID) for its complementary to pedestrian’s identity labels. However, there are few methods to utilize attribute information, which limits the development of re-ID community. In this paper, we analyze the effect of attribute information on re-ID to obtain both qualitative and quantitative results, indicating the potential for in-depth exploration of attribute information. On this basis, we propose an Identity Recognition Network (IRN) and an Attribute Recognition Network (ARN). IRN enhances the attention to pedestrian’s local information while identifying pedestrians’ identity. ARN calculates the attribute similarity among pedestrians accurately to promote the identification of IRN. The combination of them makes deep exploration of attribute information and is easy to implement. The experimental results on two large-scale re-ID benchmarks demonstrate the effectiveness of our method, which is on par with the state-of-the-art. In the DukeMTMC-reID dataset, mAP (rank-1) accuracy is improved from 58.4 (78.3) % to 66.4 (82.7) % for ResNet-50. In the Market1501 dataset, mAP (rank-1) accuracy is improved from 75.8 (90.5) % to 79.5 (92.8) % for ResNet-50.",2020,Applied Intelligence,,10.1007/s10489-020-01752-x,
a44103db15526f07b93f95c3a08cc284a207ea11,1,0,0,Intelligent Querying for Target Tracking in Camera Networks using Deep Q-Learning with n-Step Bootstrapping,"Surveillance camera networks are a useful infrastructure for various visual analytics applications, where high-level inferences and predictions could be made based on target tracking across the network. Most multi-camera tracking works focus on target re-identification and trajectory association problems to track the target. However, since camera networks can generate enormous amount of video data, inefficient schemes for making re-identification or trajectory association queries can incur prohibitively large computational requirements. In this paper, we address the problem of intelligent scheduling of re-identification queries in a multi-camera tracking setting. To this end, we formulate the target tracking problem in a camera network as an MDP and learn a reinforcement learning based policy that selects a camera for making a re-identification query. The proposed approach to camera selection does not assume the knowledge of the camera network topology but the resulting policy implicitly learns it. We have also shown that such a policy can be learnt directly from data. Using the NLPR MCT and the Duke MTMC multi-camera multi-target tracking benchmarks, we empirically show that the proposed approach substantially reduces the number of frames queried.",2020,ArXiv,2004.09632,10.1016/j.imavis.2020.104022,https://arxiv.org/pdf/2004.09632.pdf
a44bc814c38ee76e57449997c49804ee394c1f2c,1,0,0,Learning from Dances: Pose-Invariant Re-Identification for Multi-Person Tracking,"Most existing multi-person tracking approaches rely on appearance based re-identification (re-ID) to resolve fragmented tracklets. However, simply using appearance information could be insufficient for videos containing severe pose changes, such as sports or dance videos. With the goal of learning pose-invariant representations, we propose an end-to-end deep learning framework Sparse-Temporal ReID Network. Our proposed network not only realizes human pose disentanglement in an image recovery manner, but also makes efficient linkages between the identical subjects via a unique Sparse temporal identity sampling technique across time steps. Experimental results demonstrate the effectiveness of our proposed method on both multi-view re-ID benchmarks and our newly collected dance video dataset DanceReID1.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9054086,
a4958d26a3aaf77081fcb9b76393a30bf722e92e,1,1,0,A Pose-Sensitive Embedding for Person Re-identification with Expanded Cross Neighborhood Re-ranking,"Person re-identification is a challenging retrieval task that requires matching a person's acquired image across non-overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets. Code is available at https://github.com/pse-ecn.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.10378,10.1109/CVPR.2018.00051,https://arxiv.org/pdf/1711.10378.pdf
a4ae54757da23ad0551308e5a5a314bc5a9515ff,1,0,1,Global-Local Temporal Representations for Video Person Re-Identification,"This paper proposes the Global-Local Temporal Representation (GLTR) to exploit the multi-scale temporal cues in video sequences for video person Re-Identification (ReID). GLTR is constructed by first modeling the short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. Specifically, the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestrian. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences. The short and long-term temporal cues are aggregated as the final GLTR by a simple single-stream CNN. GLTR shows substantial superiority to existing features learned with body part cues or metric learning on four widely-used video ReID datasets. For instance, it achieves Rank-1 Accuracy of 87.02% on MARS dataset without re-ranking, better than current state-of-the art.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00406,
a502bee13c5eb7338efca474de049d0c98f989d1,0,1,0,Multi-task Learning with Bidirectional Language Models for Text Classification,"Multi-task learning is an effective approach to extract task-invariant features by leveraging potential information among related tasks, which improves the performance of a single task. Most existing work simply divides the whole model into shared and private spaces. Unfortunately, there is no explicit mechanism to prevent the two spaces from merging information from each other. As a result, the shared space may be mixed with task-specific features, while the private space may extract some task-invariant features. To alleviate the problem mentioned, in this paper, we propose a bidirectional language models based multi-task learning method for text classification. More specifically, we add language modelling as an auxiliary task to the private part, aiming to enhance its ability to extract task-specific features. In addition, to promote the shared part to learn common features, a loss constraint via uniform label distribution is introduced to the shared part. Finally, put task-specific features and taskinvariant features together in a weighted addition way to form the final representation, and it is then fed to the corresponding softmax layer. We do experiments on the FDU-MTL dataset which consists of 16 different text classification tasks. The experimental results show that our approach outperforms other typical methods.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852388,
a5418a2fc6713e39a95237eadc004e1e02401ae7,1,1,0,Sparse Label Smoothing Regularization for Person Re-Identification,"Person re-identification (re-id) is a cross-camera retrieval task which establishes a correspondence between images of a person from multiple cameras. Deep learning methods have been successfully applied to this problem and have achieved impressive results. However, these methods require a large amount of labeled training data. Currently, the labeled datasets in person re-id are limited in their scale and manual acquisition of such large-scale datasets from surveillance cameras is a tedious and labor-intensive task. In this paper, we propose a framework that performs intelligent data augmentation and assigns the partial smoothing label to generated data. Our approach first exploits the clustering property of existing person re-id datasets to create groups of similar objects that model cross-view variations. Each group is then used to generate realistic images through adversarial training. Our aim is to emphasize the feature similarity between generated samples and the original samples. Finally, we assign a non-uniform label distribution to the generated samples and define a regularized loss function for training. The proposed approach tackles two problems 1) how to efficiently use the generated data and 2) how to address the over-smoothness problem found in current regularization methods. The extensive experiments on four large-scale datasets show that our regularization method significantly improves the re-id accuracy compared to existing methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2901599,
a54d63c1a8c4db3c5034b1fdb08526459bb3c0b1,0,1,0,Multi-Gait Recognition Based on Attribute Discovery,"Gait recognition is an important topic in biometrics. Current works primarily focus on recognizing a single person's walking gait. However, a person's gait will change when they walk with other people. How to recognize the gait of multiple people walking is still a challenging problem. This paper proposes an attribute discovery model in a max-margin framework to recognize a person based on gait while walking with multiple people. First, human graphlets are integrated into a tracking-by-detection method to obtain a person's complete silhouette. Then, stable and discriminative attributes are developed using a latent conditional random field (L-CRF) model. The model is trained in the latent structural support vector machine (SVM) framework, in which a new constraint is added to improve the multi-gait recognition performance. In the recognition process, the attribute set of each person is detected by inferring on the trained L-CRF model. Finally, attributes based on dense trajectories are extracted as the final gait features to complete the recognition. The experimental results demonstrate that the proposed method achieves better recognition performance than traditional gait recognition methods under the condition of multiple people walking together.",2018,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2017.2726061,
a5736a7f81538145380405a4218dd424614f1cf8,0,1,0,A new patch selection method based on parsing and saliency detection for person re-identification,"Abstract Person re-identification is an important technique towards automatic recognition of a person across non-overlapping cameras. In this paper, a novel patch selection method based on parsing and saliency detection is proposed. The algorithm is divided into two stages. The first stage, primary selection: Deep Decompositional Network (DNN) is adopted to parse a pedestrian image into semantic regions, then sliding window and color matching techniques are proposed to select pedestrian patches and remove background patches. The second stage, secondary selection: saliency detection is utilized to select reliable patches according to saliency map. Finally, PHOG, HSV and SIFT features are extracted from these patches and fused with the global feature LOMO to compensate for the inherent errors of saliency detection. By applying the proposed method on such datasets as VIPeR, PRID2011, CUHK01, CUHK03, PRID 450S and iLIDS-VID, it is found that the proposed descriptor can produce results superior to many state-of-the-art feature representation methods for person identification.",2020,Neurocomputing,,10.1016/j.neucom.2019.09.073,
a59624b5ee9c6aaf5c6caba1f119158190ec4b56,1,1,0,Open Set Recognition for Unique Person Counting via Virtual Gates,"Retail shops or restaurants are interested in real-time profiling analysis of customer visit patterns, which could enable efficient management and target marketing. They need to know not only how many people entered but also if they are visiting for the first time and keep track of their exact number. As a result, in this paper we define the new variant of unique counting for videos, that is counting new persons who have not already been counted in the past. To this end, we propose a complete real-time system which is able to perform detection, tracking and unique counting in the wild with user drawn gates. A fine-tuned network on persons body is used to extract descriptors which are more privacy-oriented. Experiments of the system on the challenging DukeMTMC dataset show that our method is able to effectively count people in real time and discern between the persons which do multiple passages through the gates.",2019,ICIAP,,10.1007/978-3-030-30642-7_9,
a598868f75edeed74f14892a48fdef565da29a37,0,1,0,Semantic visual recognition in a cognitive architecture for social robots,"This work has been funded by the Spanish Government TIN2016-76515-R grant for the COMBAHO project, supported with Feder funds. This work has also been supported by a Spanish grant for PhD studies ACIF/2017/243 and FPU16/00887.",2020,Integr. Comput. Aided Eng.,,10.3233/ica-200624,
a5c5ac803bfc916654f73312daa444faa851ce70,1,0,0,Hard sample mining makes person re-identification more efficient and accurate,"Abstract In recent years, the field of person re-identification has made significant advances riding on the wave of deep learning. However, owing to the fact that there are much more easy examples than those meaningful hard examples in a dataset, the training tends to stagnate quickly and the model may suffer from over-fitting, which leads to some error matching of models especially for some hard samples during the test process. Therefore, the hard sample mining method is fateful to optimize the model and improve the learning efficiency. In this paper, an Adaptive Hard Sample Mining algorithm is proposed for training a robust person re-identification model. No need for hand-picking the images in the batch or designing the loss function for both positive and negative pairs, we can briefly calculate the hard level by comparing the prediction result with the true label of the sample. Meanwhile, taking into account the change in the number of samples required for the model during training process, an adaptive threshold of hard level can make the algorithm not only stay in step with training process harmoniously but also alleviate the under-fitting and over-fitting problem simultaneously. Besides, the designed network to implement the approach is very efficient and has good generalization performance that can be combined with various existing models readily. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 datasets clearly demonstrate the effectiveness of the proposed algorithm.",2020,Neurocomputing,,10.1016/j.neucom.2019.11.094,
a5d85550703c8d8bcfa7444a874c6c8014a2f6ef,1,1,0,Learning Similarity Attention,"We consider the problem of learning similarity functions. While there has been substantial progress in learning suitable distance metrics, these techniques in general lack decision reasoning, i.e., explaining why the input set of images is similar or dissimilar. In this work, we solve this key problem by proposing the first method to generate generic visual similarity explanations with gradient-based attention. We demonstrate that our technique is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models. Finally, we demonstrate the generality of our framework by means of experiments on a variety of tasks, including image retrieval, person re-identification, and low-shot semantic segmentation.",2019,ArXiv,1911.07381,,https://arxiv.org/pdf/1911.07381.pdf
a5db89137e1c6c607ff192ba00d5606c3db58c30,0,1,0,An improved baseline for person re-identification,"Person re-identification(Re-ID) using deep learning has made great progress in the past few years, but there is one problem that many state-of-the-art Re-ID methods all use a complex network most of which use the structure of multi-branch and multi-loss function. At present, the database used for Person re-identification is relatively small. This complex network structure may bring a problem that although current methods may perform well in the small databases, but there may be some problems of overfitting problem, once applied in the bigger dataset or real scene these complex methods may perform not well. So this paper mainly proposes a new powerful baseline network. This end-to-end network only uses a global feature and does not use multi-branch structure, but achieves state-of-the-art level. The key point is that this network has good improvement potential to adapt to larger datasets and even practical application scenarios.",2019,AIPR '19,,10.1145/3357254.3357270,
a6a6211fa3c4f61f8d5941e89561be0c351e8648,0,1,0,Contour-Guided Person Re-identification,"Feature representation is one of the crucial components in person re-identification(re-ID). Recently, local feature has attracted great attention from the re-ID community, and extra visual cues have been well exploited to guide local feature learning, such as pose cues, semantic parsing and etc. Besides, the latest research demonstrates that general CNN-based deep models have a bias to texture feature in pattern recognition, but ignore shape-based feature, which has been verified as significant for cross-domain invariance. As far as we know, there is little work focusing on shape-based feature on person re-ID. In this paper, we introduce a new data modality, pedestrian contour, into the re-ID community, which to our best knowledge is the first attempt to utilize contour explicitly in deep re-ID models. We hypothesize that, as an alternative of other exploited visual cues, pedestrian contour could guide deep models to learn robust shape-based feature, with build-in prior information. We propose several contour-guided architectures to explicitly use pedestrian contour, including plain ones and multi-scale one. Extensive experiments have validated the effectiveness of our models. Moreover, we transfer the methodology into a powerful part-based model, Part-based Convolutional Baseline(PCB), and boost the model performance, which verifies the promising prospect of contour-guided models to expand as an auxiliary mechanism in re-ID.",2019,PRCV,,10.1007/978-3-030-31726-3_25,
a6bba5ce9867c978210e3d056691b5c1e769b760,0,1,1,Deep Learning for Person Re-identification: A Survey and Outlook,"Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on both single- and cross-modality Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.",2020,ArXiv,2001.04193,,https://arxiv.org/pdf/2001.04193.pdf
a72ad9ef3517a59df740ef098f110f76e06fd4e8,1,0,0,A General Re-Ranking Method Based On Metric Learning For Person Re-Identification,"When Person Re-identification is considered as a retrieval task, re-ranking becomes a critical part of improving the re-identification accuracy. Most of the existing re-ranking methods focus on k -nearest neighbors, which requires a lot of queries and memory. In this paper, we propose a Feature Relation Map based Similarity Evaluation (FRM-SE) model to tackle this problem. The Feature Relation Map is utilized to automatically mine the latent relation between the k -neighbors through convolution operation. The re-ranking distance is learned through the FRM-SE model with metric learning. Further, we optimize the existing re-ranking method to utilize the advantage of the FRM-SE model for maintaining a balance between accuracy and complexity.The proposed approach is validated on two benchmark datasets, Market1501 and CUHK03. Results show that our re-ranking method is superior to the state-of-the-art re-ranking methods. Furthermore, in the transfer learning setting, the model trained on either Market1501 or CUHK03 can achieve a comparable accuracy improvement on the DuekMTMC dataset, which validates the generalization of our SE model.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME46284.2020.9102887,
a72c97ea3908e27fb1ecba770c49cec8e12e9008,1,0,0,Data Association for Multi-Object Tracking via Deep Neural Networks,"With recent advances in object detection, the tracking-by-detection method has become mainstream for multi-object tracking in computer vision. The tracking-by-detection scheme necessarily has to resolve a problem of data association between existing tracks and newly received detections at each frame. In this paper, we propose a new deep neural network (DNN) architecture that can solve the data association problem with a variable number of both tracks and detections including false positives. The proposed network consists of two parts: encoder and decoder. The encoder is the fully connected network with several layers that take bounding boxes of both detection and track-history as inputs. The outputs of the encoder are sequentially fed into the decoder which is composed of the bi-directional Long Short-Term Memory (LSTM) networks with a projection layer. The final output of the proposed network is an association matrix that reflects matching scores between tracks and detections. To train the network, we generate training samples using the annotation of Stanford Drone Dataset (SDD). The experiment results show that the proposed network achieves considerably high recall and precision rate as the binary classifier for the assignment tasks. We apply our network to track multiple objects on real-world datasets and evaluate the tracking performance. The performance of our tracker outperforms previous works based on DNN and comparable to other state-of-the-art methods.",2019,Sensors,,10.3390/s19030559,https://pdfs.semanticscholar.org/a72c/97ea3908e27fb1ecba770c49cec8e12e9008.pdf
a743127b44397b7a017a65a7ad52d0d7ccb4db93,1,0,0,Domain Adaptation through Synthesis for Unsupervised Person Re-identification,"Drastic variations in illumination across surveillance cameras make the person re-identification problem extremely challenging. Current large scale re-identification datasets have a significant number of training subjects, but lack diversity in lighting conditions. As a result, a trained model requires fine-tuning to become effective under an unseen illumination condition. To alleviate this problem, we introduce a new synthetic dataset that contains hundreds of illumination conditions. Specifically, we use 100 virtual humans illuminated with multiple HDR environment maps which accurately model realistic indoor and outdoor lighting. To achieve better accuracy in unseen illumination conditions we propose a novel domain adaptation technique that takes advantage of our synthetic data and performs fine-tuning in a completely unsupervised way. Our approach yields significantly higher accuracy than semi-supervised and unsupervised state-of-the-art methods, and is very competitive with supervised techniques.",2018,ECCV,1804.10094,10.1007/978-3-030-01261-8_12,https://arxiv.org/pdf/1804.10094.pdf
a74a01c5ed3b17825857ba53442376816ec9512c,1,1,0,CANU-ReID: A Conditional Adversarial Network for Unsupervised person Re-IDentification.,"Unsupervised person re-ID is the task of identifying people on a target data set for which the ID labels are unavailable during training. In this paper, we propose to unify two trends in unsupervised person re-ID: clustering & fine-tuning and adversarial learning. On one side, clustering groups training images into pseudo-ID labels, and uses them to fine-tune the feature extractor. On the other side, adversarial learning is used, inspired by domain adaptation, to match distributions from different domains. Since target data is distributed across different camera viewpoints, we propose to model each camera as an independent domain, and aim to learn domain-independent features. Straightforward adversarial learning yields negative transfer, we thus introduce a conditioning vector to mitigate this undesirable effect. In our framework, the centroid of the cluster to which the visual sample belongs is used as conditioning vector of our conditional adversarial network, where the vector is permutation invariant (clusters ordering does not matter) and its size is independent of the number of clusters. To our knowledge, we are the first to propose the use of conditional adversarial networks for unsupervised person re-ID. We evaluate the proposed architecture on top of two state-of-the-art clustering-based unsupervised person re-identification (re-ID) methods on four different experimental settings with three different data sets and set the new state-of-the-art performance on all four of them. Our code and model will be made publicly available at https://team.inria.fr/perception/canu-reid/.",2020,,1904.01308,,https://arxiv.org/pdf/1904.01308.pdf
a76241999c8a594eb18fa93e06d81b4099a8e1fe,1,0,0,Long-Term Tracking With Deep Tracklet Association,"Recently, most multiple object tracking (MOT) algorithms adopt the idea of tracking-by-detection. Relevant research shows that the performance of the detector obviously affects the tracker, while the improvement of detector is gradually slowing down in recent years. Therefore, trackers using tracklet (short trajectory) are proposed to generate more complete trajectories. Although there are various tracklet generation algorithms, the fragmentation problem still often occurs in crowded scenes. In this paper, we introduce an iterative clustering method that generates more tracklets while maintaining high confidence. Our method shows robust performance on avoiding internal identity switch. Then we propose a deep association method for tracklet association. In terms of motion and appearance, we construct motion evaluation network (MEN) and appearance evaluation network (AEN) to learn long-term features of tracklets for association. In order to explore more robust features of tracklets, a tracklet-based training mechanism is also introduced. Tracklet groups are used as the input of the networks instead of discrete detections. Experimental results show that our training method enhances the performance of the networks. In addition, our tracking framework generates more complete trajectories while maintaining the unique identity of each target as the same time. On the latest MOT 2017 benchmark, we achieve state-of-the-art results.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2993073,https://ieeexplore.ieee.org/ielx7/83/4358840/09096592.pdf
a78637cecc6d21abf8c98583fdfc656efc0ce105,0,1,0,Unpaired Pose Guided Human Image Generation,"This paper studies the task of full generative modelling of realistic images of humans, guided only by coarse sketch of the pose, while providing control over the specific instance or type of outfit worn by the user. This is a difficult problem because input and output domain are very different and direct image-to-image translation becomes infeasible. We propose an end-to-end trainable network under the generative adversarial framework, that provides detailed control over the final appearance while not requiring paired training data and hence allows us to forgo the challenging problem of fitting 3D poses to 2D images. The model allows to generate novel samples conditioned on either an image taken from the target domain or a class label indicating the style of clothing (e.g., t-shirt). We thoroughly evaluate the architecture and the contributions of the individual components experimentally. Finally, we show in a large scale perceptual study that our approach can generate realistic looking images and that participants struggle in detecting fake images versus real samples, especially if faces are blurred.",2019,CVPR Workshops,1901.02284,,https://arxiv.org/pdf/1901.02284.pdf
a7c0a0b7b7a64b8af899ce3c491b24660183cb2e,1,0,0,HPILN: A feature learning framework for cross-modality person re-identification,"Most video surveillance systems use both RGB and infrared cameras, making it a vital technique to re-identify a person cross the RGB and infrared modalities. This task can be challenging due to both the cross-modality variations caused by heterogeneous images in RGB and infrared, and the intra-modality variations caused by the heterogeneous human poses, camera views, light brightness, etc. To meet these challenges a novel feature learning framework, HPILN, is proposed. In the framework existing single-modality re-identification models are modified to fit for the cross-modality scenario, following which specifically designed hard pentaplet loss and identity loss are used to improve the performance of the modified cross-modality re-identification models. Based on the benchmark of the SYSU-MM01 dataset, extensive experiments have been conducted, which show that the proposed method outperforms all existing methods in terms of Cumulative Match Characteristic curve (CMC) and Mean Average Precision (MAP).",2019,IET Image Process.,1906.03142,10.1049/iet-ipr.2019.0699,https://arxiv.org/pdf/1906.03142.pdf
a7c6fe91d1b95bd2ca6648a24cad481b6bbdf890,0,1,0,Adaptive weight part-based convolutional network for person re-identification,"While part-based methods have been shown effective in the person re-identification task, it is unreasonable for most of them to treat each part equally, due to the retrieved image may be affected by deformation, occlusion and other factors, which makes the feature information of some parts unreliable. Instead of using the same weight of each part for the final person re-ID, we consider using an adaptive weight based on the part image information for each part for precise person retrieval. Specifically, we aim at learning discriminative part-informed features and propose an adaptive weight part-based convolutional network (AWPCN) for the person re-ID task. The core component of our AWPCN framework is an adaptive weight model, in which the part-based convolutional network and the adaptive weight model are used for feature refinement and feature-pair alignment, respectively. Given an image input at first, it outputs a convolutional descriptor consisting of several part-level features by the part-based convolutional network. And then, the corresponding weights of each part are determined by the adaptive weight model. Finally, we can use the adaptive weight part-based convolutional network joint to train each part loss and simultaneous optimization of its feature representations. We evaluate the proposed AWPCN model on Market-1501, DukeMTMC-reID and CUHK03 datasets. In extensive experiments, the AWPCN model outperforms most of the state-of-the-art methods on these representative datasets which clearly demonstrates the effectiveness of our proposed method. Our code will be released at https://github.com/deasonyuan/AWPCN.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09018-x,
a7fb5ca30d5633ee42e8e3a149e1efc85d5336e6,0,1,0,Towards Efficient Front-End Visual Sensing for Digital Retina: A Model-Centric Paradigm,"The digital retina excels at providing enhanced visual sensing and analysis capability for city brain in smart cities, and can feasibly convert the visual data from visual sensors into semantic features. With the deployment of deep learning or handcrafted models, these features are extracted on front-end devices, then delivered to back-end servers for advanced analysis. In this scenario, we propose a model generation, utilization and communication paradigm, aiming at strong front-end sensing capabilities for establishing better artificial visual systems in smart cities. In particular, we propose an integrated multiple deep learning models reuse and prediction strategy, which dramatically increases the feasibility of the digital retina in large-scale visual data analysis in smart cities. The proposed multi-model reuse scheme aims to reuse the knowledge from models cached and transmitted in digital retina to obtain more discriminative capability. To efficiently deliver these newly generated models, a model prediction scheme is further proposed by encoding and reconstructing model differences. Extensive experiments have been conducted to demonstrate the effectiveness of proposed model-centric paradigm.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2020.2966885,
a80d8506fa28334c947989ca153b70aafc63ac7f,1,1,0,Pedestrian Retrieval via Part-Based Gradation Regularization in Sensor Networks,"In this paper, we propose a novel label distribution approach named part-based gradation regularization (PGR) for pedestrian retrieval in sensor networks. Considering different importance of various body parts, we present a gradual function to assign pedestrian label for each horizontal part. In this way, we can conduct part-based supervised learning using the identification network. The proposed PGR not only learns the discriminative local convolutional neural network-based features, but also considers the significance of assigning pedestrian label for different horizontal parts. Experimental results show that the proposed PGR obtains better performance than other approaches on three pedestrian retrieval databases, i.e., Market-1501, CUHK03, and DukeMTMC-reID databases.",2018,IEEE Access,,10.1109/ACCESS.2018.2854830,
a81cf3816db39e8b0d3f6ec9c7a28247776aac70,0,1,0,Where-and-When to Look: Deep Siamese Attention Networks for Video-Based Person Re-Identification,"Video-based person re-identification (re-id) is a central application in surveillance systems with a significant concern in security. Matching persons across disjoint camera views in their video fragments are inherently challenging due to the large visual variations and uncontrolled frame rates. There are two steps crucial to person re-id, namely, discriminative feature learning and metric learning. However, existing approaches consider the two steps independently, and they do not make full use of the temporal and spatial information in the videos. In this paper, we propose a Siamese attention architecture that jointly learns spatiotemporal video representations and their similarity metrics. The network extracts local convolutional features from regions of each frame and enhances their discriminative capability by focusing on distinct regions when measuring the similarity with another pedestrian video. The attention mechanism is embedded into spatial gated recurrent units to selectively propagate relevant features and memorize their spatial dependencies through the network. The model essentially learns which parts (where) from which frames (when) are relevant and distinctive for matching persons and attaches higher importance therein. The proposed Siamese model is end-to-end trainable to jointly learn comparable hidden representations for paired pedestrian videos and their similarity value. Extensive experiments on three benchmark datasets show the effectiveness of each component of the proposed deep network while outperforming state-of-the-art methods.",2019,IEEE Transactions on Multimedia,1808.01911,10.1109/TMM.2018.2877886,https://arxiv.org/pdf/1808.01911.pdf
a88e3b8bc17df68626b8c517bccd38ce0be32dbb,1,1,0,Detecting Wearable Objects via Transfer Learning,"Transfer learning is a well known technique to circumvent the problem of small datasets in deep machine learning. It has been successfully used in the field of camera surveillance image processing which suffers from poor data quality and quantity. We focused on the task of wearable object detection, namely distinguishing if a person is or is not wearing a backpack. We created new annotations for the DukeMTMC-attribute dataset to overcome the discrepancies among the attributes. We explored transfer learning with a frozen feature extractor as well as the model fine-tuning, which turned out to perform much better. In both setups we found that the Densenet161 is the best from tested architectures. Our best model achieved about 92% balanced accuracy on the testing set.",2019,2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP),,10.1109/ICCP48234.2019.8959621,
a9c990222b57df600ad6fda79055bec394edcedd,0,0,1,Attribute-Identity Embedding and Self-Supervised Learning for Scalable Person Re-Identification,"Due to the domain shift between source dataset and target dataset, most of the existing person re-identification (PRID) algorithms trained by a supervised learning framework often fail to be well generalized to another domain. To address this challenge, we propose a self-supervised learning algorithm based on attribute-identity embedding, which can incrementally optimize the model by selecting unlabeled samples from target domain. Thus the gap between source domain and target domain is bridged. Specifically, we first develop an attribute-identity joint prediction dictionary learning model for simultaneously learning a latent attribute space, a semantic attribute dictionary and an identifier. In our method, the predicted attribute from latent attribute space is used as a bridge to establish a preliminary link between different domains so as to predict the label of the target data sample. Second, to exploit the latent label contained in the predicted samples, we propose a prediction-training cycle self-supervised learning to tune the model variables to make them more adaptive in the target domain. Finally, the similarity measurement of pedestrians is achieved by combining the attribute space with latent identity space. The experiments show that the developed method outperforms some state-of-the-art supervised PRID methods and unsupervised PRID algorithms.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2952550,
aa70a72e135d03dd3ae40ff73c3e35fbc620e3d8,1,0,0,SAMOT: Switcher-Aware Multi-Object Tracking and Still Another MOT Measure,"Multi-Object Tracking (MOT) is a popular topic in computer vision. However, identity issue, i.e., an object is wrongly associated with another object of a different identity, still remains to be a challenging problem. To address it, switchers, i.e., confusing targets thatmay cause identity issues, should be focused. Based on this motivation,this paper proposes a novel switcher-aware framework for multi-object tracking, which consists of Spatial Conflict Graph model (SCG) and Switcher-Aware Association (SAA). The SCG eliminates spatial switch-ers within one frame by building a conflict graph and working out the optimal subgraph. The SAA utilizes additional information from potential temporal switcher across frames, enabling more accurate data association. Besides, we propose a new MOT evaluation measure, Still Another IDF score (SAIDF), aiming to focus more on identity issues.This new measure may overcome some problems of the previous measures and provide a better insight for identity issues in MOT. Finally,the proposed framework is tested under both the traditional measures and the new measure we proposed. Extensive experiments show that ourmethod achieves competitive results on all measure.",2020,ArXiv,2009.10338,,https://arxiv.org/pdf/2009.10338.pdf
aaac3225452c5450f0ee9e7a46fc3120dbf665a3,1,0,0,Real-Time Multi-Target Multi-Camera Tracking with Spatial-Temporal Information,"Video security monitoring has always been an important mission for safety reason. In an entire surveillance system, there are usually several cameras distributed sparsely to cover a wide range of public areas (e.g., school, shopping mall or infrastructure). Tracking person through this cameras network is challenging due to different camera perspectives, illumination changes and pose variations. Several algorithms for Multi-Target Multi-Camera tracking (MTMCT) have been proposed in offline method which has delay in getting result. Addressing the need for real-time computation of people tracks through multi camera, the paper proposes an online tracking algorithm. The contributions include (1) online real-time framework which can be used in practical application, (2) extend a single camera multi object tracking (MOT) algorithm to be suitable for multi-camera tracking and (3) use spatial-temporal information to strengthen cross camera person recall performance. The proposed algorithm has been benchmarked against the literature review of MTMC algorithms.",2019,2019 IEEE Visual Communications and Image Processing (VCIP),,10.1109/VCIP47243.2019.8965845,
aaca2ebcd26ed668788f364dd7af8b4615492b66,0,1,0,Omnidirectional Feature Learning for Person Re-Identification,"Person re-identification (PReID) has received increasing attention due to it being an important role in intelligent surveillance. Many state-of-the-art PReID methods are part-based deep models. Most of these models focus on learning the part feature representation of a person’s body from the horizontal direction. However, the feature representation of the body from the vertical direction is usually ignored. In addition, the relationships between these part features and different feature channels are not considered. In this paper, we introduce a multi-branch deep model for PReID. Specifically, the model consists of five branches. Among the five branches, two branches learn the part features with spatial information from horizontal and vertical orientations; one branch aims to learn the interdependencies between different feature channels generated by the last convolution layer of the backbone network; the remaining two branches are identification and triplet sub-networks in which the discriminative global feature and a corresponding measurement can be learned simultaneously. All five branches can improve the quality of representation learning. We conduct extensive comparison experiments on three benchmarks, including Market-1501, CUHK03, and DukeMTMC-reID. The proposed deep framework outperforms other competitive state-of-the-art methods. The code is available at https://github.com/caojunying/person-reidentification.",2019,IEEE Access,,10.1109/ACCESS.2019.2901764,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08653287.pdf
ab2c2150ffc85f920e7ba6b7f3cbb4bf0abfb6d1,0,1,0,Self-Training With Progressive Augmentation for Unsupervised Cross-Domain Person Re-Identification,"Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function based on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: tinyurl.com/PASTReID",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1907.13315,10.1109/ICCV.2019.00831,https://arxiv.org/pdf/1907.13315.pdf
ab49a5d44f93d10c268619ff8e26d09e076724b7,1,0,0,3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels,"Crowd counting has been studied for decades and a lot of works have achieved good performance, especially the DNNs-based density map estimation methods. Most existing crowd counting works focus on single-view counting, while few works have studied multi-view counting for large and wide scenes, where multiple cameras are used. Recently, an end-to-end multi-view crowd counting method called multi-view multi-scale (MVMS) has been proposed, which fuses multiple camera views using a CNN to predict a 2D scene-level density map on the ground-plane. Unlike MVMS, we propose to solve the multi-view crowd counting task through 3D feature fusion with 3D scene-level density maps, instead of the 2D ground-plane ones. Compared to 2D fusion, the 3D fusion extracts more information of the people along z-dimension (height), which helps to solve the scale variations across multiple views. The 3D density maps still preserve the 2D density maps property that the sum is the count, while also providing 3D information about the crowd density. We also explore the projection consistency among the 3D prediction and the ground-truth in the 2D views to further enhance the counting performance. The proposed method is tested on 3 multi-view counting datasets and achieves better or comparable counting performance to the state-of-the-art.",2020,AAAI,2003.08162,10.1609/AAAI.V34I07.6980,https://arxiv.org/pdf/2003.08162.pdf
ab4a67fa4c4d0aee81e4a0d648deb458a839fa21,0,1,0,VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification,"One fundamental challenge of vehicle re-identification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of 86.07% mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.",2020,ArXiv,2004.06305,10.1109/tmm.2020.3014488,https://arxiv.org/pdf/2004.06305.pdf
ab8b29d66906e5149d963b3ad71d8965631120eb,1,0,0,Self-supervised on-line cumulative learning from video streams,"Abstract We present a novel online self-supervised method for face identity learning from video streams. The method exploits deep face feature descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative descriptor matching solution based on Reverse Nearest Neighbor and a memory based cumulative learning strategy that discards redundant descriptors while time progresses. This allows building a comprehensive and cumulative representation of all the past visual information observed so far. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information.",2020,Comput. Vis. Image Underst.,,10.1016/j.cviu.2020.102983,
abb9f70c40b3641bed51e8363a0a6b0bde5d948f,0,1,0,Full batch loss for person re-identification,"Distance metric learning between pairs of samples is the key issue for the person re-identification(re-ID). Recently, convolutional neural networks (CNNs) significantly is employed to improve performance by learning deep semantic features which requires a large amount of labeled data for train the deep model. For person re-ID task, the training data is scarce. To the end, how to use the limited data to achieve the optimal deep model is a challenging problem. We observe that sample's similarity information and identity information are complementary. Therefore, a full-batch loss function is proposed in this paper, which promotes the distance relationship of individual sample pairs in a batch to the distance matrix in all samples to make full use of the sample similarity information. At the same time, it also makes full use of the sample's identity information by integrating the identification loss function. We conduct experiments on the two public person re-ID benchmarks: Market1501 and DukeMTMC-reID. The results clearly demonstrate that our proposed full batch loss model produces more discriminative descriptors for person re-ID, which outperforms well established baselines significantly and offer new state-of-the-art performances",2019,International Conference on Graphic and Image Processing,,10.1117/12.2524336,
abcfeb2723f30105c41ffb043f2e6c173f891e90,0,0,1,Set Augmented Triplet Loss for Video Person Re-Identification,"Modern video person re-identification (re-ID) machines are often trained using a metric learning approach, supervised by a triplet loss. The triplet loss used in video reID is usually based on so-called clip features, each aggregated from a few frame features. In this paper, we propose to model the video clip as a set and instead study the distance between sets in the corresponding triplet loss. In contrast to the distance between clip representations, the distance between clip sets considers the pair-wise similarity of each element (i.e., frame representation) between two sets. This allows the network to directly optimize the feature representation at a frame level. Apart from the commonlyused set distance metrics (e.g., ordinary distance and Hausdorff distance), we further propose a hybrid distance metric, tailored for the set-aware triplet loss. Also, we propose a hard positive set construction strategy using the learned class prototypes in a batch. Our proposed method achieves state-of-the-art results across several standard benchmarks, demonstrating the advantages of the proposed method.",2020,ArXiv,2011.00774,,https://arxiv.org/pdf/2011.00774.pdf
abdb63405b0aa3cfdd028fab1afef3f3ce752b24,0,1,0,RGB-IR Cross-modality Person ReID based on Teacher-Student GAN Model,"RGB-Infrared (RGB-IR) person re-identification (ReID) is a technology where the system can automatically identify the same person appearing at different parts of a video when light is unavailable. The critical challenge of this task is the cross-modality gap of features under different modalities. To solve this challenge, we proposed a Teacher-Student GAN model (TS-GAN) to adopt different domains and guide the ReID backbone to learn better ReID information. (1) In order to get corresponding RGB-IR image pairs, the RGB-IR Generative Adversarial Network (GAN) was used to generate IR images. (2) To kick-start the training of identities, a ReID Teacher module was trained under IR modality person images, which is then used to guide its Student counterpart in training. (3) Likewise, to better adapt different domain features and enhance model ReID performance, three Teacher-Student loss functions were used. Unlike other GAN based models, the proposed model only needs the backbone module at the test stage, making it more efficient and resource-saving. To showcase our model's capability, we did extensive experiments on the newly-released SYSU-MM01 RGB-IR Re-ID benchmark and achieved superior performance to the state-of-the-art with 49.8% Rank-1 and 47.4% mAP.",2020,ArXiv,2007.07452,,https://arxiv.org/pdf/2007.07452.pdf
ac5a5e1b9be187c94deb47e81d07d92ca135e858,1,1,0,Online Multiple Object Tracking with Recurrent Neural Networks and Appearance Model,"Multiple object tracking suffers from many challenges including huge computation work, crowd scenes. In order to solve these problems, we proposed a novel online multiple object tracking algorithm based on recurrent neural networks (RNNs) and appearance model. Compared to traditional algorithms, the RNNs can handle the motion state of the target well because it is trained with a quantity of data extracted from real world scenes. In addition, RNNs is helpful to improve tracking speed because it predicts the trajectories of objects without complex appearance calculations. The appearance feature is significant for tracking, especially in crowed scenes. The appearance model is extracted by convolutional neural networks trained with MARS dataset which is more targeted for the multi object tracking. In order to balance the speed and accuracy of tracking, a novel simple decision method was proposed to decide which features should be used. Otherwise, the cascade matching is integrated into the data association to solve a lot of subproblems in tracking. The experimental evaluation shows our algorithm is fast and accurate.",2020,"2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",,10.1109/CISP-BMEI51763.2020.9263623,
ac71435d43e22daeeeb47494aedbf925b9efae1c,1,1,1,Imitating Targets from all sides: An Unsupervised Transfer Learning method for Person Re-identification,"Person re-identification (Re-ID) models usually show a limited performance when they are trained on one dataset and tested on another dataset due to the inter-dataset bias (e.g. completely different identities and backgrounds) and the intra-dataset difference (e.g. camera invariance). In terms of this issue, given a labelled source training set and an unlabelled target training set, we propose an unsupervised transfer learning method characterized by 1) bridging inter-dataset bias and intra-dataset difference via a proposed ImitateModel simultaneously; 2) regarding the unsupervised person Re-ID problem as a semi-supervised learning problem formulated by a dual classification loss to learn a discriminative representation across domains; 3) exploiting the underlying commonality across different domains from the class-style space to improve the generalization ability of re-ID models. Extensive experiments are conducted on two widely employed benchmarks, including Market-1501 and DukeMTMC-reID, and experimental results demonstrate that the proposed method can achieve a competitive performance against other state-of-the-art unsupervised Re-ID approaches.",2019,ArXiv,1904.0502,,
acdfa233c64ddc2ab108585ee2985b2eac00b442,1,0,0,Self-Critical Attention Learning for Person Re-Identification,"In this paper, we propose a self-critical attention learning method for person re-identification. Unlike most existing methods which train the attention mechanism in a weakly-supervised manner and ignore the attention confidence level, we learn the attention with a critic which measures the attention quality and provides a powerful supervisory signal to guide the learning process. Moreover, the critic model facilitates the interpretation of the effectiveness of the attention mechanism during the learning process, by estimating the quality of the attention maps. Specifically, we jointly train our attention agent and critic in a reinforcement learning manner, where the agent produces the visual attention while the critic analyzes the gain from the attention and guides the agent to maximize this gain. We design spatial- and channel-wise attention models with our critic module and evaluate them on three popular benchmarks including Market-1501, DukeMTMC-ReID, and CUHK03. The experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin of 5.9%/2.1%, 6.3%/3.0%, and 10.5%/9.5% on mAP/Rank-1, respectively.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00973,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf
ace70f1f54e38bdca7e0621a46055f236270d9be,1,0,0,Bags of Tricks and A Strong Baseline for Deep Person Re-identification.,"This paper explores a simple and efficient baseline for person re-identification (ReID). Person re-identification (ReID) with deep neural networks has made progress and achieved high performance in recent years. However, many state-of-the-arts methods design complex network structure and concatenate multi-branch features. In the literature, some effective training tricks are briefly appeared in several papers or source codes. This paper will collect and evaluate these effective training tricks in person ReID. By combining these tricks together, the model achieves 94.5% rank-1 and 85.9% mAP on Market1501 with only using global features. Our codes and models are available in Github. 1",2019,,,,
ad9daf9ffe604f0df5597c5ae4308f5e9c8494e6,0,1,0,Multi-scale Vehicle Re-identification Using Self-adapting Label Smoothing Regularization,"Vehicle re-identification (re-id) plays an important role in intelligent surveillance. Since difference vehicle models may have similar appearances, together with the problem of image scale variations, the vehicle re-id remains long-term challenging. We present a novel multi-scale vehicle re-id framework using self-adapting label smoothing regularization (SLSR). It integrates the appearance information from multi-scale images to alleviate the influence of scale changes caused by perspectives. To enhance the generalization ability in feature representations, we design the self-adapting label smoothing regulation in semi-supervised training process. It dynamically assigns labels to fake images to realize data augmentation. We validate the effectiveness of our proposed framework on popular VeRi and VehicleID datasets. Extensive experimental results demonstrate that our method outperforms most state-of-the-art methods on both datasets. Especially, we exceeds the latest method by 3.81% in mAP and 5.32% in rank-1 on VeRi dataset.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8682599,
ae47057e1ad945a013f08f5d9cd1bc284bae439b,0,1,0,An Interactive Framework for Cross-modal Attribute-based Person Retrieval,"Person re-identification systems generally rely on a query person image to find additional occurrences of this person across a camera network. In many real-world situations, however, no such query image is available and witness testimony is the only clue upon which to base a search. Cross-modal re-identification based on attribute queries can help in such cases but currently yields a low matching accuracy which is often not sufficient for practical applications. In this work we propose an interactive feedback-driven framework, which successfully bridges the modality gap and achieves a significant increase in accuracy by 47% in mean average precision (mAP) compared to the fully automatic cross-modal state-of-the-art. We further propose a cluster-based feedback method as part of the framework, which outperforms naïve user feedback by more than 9% mAP. Our results set a new state-of-the-art for fully automatic and feedback-driven cross-modal attribute-based re-identification on two public datasets.",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909832,
ae4d06143a6b46ffaf5d228b1fa464dc322ddc18,1,0,1,R4-A.3: Human Detection & Re-Identification for Mass Transit Environments,"Large networks of cameras are ubiquitous in urban life, especially in densely populated environments such as airports, train stations, and sports arenas. For cost and practicality, most cameras in such networks are spaced widely so that their fields of view are nonoverlapping. Automatically matching humans that reappear across different cameras in such networks is a critical problem in homeland security-related surveillance applications.",2017,,,,https://pdfs.semanticscholar.org/41dd/d29d9e56bb87b9f988afc75cd597657b2600.pdf
ae5983048e59a339c77fee89e9279a4a787ba985,0,1,0,Part-Based Deep Hashing for Large-Scale Person Re-Identification,"Large-scale is a trend in person re-identi- fication (re-id). It is important that real-time search be performed in a large gallery. While previous methods mostly focus on discriminative learning, this paper makes the attempt in integrating deep learning and hashing into one framework to evaluate the efficiency and accuracy for large-scale person re-id. We integrate spatial information for discriminative visual representation by partitioning the pedestrian image into horizontal parts. Specifically, Part-based Deep Hashing (PDH) is proposed, in which batches of triplet samples are employed as the input of the deep hashing architecture. Each triplet sample contains two pedestrian images (or parts) with the same identity and one pedestrian image (or part) of the different identity. A triplet loss function is employed with a constraint that the Hamming distance of pedestrian images (or parts) with the same identity is smaller than ones with the different identity. In the experiment, we show that the proposed PDH method yields very competitive re-id accuracy on the large-scale Market-1501 and Market-1501+500K datasets.",2017,IEEE Transactions on Image Processing,1705.02145,10.1109/TIP.2017.2695101,https://opus.lib.uts.edu.au/bitstream/10453/118092/4/CEDC9F66-1F11-46FE-9702-B48259EFF456%20am.pdf
ae9b1fbc3484d41c8fa4d6f82d94bd05a8b96091,0,0,1,Superpixel-Based Temporally Aligned Representation for Video-Based Person Re-Identification †,"Most existing person re-identification methods focus on matching still person images across non-overlapping camera views. Despite their excellent performance in some circumstances, these methods still suffer from occlusion and the changes of pose, viewpoint or lighting. Video-based re-id is a natural way to overcome these problems, by exploiting space–time information from videos. One of the most challenging problems in video-based person re-identification is temporal alignment, in addition to spatial alignment. To address the problem, we propose an effective superpixel-based temporally aligned representation for video-based person re-identification, which represents a video sequence only using one walking cycle. Particularly, we first build a candidate set of walking cycles by extracting motion information at superpixel level, which is more robust than that at the pixel level. Then, from the candidate set, we propose an effective criterion to select the walking cycle most matching the intrinsic periodicity property of walking persons. Finally, we propose a temporally aligned pooling scheme to describe the video data in the selected walking cycle. In addition, to characterize the individual still images in the cycle, we propose a superpixel-based representation to improve spatial alignment. Extensive experimental results on three public datasets demonstrate the effectiveness of the proposed method compared with the state-of-the-art approaches.",2019,Sensors,,10.3390/s19183861,https://pdfs.semanticscholar.org/0f17/81acbb438ad8dfdc5083a40f0a9ae1f519e4.pdf
af1c3758e455193e39966132725a8eb365a01662,1,0,0,Learning Posterior and Prior for Uncertainty Modeling in Person Re-Identification,"Data uncertainty in practical person reID is ubiquitous, hence it requires not only learning the discriminative features, but also modeling the uncertainty based on the input. This paper proposes to learn the sample posterior and the class prior distribution in the latent space, so that not only representative features but also the uncertainty can be built by the model. The prior reflects the distribution of all data in the same class, and it is the trainable model parameters. While the posterior is the probability density of a single sample, so it is actually the feature defined on the input. We assume that both of them are in Gaussian form. To simultaneously model them, we put forward a distribution loss, which measures the KL divergence from the posterior to the priors in the manner of supervised learning. In addition, we assume that the posterior variance, which is essentially the uncertainty, is supposed to have the second-order characteristic. Therefore, a $\Sigma-$net is proposed to compute it by the high order representation from its input. Extensive experiments have been carried out on Market1501, DukeMTMC, MARS and noisy dataset as well.",2020,ArXiv,2007.08785,,https://arxiv.org/pdf/2007.08785.pdf
af7bc853cfec0e31e6af6b673ee4dad1682a57fc,0,1,0,Circulant Binary Convolutional Networks for Object Recognition,"The rapidly decreasing computation and memory cost has recently driven the success of many applications in the field of deep learning. Practical applications of deep learning in resource-limited hardware, such as embedded devices and smart phones, however, remain challenging. For binary convolutional networks, the reason lies in the degraded representation caused by binarizing full-precision filters. To address this problem, we propose new circulant filters (CiFs) and a circulant binary convolution (CBConv) to enhance the capacity of binarized convolutional features via our circulant back propagation (CBP). The CiFs can be easily incorporated into existing deep convolutional neural networks (DCNNs), which leads to new Circulant Binary Convolutional Networks (CBCNs). Extensive experiments confirm that the performance gap between the 1-bit and full-precision DCNNs is minimized by increasing the filter diversity, which further increases the representational ability in our networks. Our experiments on ImageNet show that CBCNs achieve 61.4% top-1 accuracy with ResNet18. Compared to the state-of-the-art such as XNOR, CBCNs can achieve up to 10% higher top-1 accuracy with more powerful representational ability. Also, CBCNs approximately achieve a storage reduction about 32 times. In particular, our method shows strong generalization on the object recognition task, i.e., face recognition, facial expression recognition and person re-identification.",2020,IEEE Journal of Selected Topics in Signal Processing,,10.1109/JSTSP.2020.2969516,
af8a72cee9150d52512c238fc5d5f8b400adaa40,0,1,0,Dual Encoder-Decoder Based Generative Adversarial Networks for Disentangled Facial Representation Learning,"To learn disentangled representations of facial images, we present a Dual Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed method, both the generator and discriminator are designed with deep encoder-decoder architectures as their backbones. To be more specific, the encoder-decoder structured generator is used to learn a pose disentangled face representation, and the encoder-decoder structured discriminator is tasked to perform real/fake classification, face reconstruction, determining identity and estimating face pose. We further improve the proposed network architecture by minimizing the additional pixel-wise loss defined by the Wasserstein distance at the output of the discriminator so that the adversarial framework can be better trained. Additionally, we consider face pose variation to be continuous, rather than discrete in existing literature, to inject richer pose information into our model. The pose estimation task is formulated as a regression problem, which helps to disentangle identity information from pose variations. The proposed network is evaluated on the tasks of pose-invariant face recognition (PIFR) and face synthesis across poses. An extensive quantitative and qualitative evaluation carried out on several controlled and in-the-wild benchmarking datasets demonstrates the superiority of the proposed DED-GAN method over the state-of-the-art approaches.",2020,IEEE Access,1909.08797,10.1109/ACCESS.2020.3009512,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09141259.pdf
b0025636e687479f1e99343ab630ff8053569528,1,1,0,Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification,"Person re-identification is a challenging task due to various complex factors. Recent studies have attempted to integrate human parsing results or externally defined attributes to help capture human parts or important object regions. On the other hand, there still exist many useful contextual cues that do not fall into the scope of predefined human parts or attributes. In this paper, we address the missed contextual cues by exploiting both the accurate human parts and the coarse non-human parts. In our implementation, we apply a human parsing model to extract the binary human part masks and a self-attention mechanism to capture the soft latent (non-human) part masks. We verify the effectiveness of our approach with new state-of-the-art performance on three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our implementation is available at https://github.com/ggjy/P2Net.pytorch.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1910.10111,10.1109/ICCV.2019.00374,https://arxiv.org/pdf/1910.10111.pdf
b00a30fd43a8a90f362f0b1b4a6cbdb13bc4ae73,1,0,0,Collaborative Attention Network for Person Re-identification,,2019,ArXiv,1911.13008,,https://arxiv.org/pdf/1911.13008.pdf
b00f68e54dacf6aac51899f5194c64259f06d04c,1,0,0,Coarse-to-Fine Multi-camera Network Topology Estimation,"In multiple camera networks, the correlation of multiple cameras can provide us with a richer information than a single camera. In order to make full use of the association information between multiple cameras. We propose a novel approach to estimate a camera topology relationship in a multi-camera surveillance network, which is unsupervised and gradually refined from coarse to fine. First, an improved cross-correlation function is used to get a preliminary result, then a time constraint feature matching model is used to reduce the error caused by external environment and noise, which can increase the accuracy of our results. Finally, we test the proposed method on several different datasets, and its result indicates that our approach perform well on recovering the topology of the camera and can improve the accuracy on over camera tracking.",2017,PCM,,10.1007/978-3-319-77383-4_96,
b030bf2b14c09f4980cbf5a8d77057413f23aee8,1,1,0,Interpretable and Generalizable Person Re-identification with Query-Adaptive Convolution and Temporal Lifting,"For person re-identification, existing deep networks often focus on representation learning. However, without domain adaptation or transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and result are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this image matching architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation without further transfer learning, the proposed Query-Adaptive Convolution (QAConv) method achieves better results than many transfer learning methods for person re-identification. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, resulting in state-of-the-art results in cross-dataset evaluations.",2020,ECCV,1904.10424,10.1007/978-3-030-58621-8_27,https://arxiv.org/pdf/1904.10424.pdf
b08085f36df9ea76010c6c921e1bc6d6410cf8de,1,1,0,Local-Global Extraction Unit for Person Re-identification,"The huge variance of human pose and inaccurate detection significantly increase the difficulty of person re-identification. Existing deep learning methods mostly focus on extraction of global feature and local feature, or combine them to learn a discriminative pedestrian descriptor. However, rare traditional methods have been exploited the association of the local and global features in convolutional neural networks (CNNs), and some important part-wise information is not captured sufficiently when training. In this paper, we propose a novel architecture called Local-Global Extraction Unit (LGEU), which is able to adaptively re-calibrate part-wise information with integrating the channel-wise information. Extensive experiments on Market-1501, CUHK03, and DukeMTMC-reID datasets achieve competitive results with the state-of-the-art methods. On Market-1501, for instance, LGEU achieves 91.8% rank-1 accuracy and especially 88.0% mAP.",2018,BICS,,10.1007/978-3-030-00563-4_39,
b0cf64b02525dd4b623c9adbc110a16c7fa2094a,1,1,0,Discriminative representation learning for person re-identification via multi-loss training,"Abstract The identification model that employs softmax loss to minimize person identity classification errors has gradually gained popularity in person re-identification community due to its easy implementations. However, the softmax loss only encourages the separation of different identities. The intra-class differences caused by large view variations such as spatial misalignment and human pose change are not considered in the model training process. In this paper, we present a hybrid deep model that combines multiple loss functions to handle this problem. Specifically, the multi-loss function contains three terms, namely softmax loss, center loss, and a novel loss called inter-center loss. The center loss penalizes the distance between deep features and their center, aiming to reduce intra-class differences. The inter-center loss maximizes the distances between different class centers, aiming to further enlarge inter-class separation. Extensive experiments conducted on three public benchmark datasets including Market1501, CUHK03, and DukeMTMC-reID demonstrate the effectiveness of our method.",2019,J. Vis. Commun. Image Represent.,,10.1016/J.JVCIR.2019.06.001,
b0db147f7d84111887e6027ea4ef8ca9f9862b92,1,0,0,ATRW: A Benchmark for Amur Tiger Re-identification in the Wild,"Monitoring the population and movements of endangered species is an important task to wildlife conversation. Traditional tagging methods do not scale to large populations, while applying computer vision methods to camera sensor data requires re-identification (re-ID) algorithms to obtain accurate counts and moving trajectory of wildlife. However, existing re-ID methods are largely targeted at persons and cars, which have limited pose variations and constrained capture environments. This paper tries to fill the gap by introducing a novel large-scale dataset, the Amur Tiger Re-identification in the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. In contrast to typical re-ID datasets, the tigers are captured in a diverse set of unconstrained poses and lighting conditions. We demonstrate with a set of baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we propose a novel method for tiger re-identification, which introduces precise pose parts modeling in deep neural networks to handle large pose variation of tigers, and reaches notable performance improvement over existing re-ID methods. The ATRW dataset is public available at https://cvwc2019.github.io/challenge.html",2020,ACM Multimedia,1906.05586,10.1145/3394171.3413569,https://arxiv.org/pdf/1906.05586.pdf
b16c89bd318e88a44e6abdd6da48caddea5b647a,0,1,0,Cross-Resolution Adversarial Dual Network for Person Re-Identification and Beyond,"Person re-identification (re-ID) aims at matching images of the same person across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Extensive experimental results on five standard person re-ID benchmarks confirm the effectiveness of our method and the superiority over the state-of-the-art approaches, especially when the input resolutions are not seen during training. Furthermore, the experimental results on two vehicle re-ID benchmarks also confirm the generalization of our model on cross-resolution visual tasks. The extensions of semi-supervised settings further support the use of our proposed approach to real-world scenarios and applications.",2020,ArXiv,2002.09274,,https://arxiv.org/pdf/2002.09274.pdf
b17c685237bdf9c59d3ab31c43433c96923a168b,1,0,0,Memory-Based Neighbourhood Embedding for Visual Recognition,"Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, i.e., how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.04992,10.1109/ICCV.2019.00620,https://arxiv.org/pdf/1908.04992.pdf
b1f9109e57c2b69954d9fc58319869d599bb799d,1,1,0,Part-Based Attribute-Aware Network for Person Re-Identification,"Despite the rapid progress over the past decade, person re-identification (reID) remains a challenging task due to the fact that discriminative features underlying different granularities are easily affected by illumination and camera-view variation. Most deep learning-based algorithms for reID extract global embedding as the representation of the pedestrian from the convolutional neural network. Considering that person attributes are robust and informative to identify pedestrians. This paper proposes a multi-branch model, namely part-based attribute-aware network (PAAN), to leverage both person reID and attribute performance, which not only utilizes ID label visible to the whole image but also utilizes attribute information. In order to learn discriminative and robust global representation which is invariant to the fact mentioned above, we resort to global and local person attributes to build global and local representation, respectively, utilizing our proposed layered partition strategy. Our goal is to exploit global or local semantic information to guide the optimization of global representation. Besides, in order to enhance the global representation, we design a semantic bridge replenishing mid-level semantic information for the final representation, which contains high-level semantic information. The extensive experiments are conducted to demonstrate the effectiveness of our proposed approach on two large-scale person re-identification datasets including Market-1501 and DukeMTMC-reID, and our approach achieves rank-1 of 92.40% on Market-1501 and 82.59% on DukeMTMC-reID showing strong competitiveness among the start of the art.",2019,IEEE Access,,10.1109/ACCESS.2019.2912844,
b22c08f71a8292d3848dde856839cd1f62d5fa86,1,0,0,Deep Classification Consistency for Person Re-Identification,"Person re-identification (Re-ID) has greatly benefited from utilizing features of different levels. Most methods draw support from elegant network designs and elaborate fusion modules. In this paper, different from employing multi-level feature fusion like mainstream skip connection methods, we propose a Deep Classification Consistency(DCC) layer which imposes classification consistency on deep features from different stages in CNNs. DCC regularizes the training process of networks. It profoundly changes the distribution of learned features, making them more discriminative and transferable. Extensive experiments on Market-1501, DukeMTMC-reID, and CUHK03 datasets prove that the proposed method has obtained competitive performances when compared with state-of-the-art approaches, especially those advanced methods merely based on metric learning.",2020,IEEE Access,,10.1109/ACCESS.2020.3031935,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09229164.pdf
b232f7b890044dc1856101ef2820563d7a1bbb29,1,0,0,Adaptive Attention-Aware Network for unsupervised person re-identification,"Abstract Person re-identification (Re-ID) has attracted more attention in computer vision tasks recently and achieved high accuracy in some public available datasets in a supervised manner. The performance drops significantly when datasets are unlabeled, which limits the scalability of Re-ID algorithms in practical applications. Despite some unsupervised methods are proposed to address the scalability problem of Re-ID, it’s hard to learn discriminative feature representations due to the lack of pairwise labels in different camera views. To overcome this problem, we propose an end-to-end network named Adaptive Attention-Aware Network for unsupervised person re-identification. Specifically, we propose a novel adaptive attention-aware module that could be easily embedded into Re-ID architecture. The proposed module focuses on learning strong expressive relationship among channels of feature maps, and alleviating the key problems of Re-ID, e.g., occlusion and local deformation. In addition, we extract the camera-invariant features by adopting camera-style transfer feature learning since matching pairs in Re-ID suffers from appearance changes under different camera views. Besides, unsupervised hard negative mining is introduced to learn large intra-person appearance variance and discriminate high inter-person appearance similarity in an unlabeled target dataset with an auxiliary labeled dataset. Comprehensive experiments on three public available Re-ID datasets demonstrate that our method can achieve the state-of-the-art results of unsupervised Re-ID and is competitive with supervised learning.",2020,Neurocomputing,,10.1016/j.neucom.2020.05.094,
b23e760065eeac8ec4431b64bf18497ff78398e2,1,1,0,Incomplete Descriptor Mining with Elastic Loss for Person Re-Identification,"In this paper, we propose a novel person Re-ID model, Consecutive Batch DropBlock Network (CBDB-Net), to help the person Re-ID model to capture the attentive and robust person descriptor. The CBDB-Net contains two novel modules: the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss. In the Consecutive Batch DropBlock Module (CBDBM), it firstly conducts uniform partition on the feature maps. And then, the CBDBM independently and continuously drops each patch from top to bottom on the feature maps, which outputs multiple incomplete features to push the model to capture the robust person descriptor. In the Elastic Loss, we design a novel weight control item to help the deep model adaptively balance hard sample pairs and easy sample pairs in the whole training process. Through an extensive set of ablation studies, we verify that the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss each contribute to the performance boosts of CBDB-Net. We demonstrate that our CBDB-Net can achieve the competitive performance on the three generic person Re-ID datasets (the Market-1501, the DukeMTMC-Re-ID, and the CUHK03 dataset), three occlusion Person Re-ID datasets (the Occluded DukeMTMC, the Partial-REID, and the Partial iLIDS dataset), and the other image retrieval dataset (In-Shop Clothes Retrieval dataset).",2020,,2008.0401,,https://arxiv.org/pdf/2008.04010.pdf
b2423a5a81f736de33945fa72ea9027b3178ccab,0,1,0,Hdrnet: Person Re-Identification Using Hybrid Sampling in Deep Reconstruction Network,"Person re-identification (re-id) is the task of identifying a person across non-overlapping cameras. Most of the current techniques apply deep learning and achieve a significant accuracy. However, learning a deep model that can generalize well against the challenges of pose variation, occlusion, illumination changes, and low resolution is a difficult task. Toward this, we propose a deep reconstruction re-id network, comprising of an encoder and a multi-resolution decoder, which can learn embeddings invariant to pose, occlusion, illumination, and low resolution. In our model, the encoder acts as a conventional deep re-id network and outputs a discriminative feature embedding. The output feature is then used as an input to the multi-resolution decoder to reconstruct the input images of the same identity under different resolutions, such that they are similar in pose and illumination as well as free from occlusion. We further propose a hybrid sampling strategy to boost the effectiveness of the training loss function. In addition, we propose test set augmentation using the reconstructed images to explicitly transform single query to multi-query setting. In our multi-tasking approach, the feature robustness is enhanced by the multi-resolution decoder, and the overall accuracy is further improved by a sampling strategy and test data augmentation. Furthermore, we empirically show that the proposed network is robust to pose variations, occlusion, and low resolution. We perform rigorous qualitative and quantitative analysis in order to demonstrate that we achieve state-of-the-art person re-id accuracy.",2019,IEEE Access,,10.1109/ACCESS.2019.2908344,
b25aa5113b12bb7818da9e35c727457e84244f13,1,0,0,"MVP Matching: A Maximum-Value Perfect Matching for Mining Hard Samples, With Application to Person Re-Identification","How to correctly stress hard samples in metric learning is critical for visual recognition tasks, especially in challenging person re-ID applications. Pedestrians across cameras with significant appearance variations are easily confused, which could bias the learned metric and slow down the convergence rate. In this paper, we propose a novel weighted complete bipartite graph based maximum-value perfect (MVP) matching for mining the hard samples from a batch of samples. It can emphasize the hard positive and negative sample pairs respectively, and thus relieve adverse optimization and sample imbalance problems. We then develop a new batch-wise MVP matching based loss objective and combine it in an end-to-end deep metric learning manner. It leads to significant improvements in both convergence rate and recognition performance. Extensive empirical results on five person re-ID benchmark datasets, i.e., Market-1501, CUHK03-Detected, CUHK03-Labeled, Duke-MTMC, and MSMT17, demonstrate the superiority of the proposed method. It can accelerate the convergence rate significantly while achieving state-of-the-art performance. The source code of our method is available at \url{https://github.com/IAAI-CVResearchGroup/MVP-metric}.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00684,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.pdf
b273769cdccf1b83476c0f1649efeeb203a4bd19,0,1,0,Toward Intelligent Visual Sensing and Low-cost Analysis: A Collaborative Computing Approach,"In the big data era, there has been an increasing consensus that the label information, computational resources and communication bandwidth are particularly precious. State-of-the-art research is revolutionizing the vision systems of the smart city, which converts the visual signals from sensory input into feature representations and conveys the compact feature for analysis by using the computational resources of both front and back ends. To deploy a robust model, large amounts of labeled data are usually required, and thereby heavy computational and communication resources are incurred in model training as well as inference. However, the computational resources in front-end devices are usually constrained, and heavy transmission burden is imposed when leveraging multiple models amongst different ends. In this work, we propose a novel collaborative computing approach for intelligent sensing and low-cost analysis, which reduces the requirement of labeled data and communication cost, and balances the computational load in model training and inference. By incorporating the adversarial learning mechanism into collaborative model training, knowledge of different domains can be better exploited. Moreover, the learned models are deployed for inference in a collaborative manner, in which part of model is placed in front-ends for extracting intermediate feature maps, and part of the model remains in back ends for inference with received feature maps. The effectiveness of the proposed approach has been validated in the context of an emerging digital retina system for smart city intelligent applications.",2019,2019 IEEE Visual Communications and Image Processing (VCIP),,10.1109/VCIP47243.2019.8965808,
b27726edf7f8ef4ab562f484cfcec59f616800fd,0,1,0,Self-attention Learning for Person Re-identification,"Person re-identification is a critical yet challenging task in video surveillance. It aims to match the same person across cameras. Practically, people’s appearances vary greatly across cameras. Most deep learning methods rely on single-level features of deep layer while ignoring low-level detailed features of shallow layers, since different layers have different sizes of feature maps and different layers’ features cannot be concatenated without extra downsampling or upsampling. To remedy this problem, we propose a novel yet simple self-attention learning method for person re-identification. We design a convolutional neural network(CNN) to capture multi-level information from different layers while keeping spatial resolution of feature maps unchanged by using dilated convolution. Multi-level information consists of two parts: multi-level attention maps and multi-level feature maps. Multi-level attention maps are constrained and multi-level feature maps are concatenated easily. And we combine softmax loss with quadruplet loss, taking full advantages of labels and metric learning at the same time. Experimental results demonstrate the proposed method achieves excellent performance for person re-identification and self-attention constraint can also be used in many other tasks.",2018,BMVC,,,http://bmvc2018.org/contents/papers/0613.pdf
b2ad0d45b8173961d410685dc40b20ace5e5a6be,1,0,0,Progressive Learning Algorithm for Efficient Person Re-Identification,"This paper studies the problem of Person Re-Identification (ReID)for large-scale applications. Recent research efforts have been devoted to building complicated part models, which introduce considerably high computational cost and memory consumption, inhibiting its practicability in large-scale applications. This paper aims to develop a novel learning strategy to find efficient feature embeddings while maintaining the balance of accuracy and model complexity. More specifically, we find by enhancing the classical triplet loss together with cross-entropy loss, our method can explore the hard examples and build a discriminant feature embedding yet compact enough for large-scale applications. Our method is carried out progressively using Bayesian optimization, and we call it the Progressive Learning Algorithm (PLA). Extensive experiments on three large-scale datasets show that our PLA is comparable or better than the-state-of-the-arts. Especially, on the challenging Market-1501 dataset, we achieve Rank-1=94.7\%/mAP=89.4\% while saving at least 30\% parameters than strong part models.",2019,ArXiv,1912.07447,,https://arxiv.org/pdf/1912.07447.pdf
b2be518280db9f7cd1c73f88bcc7c53401c8ce03,1,1,0,Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification,"Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins.",2020,ECCV,2007.10315,10.1007/978-3-030-58536-5_6,https://arxiv.org/pdf/2007.10315.pdf
b2d25f22a2add6350bf07044034a0694420be470,0,1,0,"Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11, 2019, Proceedings, Part I","We propose the channel feature enhanced detector (CFED) for ball detection, a challenging small object detection task. The proposed method achieves a good performance on small ball detection since we design a channel feature enhanced module to increase the discriminability of the target features. Moreover, we set up the BALL dataset for training and evaluation. Experimental results show that our method achieves a mAP of 90.2% on BALL dataset with an inference time of 11.4 milliseconds per image. The proposed lightweight network makes it possible to apply real-time detection to mobile devices.",2019,PRCV,,10.1007/978-3-030-31654-9,
b2ed3b754f8598af7d0604c64de77f8399ab1b1b,1,1,0,Learning Incremental Triplet Margin for Person Re-identification,"Person re-identification (ReID) aims to match people across multiple non-overlapping video cameras deployed at different locations. To address this challenging problem, many metric learning approaches have been proposed, among which triplet loss is one of the state-of-the-arts. In this work, we explore the margin between positive and negative pairs of triplets and prove that large margin is beneficial. In particular, we propose a novel multi-stage training strategy which learns incremental triplet margin and improves triplet loss effectively. Multiple levels of feature maps are exploited to make the learned features more discriminative. Besides, we introduce global hard identity searching method to sample hard identities when generating a training batch. Extensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our approach yields a performance boost and outperforms most existing state-of-the-art methods.",2019,AAAI,1812.06576,10.1609/AAAI.V33I01.33019243,https://arxiv.org/pdf/1812.06576.pdf
b34b630a730a1ffee70fb260d1a08175707c099d,1,1,0,Taking A Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification,"Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has achieved remarkable performance. However, in pursuit of high accuracy, researchers in the academic always focus on training with large-scale datasets at a high cost of time and label expenses, while neglect to explore the potential of performing efficient training from millions of synthetic data. To facilitate development in this field, we reviewed the previously developed synthetic dataset GPR and built an improved one (GPR+) with larger number of identities and distinguished attributes. Based on it, we quantitatively analyze the influence of dataset attribute on re-ID system. To our best knowledge, we are among the first attempts to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. This research helps us have a deeper understanding of the fundamental problems in person re-ID, which also provides useful insights for dataset building and future practical usage.",2020,ArXiv,2010.08145,,https://arxiv.org/pdf/2010.08145.pdf
b36877dc7eac3b356e60d768bd624f8b9e867cec,1,1,0,Illumination adaptive person reid based on teacher-student model and adversarial training,"Most existing works in Person Re-identification (ReID) focus on settings where illumination either is kept the same or has very little fluctuation. However, the changes in the illumination degree may affect the robustness of a ReID algorithm significantly. To address this problem, we proposed a Two-Stream Network that can separate ReID features from lighting features to enhance ReID performance. Its innovations are threefold: (1) A discriminative entropy loss to ensure the ReID features contain no lighting information. (2) A ReID Teacher model trained by images under ""neutral"" lighting conditions to guide ReID classification. (3) An illumination Teacher model trained by the differences between the illumination-adjusted and original images to guide illumination classification. We construct two augmented datasets by synthetically changing a set of predefined lighting conditions in two of the most popular ReID benchmarks: Market1501 and DukeMTMC-ReID. Experiments demonstrate that our algorithm outperforms other state-of-the-art works and particularly potent in handling images under extremely low light.",2020,ICIP,2002.01625,10.1109/icip40778.2020.9190796,https://arxiv.org/pdf/2002.01625.pdf
b3a45118534144f50a56653dac8109c73fc2c0e8,1,0,0,A Dataset for Persistent Multi-target Multi-camera Tracking in RGB-D,"Video surveillance systems are now widely deployed to improve our lives by enhancing safety, security, health monitoring and business intelligence. This has motivated extensive research into automated video analysis. Nevertheless, there is a gap between the focus of contemporary research, and the needs of end users of video surveillance systems. Many existing benchmarks and methodologies focus on narrowly defined problems in detection, tracking, re-identification or recognition. In contrast, end users face higher-level problems such as long-term monitoring of identities in order to build a picture of a person's activity across the course of a day, producing usage statistics of a particular area of space, and that these capabilities should be robust to challenges such as change of clothing. To achieve this effectively requires less widely studied capabilities such as spatio-temporal reasoning about people identities and locations within a space partially observed by multiple cameras over an extended time period. To bridge this gap between research and required capabilities, we propose a new dataset LIMA that encompasses the challenges of monitoring a typical home / office environment. LIMA contains 4.5 hours of RGB-D video from three cameras monitoring a four room house. To reflect the challenges of a realistic practical application, the dataset includes clothes changes and visitors to ensure the global reasoning is a realistic open-set problem. In addition to raw data, we provide identity annotation for benchmarking, and tracking results from a contemporary RGB-D tracker – thus allowing focus on the higher level monitoring problems.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.189,https://www.pure.ed.ac.uk/ws/files/36162552/layne2017lima.pdf
b3bbf0efbd272831e89fbb09634636c70bcf2dfa,0,1,0,Person Re-identification through Clustering and Partial Label Smoothing Regularization,"In this paper, we propose a new label smoothing regularization scheme for person re-identification. We first use an unsupervised method for discriminative learning representation. We apply a clustering algorithm on the learned feature to partition the training set into k groups of equal variance and derive a shared space for similar images. Secondly, a GAN model is fed with each cluster to produce samples with relatively similar features to the original space. Our method consists of assigning an adaptive smooth label distribution to each generated sample according to their original cluster. To train our model, we define a new objective function which takes into account the generated samples and fine-tuned a CNN baseline using the objective function. Our model learns to exploit the samples generated by the GAN model to boost the performance of the person re-id by improving generalization. Extensive evaluations were conducted on four large-scale datasets to validate the advantage of the proposed model.",2019,ICSIM 2019,,10.1145/3305160.3305205,
b3c4908ef07fe3909bca75545192d16b735d9470,0,1,0,Effective Data Augmentation with Multi-Domain Learning GANs,"For deep learning applications, the massive data development (e.g., collecting, labeling), which is an essential process in building practical applications, still incurs seriously high costs. In this work, we propose an effective data augmentation method based on generative adversarial networks (GANs), called Domain Fusion. Our key idea is to import the knowledge contained in an outer dataset to a target model by using a multi-domain learning GAN. The multi-domain learning GAN simultaneously learns the outer and target dataset and generates new samples for the target tasks. The simultaneous learning process makes GANs generate the target samples with high fidelity and variety. As a result, we can obtain accurate models for the target tasks by using these generated samples even if we only have an extremely low volume target dataset. We experimentally evaluate the advantages of Domain Fusion in image classification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor Scene Recognition. When trained on each target dataset reduced the samples to 5,000 images, Domain Fusion achieves better classification accuracy than the data augmentation using fine-tuned GANs. Furthermore, we show that Domain Fusion improves the quality of generated samples, and the improvements can contribute to higher accuracy.",2020,AAAI,1912.11597,10.1609/AAAI.V34I04.6131,https://arxiv.org/pdf/1912.11597.pdf
b3dbb682bb8b71723538f433b1dd007ed0b9b244,1,1,1,Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification,"Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a ""learning via translation"" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation. Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.07027,10.1109/CVPR.2018.00110,https://arxiv.org/pdf/1711.07027.pdf
b3f6e8fdc4635ce5aa39781239598f28a23fdbac,0,1,0,Unsupervised Domain Adaptation in Person re-ID via k-Reciprocal Clustering and Large-Scale Heterogeneous Environment Synthesis,"An ongoing major challenge in computer vision is the task of person re-identification, where the goal is to match individuals across different, non-overlapping camera views. While recent success has been achieved via supervised learning using deep neural networks, such methods have limited widespread adoption due to the need for large-scale, customized data annotation. As such, there has been a recent focus on unsupervised learning approaches to mitigate the data annotation issue; however, current approaches in literature have limited performance compared to supervised learning approaches as well as limited applicability for adoption in new environments. In this paper, we address the aforementioned challenges faced in person re-identification for real-world, practical scenarios by introducing a novel, unsupervised domain adaptation approach for person re-identification. This is accomplished through the introduction of: i) k-reciprocal tracklet Clustering for Unsupervised Domain Adaptation (ktCUDA) (for pseudo-label generation on target domain), and ii) Synthesized Heterogeneous RE-id Domain (SHRED) composed of large-scale heterogeneous independent source environments (for improving robustness and adaptability to a wide diversity of target environments). Experimental results across four different image and video benchmark datasets show that the proposed ktCUDA and SHRED approach achieves an average improvement of +5.7 mAP in re-identification performance when compared to existing state-of-the-art methods, as well as demonstrate better adaptability to different types of environments.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),2001.04928,10.1109/WACV45572.2020.9093606,https://arxiv.org/pdf/2001.04928.pdf
b4db7ff14ae6099d6a823a25a876eaca28bf821a,0,1,0,Learning Modality-Specific Representations for Visible-Infrared Person Re-Identification,"Traditional person re-identification (re-id) methods perform poorly under changing illuminations. This situation can be addressed by using dual-cameras that capture visible images in a bright environment and infrared images in a dark environment. Yet, this scheme needs to solve the visible-infrared matching issue, which is largely under-studied. Matching pedestrians across heterogeneous modalities is extremely challenging because of different visual characteristics. In this paper, we propose a novel framework that employs modality-specific networks to tackle with the heterogeneous matching problem. The proposed framework utilizes the modality-related information and extracts modality-specific representations (MSR) by constructing an individual network for each modality. In addition, a cross-modality Euclidean constraint is introduced to narrow the gap between different networks. We also integrate the modality-shared layers into modality-specific networks to extract shareable information and use a modality-shared identity loss to facilitate the extraction of modality-invariant features. Then a modality-specific discriminant metric is learned for each domain to strengthen the discriminative power of MSR. Eventually, we use a view classifier to learn view information. The experiments demonstrate that the MSR effectively improves the performance of deep networks on VI-REID and remarkably outperforms the state-of-the-art methods.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2928126,
b4e902118d439adce28a92bc36ac6a37ad293b7c,0,1,0,Distributed person re-identification through network-wise rank fusion consensus,"Abstract The problem of re-identify persons across single disjoint camera-pairs has received great attention from the community. Despite this, when the re-identification process has to be carried out on a wide camera network additional problems arise and deny the direct application of existing solutions. Thus, a different approach has to be considered. In particular, existing approaches have neglected the importance of the network topology (i.e., the configuration of the monitored area) in such a process. To try filling such a gap, we propose a distributed person re-identification framework which brings in the following contributions: (i) a weighted camera matching cost that measures the re-identification performance between cameras in the network; (ii) a derivation of the distance vector algorithm that yields to network topology learning and allows us to prioritize and limit the cameras inquired for the re-identification; (iii) a network consensus weighted rank fusion solution that allows us to perform the re-identification in a robust fashion. Results on four benchmark datasets show that the proposed approach brings to significant network-wise re-identification improvements.",2019,Pattern Recognit. Lett.,,10.1016/J.PATREC.2018.12.015,
b4ff63329ddc7485fc94b7b1ff87740efdbfafef,1,1,0,Meta-Test Source Datasets LLmmmmrr LLmmmmmm LLTTrrTT LLTTrrTT LLMM ∇∇ΘΘ LLmmmmrr ΘΘ + LLmmmmmm ΘΘ ′ Meta-Train Memories Backbone Backbone Copy Model Meta-Train ⋯ ⋯,"Recent advances in person re-identification (ReID) obtain impressive accuracy in the supervised and unsupervised learning settings. However, most of the existing methods need to train a new model for a new domain by accessing data. Due to public privacy, the new domain data are not always accessible, leading to a limited applicability of these methods. In this paper, we study the problem of multi-source domain generalization in ReID, which aims to learn a model that can perform well on unseen domains with only several labeled source domains. To address this problem, we propose the Memory-based Multi-Source Meta-Learning (M$^3$L) framework to train a generalizable model for unseen domains. Specifically, a meta-learning strategy is introduced to simulate the train-test process of domain generalization for learning more generalizable models. To overcome the unstable meta-optimization caused by the parametric classifier, we propose a memory-based identification loss that is non-parametric and harmonizes with meta-learning. We also present a meta batch normalization layer (MetaBN) to diversify meta-test features, further establishing the advantage of meta-learning. Experiments demonstrate that our M$^3$L can effectively enhance the generalization ability of the model for unseen domains and can outperform the state-of-the-art methods on four large-scale ReID datasets.",,,,,https://arxiv.org/pdf/2012.00417.pdf
b50520a551989151096fb7c323783db15061b82a,1,0,0,University of Applied Sciences Mittweida and Chemnitz University of Technology at TRECVID Instance Search 2019,"Identifying activities in large video collections remains a difficult process. (Partially) automated systems can be used to address different parts of these challenges. Object detection and classification are achieving ever higher detection rates using the latest state-of-the-art neural convolution networks. In our contribution to the task of instance search we specifically discuss the extension of a heterogeneous system that enables the identification performance for the recognition and localization of individuals and their activities by heuristically combining several state-of-the-art activity recognition, object recognition and classification frameworks. In our first approach to the task of Instance Search. (INS), which deals with the recognition of complex activities of persons or objects, we also integrate state-of-the-art neural network object recognition and classification frames to extract boundary frames from prominent regions or objects that can be used for further processing. However, basic tracking of objects detected by bounding boxes requires special algorithmic or feature-driven handling to include statistical correlations between frames. Our approach describes a simple yet powerful way to track objects across video images.",2020,,,,https://pdfs.semanticscholar.org/b505/20a551989151096fb7c323783db15061b82a.pdf
b572b85afa982e765190364ff710014267aabfb3,1,1,0,Coarse-Fine Convolutional Neural Network for Person Re-Identification in Camera Sensor Networks,"In this paper, we present a novel deep model named coarse-fine convolutional neural network (CFCNN) for person re-identification in camera sensor networks, which jointly learns global and multi-scale local features simultaneously. To this end, we design the CFCNN as a multi-branch network, which is composed of one coarse and two fine branches. Specifically, the global feature is learned from the coarse branch, and the two fine branches are developed to extract two kinds of local features with different scales. Afterward, each branch is followed by a classification loss to make the identity prediction. Finally, we obtain completed pedestrian representations via concatenating the learned global and all local features. We conduct a number of experiments to evaluate the effectiveness of the CFCNN on three datasets. The CFCNN achieves high rank-1 and mAP accuracy with 94.0%/81.2%, 64.6%/58.4%, and 85.7%/72.4% on Market-1501, CUHK03, and DukeMTMC-reID, respectively. These results significantly outperform the prior state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2917939,
b5989d2088c40075d59a4de9a46c7e4fd04a89dd,0,1,0,Multi-level feature fusion model-based real-time person re-identification for forensics,"Person forensics aims to retrieve the specified person across non-overlapping cameras. It is difficult owing to the appearance variations caused by occlusion, human pose change, background clutter, illumination variation, etc. In this scenario, current models face great challenges in extracting effective features. Recent deep learning models mainly focus on extracting representative deep features to cope with appearance variations, while handcrafted features are not fully explored. In this paper, a multi-level feature fusion model (MFFM) is designed to combine both deep features and handcrafted features in real time. MFFM is first utilized to describe person appearance. Then, local binary pattern (LBP) and histogram of oriented gradient (HOG) are extracted to cope with geometric change and illumination variance. Using LBP and HOG, 11.89% on the CUHK03, 15.30% on the Market-1501 and 8.25% on the VIPeR top-1 recognition accuracy improvement for the proposed method are achieved with only 9.66%, 4.90%, and 7.59% extra processing time. Experimental results indicate MFFM can achieve the best performance compared to the state-of-the-art models on the Market1501, CUHK03, and VIPeR datasets.",2019,Journal of Real-Time Image Processing,,10.1007/s11554-019-00908-4,
b5a9f87daed5b3a4b0d0381dc2b2480766a581a3,1,1,0,Deep Group-Shuffling Random Walk for Person Re-identification,"Person re-identification aims at finding a person of interest in an image gallery by comparing the probe image of this person with all the gallery images. It is generally treated as a retrieval problem, where the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. However, most existing methods only consider P2G affinities but ignore the affinities between all the gallery images (G2G affinity). Some frameworks incorporated G2G affinities into the testing process, which is not end-to-end trainable for deep neural networks. In this paper, we propose a novel group-shuffling random walk network for fully utilizing the affinity information between gallery images in both the training and testing processes. The proposed approach aims at end-to-end refining the P2G affinities based on G2G affinity information with a simple yet effective matrix operation, which can be integrated into deep neural networks. Feature grouping and group shuffle are also proposed to apply rich supervisions for learning better person features. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets by large margins, which demonstrate the effectiveness of our approach.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1807.11178,10.1109/CVPR.2018.00241,https://arxiv.org/pdf/1807.11178.pdf
b5cbb43c8442d847c0bb449bf10669adf06c0472,1,1,1,Unsupervised Attention Based Instance Discriminative Learning for Person Re-Identification,"Recent advances in person re-identification have demonstrated enhanced discriminability, especially with supervised learning or transfer learning. However, since the data requirements—including the degree of data curations—are becoming increasingly complex and laborious, there is a critical need for unsupervised methods that are robust to large intra-class variations, such as changes in perspective, illumination, articulated motion, resolution, etc. Therefore, we propose an unsupervised framework for person re-identification which is trained in an end-to-end manner without any pre-training. Our proposed framework leverages a new attention mechanism that combines group convolutions to (1) enhance spatial attention at multiple scales and (2) reduce the number of trainable parameters by 59.6%. Additionally, our framework jointly optimizes the network with agglomerative clustering and instance learning to tackle hard samples. We perform extensive analysis using the Market1501 and DukeMTMC-reID datasets to demonstrate that our method consistently outperforms the state-of-the-art methods (with and without pre-trained weights).",2020,ArXiv,2011.01888,,https://arxiv.org/pdf/2011.01888.pdf
b5d3ee3cf721d361c875f9c371325c2ab59645a9,0,1,0,Learning a Robust Representation via a Deep Network on Symmetric Positive Definite Manifolds,"Recent studies have shown that aggregating convolutional features of a pre-trained Convolutional Neural Network (CNN) can obtain impressive performance for a variety of visual tasks. The symmetric Positive Definite (SPD) matrix becomes a powerful tool due to its remarkable ability to learn an appropriate statistic representation to characterize the underlying structure of visual features. In this paper, we propose to aggregate deep convolutional features into an SPD matrix representation through the SPD generation and the SPD transformation under an end-to-end deep network. To this end, several new layers are introduced in our network, including a nonlinear kernel aggregation layer, an SPD matrix transformation layer, and a vectorization layer. The nonlinear kernel aggregation layer is employed to aggregate the convolutional features into a real SPD matrix directly. The SPD matrix transformation layer is designed to construct a more compact and discriminative SPD representation. The vectorization and normalization operations are performed in the vectorization layer for reducing the redundancy and accelerating the convergence. The SPD matrix in our network can be considered as a mid-level representation bridging convolutional features and high-level semantic features. To demonstrate the effectiveness of our method, we conduct extensive experiments on visual classification. Experiment results show that our method notably outperforms state-of-the-art methods.",2019,Pattern Recognit.,1711.0654,10.1016/j.patcog.2019.03.007,https://arxiv.org/pdf/1711.06540.pdf
b5da737c46acffad8d59a1a9ae1ecb28a15da5c2,0,1,0,Memorizing Comprehensively to Learn Adaptively: Unsupervised Cross-Domain Person Re-ID with Multi-level Memory,"Unsupervised cross-domain person re-identification (Re-ID) aims to adapt the information from the labelled source domain to an unlabelled target domain. Due to the lack of supervision in the target domain, it is crucial to identify the underlying similarity-and-dissimilarity relationships among the unlabelled samples in the target domain. In order to use the whole data relationships efficiently in mini-batch training, we apply a series of memory modules to maintain an up-to-date representation of the entire dataset. Unlike the simple exemplar memory in previous works, we propose a novel multi-level memory network (MMN) to discover multi-level complementary information in the target domain, relying on three memory modules, i.e., part-level memory, instance-level memory, and domain-level memory. The proposed memory modules store multi-level representations of the target domain, which capture both the fine-grained differences between images and the global structure for the holistic target domain. The three memory modules complement each other and systematically integrate multi-level supervision from bottom to up. Experiments on three datasets demonstrate that the multi-level memory modules cooperatively boost the unsupervised cross-domain Re-ID task, and the proposed MMN achieves competitive results.",2020,ArXiv,2001.04123,,https://arxiv.org/pdf/2001.04123.pdf
b5f8adcd9e9441bb3a244c739483f62825357452,1,0,0,Occlusion-robust Online Multi-object Visual Tracking using a GM-PHD Filter with a CNN-based Re-identification,"We propose a novel online multi-object visual tracking algorithm via a tracking-by-detection paradigm using a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter and deep Convolutional Neural Network (CNN) appearance representations learning. The GM-PHD filter has a linear complexity with the number of objects and observations while estimating the states and cardinality of unknown and time-varying number of objects in the scene. Though it handles object birth, death and clutter in a unified framework, it is susceptible to miss-detections and does not include the identity of objects. We use visual-spatio-temporal information obtained from object bounding boxes and deeply learned appearance representations to perform estimates-to-tracks data association for labeling of each target. We learn the deep CNN appearance representations by training an identification network (IdNet) on large-scale person re-identification data sets. We also employ additional unassigned tracks prediction after the data association step to overcome the susceptibility of the GM-PHD filter towards miss-detections caused by occlusion. Our tracker which runs in real-time is applied to track multiple objects in video sequences acquired under varying environmental conditions and objects density. Lastly, we make extensive evaluations on Multiple Object Tracking 2016 (MOT16) and 2017 (MOT17) benchmark data sets and find out that our online tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and identification.",2019,ArXiv,1912.05949,,https://arxiv.org/pdf/1912.05949.pdf
b633239f9300e8d3a29bf29a6724b606293f3f05,1,1,0,Building Computationally Efficient and Well-Generalizing Person Re-Identification Models with Metric Learning,"This work considers the problem of domain shift in person re-identification.Being trained on one dataset, a re-identification model usually performs much worse on unseen data. Partially this gap is caused by the relatively small scale of person re-identification datasets (compared to face recognition ones, for instance), but it is also related to training objectives. We propose to use the metric learning objective, namely AM-Softmax loss, and some additional training practices to build well-generalizing, yet, computationally efficient models. We use recently proposed Omni-Scale Network (OSNet) architecture combined with several training tricks and architecture adjustments to obtain state-of-the art results in cross-domain generalization problem on a large-scale MSMT17 dataset in three setups: MSMT17-all->DukeMTMC, MSMT17-train->Market1501 and MSMT17-all->Market1501.",2020,ArXiv,2003.07618,,https://arxiv.org/pdf/2003.07618.pdf
b689e97cf90559e12cb2267ab00479fb9a627515,0,1,0,Accelerated low-rank sparse metric learning for person re-identification,"Abstract Person re-identification is an open and challenging problem in computer vision. A surge of effort has been spent design the best feature representation, and to learn either the transformation of such features across cameras or an optimal matching metric. Metric learning solutions which are currently in vogue in the field generally require a dimensionality reduction pre-processing stage to handle the high-dimensionality of the adopted feature representation. Such an approach is suboptimal and a better solution can be achieved by combining such a step in the metric learning process. Towards this objective, a low-rank matrix which projects the high-dimensional vectors to a low-dimensional manifold with a discriminative Euclidean distance is introduced. The goal is achieved with a stochastic accelerated proximal gradient method. Experiments on two public benchmark datasets show that better performances than state-of-the-art methods are achieved.",2018,Pattern Recognit. Lett.,,10.1016/j.patrec.2018.07.033,
b6933b75bdb0af81711487140fe1175b19fd103e,0,0,1,An Efficient Non-local Attention Network for Video-based Person Re-identification,"A spatial and temporal attention strategy based on Non-local Networks is proposed for video-based person re-identification. The most existing methods design attention mechanisms on high-level features, which ignore the low-level features with more details. The proposed method adopts non-local networks which can aggregate features according to feature correlation at any level. There are two contributions of this work can be summarized as follows: (i) The spatial and temporal redundancy in video-based person Re-ID is analyzed in this work; (ii) An Efficient Non-local Attention Network is designed to reduce the computation complexity by exploring spatial and temporal redundancy for video-based person Re-ID. We conduct extensive experiments on two large-scale benchmarks, i.e. MARS and DukeMTMC-VideoReID. The experiments show that our model achieve 85.2% mAP, 88.3% rank-1 accuracy on MARS dataset and 95.4% mAP, 95.6% rank-1 on DukeMTMC-VideoReID without re-ranking, which significantly outperforms the state-of-arts.",2019,ICIT,,10.1145/3377170.3377253,
b6b9c346c3fd80dffe8a5ab878edff68ff9720b4,1,1,0,Deep Semi-Supervised Person Re-Identification with External Memory,"To overcome the scalability problem of supervised person re-identification (Re-ID), we consider the semi-supervised person Re-ID problem of learning from a limited number of labeled images of a few identities and a large number of unlabeled images. To this end, we propose an external-memory-based deep semi-supervised person Re-ID model (EDS). Based on the external memory, two loss functions are designed so as to effectively cope with the relation between labeled and unlabeled data for overcoming the limitation of batch size in each epoch in deep learning. Therefore, an effective deep semi-supervised learning method can be performed. Extensive experiments validate the superiority of the proposed method for semi-supervised person Re-ID.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME.2019.00192,
b6f959dea4ae8adc131e571096845a28120997f1,1,0,0,Rethinking Temporal Fusion for Video-based Person Re-identification on Semantic and Time Aspect,"Recently, the research interest of person re-identification (ReID) has gradually turned to video-based methods, which acquire a person representation by aggregating frame features of an entire video. However, existing video-based ReID methods do not consider the semantic difference brought by the outputs of different network stages, which potentially compromises the information richness of the person features. Furthermore, traditional methods ignore important relationship among frames, which causes information redundancy in fusion along the time axis. To address these issues, we propose a novel general temporal fusion framework to aggregate frame features on both semantic aspect and time aspect. As for the semantic aspect, a multi-stage fusion network is explored to fuse richer frame features at multiple semantic levels, which can effectively reduce the information loss caused by the traditional single-stage fusion. While, for the time axis, the existing intra-frame attention method is improved by adding a novel inter-frame attention module, which effectively reduces the information redundancy in temporal fusion by taking the relationship among frames into consideration. The experimental results show that our approach can effectively improve the video-based re-identification accuracy, achieving the state-of-the-art performance.",2020,AAAI,1911.12512,10.1609/AAAI.V34I07.6770,https://arxiv.org/pdf/1911.12512.pdf
b74f266c8a9fdb1e76ff02842c7b478c5da94ef4,1,1,0,Multi-level and multi-scale horizontal pooling network for person re-identification,"Person re-identification (Re-ID) is the task of matching a target person across different cameras, which has drawn extensive attention in computer vision and has become an essential component in the video surveillance system. Despite recent remarkable progress, person re-identification methods are either subject to the power of feature representation, or give equal importance to all examples. To mitigate these issues, we introduce a simple, yet effective, Multi-level and Multi-scale Horizontal Pooling Network (MMHPN) for person re-identification. Concretely, our contributions are three-fold:1) we take partial feature representation into account at different pooling scales and different semantic levels so that various partial information is obtained to form a robust descriptor; 2) we introduce a Part Sensitive Loss (PSL) to reduce the effect of easily classified partition to facilitate training of the person re-identification network, 3) we conduct extensive experimental results using the Market-1501, DukeMTMC-reID and CUHK03 datasets and achieve mAP scores of 83.4%, 75.1% and 65.4% respectively on these challenging datasets.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09427-y,https://link.springer.com/content/pdf/10.1007/s11042-020-09427-y.pdf
b76ed28a9b8a8b9061e443c85e39de8ef0d63605,0,1,0,Domain adaptation for cross-dataset person re-identification,"Most of the studies that have been conducted on person re-identification utilizes a single dataset to train, validate, and test the proposed system. Although these subsets do not overlap, since they were collected under similar conditions, experimental results obtained from such a setup are not good indicators in terms of the generalizability of the developed systems. Therefore, to obtain a better measure for the generalization capability of the proposed systems, cross-dataset experimental setups would be more appropriate. In the cross-dataset setup, the developed systems are trained and validated on one dataset and then tested using another one. In this work, to reduce the difference between the distributions of the utilized datasets in a cross-dataset setup, we proposed a cycle-consistent generative adversarial network based deep learning approach. The proposed method makes source dataset and target dataset look more similar. In the experiments, Market-1501 dataset was used as the source and PRID2011 was used as the target dataset. In the experiments, by benefiting from the proposed domain adaptation method, superior results have been achieved.",2018,2018 26th Signal Processing and Communications Applications Conference (SIU),,10.1109/SIU.2018.8404852,
b77a0cfe83281edccce6661e418e83152ea062a9,1,1,0,Intra-Camera Supervised Person Re-Identification: A New Benchmark,"Existing person re-identification (re-id) methods rely mostly on a large set of inter-camera identity labelled training data, requiring a tedious data collection and annotation process therefore leading to poor scalability in practical re-id applications. To overcome this fundamental limitation, we consider person re-identification without inter-camera identity association but only with identity labels independently annotated within each individual camera-view. This eliminates the most time-consuming and tedious inter-camera identity labeling process in order to significantly reduce the amount of human efforts required during annotation. It hence gives rise to a more scalable and more feasible learning scenario, which we call Intra-Camera Supervised (ICS) person re-id. Under this ICS setting with weaker label supervision, we formulate a Multi-Task Multi-Label (MTML) deep learning method. Given no inter-camera association, MTML is specially designed for self-discovering the inter-camera identity correspondence. This is achieved by inter-camera multi-label learning under a joint multi-task inference framework. In addition, MTML can also efficiently learn the discriminative re-id feature representations by fully using the available identity labels within each camera-view. Extensive experiments demonstrate the performance superiority of our MTML model over the state-of-the-art alternative methods on three large-scale person re-id datasets in the proposed intra-camera supervised learning setting.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),1908.10344,10.1109/ICCVW.2019.00138,https://arxiv.org/pdf/1908.10344.pdf
b7b48683002571fbc0ac0cf57bc63815c0305190,1,0,0,The Computers Have a Thousand Eyes: Towards a Practical and Ethical Video Analytics System for Person Tracking,"With the rise of computer vision, video analytics systems have become more prevalent in the real-world, which automatically process video camera footage to produce information for human users. However, the majority of the existing research has looked at small parts of the overall puzzle in isolation, oen focusing on accuracy while ignoring other practical requirements such as computation time and protecting privacy. e research presented in this thesis focuses on the development of a person tracking video analytics system that moves towards what is needed for real-world implementation with embedded systems. is is contextualised in a motivating scenario based on commercial market research, a modern application of computer vision moving away from traditional state-controlled surveillance. e aim is to balance the trade-o between accuracy and speed, with a modular pipeline to allow for beer control and transparency into the algorithms being used. e developed video analytics system consists of a novel superpixel-based background estimation algorithm, person detection using o-the-shelf methods, an unsupervised person re-identication approach for classifying identities across multiple camera views, a Kalman Filter-based spatio-temporal model for tracking people by their positions, and a model fusion module that combines the appearance-based re-identication and spatio-temporal models together to improve the classication accuracy. In addition to the algorithms, this thesis also investigates the privacy loss encountered by these types of video analytics systems, rstly through a survey into public perceptions of privacy around surveillance cameras, and secondly through the proposal of a system architecture that uses computer vision and embedded systems to help protect privacy by default. is is then implemented with the use of smart cameras, and the impacts on accuracy, speed, and networking constraints are discussed. Lastly, further techniques for accelerating computer vision tasks in embedded system contexts are presented, with a case study demonstrating the use of Hardware/Soware Co-design. e combination of all of these dierent factors brings a holistic view to the development of practical and ethical video analytics systems for person tracking, making progress towards overcoming the challenges faced by system designers and developers in real-world implementation.",2019,,,,
b80c3c39e4c1aa10fed82907d91594fde6d0d042,0,1,0,Deep Constrained Dominant Sets,001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 ICCV #3272 ICCV #3272 ICCV 2019 Submission #3272. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.,2019,,,,https://www.crcv.ucf.edu/wp-content/uploads/2019/08/Publications_Deep-Constrained-Dominant-Sets-for-Person-Re-Identification_Supplementary.pdf
b834695e6f4b95665c2e5bb4117c87fc88293b9e,1,0,0,Data association framework based on biconnected gated recurrent unit network for multiple object tracking,"Abstract. In the tracking-by-detection scheme of multiple object tracking (MOT), the data association process in which existing tracking data and new detections are matched over time is very important. A framework is proposed to solve the data association problem in MOT in scenarios where there are potential target interactions and occlusions in crowded environments. This framework consists of an input layer and an association layer. The input layer is an end-to-end feature-map extraction model that incorporates a simplified Siamese convolutional neural network, which effectively distinguishes similar objects based on their appearance and motion. The association layer is composed of a bidirectional gated recurrent unit network with three layers of fully connected networks (FCNs), whose outputs are fed into the FCNs and transformed into an association matrix that reflects the matching scores between the detections and existing tracks. The matrix is then used to minimize the loss of the framework. The experimental results show that the proposed framework demonstrates outstanding performance for MOT, with its accuracy and precision in MOT reaching values as high as 26.1% and 71.2%, respectively.",2020,J. Electronic Imaging,,10.1117/1.JEI.29.5.053017,
b837136c154903d23e79e5695270bc7d5bdf084f,0,0,1,Local-Global Feature for Video-Based One-Shot Person Re-Identification,"One-shot video-based re-identification, which uses only one labeled tracklet for each identity, is challenging since the framework usually suffers misalignment and inefficient utilizing of unlabeled data. In this paper we propose a novel local-global progressive learning framework to overcome the limitations. To obtain robust features in a tracklet, we first design sub-networks to learn four discriminative part-based feature maps and one global feature map which is insensitive to misalignment. Then a novel adaptive loss is proposed to balance the part-based and global feature properly. To utilize unlabeled data, our framework gradually select most reliable pseudo-labeled tracklets to the training set for iterative training. Extensive experiments are conducted on two video-based Re-ID datasets, MARS and DukeMTMC-VideoReID. The mAP of our model outperforms the state-of-the-art methods by 20.8% on the DukeMTMC-VideoReID dataset.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053134,
b83964876faa536489b4c189fdeb1f9d50172862,1,1,0,Exploit the Unknown Gradually: One-Shot Video-Based Person Re-identification by Stepwise Learning,"We focus on the one-shot learning for video-based person re-Identification (re-ID). Unlabeled tracklets for the person re-ID tasks can be easily obtained by preprocessing, such as pedestrian detection and tracking. In this paper, we propose an approach to exploiting unlabeled tracklets by gradually but steadily improving the discriminative capability of the Convolutional Neural Network (CNN) feature representation via stepwise learning. We first initialize a CNN model using one labeled tracklet for each identity. Then we update the CNN model by the following two steps iteratively: 1. sample a few candidates with most reliable pseudo labels from unlabeled tracklets; 2. update the CNN model according to the selected data. Instead of the static sampling strategy applied in existing works, we propose a progressive sampling method to increase the number of the selected pseudo-labeled candidates step by step. We systematically investigate the way how we should select pseudo-labeled tracklets into the training set to make the best use of them. Notably, the rank-1 accuracy of our method outperforms the state-of-the-art method by 21.46 points (absolute, i.e., 62.67% vs. 41.21%) on the MARS dataset, and 16.53 points on the DukeMTMC-VideoReID dataset1.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00543,https://opus.lib.uts.edu.au/bitstream/10453/131055/4/Exploit%20the%20Unknown%20Gradually%20One-Shot%20Video-Based%20Person%20Re-Identification%20by%20Stepwise%20Learning.pdf
b852634098dd8c1fcdfc3c96c86d599d47f7c302,1,1,0,"Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking","The success of DNNs has driven the extensive applications of person re-identification (ReID) into a new era. However, whether ReID inherits the vulnerability of DNNs remains unexplored. To examine the robustness of ReID systems is rather important because the insecurity of ReID systems may cause severe losses, e.g., the criminals may use the adversarial perturbations to cheat the CCTV systems. In this work, we examine the insecurity of current best-performing ReID models by proposing a learning-to-mis-rank formulation to perturb the ranking of the system output. As the cross-dataset transferability is crucial in the ReID domain, we also perform a back-box attack by developing a novel multi-stage network architecture that pyramids the features of different levels to extract general and transferable features for the adversarial perturbations. Our method can control the number of malicious pixels by using differentiable multi-shot sampling. To guarantee the inconspicuousness of the attack, we also propose a new perception loss to achieve better visual quality. Extensive experiments on four of the largest ReID benchmarks (i.e., Market1501, CUHK03, DukeMTMC, and MSMT17) not only show the effectiveness of our method, but also provides directions of the future improvement in the robustness of ReID systems. For example, the accuracy of one of the best-performing ReID systems drops sharply from 91.8% to 1.4% after being attacked by our method. Some attack results are shown in Fig. 1. The code is available at: https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.04199,10.1109/CVPR42600.2020.00042,https://arxiv.org/pdf/2004.04199.pdf
b856c0eb039effce7da9ff45c3f5987f18928bef,1,1,0,Pedestrian Alignment Network for Large-scale Person Re-Identification,"Person re-identification (re-ID) is mostly viewed as an image retrieval problem. This task aims to search a query person in a large image pool. In practice, person re-ID usually adopts automatic detectors to obtain cropped pedestrian images. However, this process suffers from two types of detector errors: excessive background and part missing. Both errors deteriorate the quality of pedestrian alignment and may compromise pedestrian matching due to the position and scale variances. To address the misalignment problem, we propose that alignment be learned from an identification procedure. We introduce the pedestrian alignment network (PAN) which allows discriminative embedding learning pedestrian alignment without extra annotations. We observe that when the convolutional neural network learns to discriminate between different identities, the learned feature maps usually exhibit strong activations on the human body rather than the background. The proposed network thus takes advantage of this attention mechanism to adaptively locate and align pedestrians within a bounding box. Visual examples show that pedestrians are better aligned with PAN. Experiments on three large-scale re-ID datasets confirm that PAN improves the discriminative ability of the feature embeddings and yields competitive accuracy with the state-of-the-art methods.",2019,IEEE Transactions on Circuits and Systems for Video Technology,1707.00408,10.1109/TCSVT.2018.2873599,https://arxiv.org/pdf/1707.00408.pdf
b8ea9be9b879825bb2fefd99711f2268cbb5bff3,1,0,0,Visual Appearance Based Person Retrieval in Unconstrained Environment Videos,"Abstract Visual appearance-based person retrieval is a challenging problem in surveillance. It uses attributes like height, cloth color, cloth type and gender to describe a human. Such attributes are known as soft biometrics. This paper proposes person retrieval from surveillance video using height, torso cloth type, torso cloth color and gender. The approach introduces an adaptive torso patch extraction and bounding box regression to improve the retrieval. The algorithm uses fine-tuned Mask R-CNN and DenseNet-169 for person detection and attribute classification respectively. The performance is analyzed on AVSS 2018 challenge II dataset and it achieves 11.35% improvement over state-of-the-art based on average Intersection over Union measure.",2019,Image Vis. Comput.,1910.14565,10.1016/j.imavis.2019.10.002,https://arxiv.org/pdf/1910.14565.pdf
b8fdbf9cf56247b38a041fb6fdcab3d42bbbcc87,1,0,0,Deep Learning for Person Re-identification in Surveillance Videos,"In the recent years, Closed Circuit Television (CCTV) is viewed as the basis for providing security. One of the most important aspects of CCTV surveillance systems security mechanism is to re-identify a person captured in one of the camera across different surveillance cameras. Re-identification has a major role in several applications like automated surveillance of universities, offices, malls, home and restricted environments like embassies or laboratories with strong security restrictions. Traditionally, identifying a person in a video was practiced under the set of same external conditions (like same illumination, viewpoint, back ground conditions etc.). But when it comes to automated re-identification in a CCTV surveillance system, several challenges emerge as the environment is uncontrolled and keeps varying, further the poses of the person and the angles of the cameras capturing the videos also incur additional challenge for the task considered. When a person disappears from one camera view for a period of time, he should be recognized in another view of camera at a different location when there are environmental disturbances like variation in illumination, crowded scene, partial occlusions, physical appearance variations, full occlusions, view point variations, background clutter, shadows and reflections, etc. In this chapter, the major focus is on the techniques of deep learning used to develop an end-to-end re-identification system highlighting the methods to handle the uncontrolled environment challenges mentioned. An end-to-end re-identification task consists of sequence of steps namely pedestrian detection, person tracking followed by person re-identification. Given a video sequence or an image as an input, firstly the humans are detected from the video sequence as a process of pedestrian detection. The person tracking within the camera is conducted, to find the different poses of the probe if needed. Then the re-identification process is conducted where the deep learning models are used to re-identify the person with the help of gallery set of videos and evaluates the similarities of gallery set and the person of interest by using deep learning metrics. The re-identification results end as a retrieval process where all similar images of the person of interest are retrieved. Several bench mark datasets considered in literature for re-identification system are VIPeR, ETHZ, PRID, CAVIAR, CUHK01, CUHK02, CUHK03, i-LIDS, RAiD, MARS, etc.",2020,,,10.1007/978-3-030-31760-7_9,
b918ceeda271fce750061ee957ab0a1f31bf709a,1,0,0,How Trustworthy are the Existing Performance Evaluations for Basic Vision Tasks?,"Performance evaluation is indispensable to the advancement of machine vision, yet its consistency and rigour have not received proportionate attention. This paper examines performance evaluation criteria for basic vision tasks namely, object detection, instance-level segmentation and multi-object tracking. Specifically, we advocate the use of criteria that are (i) consistent with mathematical requirements such as the metric properties, (ii) contextually meaningful in sanity tests, and (iii) robust to hyper-parameters for reliability. We show that many widely used performance criteria do not fulfill these requirements. Moreover, we explore alternative criteria for detection, segmentation, and tracking, using metrics for sets of shapes, and assess them against these requirements.",2020,ArXiv,2008.03533,,https://arxiv.org/pdf/2008.03533.pdf
b96017e1023e0431e326b78bef3cca2a083212dd,1,0,0,OSMO: Online Specific Models for Occlusion in Multiple Object Tracking under Surveillance Scene,"With demands of the intelligent monitoring, multiple object tracking (MOT) in surveillance scene has become an essential but challenging task. Occlusion is the primary difficulty in surveillance MOT, which can be categorized into the inter-object occlusion and the obstacle occlusion. Many current studies on general MOT focus on the former occlusion, but few studies have been conducted on the latter one. In fact, there are useful prior knowledge in surveillance videos, because the scene structure is fixed. Hence, we propose two models for dealing with these two kinds of occlusions. The attention-based appearance model is proposed to solve the inter-object occlusion, and the scene structure model is proposed to solve the obstacle occlusion. We also design an obstacle map segmentation method for segmenting obstacles from the surveillance scene. Furthermore, to evaluate our method, we propose four new surveillance datasets that contain videos with obstacles. Experimental results show the effectiveness of our two models.",2018,MM '18,,10.1145/3240508.3240548,http://www.jdl.link/doc/2011/2019110_p201-gao.pdf
b97209f50affe71d0b76f300d98ab96b6fe45b06,1,0,0,Group Re-Identification with Multi-grained Matching and Integration,"The task of reidentifying groups of people under different camera views is an important yet less-studied problem. Group reidentification (Re-ID) is a very challenging task since it is not only adversely affected by common issues in traditional single-object Re-ID problems, such as viewpoint and human pose variations, but also suffers from changes in group layout and group membership. In this paper, we propose a novel concept of group granularity by characterizing a group image by multigrained objects: individual people and subgroups of two and three people within a group. To achieve robust group Re-ID, we first introduce multigrained representations which can be extracted via the development of two separate schemes, that is, one with handcrafted descriptors and another with deep neural networks. The proposed representation seeks to characterize both appearance and spatial relations of multigrained objects, and is further equipped with importance weights which capture variations in intragroup dynamics. Optimal group-wise matching is facilitated by a multiorder matching process which, in turn, dynamically updates the importance weights in iterative fashion. We evaluated three multicamera group datasets containing complex scenarios and large dynamics, with experimental results demonstrating the effectiveness of our approach.",2019,IEEE transactions on cybernetics,1905.07108,10.1109/TCYB.2019.2917713,https://arxiv.org/pdf/1905.07108.pdf
b986a535e45751cef684a30631a74476e911a749,1,1,0,Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models,"Given a video or an image of a person acquired from a camera, person re-identification is the process of retrieving all instances of the same person from videos or images taken from a different camera with non-overlapping view. This task has applications in various fields, such as surveillance, forensics, robotics, multimedia. In this paper, we present a novel framework, named Saliency-Semantic Parsing Re-Identification (SSP-ReID), for taking advantage of the capabilities of both clues: saliency and semantic parsing maps, to guide a backbone convolutional neural network (CNN) to learn complementary representations that improves the results over the original backbones. The insight of fusing multiple clues is based on specific scenarios in which one response is better than another, thus favoring the combination of them to increase performance. Due to its definition, our framework can be easily applied to a wide variety of networks and, in contrast to other competitive methods, our training process follows simple and standard protocols. We present extensive evaluation of our approach through five backbones and three benchmarks. Experimental results demonstrate the effectiveness of our person re-identification framework. In addition, we combine our framework with re-ranking techniques to achieve state-of-the-art results on three benchmarks.",2019,Image Vis. Comput.,1807.05618,10.1016/j.imavis.2019.07.009,https://arxiv.org/pdf/1807.05618.pdf
b9a513cef452de44c220f675e2ef7dab21dbac47,1,1,0,PGAN: Part-Based Nondirect Coupling Embedded GAN for Person Reidentification,"The block-based representation learning method has been proven to be a very effective method for person reidentification (Re-ID), but the features extracted by the existing block-based approach tend to have a high correlation among different blocks. Also, these methods perform less well for persons with large posture changes. Thus, part-based nondirect coupling representation learning method is proposed by introducing a similarity measure loss to constrain features of different blocks. Moreover, part-based nondirect coupling embedded GAN method is proposed, which aims to extract more common features of different postures of a same person. In this way, the extracted features of the network are robust for posture changes of a person, and there are no auxiliary pose information and additional computational cost required in the test stage. Experimental results on public datasets show that our proposed method achieves good performances, especially, it outperforms the state-of-the-art GAN-based methods for person Re-ID.",2020,IEEE MultiMedia,,10.1109/MMUL.2020.2999445,http://eprints.gla.ac.uk/217561/1/217561.pdf
ba12b1a8ab4995efb1aa1560bce19794a0358dec,0,1,0,Unsupervised Data Uncertainty Learning in Visual Retrieval Systems,"We introduce an unsupervised formulation to estimate heteroscedastic uncertainty in retrieval systems. We propose an extension to triplet loss that models data uncertainty for each input. Besides improving performance, our formulation models local noise in the embedding space. It quantifies input uncertainty and thus enhances interpretability of the system. This helps identify noisy observations in query and search databases. Evaluation on both image and video retrieval applications highlight the utility of our approach. We highlight our efficiency in modeling local noise using two real-world datasets: Clothing1M and Honda Driving datasets. Qualitative results illustrate our ability in identifying confusing scenarios in various domains. Uncertainty learning also enables data cleaning by detecting noisy training labels.",2019,ArXiv,1902.02586,,https://arxiv.org/pdf/1902.02586.pdf
ba1408c1069e90ddf06079cf8f28347d500d33a8,1,0,1,Adaptive Graph Representation Learning for Video Person Re-Identification,"Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes’ information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. Code is available at https://github.com/weleen/AGRL.pytorch.",2020,IEEE Transactions on Image Processing,1909.0224,10.1109/TIP.2020.3001693,https://arxiv.org/pdf/1909.02240.pdf
ba5069f5a4f19005ab741b6cac4690e7c603472d,1,1,0,Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification,"Most existing person re-identification(Re-ID) approaches achieve superior results based on the assumption that a large amount of pre-labelled data is usually available and can be put into training phrase all at once. However, this assumption is not applicable to most real-world deployment of the Re-ID task. In this work, we propose an alternative reinforcement learning based human-in-the-loop model which releases the restriction of pre-labelling and keeps model upgrading with progressively collected data. The goal is to minimize human annotation efforts while maximizing Re-ID performance. It works in an iteratively updating framework by refining the RL policy and CNN parameters alternately. In particular, we formulate a Deep Reinforcement Active Learning (DRAL) method to guide an agent (a model in a reinforcement learning process) in selecting training samples on-the-fly by a human user/annotator. The reinforcement learning reward is the uncertainty value of each human selected sample. A binary feedback (positive or negative) labelled by the human annotator is used to select the samples of which are used to fine-tune a pre-trained CNN Re-ID model. Extensive experiments demonstrate the superiority of our DRAL method for deep reinforcement learning based human-in-the-loop person Re-ID when compared to existing unsupervised and transfer learning models as well as active learning models.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00622,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/64521/2/Gong%20Deep%20reinforcement%20active%202020%20Accepted.pdf
ba82447e42ec9d21badd8172e830af9f8266657c,1,1,0,Dynamic Gallery for Real-Time Multi-Target Multi-Camera Tracking,"For multi-target multi-camera recognition tasks, tracking of objects of interest is one of the essential yet challenging issues due to the fact that the task requires re-identifying identical targets across distinct views. Multi-target multi-camera tracking (MTMCT) applications span a wide range of variety (e.g. crowd behavior analysis, anomaly individual tracking and sport player tracking), so how to make the system perform real-time tracking becomes a crucial research issue. In this paper, we propose an online hierarchical algorithm for extreme clustering based MTMCT framework. The system can automatically create a dynamic gallery with real-time fashion by collecting appearance information of multi-object tracking in single-camera view. We evaluate the effectiveness and efficiency of our framework, and compare the state-of-the-art methods on MOT16 as well as DukeMTMC for single and multiple camera tracking. The high-frame-rate performance and promising tracking results confirm our system can be used in realworld applications.",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909837,
bab66082d01b393e6b9e841e5e06782a6c61ec88,1,1,0,Pose-Driven Deep Models for Person Re-Identification,"Person re-identification (re-id) is the task of recognizing and matching persons at different locations recorded by cameras with non-overlapping views. One of the main challenges of re-id is the large variance in person poses and camera angles since neither of them can be influenced by the re-id system. In this work, an effective approach to integrate coarse camera view information as well as fine-grained pose information into a convolutional neural network (CNN) model for learning discriminative re-id embeddings is introduced. In most recent work pose information is either explicitly modeled within the re-id system or explicitly used for pre-processing, for example by pose-normalizing person images. In contrast, the proposed approach shows that a direct use of camera view as well as the detected body joint locations into a standard CNN can be used to significantly improve the robustness of learned re-id embeddings. On four challenging surveillance and video re-id datasets significant improvements over the current state of the art have been achieved. Furthermore, a novel reordering of the MARS dataset, called X-MARS is introduced to allow cross-validation of models trained for single-image re-id on tracklet data.",2018,ArXiv,1803.08709,,https://arxiv.org/pdf/1803.08709.pdf
bac41d228c071eaf8d21401e34157f4e793a41f9,1,1,0,Uniformity Attentive Learning-Based Siamese Network for Person Re-Identification,"Person re-identification (Re-ID) has a problem that makes learning difficult such as misalignment and occlusion. To solve these problems, it is important to focus on robust features in intra-class variation. Existing attention-based Re-ID methods focus only on common features without considering distinctive features. In this paper, we present a novel attentive learning-based Siamese network for person Re-ID. Unlike existing methods, we designed an attention module and attention loss using the properties of the Siamese network to concentrate attention on common and distinctive features. The attention module consists of channel attention to select important channels and encoder-decoder attention to observe the whole body shape. We modified the triplet loss into an attention loss, called uniformity loss. The uniformity loss generates a unique attention map, which focuses on both common and discriminative features. Extensive experiments show that the proposed network compares favorably to the state-of-the-art methods on three large-scale benchmarks including Market-1501, CUHK03 and DukeMTMC-ReID datasets.",2020,Sensors,,10.3390/s20123603,https://pdfs.semanticscholar.org/df62/090ec4924cc37a07615f5a785216dbfe34b4.pdf
badf092d7daee1cab35d2d584778f44d72b5d871,0,1,0,"Enhancing Person Retrieval with Joint Person Detection, Attribute Learning, and Identification","Person re-identification receives increasing attention in recent years. However, most works assume the persons have been well cropped from the whole scene images, and only focus on learning features and metrics. This paper considers the person re-identification problem in a real-world scenario, which should consider detection and identification simultaneously. This paper proposes a multi-task learning framework for person retrieval in the wild. Person attribute learning is exploited in our framework to enhance person retrieval. Our work consists of two main contributions: (1) we present a 11 image-level attribute annotations for each image in the large-scale PRW [27] dataset, and (2) we develop an end-to-end person retrieval framework which jointly learns person detector, attribute detectors, and visual embeddings in a multi-task learning manner. We evaluate the effectiveness of the proposed approach on two tasks, i.e. person attribute recognition and person re-identification. Experimental results have demonstrated the effectiveness of the proposed approach.",2018,PCM,,10.1007/978-3-030-00767-6_11,
baf8f45a749172a417e566698282659770287b4f,0,1,0,Spatial Preserved Graph Convolution Networks for Person Re-identification,"Person Re-identification is a very challenging task due to inter-class ambiguity caused by similar appearances, and large intra-class diversity caused by viewpoints, illuminations, and poses. To address these challenges, in this article, a graph convolution network based model for person re-identification is proposed to learn more discriminative feature embeddings, where a graph-structured relationship between person images and person parts are together integrated. Graph convolution networks extract common characteristics of the same person, while pyramid feature embedding exploits parts relations and learns stable representation with each person image. We achieve a very competitive performance respectively on three widely used datasets, indicating that the proposed approach significantly outperforms the baseline methods and achieves the state-of-the-art performance.",2020,,,10.1145/3362988,
bafa74ff81e5de7d33bce36106de38d5ba585baf,0,1,1,Real-Time Person Re-Identification for Mobile Robots to Improve Human-Robot Interaction,"Mobile robots operating in seniors’ homes can serve as social companions and assist with daily tasks, thus enhancing the seniors’ quality of life [104]. In order for robots to assist seniors, it is crucial that they are equipped with sets of social and interactive skills to enable them to have natural and personalized interactions. Personalized interactions, such as using patients’ proper names or remembering personal preferences, is necessary to establish strong social relationships [4, 45], and is a key factor to improve trust in human-robot interaction [37]. A prerequisite for robots to achieve personalized interactions, however, is the ability to automatically recognize and re-identify people around them [4]. Existing person re-identification systems for mobile robots are highly restricted in terms of where robots can operate, and do not stimulate natural and personalized interactions because they need preliminary knowledge about the robot’s users [12, 18], rely on facial cues [113, 115], or use data collected from external sensors [45]. This thesis introduces two lightweight Siamese convolutional neural networks, LuNet Light and LuNet Lightest, designed for the problem of person re-identification in a robotic setting without relying on the aforementioned restrictions. Despite being significantly more lightweight than other person re-identification systems [3, 120], LuNet Lightest achieves near state-ofthe-art results on the MARS dataset evaluation protocols [135]. This thesis additionally presents a set of evaluation measures tailored to evaluate reidentification systems for robots operating in various environments. When simulating crowded environments, LuNet Lightest reaches 92.4% balanced accuracy on the proposed evaluation protocol. As a result of the lightweight architecture, LuNet Lightest achieves real-time frame-rates of 71.6 frames per second when using a GPU, 33.9 frames per second when using a CPU without GPU, and 15.7 frames per second when using only one core of the same CPU, rendering the proposed system highly suitable for low-cost, hardware-constrained robots. The proposed person re-identification system will enable assistive mobile robots to robustly and accurately identify their users, and is a preliminary step to improve trust and attain natural and personalized interaction between robots and patients.",2019,,,,
bafc809b6258d837932e50502ab4697c3bb6e8bd,0,1,0,Efficient Feature Extraction for Person Re-Identification via Distillation,"Person re-identification has received increasing attention due to the high performance achieved by new methods based on deep learning. With larger networks of cameras being deployed, more surveillance videos need to be parsed, and extracting features for each frame remains a bottleneck. In addition, the feature extraction needs to be robust to images captured in a variety of scenarios. We propose using deep neural network distillation for training a feature extractor with a lower computational cost, while keeping track of its cross-domain ability. In the end, the proposed model is three times faster, without a decrease in accuracy. Results are validated on two popular person re-identification benchmark datasets and compared to a solution using ResNet.",2019,2019 27th European Signal Processing Conference (EUSIPCO),,10.23919/EUSIPCO.2019.8903080,
bb0b11a20c9460e1cbac1a0ab1aaeb7c060bd64d,0,1,0,Person re-identification algorithm based on the fusion of deep feature and LOMO feature,"Person Re-identification is a sub-problem of image retrieval, using computer vision techniques to judge whether a certain identical pedestrian exists among different images or video sequences, which has attracted more and more attention of researchers. In this paper, regarding the fact that under non-overlapping multi-camera, traditional handcrafted features have a limited presentation power in re-identifying the pedestrians and that deep features have complicated parameters while training. A re-identification method based on the deep fusion of handcrafted features and deep features was proposed, which cut down the number of parameters but still guaranteed the accuracy, achieving the advancement of both precision and capacity. In our model, the LOMO algorithm is used to extract the handcrafted features from the images first. Then, the dimensionality of those features are reduced by Guassian Pooling for efficiency. After that, they are connected to the deep fusion network with the deep features extracted from the same images by a modification of ResNet50. Finally, the fused features are sent to the classifier for the re-identification. In the training process, we proposed a training strategy called Gradient Freezing after studying the training details in the application of transfer learning on neural network. Experiments have proved that the accuracy of applying the deep fusion network that fused with deep features and handcrafted features is 30% higher than that of the ResNet50 alone, and that the time it consumes is reduced by 10 epoches through the gradient freezing method. Moreover, several experiments carried out on dataset Marketl501 indicate that under Single Query on Marketl501, Rankl(the probability of matching successfully for the first time) can reach a high number of 81.74% and mAP(mean average Precision) of 68.75%.",2019,International Conference on Digital Image Processing,,10.1117/12.2541005,
bb4f83458976755e9310b241a689c8d21b481238,0,1,0,Improving Face Verification and Person Re-Identification Accuracy Using Hyperplane Similarity,"The standard framework for using a convolutional neural network (CNN) for face verification is to compare the feature vectors taken from the penultimate network layer of a CNN trained to classify the identity of an input face using a softmax loss over identities. Feature vectors are typically compared using the simple L2 distance. We demonstrate that the L2 distance is not the best distance to use in this scenario, and propose the hyperplane similarity as a more appropriate similarity function that is derived from the soft-max loss function used to train the network. We demonstrate that hyperplane similarity improves verification results especially for low false acceptance rates which are usually the most important operating regimes for real applications. We also propose a fast algorithm for finding the separating hyperplanes needed to compute hyperplane similarity.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.183,http://www.merl.com/publications/docs/TR2017-155.pdf
bb52afd06e51a7b96802f6db9bd18cbc80b9a848,1,0,0,Multiple hypothesis tracking algorithm for multi-target multi-camera tracking with disjoint views,"In this study, a multiple hypothesis tracking (MHT) algorithm for multi-target multi-camera tracking (MCT) with disjoint views is proposed. The authors' method forms track-hypothesis trees, and each branch of them represents a multi-camera track of a target that may move within a camera as well as move across cameras. Furthermore, multi-target tracking within a camera is performed simultaneously with the tree formation by manipulating a status of each track hypothesis. Each status represents three different stages of a multi-camera track: tracking , searching , and end-of-track . The tracking status means targets are tracked by a single camera tracker. In the searching status, the disappeared targets are examined if they reappear in other cameras. The end-of-track status does the target exited the camera network due to its lengthy invisibility. These three status assists MHT to form the track-hypothesis trees for multi-camera tracking. Furthermore, a gating  technique which eliminates the unlikely observation-to-track association using space-time information has been introduced. In the experiments, the proposed method has been tested using two datasets, DukeMTMC and NLPR\_MCT, which demonstrates that the method outperforms the state-of-the-art method in terms of improvement of the accuracy. In addition, real-time and online performance of proposed method is also showed in this study.",2018,IET Image Process.,1901.08787,10.1049/iet-ipr.2017.1244,https://arxiv.org/pdf/1901.08787.pdf
bbd6c67f6268cd69b1eea02ffb8e018e8e88ce0e,1,0,0,HorNet: A Hierarchical Offshoot Recurrent Network for Improving Person Re-ID via Image Captioning,"Person re-identification (re-ID) aims to recognize a person-of-interest across different cameras with notable appearance variance. Existing research works focused on the capability and robustness of visual representation. In this paper, instead, we propose a novel hierarchical offshoot recurrent network (HorNet) for improving person re-ID via image captioning. Image captions are semantically richer and more consistent than visual attributes, which could significantly alleviate the variance. We use the similarity preserving generative adversarial network (SPGAN) and an image captioner to fulfill domain transfer and language descriptions generation. Then the proposed HorNet can learn the visual and language representation from both the images and captions jointly, and thus enhance the performance of person re-ID. Extensive experiments are conducted on several benchmark datasets with or without image captions, i.e., CUHK03, Market-1501, and Duke-MTMC, demonstrating the superiority of the proposed method. Our method can generate and extract meaningful image captions while achieving state-of-the-art performance.",2019,IJCAI,1908.04915,10.24963/ijcai.2019/742,https://arxiv.org/pdf/1908.04915.pdf
bbdc130ec6a448c11f4a1e507175062013296d82,1,1,0,Person re-identification based on multi-level feature complementarity of cross-attention with part metric learning,"Person re-identification is an image retrieval task, and its task is to perform a person matching in different cameras by a given person target. This research has been noticed and studied by more and more people. However, pose changes and occlusions often occur during a person walking. Especially in the most related methods, local features are not used to simply and effectively solve the problems of occlusion and pose changes. Moreover, the metric loss functions only consider the image-level case, and it cannot adjust the distance between local features well. To tackle the above problems, a novel person re-identification scheme is proposed. Through experiments, we found that we paid more attention to different parts of a person when we look at him from a horizontal or vertical perspective respectively. First, in order to solve the problem of occlusion and pose changes, we propose a Cross Attention Module (CAM). It enables the network to generate a cross attention map and improve the accuracy of person re-identification via the enhancement of the most significant local features of persons. The horizontal and vertical attention vectors of the feature maps are extracted and a cross attention map is generated, and the local key features are enhanced by this attention map. Second, in order to solve the problem of the lack of expression ability of the single-level feature maps, we propose a Multi-Level Feature Complementation Module (MLFCM). In this module, the missing information of high-level features is complemented by low-level features via short skip. Feature selection is also performed among deep features maps. The purpose of this module is to get the feature maps with complete information. Further, this module solves the problem of missing contour features in high-level semantic features. Third, in order to solve the problem that the current metric loss function cannot adjust the distance between local features, we propose Part Triple Loss Function (PTLF). It can reduce both within-class and increase between-class distance of the person parts. Experimental results show that our model achieves high values on Rank-k and mAP on Market-1501, Duke-MTMC and CUHK03-NP.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-08972-w,
bbffadf57625f262667caba3a38b0695efc37967,1,1,1,Semantic Alignment in Multiple Stages of Networks for Person Re-ID: Towards Generalizable Models,"Person re-identification (re-ID) is a task that aims to associate the same people across different cameras. One of the many important problems a person re-ID system has to address in order to achieve good performance is the feature misalignment problem. Past research has attempted to address this problem by using attention networks, pose-estimation modules, or semantic segmentation networks. However, they all eventually tend to pool these features to a single feature embedding, thereby not distinguishing regions with different semantic meanings such as the head, torso, and lower body. Most approaches also do not make use of all the information available throughout multiple layers (stages) of the feature extractor. Furthermore, although these additional features are used to provide extra information to the re-ID network during training, they do not take into account the importance of different regions of the image due to, for example, occlusion. To circumvent these problems, we propose a network that is capable of extracting regional feature embeddings that are associated with specific body parts of an identity, i.e., head, upper-body, lower-body, shoes, and foreground image. We extract these features from multiple stages of a feature extractor using a semantic-segmentation module. We then use multi-branch learning to ensure that these features are independently optimized by introducing separate modules (branches) for each regional feature embedding. To increase the robustness of the model, we also propose a novel testing strategy that makes use of the importance and visibility of specific body parts in both the query and gallery images in order to calculate a ranking list. Finally, to address the current lack of datasets that contain images from overhead face-down cameras, we introduce a new dataset named MatchNMingle-reID. Because of the viewpoint of the cameras, this dataset presents unique challenges that are not seen in current datasets and opens possibilities to create more generalizable models that can effectively address the feature misalignment problem.",2019,,,,https://pdfs.semanticscholar.org/bbff/adf57625f262667caba3a38b0695efc37967.pdf
bc2563999c44e18d15285ce795097e4db1685d48,1,0,0,Crowdsourcing-Based Ranking Aggregation for Person Re-Identification,"Person re-identification (re-ID) is widely applied in surveillance and criminal detection applications. The existing research focus on devising the stand-alone re-ID methods, ignoring their practical application in the multi-person collaboration scenario. To improve the search efficiency, a group of investigators are usually assigned the same task to re-identify a suspect from a shared gallery set. Due to their personalized viewpoints and search feedback operations, different investigators may obtain diverse search results of the same query target. In this case, merging different rankings and generating an improved result is of great importance. To this end, this paper proposes a crowdsourcing-based ranking aggregation to adaptively fuse multiple ranking lists for re-ID problem. The method estimates the reliability of individual investigators, with a specifically designed long tail distribution to fit the top ranking demand, and is feasible for human-machine interaction. Extensive experiments conducted on four dataset-s demonstrate the superiority of the proposed method.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053496,
bc3cb9cfc77665fc2ef0c6391d5212e405cad464,1,1,0,Person re-identification via adaptive verification loss,"Abstract Person re-identification (re-ID) is the task of associating the image patches of the identical person across disjoint cameras, which is a very challenging topic in computer vision. Recently, with the advance of deep learning techniques and the public of huge datasets, person re-ID has achieved a great success. However, an obvious but ignored issue is that the sample pairs are biased, where the negative sample pairs are far more than the positive ones. In this paper, we develop an adaptive verification loss, termed as ADV-Loss to handle the imbalance of sample pairs. Our ADV-Loss empowers the popular verification loss with the flexibility of adaptively addressed the hard triple units. Specifically, we learn a re-weighted strategy from the triplet loss to balance the sample pairs. According to the objective values of triple loss, the hard triple units will be endowed with larger weights, while the less important triple units are de-emphasized or simply dropped. Thus, ADV-Loss, on the basis of the cross-entropy loss, handles the biased issue by only holding the equal number but informative sample pairs. Moreover, theoretical analysis of our designed ADV-Loss is provided to guarantee the important sample pairs not to be despised during the backpropagation. Ablation analysis verifies the potential efficacy of the proposed loss. Moreover, experiments of person re-ID on three large-scale benchmark datasets show that two popular deep networks with the proposed loss achieve satisfactory performance against several well-established person re-ID methods.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.05.037,
bc3df092a7ae8f5b54045194f182bc4ff06300a4,1,0,0,VMRFANet: View-Specific Multi-Receptive Field Attention Network for Person Re-identification,"Person re-identification (re-ID) aims to retrieve the same person across different cameras. In practice, it still remains a challenging task due to background clutter, variations on body poses and view conditions, inaccurate bounding box detection, etc. To tackle these issues, in this paper, we propose a novel multi-receptive field attention (MRFA) module that utilizes filters of various sizes to help network focusing on informative pixels. Besides, we present a view-specific mechanism that guides attention module to handle the variation of view conditions. Moreover, we introduce a Gaussian horizontal random cropping/padding method which further improves the robustness of our proposed network. Comprehensive experiments demonstrate the effectiveness of each component. Our method achieves 95.5% / 88.1% in rank-1 / mAP on Market-1501, 88.9% / 80.0% on DukeMTMC-reID, 81.1% / 78.8% on CUHK03 labeled dataset and 78.9% / 75.3% on CUHK03 detected dataset, outperforming current state-of-the-art methods.",2020,ICAART,2001.07354,10.5220/0008917004130420,https://arxiv.org/pdf/2001.07354.pdf
bc8aee2f6b7d9d7b126d1e8dbcd33ea355aa3b37,0,1,0,SeqFace: Make full use of sequence information for face recognition,"Deep convolutional neural networks (CNNs) have greatly improved the Face Recognition (FR) performance in recent years. Almost all CNNs in FR are trained on the carefully labeled datasets containing plenty of identities. However, such high-quality datasets are very expensive to collect, which restricts many researchers to achieve state-of-the-art performance. In this paper, we propose a framework, called SeqFace, for learning discriminative face features. Besides a traditional identity training dataset, the designed SeqFace can train CNNs by using an additional dataset which includes a large number of face sequences collected from videos. Moreover, the label smoothing regularization (LSR) and a new proposed discriminative sequence agent (DSA) loss are employed to enhance discrimination power of deep face features via making full use of the sequence data. Our method achieves excellent performance on Labeled Faces in the Wild (LFW), YouTube Faces (YTF), only with a single ResNet. The code and models are publicly available on-line (this https URL).",2018,ArXiv,1803.06524,,https://arxiv.org/pdf/1803.06524.pdf
bcad284af2a484d508a695dc534b0363812b1993,1,0,0,Inserting Videos Into Videos,"In this paper, we introduce a new problem of manipulating a given video by inserting other videos into it. Our main task is, given an object video and a scene video, to insert the object video at a user-specified location in the scene video so that the resulting video looks realistic. We aim to handle different object motions and complex backgrounds without expensive segmentation annotations. As it is difficult to collect training pairs for this problem, we synthesize fake training pairs that can provide helpful supervisory signals when training a neural network with unpaired real data. The proposed network architecture can take both real and fake pairs as input and perform both supervised and unsupervised training in an adversarial learning scheme. To synthesize a realistic video, the network renders each frame based on the current input and previous frames. Within this framework, we observe that injecting noise into previous frames while generating the current frame stabilizes training. We conduct experiments on real-world videos in object tracking and person re-identification benchmark datasets. Experimental results demonstrate that the proposed algorithm is able to synthesize long sequences of realistic videos with a given object video inserted.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.06571,10.1109/CVPR.2019.01030,https://arxiv.org/pdf/1903.06571.pdf
bd065cb99ad7236e2222604b7a4ef992447c565c,1,0,1,Robust Re-Identification by Multiple Views Knowledge Distillation,"To achieve robustness in Re-Identification, standard methods leverage tracking information in a Video-To-Video fashion. However, these solutions face a large drop in performance for single image queries (e.g., Image-To-Video setting). Recent works address this severe degradation by transferring temporal information from a Video-based network to an Image-based one. In this work, we devise a training strategy that allows the transfer of a superior knowledge, arising from a set of views depicting the target object. Our proposal - Views Knowledge Distillation (VKD) - pins this visual variety as a supervision signal within a teacher-student framework, where the teacher educates a student who observes fewer views. As a result, the student outperforms not only its teacher but also the current state-of-the-art in Image-To-Video by a wide margin (6.3% mAP on MARS, 8.6% on Duke-Video-ReId and 5% on VeRi-776). A thorough analysis - on Person, Vehicle and Animal Re-ID - investigates the properties of VKD from a qualitatively and quantitatively perspective. Code is available at this https URL.",2020,ECCV,2007.04174,10.1007/978-3-030-58607-2_6,https://arxiv.org/pdf/2007.04174.pdf
bd27161a32d1f6c1d9b62b46ce23878046a57b98,1,0,0,"Exploiting Point Motion, Shape Deformation, and Semantic Priors for Dynamic 3D Reconstruction in the Wild","With the advent of affordable and high-quality smartphone cameras, any significant events will be massively captured both actively and passively from multiple perspectives.This opens up exciting opportunities for low-cost high-end VFX effects and large scale media analytics. However, automatically organizing large scale visual data and creating a comprehensive 3D scene model is still an unsolved problem. State of the art 3D reconstruction algorithms are mostly applicable to static scenes, mainly due to the lack of triangulation constraints for dynamic objects observedby unsynchronized cameras and the difficulties in finding reliable correspondences across cameras in diverse and dynamic settings. This thesis aims to provide a computational pipeline for high-quality 3D reconstructionof the dynamic scene captured by multiple unsynchronized video cameras in the wild. The key is to exploit the physics of motion dynamics, shape deformation, scene semantics, and the interplay between them. Toward this end, this thesismakes four enabling technical contributions. First, this thesis introduces a spatiotemporal bundle adjustment algorithm to accurately estimate a sparse set of 3D trajectories of dynamic objects from multiple unsynchronized mobile video cameras. The lack of triangulation constraint on dynamic points is solved by carefully integrating physics-based motion prior describing how points move over time. This algorithm takes advantage of the unsynchronized video streams to estimate 3D motion reconstruction in the wild at much higher temporalresolution than the input videos. Second, this thesis presents a simple but powerful self-supervised framework toadapt a generic person appearance descriptor to the unlabeled videos by exploiting motion tracking, mutual exclusion constraints, and multi-view geometry without anymanual annotations. The adapted descriptor is strongly discriminative and enables a tracking-by-clustering formulation. This advantage enables a first-of-a-kind accurateand consistent markerless motion tracking of multiple people participating in a complex group activity from mobile cameras in the wild with further application tomulti-angle video cutting for intuitive tracking visualization.Third, this thesis creates a framework for 3D tracking of the rigidly moving objects even in severe occlusions by fusing single-view unstructured tracklets and multi-view semantic structured keypoints reconstruction. No spatial correspondences are needed for the unstructured points. No temporal correspondences are needed for the structured points. The imprecise but accurate 3D structured keypoint is compensated by the sparse but precise 3D unstructured tracks, leading to improvements in both structured keypoints localization and motion tracking of the entire object.Fourth, this thesis presents a single-shot illumination decomposition method for dense dynamic shape capture of highly textured surfaces illuminated by multiple projectors. The decomposition scheme assumes smooth shape deformation and can accurately recover the illumination image of different projectors and the texture imagesof the scene from their mixed appearances.",2019,,,10.1184/R1/9816818.V1,http://www.cs.cmu.edu/~ILIM/publications/PDFs/MV-THESIS-19.pdf
bd963ee8ae9afee34b766913a9891b1db36275ca,0,1,0,A novel data augmentation scheme for pedestrian detection with attribute preserving GAN,"Abstract Recently pedestrian detection has progressed significantly. However, detecting pedestrians of small scale or in heavy occlusions is still notoriously difficult. Besides, the generalization ability of pre-trained detectors across different datasets remains to be improved. Both of these issues can be attributed to insufficient training data coverage. To cope with this, we present an efficient data augmentation scheme by transferring pedestrians from other datasets into the target scene with a novel Attribute Preserving Generative Adversarial Networks (APGAN). The proposed methodology consists of two steps: pedestrian embedding and style transfer. The former step can simulate pedestrian images of various scale and occlusion, in any pose or background, thus greatly promoting the data variation. The latter step aims to make the generated samples more realistic while guarantee the data coverage. To achieve this goal, we propose APGAN, which pursues both good visual quality and attribute preserving after style transfer. With the proposed method, we can make effective sample augmentations to improve the generalization ability of the trained detectors and enhance its robustness to scale change and occlusions. Extensive experiment results validate the effectiveness and advantages of our method.",2020,Neurocomputing,,10.1016/j.neucom.2020.02.094,
bdcc60fbc992cf34ad714cad9a327f572c100012,1,1,0,Perceive Where to Focus: Learning Visibility-Aware Part-Level Features for Partial Person Re-Identification,"This paper considers a realistic problem in person re-identification (re-ID) task, i.e., partial re-ID. Under partial re-ID scenario, the images may contain a partial observation of a pedestrian. If we directly compare a partial pedestrian image with a holistic one, the extreme spatial misalignment significantly compromises the discriminative ability of the learned representation. We propose a Visibility-aware Part Model (VPM) for partial re-ID, which learns to perceive the visibility of regions through self-supervision. The visibility awareness allows VPM to extract region-level features and compare two images with focus on their shared regions (which are visible on both images). VPM gains two-fold benefit toward higher accuracy for partial re-ID. On the one hand, compared with learning a global feature, VPM learns region-level features and thus benefits from fine-grained information. On the other hand, with visibility awareness, VPM is capable to estimate the shared regions between two images and thus suppresses the spatial misalignment. Experimental results confirm that our method significantly improves the learned feature representation and the achieved accuracy is on par with the state of the art.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.00537,10.1109/CVPR.2019.00048,https://arxiv.org/pdf/1904.00537.pdf
bddba49c6dd0863b1950dfd2a5bdc9856937d457,1,0,0,A Detailed performance of alternative methods for recognition of social relation 35 B Contact information 35,,2018,,,,https://pdfs.semanticscholar.org/bddb/a49c6dd0863b1950dfd2a5bdc9856937d457.pdf
be01ce9c11f1187b6e395322b8517348264df63b,1,0,0,DotSCN: Group Re-identification via Domain-Transferred Single and Couple Representation Learning.,"Group re-identification (G-ReID) is an important yet less-studied task. Its challenges not only lie in appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also derive from group layout and membership changes. So the key task of G-ReID is to learn representations robust to such changes. To address this issue, we propose a Transferred Single and Couple Representation Learning Network (TSCN). Its merits are two aspects: 1) Due to the lack of labelled training samples, existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To gain the superiority of deep learning models, we treat a group as multiple persons and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style to learn single representations. 2) Taking into account the neighborhood relationship in a group, we further propose learning a novel couple representation between two group members, that achieves more discriminative power in G-ReID tasks. In addition, an unsupervised weight learning method is exploited to adaptively fuse the results of different views together according to result patterns. Extensive experimental results demonstrate the effectiveness of our approach that significantly outperforms state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by 39.0\% CMC-1 on the DukeMCMT dataset.",2020,,1905.04854,10.1109/tcsvt.2020.3031303,https://arxiv.org/pdf/1905.04854.pdf
be501779ba57eb1ecc1d9ec96b59e7bdd365f2f3,0,1,0,Person Re-Identification Based on Attribute Heterogeneity,"In order to improve the recognition accuracy of person re-identification. We proposed a neural network model based on heterogeneity of person attributes. Compared with the existing methods, this model has the following two advantages. First, based on the heterogeneity between attributes. We design the different identification methods to identification different kinds of attributes. Second, aim at the different of loss measurement for the attribute recognition method. We proposed an algorithm consistent with the heterogeneous loss function. The experimental results show that compared with the existing methods, the proposed model is effectively enhance the person recognition rank-1 accuracy. Ours method reached the rank-1 accuracy 88.13% on Market1501, 74.96% on DukeMTMC and 77.64% on PETA dataset.",2018,2018 2nd International Conference on Data Science and Business Analytics (ICDSBA),,10.1109/ICDSBA.2018.00077,
be64826a5eebee8f604322abf06b42ceba2a0cab,1,1,0,Camera Style and Identity Disentangling Network for Person Re-identification,"Camera style (camstyle) is a main factor that affects the performance of person reidentification (ReID). In the past years, existing works mainly exploit implicit solutions from the inputs by designing some strong constraints. However, these methods cannot consistently work as the camstyle still exists in the inputs as well as in the intermediate features. To address this problem, we propose a Camstyle-Identity Disentangling (CID) network for person ReID. More specifically, we disentangle the ID feature and camstyle feature in the latent space. In order to disentangle the features successfully, we present a Camstyle Shuffling and Retraining (CSR) scheme to generate more ID-preserved and camstyle variation samples for training. The proposed scheme ensures the success of disentangling and is able to eliminate the camstyle features in the backbone during the training process. Numerous experimental results on the Market-1501 and DukeMTMCreID datasets demonstrate that our network can effectively disentangle the features and facilitate the person ReID networks.",2019,BMVC,,,https://bmvc2019.org/wp-content/uploads/papers/0335-paper.pdf
be79ad118d0524d9b493f4a14a662c8184e6405a,1,1,0,Attend and Align: Improving Deep Representations with Feature Alignment Layer for Person Retrieval,"In fine-grained recognition, object misalignment and background noise are two long-standing factors that influence the robustness of deep learning models. This paper mainly focuses on person re-identification (re-ID) and introduces a feature alignment layer (FAL) which alleviates the target misalignment and the background noise simultaneously. Through attention mechanism, FAL informs the underlying importance of each pixel on feature maps, i.e., whether the pixel is beneficial towards discriminating different persons. Then the discriminative regions relocate to the center and are stretched to fill the feature maps. Such an “attend and align” mechanism is specified into two steps: target position prediction and value assignment. In the first step, a pixel on feature maps learns to find a target position which is ID-discriminative. In the second step, the pixel is assigned with a new value using the context of the predicted position. Moreover, FAL can be easily plugged into a canonical Convolutional Neural Network (CNN) and learned in an end-to-end manner. In experiment, our method yields competitive results compared with the state-of-the-art approaches on three person re-ID datasets, Market-1501, DukeMTMC-reID and CUHK03. We also demonstrate that our method improves a competitive fine-grained recognition baseline on CUB-200-2011.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545850,
be90dc791ebc764e242b24b83ff40ba797c09114,1,0,0,Key-Track: A Lightweight Scalable LSTM-based Pedestrian Tracker for Surveillance Systems,"There has been a growing interest in leveraging state of the art deep learning techniques for tracking objects in recent years. Most of this work focuses on using redundant appearance models for predicting object tracklets for the next frame. Moreover, not much work has been done to explore the sequence learning properties of Long Short Term Memory (LSTM) Neural Networks for object tracking in video sequences. In this work we propose a novel LSTM tracker, Key-Track, which effectively learns the spatial and temporal behavior of pedestrians after analyzing movement patterns of human key-points provided to it by OpenPose [3]. We train Key-Track on single person sequences that we curated from the Duke Multi-target Multi-Camera (Duke-MTMC) [26] dataset and scale it to track multiple people at run-time, further testing its scalability. We report our results on the Duke-MTMC dataset for different time-series sequence lengths we feed to Key-Track and find three as the optimum time-step sequence length producing the highest Average Overlap Score (AOS). We further present our qualitative analysis on these different time-series sequence lengths producing different results depending on the type of video sequence. The total observed size of Key-Track is under 1 megabytes which paves its way into mobile devices for the purpose of tracking in real-time.",2019,ICIAR,,10.1007/978-3-030-27272-2_18,
bf037ae7e27d60514428126bb840f3674fac981f,0,1,0,Continual Representation Learning for Biometric Identification,"With the explosion of digital data in recent years, continuously learning new tasks from a stream of data without forgetting previously acquired knowledge has become increasingly important. In this paper, we propose a new continual learning (CL) setting, namely ``continual representation learning'', which focuses on learning better representation in a continuous way. We also provide two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant. In contrast to requiring the model to recognize more learned classes, we aim to learn feature representation that can be better generalized to not only previously unseen images but also unseen classes/identities. For the new setting, we propose a novel approach that performs the knowledge distillation over a large number of identities by applying the neighbourhood selection and consistency relaxation strategies to improve scalability and flexibility of the continual learning model. We demonstrate that existing CL methods can improve the representation in the new setting, and our method achieves better results than the competitors.",2020,ArXiv,2006.04455,,https://arxiv.org/pdf/2006.04455.pdf
bf3dbd231790cf3f9c50af6a0a958482762e48ea,0,1,0,Self-Guided Hash Coding for Large-Scale Person Re-identification,"The laborious manual person ID annotation results in limited training data and increased difficulty in learning discriminative representations. Meanwhile, high dimensional deep features are not ready for fast indexing and matching. Those challenges hinder the application of person Re-Identification (ReID) in large-scale data. To conquer those challenges, we propose a novel training strategy to learn compact binary hash codes. To facilitate feature learning, person images are decomposed into body parts, which are then composed across images into new positive and negative training samples. Binary code quality restrictions are also applied the during training procedure. Requiring no extra annotation costs, our algorithm iteratively generates hard training samples by itself and makes discriminative hash code learning with a limited number of labeled data possible. We hence use ""self-guided"" to describe this training procedure. Extensive experiments are conducted on two large-scale person ReID datasets, i.e., Market1501 and MSMT17 with distractors, showing our method is competitive compared with recent works.",2019,2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),,10.1109/MIPR.2019.00051,
bf549ded8c9ee32077a894f227886a07e8a62d25,0,1,0,Deep Imbalanced Attribute Classification using Visual Attention Aggregation,"For many computer vision applications, such as image description and human identification, recognizing the visual attributes of humans is an essential yet challenging problem. Its challenges originate from its multi-label nature, the large underlying class imbalance and the lack of spatial annotations. Existing methods follow either a computer vision approach while failing to account for class imbalance, or explore machine learning solutions, which disregard the spatial and semantic relations that exist in the images. With that in mind, we propose an effective method that extracts and aggregates visual attention masks at different scales. We introduce a loss function to handle class imbalance both at class and at an instance level and further demonstrate that penalizing attention masks with high prediction variance accounts for the weak supervision of the attention mechanism. By identifying and addressing these challenges, we achieve state-of-the-art results with a simple attention mechanism in both PETA and WIDER-Attribute datasets without additional context or side information.",2018,ECCV,1807.03903,10.1007/978-3-030-01252-6_42,https://arxiv.org/pdf/1807.03903.pdf
bf5dc0958e04a4c77ed70593c90c274c56882eff,0,1,0,Disjoint Label Space Transfer Learning with Common Factorised Space,"In this paper, a unified approach is presented to transfer learning that addresses several source and target domain label-space and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.",2019,AAAI,1812.02605,10.1609/aaai.v33i01.33013288,https://arxiv.org/pdf/1812.02605.pdf
bf9a30ee3ef6320780c6980f21f3c3ea43fc2c62,1,0,0,PANDA: A Gigapixel-Level Human-Centric Video Dataset,"We present PANDA, the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world scenes with both wide field-of-view (~1 square kilometer area) and high-resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100× scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions. We benchmark the human detection and tracking tasks. Due to the vast variance of pedestrian pose, scale, occlusion and trajectory, existing approaches are challenged by both accuracy and efficiency. Given the uniqueness of PANDA with both wide FoV and high resolution, a new task of interaction-aware group detection is introduced. We design a ‘global-to-local zoom-in’ framework, where global trajectories and local interactions are simultaneously encoded, yielding promising results. We believe PANDA will contribute to the community of artificial intelligence and praxeology by understanding human behaviors and interactions in large-scale real-world scenes. PANDA Website: http://www.panda-dataset.com.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.04852,10.1109/cvpr42600.2020.00333,https://arxiv.org/pdf/2003.04852.pdf
bfab573b064217d0533f2d1e8651746cf295c182,0,1,0,Single Camera Training for Person Re-identification,"Person re-identification (ReID) aims at finding the same person in different cameras. Training such systems usually requires a large amount of cross-camera pedestrians to be annotated from surveillance videos, which is labor-consuming especially when the number of cameras is large. Differently, this paper investigates ReID in an unexplored single-camera-training (SCT) setting, where each person in the training set appears in only one camera. To the best of our knowledge, this setting was never studied before. SCT enjoys the advantage of low-cost data collection and annotation, and thus eases ReID systems to be trained in a brand new environment. However, it raises major challenges due to the lack of cross-camera person occurrences, which conventional approaches heavily rely on to extract discriminative features. The key to dealing with the challenges in the SCT setting lies in designing an effective mechanism to complement cross-camera annotation. We start with a regular deep network for feature extraction, upon which we propose a novel loss function named multi-camera negative loss (MCNL). This is a metric learning loss motivated by probability, suggesting that in a multi-camera system, one image is more likely to be closer to the most similar negative sample in other cameras than to the most similar negative sample in the same camera. In experiments, MCNL significantly boosts ReID accuracy in the SCT setting, which paves the way of fast deployment of ReID systems with good performance on new target scenes.",2020,AAAI,1909.10848,10.1609/AAAI.V34I07.6985,https://arxiv.org/pdf/1909.10848.pdf
bfc8d835a3921362c9add97ddbfb28375cc22ca2,1,1,0,A divide-and-unite deep network for person re-identification,"Person re-identification (person re-ID) is one of the most challenging tasks in the field of computer vision as it involves large variations in human appearances, human poses, background illuminations, camera views, etc. In recent literature, using part-level features for the person re-ID task provides fine-grained information, and has been proven to be effective. Instead of relying on additional skeleton key points or pose estimation models, this paper proposes a Divide-and-Unite Network to obtain feature embedding end-to-end. We design a deep network guided by image contents, which divides pedestrians into parts and obtains the part features with different contributions. These part features and the global feature are united to obtain the pedestrian descriptor for person re-ID. To summarize, the contributions of this work are two-fold. Firstly, a novel architecture of discriminative descriptor learning is proposed, which is based on the global feature and supplemented by part features. Secondly, a Feature Division Network is constructed to generate the part features with different contributions, where the divided parts maintain the consistency of content between different images. Extensive experiments are conducted on three widely-used benchmarks including Market1501, CUHK03, and DukeMTMC-reID. The results have demonstrated that the proposed model can achieve remarkable performance against numerous state-of-the-arts.",2020,,,10.1007/S10489-020-01880-4,
c03e0a90f2cc33f73ce0c305475f87990438a05b,1,0,1,Multi-shot Person Re-identification through Set Distance with Visual Distributional Representation,"Person re-identification aims to identify a specific person at distinct times and locations. It is challenging because of occlusion, illumination, and viewpoint change in camera views. Recently, multi-shot person re-id task receives more attention since it is closer to real-world application. A key point of a good algorithm for multi-shot person re-id is the temporal aggregation of the person appearance features. While most of the current approaches apply pooling strategies and obtain a fixed-size vector representation, these may lose the matching evidence between examples. In this work, we propose the idea of visual distributional representation, which interprets an image set as samples drawn from an unknown distribution in appearance feature space. Based on the supervision signals from a downstream task of interest, the method reshapes the appearance feature space and further learns the unknown distribution of each image set. In the context of multi-shot person re-id, we apply this novel concept along with Wasserstein distance and jointly learn a distributional set distance function between two image sets. In this way, the proper alignment between two image sets can be discovered naturally in a non-parametric manner. Our experiment results on three public datasets show the advantages of our proposed method compared to other state-of-the-art approaches.",2019,ICMR,1808.01119,10.1145/3323873.3325030,https://arxiv.org/pdf/1808.01119.pdf
c0c390dcd3f980dc555e3add7e370e947fffc62c,0,1,0,Deep learning-based methods for person re-identification: A comprehensive review,"Abstract In recent years, person re-identification (ReID) has received much attention since it is a fundamental task in intelligent surveillance systems and has widespread application prospects in numerous fields. Given an image of a pedestrian captured from one camera, the task is to identify this pedestrian from the gallery set captured by other multiple cameras. It is a challenging issue since the appearance of a pedestrian may suffer great changes across different cameras. The task has been greatly boosted by deep learning technology. There are mainly six types of deep learning-based methods designed for this issue, i.e. identification deep model, verification deep model, distance metric-based deep model, part-based deep model, video-based deep model and data augmentation-based deep model. In this paper, we first give a comprehensive review of current six types of deep learning methods. Second, we present the detailed descriptions of existing person ReID datasets. Then, some state-of-the-art performances of methods over recent years on several representative ReID datasets are summarized. Finally, we conclude this paper and discuss the future directions of the person ReID.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.01.079,
c0f01b8174a632448c20eb5472cd9d5b2c595e39,1,1,0,Features for Multi-target Multi-camera Tracking and Re-identification,"Multi-Target Multi-Camera Tracking (MTMCT) tracks many people through video taken from several cameras. Person Re-Identification (Re-ID) retrieves from a gallery images of people similar to a person query image. We learn good features for both MTMCT and Re-ID with a convolutional neural network. Our contributions include an adaptive weighted triplet loss for training and a new technique for hard-identity mining. Our method outperforms the state of the art both on the DukeMTMC benchmarks for tracking, and on the Market-1501 and DukeMTMC-ReID benchmarks for Re-ID. We examine the correlation between good Re-ID and good MTMCT scores, and perform ablation studies to elucidate the contributions of the main components of our system. Code is available1.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.10859,10.1109/CVPR.2018.00632,https://arxiv.org/pdf/1803.10859.pdf
c0fda1dd28e355db93e88eeeb85b420099b475e6,1,1,0,Attribute analysis with synthetic dataset for person re-identification,"Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, have achieved remarkable performance. However, existing synthetic datasets are in small size and lack of diversity, which hinders the development of person re-ID in real-world scenarios. To address this problem, firstly, we develop a large-scale synthetic data engine, the salient characteristic of this engine is controllable. Based on it, we build a large-scale synthetic dataset, which are diversified and customized from different attributes, such as illumination and viewpoint. Secondly, we quantitatively analyze the influence of dataset attributes on re-ID system. To our best knowledge, this is the first attempt to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. Comprehensive experiments help us have a deeper understanding of the fundamental problems in person re-ID. Our research also provides useful insights for dataset building and future practical usage.",2020,ArXiv,2006.07139,,https://arxiv.org/pdf/2006.07139.pdf
c142f7d7f00e016db046f0d7cdb5895f852cfb2e,0,1,0,Learning Visual Instance Retrieval from Failure: Efficient Online Local Metric Adaptation from Negative Samples,"Existing visual instance retrieval (VIR) approaches attempt to learn a faithful global matching metric or discriminative feature embedding offline to cover enormous visual appearance variations, so as to directly use it online on various unseen probes for retrieval. However, their requirement for a huge set of positive training pairs is very demanding in practice and the performance is largely constrained for the unseen testing samples due to the severe data shifting issue. In contrast, this paper advocates a different paradigm: part of the learning can be performed online but with nominal costs, so as to achieve online metric adaptation for different query probes. By exploiting easily-available negative samples, we propose a novel solution to achieve the optimal local metric adaptation effectively and efficiently. The insight of our method is the local hard negative samples can actually provide tight constraints to fine tune the metric locally. Our local metric adaptation method is generally applicable to be used on top of any offline-learned baselines. In addition, this paper gives in-depth theoretical analyses of the proposed method to guarantee the reduction of the classification error both asymptotically and practically. Extensive experiments on various VIR tasks have confirmed our effectiveness and superiority.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2019.2918208,
c19e05afbd7cf6f31a32093e91b91c1d2d72ad67,1,0,0,Enhance Part-Based Model for Person Re-Identification with Fused Multi-Scale Features,"In recent years, part-based models have been verified their effectiveness for person Re-identification (Re-ID). Since they learn an embedding only by partitioning single-scale features of the highest layer in the backbone network, their performances highly depend on the well-aligned parts of the extracted feature maps. However, misalignments occur very commonly in person Re-ID tasks due to the variations of viewpoints and poses. To address the part-misalignment problem and learn a more discriminative embedding for person Re-ID, we propose a novel Part-based model with fused Multi-Scale features (PMS), which innovatively upscales the low-layer features by using UpShuffle Modules and smoothly integrates the high-layer features. The fused multi-scale features are very robust to the variations of pedestrian scale and beneficial to resolve the part-misalignment problem. Experimental results on three commonly used datasets, including Market-1501, DukeMTMC-reID and CUHK03, have validated our model by outperforming the state-of-the-art methods with no need of re-ranking.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9054659,
c1b874a9df0d8145b31e7305955658e091c9c01f,1,0,0,Improving open-set person re-identification by statistics-driven gallery refinement,"Person re-identification (re-ID) is a valuable tool for multi-camera tracking of persons. Up till now, research on person re-ID has mainly focused on the closed-set case, where a given query is assumed to always have a correct match in the gallery set, which does not hold for practical scenarios. In this study, we explore the open-set person re-ID problem with queries not always included in the gallery set. First, we convert the popular closed-set person re-ID datasets into the open-set scenario. Second, we compare the performances of six state-of-the-art closed-set person re-ID methods under open-set conditions. Third, we investigate the impact of a simple and fast statistics-driven gallery refinement approach on the open-set person re-ID performance. Extensive experimental evaluations show that, gallery refinement increases the performance of existing methods in the low false-accept rate (FAR) region, while simultaneously reducing the computational demands of retrieval. Results show an average detection and identification rate (DIR) increase of 7.91% and 3.31% on the DukeMTMC-reID and Market1501 datasets, respectively, for an FAR of 1%.",2020,International Conference on Machine Vision,,10.1117/12.2559441,
c1bd9c6fe4fc8b0c955ac760db7f2cbe19ea8b8d,0,1,0,Beyond Intra-modality Discrepancy: A Comprehensive Survey of Heterogeneous Person Re-identification,"An effective and efficient person re-identification (ReID) algorithm will alleviate painful video watching, and accelerate the investigation progress. Recently, with the explosive requirements of practical applications, a lot of research efforts have been dedicated to heterogeneous person re-identification (He-ReID). In this paper, we review the state-of-the-art methods comprehensively with respect to four main application scenarios -- low-resolution, infrared, sketch and text. We begin with a comparison between He-ReID and the general Homogeneous ReID (Ho-ReID) task. Then, we survey the models that have been widely employed in He-ReID. Available existing datasets for performing evaluation are briefly described. We then summarize and compare the representative approaches. Finally, we discuss some future research directions.",2019,ArXiv,1905.10048,,https://arxiv.org/pdf/1905.10048.pdf
c1f9d7d6df92550b7ce62d44f488d849a207902d,0,1,0,Similarity Preserved Camera-to-Camera Gan for Person Re-Identification,"Person re-identification (re-ID) is a challenge task, which suffers from detector errors and background variations. The existing works target the problem by generating persons images with GANs for data augmentation against over-fitting. However, the previous approaches still arise the person appearance changes during transferring. To circumvent these issues, we propose a similarity preserved camera-to-camera GAN (SPCGAN) for learning foreground-related and background-unrelated representation. It is a novel structure based on StarGAN and used specifically for camera domain transferring in re-ID with similarity preserved. We introduce augmented datasets with transferred pedestrian images, whose foregrounds are basically unchanged, to different camera domains. Our augmented and synthetic datasets effectively improve the performance of deep learning models for re-ID. The proposed method is evaluated on Market-1501 and DukeMTMC-reID datasets. Experimental results show that our method is effective and achieve outstanding re-ID accuracy compared with state-of-the-art GAN based person re-ID approaches.",2019,2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),,10.1109/ICMEW.2019.00097,
c2359b36e2c25fba3648f093db125579077f2275,1,1,0,Hierarchical Bi-Directional Feature Perception Network for Person Re-Identification,"Previous Person Re-Identification (Re-ID) models aim to focus on the most discriminative region of an image, while its performance may be compromised when that region is missing caused by camera viewpoint changes or occlusion. To solve this issue, we propose a novel model named Hierarchical Bi-directional Feature Perception Network (HBFP-Net) to correlate multi-level information and reinforce each other. First, the correlation maps of cross-level feature-pairs are modeled via low-rank bilinear pooling. Then, based on the correlation maps, Bi-directional Feature Perception (BFP) module is employed to enrich the attention regions of high-level feature, and to learn abstract and specific information in low-level feature. And then, we propose a novel end-to-end hierarchical network which integrates multi-level augmented features and inputs the augmented low- and middle-level features to following layers to retrain a new powerful network. What's more, we propose a novel trainable generalized pooling, which can dynamically select any value of all locations in feature maps to be activated. Extensive experiments implemented on the mainstream evaluation datasets including Market-1501, CUHK03 and DukeMTMC-ReID show that our method outperforms the recent SOTA Re-ID models.",2020,ACM Multimedia,2008.03509,10.1145/3394171.3413689,https://arxiv.org/pdf/2008.03509.pdf
c2395baaaf9eb7923109764cbd30ef40f32bdf8d,1,0,1,Unsupervised Constrative Person Re-identification,"Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Unsupervised person ReID attracts a lot of attention recently, due to it works without intensive manual annotation and thus shows great potential of adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively sampled negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic dictionaries, among which the global and local memory banks are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state-of-the-arts.",2020,ArXiv,2010.07608,,https://arxiv.org/pdf/2010.07608.pdf
c25e78f6f4e79ee88b0a78a3eb6e801ba7046330,1,1,0,Group Re-Identification with Hybrid Attention Model and Residual Distance,"Group re-identification (Re-ID) is an important task of computer vision and involves multiple challenges. In this paper, we propose a Hybrid Attention Model (HAM) to address the problem of group Re-ID, solving the spatial variation in the challenging still-image-based group Re-ID task. HAM consists of both the position and channel attention to make the network focus more on the crucial areas and features of the group images. Furthermore, we propose a novel Least Squares Residual Distance (LSRD) based on the least squares algorithm. LSRD can leverage the residual of the fitting function achieved by least squares method, better learning the metric between group image pairs. To evaluate the performance, we propose a new largest group Re-ID dataset. Extensive experimental results conducted on the dataset demonstrate the effectiveness of our approaches.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803758,
c27ecf5b8943a643584424fab38a902f344165e0,1,1,0,Resource Aware Person Re-identification Across Multiple Resolutions,"Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about high- and low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-of-the-art results on all five datasets that we evaluate on. We then propose two new formulations of the person reID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1805.08805,10.1109/CVPR.2018.00839,https://arxiv.org/pdf/1805.08805.pdf
c2981caccc49e67c4f4a9cc66911faf954c01e9a,0,0,1,Spatially and Temporally Efficient Non-local Attention Network for Video-based Person Re-Identification,"Video-based person re-identification (Re-ID) aims at matching video sequences of pedestrians across non-overlapping cameras. It is a practical yet challenging task of how to embed spatial and temporal information of a video into its feature representation. While most existing methods learn the video characteristics by aggregating image-wise features and designing attention mechanisms in Neural Networks, they only explore the correlation between frames at high-level features. In this work, we target at refining the intermediate features as well as high-level features with non-local attention operations and make two contributions. (i) We propose a Non-local Video Attention Network (NVAN) to incorporate video characteristics into the representation at multiple feature levels. (ii) We further introduce a Spatially and Temporally Efficient Non-local Video Attention Network (STE-NVAN) to reduce the computation complexity by exploring spatial and temporal redundancy presented in pedestrian videos. Extensive experiments show that our NVAN outperforms state-of-the-arts by 3.8% in rank-1 accuracy on MARS dataset and confirms our STE-NVAN displays a much superior computation footprint compared to existing methods.",2019,BMVC,1908.01683,,https://arxiv.org/pdf/1908.01683.pdf
c2a5f27d97744bc1f96d7e1074395749e3c59bc8,1,1,0,Horizontal Pyramid Matching for Person Re-identification,"Despite the remarkable recent progress, person re-identification (Re-ID) approaches are still suffering from the failure cases where the discriminative body parts are missing. To mitigate such cases, we propose a simple yet effective Horizontal Pyramid Matching (HPM) approach to fully exploit various partial information of a given person, so that correct person candidates can be still identified even even some key parts are missing. Within the HPM, we make the following contributions to produce a more robust feature representation for the Re-ID task: 1) we learn to classify using partial feature representations at different horizontal pyramid scales, which successfully enhance the discriminative capabilities of various person parts; 2) we exploit average and max pooling strategies to account for person-specific discriminative information in a global-local manner. To validate the effectiveness of the proposed HPM, extensive experiments are conducted on three popular benchmarks, including Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP scores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new state-of-the-arts. Our code is available on Github",2019,AAAI,1804.05275,10.1609/aaai.v33i01.33018295,https://arxiv.org/pdf/1804.05275.pdf
c2bb9d275dfab4b7798fe1cadd3bf9209de3ef5e,1,0,1,Exploiting Global Camera Network Constraints for Unsupervised Video Person Re-identification,"Many unsupervised approaches have been proposed recently for the video-based re-identification problem since annotations of samples across cameras are time-consuming. However, higher-order relationships across the entire camera network are ignored by these methods, leading to contradictory outputs when matching results from different camera pairs are combined. In this paper, we address the problem of unsupervised video-based re-identification by proposing a consistent cross-view matching (CCM) framework, in which global camera network constraints are exploited to guarantee the matched pairs are with consistency. Specifically, we first propose to utilize the first neighbor of each sample to discover relations among samples and find the groups in each camera. Additionally, a cross-view matching strategy followed by global camera network constraints is proposed to explore the matching relationships across the entire camera network. Finally, we learn metric models for camera pairs progressively by alternatively mining consistent cross-view matching pairs and updating metric models using these obtained matches. Rigorous experiments on two widely-used benchmarks for video re-identification demonstrate the superiority of the proposed method over current state-of-the-art unsupervised methods; for example, on the MARS dataset, our method achieves an improvement of 4.2\% over unsupervised methods, and even 2.5\% over one-shot supervision-based methods for rank-1 accuracy.",2019,,1908.10486,,https://arxiv.org/pdf/1908.10486.pdf
c2cb8ecbe281793401b605634805c47daba6b944,0,1,0,FastReID: A Pytorch Toolbox for Real-world Person Re-identification,"We present FastReID, as a widely used object reidentification (re-id) software system in JD AI Research. High modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art algorithms, including person re-id, partial re-id, crossdomain re-id and vehicle re-id, and plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most complete and high-performance toolbox supports single and multiple GPU servers, you can reproduce our project results very easily and are very welcome to use it, the code and models are available at https://github.com/JDAI-CV/fast-reid.",2020,,,,
c3725184a31e281b543467673753c719dcc2393e,1,0,0,"MSBA: Multiple Scales, Branches and Attention Network With Bag of Tricks for Person Re-Identification","Person re-identification (Re-ID) has become a hot topic in both research and industry. We joined in a person Re-ID challenge of the First National Artificial Intelligence Challenge (China, 2019) and found some model designs and training tricks work great or not on a super big private dataset. In this paper, we propose a model that combines the most effective designs, including multi-scale, multi-branch and attention mechanism, and report training tricks that are no less or even more important in improving person Re-ID performance. We analyze four commonly used public datasets: Market1501, DukeMTMC-ReID, CUHK03, and MSMT17, and achieve the state-of-the-art performance. Besides, we analyze and confirm the effectiveness of the designs by ablation studies. We also share strategies that play a key role in the challenge and experience of model designs that do not generalize well on large datasets.",2020,IEEE Access,,10.1109/ACCESS.2020.2984915,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09052718.pdf
c37bc234a97099e15753f19feb6653d436a09368,1,0,0,Multi-Camera Vehicle Tracking with Powerful Visual Features and Spatial-Temporal Cue,"Vehicle re-identification and multi-camera multi-object vehicle tracking are important components in the field of intelligent traffic, which is attracting more and more attention. In the NVIDIA AI City Challenge, we propose our solutions to solve these issues. In Track1 task, clustering loss and trajectory consistent loss are introduced into the vehicle re-identification training framework to train more suitable trajectory-based features for the clustering task. Besides, spatial-temporal cue is fully excavated to make up the deficiency of appearance feature and constrained hierarchical clustering is introduced into the pipeline to get the final cluster results. In Track2 task, we propose an effective vehicle training framework and trajectory-based weighted ranking method, which greatly improve the performance. Furthermore, an efficient way to mining the additional data to train more robust features is proposed to enlarge the training data. Finally, our algorithm achieves the state-of-theart performance in the competition.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/c37b/c234a97099e15753f19feb6653d436a09368.pdf
c37c3853ab428725f13906bb0ff4936ffe15d6af,1,1,0,Unsupervised Person Re-identification by Deep Learning Tracklet Association,"Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation re-id methods using six person re-id benchmarking datasets.",2018,ECCV,1809.02874,10.1007/978-3-030-01225-0_45,https://arxiv.org/pdf/1809.02874.pdf
c380808a70fc2325ebd2be2e92b76b7e5c48bca8,1,1,0,Deep Association: End-to-end Graph-Based Learning for Multiple Object Tracking with Conv-Graph Neural Network,"Multiple Object Tracking (MOT) has a wide range of applications in surveillance retrieval and autonomous driving. The majority of existing methods focus on extracting features by deep learning and hand-crafted optimizing bipartite graph or network flow. In this paper, we proposed an efficient end-to-end model, Deep Association Network (DAN), to learn the graph-based training data, which are constructed by spatial-temporal interaction of objects. DAN combines Convolutional Neural Network (CNN), Motion Encoder (ME) and Graph Neural Network (GNN). The CNNs and Motion Encoders extract appearance features from bounding box images and motion features from positions respectively, and then the GNN optimizes graph structure to associate the same object among frames together. In addition, we presented a novel end-to-end training strategy for Deep Association Network. Our experimental results demonstrate the effectiveness of DAN up to the state-of-the-art methods without extra-dataset on MOT16 and DukeMTMCT.",2019,ICMR,,10.1145/3323873.3325010,
c3923b66b737f4f8c1f7ca8346b7dd6da078e6f1,0,1,0,Global Distance-distributions Separation for Unsupervised Person Re-identification,"Supervised person re-identification (ReID) often has poor scalability and usability in real-world deployments due to domain gaps and the lack of annotations for the target domain data. Unsupervised person ReID through domain adaptation is attractive yet challenging. Existing unsupervised ReID approaches often fail in correctly identifying the positive samples and negative samples through the distance-based matching/ranking. The two distributions of distances for positive sample pairs (Pos-distr) and negative sample pairs (Neg-distr) are often not well separated, having large overlap. To address this problem, we introduce a global distance-distributions separation (GDS) constraint over the two distributions to encourage the clear separation of positive and negative samples from a global view. We model the two global distance distributions as Gaussian distributions and push apart the two distributions while encouraging their sharpness in the unsupervised training process. Particularly, to model the distributions from a global view and facilitate the timely updating of the distributions and the GDS related losses, we leverage a momentum update mechanism for building and maintaining the distribution parameters (mean and variance) and calculate the loss on the fly during the training. Distribution-based hard mining is proposed to further promote the separation of the two distributions. We validate the effectiveness of the GDS constraint in unsupervised ReID networks. Extensive experiments on multiple ReID benchmark datasets show our method leads to significant improvement over the baselines and achieves the state-of-the-art performance.",2020,ECCV,2006.00752,10.1007/978-3-030-58571-6_43,https://arxiv.org/pdf/2006.00752.pdf
c3980c4ce42ebcd493514180897829096c189337,0,1,0,"Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part II","In selection processes, decisions follow a sequence of stages. Early stages have more applicants and general information, while later stages have fewer applicants but specific data. This is represented by a dual funnel structure, in which the sample size decreases from one stage to the other while the information increases. Training classifiers for this case is challenging. In the early stages, the information may not contain distinct patterns to learn, causing underfitting. In later stages, applicants have been filtered out and the small sample can cause overfitting. We redesign the multi-stage problem to address both cases by combining adversarial autoencoders (AAE) and multi-task semi-supervised learning (MTSSL) to train an end-to-end neural network for all stages together. The AAE learns the representation of the data and performs data imputation in missing values. The generated dataset is fed to an MTSSL mechanism that trains all stages together, encouraging related tasks to contribute to each other using a temporal regularization structure. Using real-world data, we show that our approach outperforms other state-ofthe-art methods with a gain of 4x over the standard case and a 12% improvement over the second-best method.",2020,PAKDD,,10.1007/978-3-030-47436-2,
c433ac3494a01094254aecc3d55b3d53bcb53edf,1,0,1,Spatial-Temporal Graph Convolutional Network for Video-Based Person Re-Identification,"While video-based person re-identification (Re-ID) has drawn increasing attention and made great progress in recent years, it is still very challenging to effectively overcome the occlusion problem and the visual ambiguity problem for visually similar negative samples. On the other hand, we observe that different frames of a video can provide complementary information for each other, and the structural information of pedestrians can provide extra discriminative cues for appearance features. Thus, modeling the temporal relations of different frames and the spatial relations within a frame has the potential for solving the above problems. In this work, we propose a novel Spatial-Temporal Graph Convolutional Network (STGCN) to solve these problems. The STGCN includes two GCN branches, a spatial one and a temporal one. The spatial branch extracts structural information of a human body. The temporal branch mines discriminative cues from adjacent frames. By jointly optimizing these branches, our model extracts robust spatial-temporal information that is complementary with appearance information. As shown in the experiments, our model achieves state-of-the-art results on MARS and DukeMTMC-VideoReID datasets.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.00335,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Spatial-Temporal_Graph_Convolutional_Network_for_Video-Based_Person_Re-Identification_CVPR_2020_paper.pdf
c43d185fd701cde6ab44f0bc2fd7eb1cc1a74935,0,1,0,Attention-guided GANs for human pose transfer,"This paper presents a novel generative adversarial network for the task of human pose transfer, which aims at transferring the pose of a given person to a target pose. In order to deal with pixel-to-pixel misalignment due to the pose differences, we introduce an attention mechanism and propose Pose-Guided Attention Blocks. With these blocks, the generator can learn how to transfer the details from the conditional image to the target image based on the target pose. Our network can make the target pose truly guide the transfer of features. The effectiveness of the proposed network is validated on DeepFasion and Market-1501 datasets. Compared with state-of-the-art methods, our generated images are more realistic with better facial details.",2019,SPIE/COS Photonics Asia,,10.1117/12.2538638,
c43ed9b34cad1a3976bac7979808eb038d88af84,0,1,0,Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model,"We propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with a wide range of expressions, poses, and illuminations conditioned by synthetic images sampled from a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with a large volume of paired data or train highly under-constrained two-way generative networks in an unsupervised fashion. We propose a semi-supervised adversarial learning framework to constrain the two-way networks by a small number of paired real and synthetic images, along with a large volume of unpaired data. A set-based loss is also proposed to preserve identity coherence of generated images. Qualitative results show that generated face images of new identities contain pose, lighting and expression diversity. They are also highly constrained by the synthetic input images while adding photorealism and retaining identity information. We combine face images generated by the proposed method with a real data set to train face recognition algorithms and evaluate the model quantitatively on two challenging data sets: LFW and IJB-A. The generated images by our framework consistently improve the performance of deep face recognition networks trained with the Oxford VGG Face dataset, and achieve comparable results to the state-of-the-art.",2018,ECCV,1804.03675,10.1007/978-3-030-01252-6_14,https://arxiv.org/pdf/1804.03675.pdf
c45c3286f06b93e086d0488ec39d41e95ef9b2d5,1,1,1,Unsupervised Graph Association for Person Re-Identification,"In this paper, we propose an unsupervised graph association (UGA) framework to learn the underlying viewinvariant representations from the video pedestrian tracklets. The core points of UGA are mining the underlying cross-view associations and reducing the damage of noise associations. To this end, UGA is adopts a two-stage training strategy: (1) intra-camera learning stage and (2) intercamera learning stage. The former learns the intra-camera representation for each camera. While the latter builds a cross-view graph (CVG) to associate different cameras. By doing this, we can learn view-invariant representation for all person. Extensive experiments and ablation studies on seven re-id datasets demonstrate the superiority of the proposed UGA over most state-of-the-art unsupervised and domain adaptation re-id methods.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00841,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.pdf
c52f35d2888a2c197eca5398e2a3adc3f2dc987b,1,0,0,Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification,"The trained person re-identification systems fundamentally need to be deployed on different target environments. Learning the crossdomain model has great potential for the scalability of real-world applications. In this paper, we propose a deep credible metric learning (DCML) method for unsupervised domain adaptation person re-identification. Unlike existing methods that directly finetune the model in the target domain with pseudo labels generated by the source pre-trained model, our DCML method adaptively mines credible samples for training to avoid the misleading from noise labels. Specifically, we design two credibility metrics for sample mining including the k-Nearest Neighbor similarity for density evaluation and the prototype similarity for centrality evaluation. As the increasing of the pseudo label credibility, we progressively adjust the sampling strategy in the training process. In addition, we propose an instance margin spreading loss to further increase instancewise discrimination. Experimental results demonstrate that our DCML method explores credible and valuable training data and improves the performance of unsupervised domain adaptation.",2020,ECCV,,10.1007/978-3-030-58598-3_38,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530630.pdf
c5485948fa7f9639fbabef11a7ba2dc4de049336,1,0,0,FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking.,"There has been remarkable progress on object detection and re-identification (re-ID) in recent years which are the key components of multi-object tracking. However, little attention has been focused on jointly accomplishing the two tasks in a single network. Our study shows that the previous attempts ended up with degraded accuracy mainly because the re-ID task is not fairly learned which causes many identity switches. The unfairness lies in two-fold: (1) they treat re-ID as a secondary task whose accuracy heavily depends on the primary detection task. So training is largely biased to the detection task but ignores the re-ID task; (2) they use ROI-Align to extract re-ID features which is directly borrowed from object detection. However, this introduces a lot of ambiguity in characterizing objects because many sampling points may belong to disturbing instances or background. To solve the problems, we present a simple approach \emph{FairMOT} which consists of two homogeneous branches to predict pixel-wise objectness scores and re-ID features. The achieved fairness between the tasks allows \emph{FairMOT} to obtain high levels of detection and tracking accuracy and outperform previous state-of-the-arts by a large margin on several public datasets. The source code and pre-trained models are released at this https URL.",2020,,2004.01888,,https://arxiv.org/pdf/2004.01888.pdf
c56819a4918bdcd29038b17dc35b2bd549641b83,0,1,0,Transfer Adaptation Learning: A Decade Survey,"The world we see is ever-changing and it always changes with people, things, and the environment. Domain is referred to as the state of the world at a certain moment. A research problem is characterized as domain transfer adaptation when it needs knowledge correspondence between different moments. Conventional machine learning aims to find a model with the minimum expected risk on test data by minimizing the regularized empirical risk on the training data, which, however, supposes that the training and test data share similar joint probability distribution. Transfer adaptation learning aims to build models that can perform tasks of target domain by learning knowledge from a semantic related but distribution different source domain. It is an energetic research filed of increasing influence and importance. This paper surveys the recent advances in transfer adaptation learning methodology and potential benchmarks. Broader challenges being faced by transfer adaptation learning researchers are identified, i.e., instance re-weighting adaptation, feature adaptation, classifier adaptation, deep network adaptation, and adversarial adaptation, which are beyond the early semi-supervised and unsupervised split. The survey provides researchers a framework for better understanding and identifying the research status, challenges and future directions of the field.",2019,ArXiv,1903.04687,,https://arxiv.org/pdf/1903.04687.pdf
c583e6df0613e6e985850eaadd10787eade204ba,0,1,0,An Adversarial Regularisation for Semi-Supervised Training of Structured Output Neural Networks,"We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator as a source of error signal for unlabelled data. This effectively boosts the performance of a network on a held out test set. Initial experiments in image segmentation demonstrate that the proposed framework enables achieving the same network performance as in a fully supervised scenario, while using two times less annotations.",2017,NIPS 2017,1702.02382,,https://arxiv.org/pdf/1702.02382.pdf
c58a6b8fb1b78b70176592db9590ba92f2a47c03,1,1,0,Adaptive Alignment Network for Person Re-identification,"Person re-identification aims at identifying a target pedestrian across non-overlapping camera views. Pedestrian misalignment, which mainly arises from inaccurate person detection and pose variations, is a critical challenge for person re-identification. To address this, this paper proposes a new Adaptive Alignment Network (AAN), towards robust and accurate person re-identification. AAN automatically aligns pedestrian images from coarse to fine by learning both patch-wise and pixel-wise alignments, leading to effective pedestrian representation invariant to the variance of human pose and location across images. In particular, AAN consists of a patch alignment module, a pixel alignment module and a base network. The patch alignment module estimates the alignment offset for each image patch and performs patch-wise alignment with the offsets. The pixel alignment module is for fine-grained pixel-wise alignment. It learns the subtle local offset for each pixel and produces finely aligned feature map. Extensive experiments on three benchmarks, i.e., Market1501, DukeMTMC-reID and MSMT17 datasets, have demonstrated the effectiveness of the proposed approach.",2019,MMM,,10.1007/978-3-030-05716-9_2,
c5bfbda61a29cd1206ea6670094c7f38c589c51b,0,1,0,ReadNet: Towards Accurate ReID with Limited and Noisy Samples,"Person re-identification (ReID) is an essential cross-camera retrieval task to identify pedestrians. However, the photo number of each pedestrian usually differs drastically, and thus the data limitation and imbalance problem hinders the prediction accuracy greatly. Additionally, in real-world applications, pedestrian images are captured by different surveillance cameras, so the noisy camera related information, such as the lights, perspectives and resolutions, result in inevitable domain gaps for ReID algorithms. These challenges bring difficulties to current deep learning methods with triplet loss for coping with such problems. To address these challenges, this paper proposes ReadNet, an adversarial camera network (ACN) with an angular triplet loss (ATL). In detail, ATL focuses on learning the angular distance among different identities to mitigate the effect of data imbalance, and guarantees a linear decision boundary as well, while ACN takes the camera discriminator as a game opponent of feature extractor to filter camera related information to bridge the multi-camera gaps. ReadNet is designed to be flexible so that either ATL or ACN can be deployed independently or simultaneously. The experiment results on various benchmark datasets have shown that ReadNet can deliver better prediction performance than current state-of-the-art methods.",2020,ArXiv,2005.0574,,https://arxiv.org/pdf/2005.05740.pdf
c5e645dc2d34c7c3777456a25e04ac8b05f78177,0,1,0,Cross-view Identical Part Area Alignment for Person Re-identification,"Person re-identification aims to associate images captured by non-overlapping cameras. It is a challenging task because images are often in different conditions such as background clutter, illumination variation, viewpoint changes and different camera settings. Viewpoint changes and pose variations often cause body part self-occlusion and misalignment. To deal with the problem, local features from human body parts are extracted. However, with viewpoint changes, the body parts also rotate horizontally. It is inappropriate to extract feature from entire area of body parts directly because the visible surface of body parts would turn away if viewpoint changes. Comparing identical areas provides a new way to pay attention to the details of person images. In this paper, we propose a Rotation Invariant Network to find the identical areas in cross-view images to extract robust local features. Extensive experiment show the effectiveness of our method on public datasets including CUHK03, Market1501 and DukeMTMC.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683137,http://mmap.whu.edu.cn/wp-content/uploads/2019/03/20190218051144_367651_4317.pdf
c5fcc26760ac57ca2224d7311966a9621df7983d,1,0,0,Multi-Similarity Re-Ranking for Person Re-Identification,"Re-ranking has been proved an effective method to boost the performance of person re-identification. Existing works focus on contextual or graph-based similarity to improve the initial ranking result. The former mainly concentrates on more accurate similarity description but neglects the manifold constraint. While, the later centers on solving similarities with manifold constraint, which acquires several accurate top ranks. In this paper, we propose a novel method which not only takes contextual similarity to generate top ranks accurately but also refines the ranks based on graph-based similarity. Specifically, given initial Euclidean distances between a probe and galleries, we mine contextual and graph-based similarities respectively and then re-rank all galleries with a diffusion procedure under constraints of both similarities. Experiments on two person re-ID datasets demonstrate that our method outperforms state-of-the-art re-ranking approaches in person re-identification.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8804399,
c62d524b3f74137efcf824c32252bd5993c07c4e,0,0,1,Supplementary Material for “Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification”,"for Video-based Person Re-identification” Jinrui Yang , Wei-Shi Zheng1,2,3∗ , Qize Yang , Yingcong Chen , and Qi Tian 1 School of Data and Computer Science, Sun Yat-sen University, China 2 Peng Cheng Laboratory, Shenzhen 518005, China 3 Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 4 The Chinese University of Hong Kong, China The Huawei Noah’s Ark Lab, China {yangjr27,yangqz}@mail2.sysu.edu.cn,wszheng@ieee.org,yingcong.ian.chen@gmail.com ,tian.qi1@huawei.com",2020,,,,http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yang_Spatial-Temporal_Graph_Convolutional_CVPR_2020_supplemental.pdf
c640807b029250bc6d302f60f0966d9cb49eb89a,1,0,0,Re-identification in Home Environments using Facial and Appearance Features,"Re-identification is a vital task in video surveillance, and it is often viewed as an image retrieval task (i.e. given one image to search relevant images from all other images captured by different cameras.). The majority of the state-of-art methodologies focus on data which is obtained in an outdoor environment and are based on spatial and temporal features [1]. However, there is a need for methodologies that can handle data from the home environment due to the increasing demand for health monitoring. The LIMA dataset [2] , where the images are captured by a low-cost camera in the home environment, provided by the SPHERE project, includes realistic challenges. As an example, people in the scene may change clothes, and an unknown identity may appear in the scene. Since these are challenging situations, facial features can help to discriminate better amongst different identities that appear in the scene. Therefore, We have provided some re-identification results for LIMA dataset and methodologies focus on both facial and appearance features. Besides, we also label each image in the LIMA dataset to provide a LIMA face dataset (binary label, face or not containing face) to support the face detection performance measurement. Both traditional and modern approaches are employed to compare the performance in face detection and recognition. Furthermore, CNN with only appearance features is trained to see the drawbacks of the appearance features. Then, we propose a two-stream convolutional neural network to combine facial and appearance features together to predict the ID of the person, and it can be shown that the facial features indeed improve the performance through a simple experiment.",2018,,,,https://pdfs.semanticscholar.org/c640/807b029250bc6d302f60f0966d9cb49eb89a.pdf
c66779a993292bef4ac7766d25703705f9d7e3a7,1,0,0,Multi-Domain Learning and Identity Mining for Vehicle Re-Identification,"This paper introduces our solution for the Track2 in AI City Challenge 2020 (AICITY20). The Track2 is a vehicle re-identification (ReID) task with both the real-world data and synthetic data.Our solution is based on a strong baseline with bag of tricks (BoT-BS) proposed in person ReID. At first, we propose a multi-domain learning method to joint the real-world and synthetic data to train the model. Then, we propose the Identity Mining method to automatically generate pseudo labels for a part of the testing data, which is better than the k-means clustering. The tracklet-level re-ranking strategy with weighted features is also used to post-process the results. Finally, with multiple-model ensemble, our method achieves 0.7322 in the mAP score which yields third place in the competition. The codes are available at https://github.com/heshuting555/AICITY2020_DMT_VehicleReID.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2004.10547,10.1109/CVPRW50498.2020.00299,https://arxiv.org/pdf/2004.10547.pdf
c6b28d6bc3b99a2a2b62585c0d5585ee61cfca1a,1,1,0,Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective,"This paper considers learning deep features from long-tailed data. We observe that in the deep feature space, the head classes and the tail classes present different distribution patterns. The head classes have a relatively large spatial span, while the tail classes have a significantly small spatial span, due to the lack of intra-class diversity. This uneven distribution between head and tail classes distorts the overall feature space, which compromises the discriminative ability of the learned features. In response, we seek to expand the distribution of the tail classes during training, so as to alleviate the distortion of the feature space. To this end, we propose to augment each instance of the tail classes with certain disturbances in the deep feature space. With the augmentation, a specified feature vector becomes a set of probable features scattered around itself, which is analogical to an atomic nucleus surrounded by the electron cloud. Intuitively, we name it as ``feature cloud''. The intra-class distribution of the feature cloud is learned from the head classes, and thus provides higher intra-class variation to the tail classes. Consequentially, it alleviates the distortion of the learned feature space, and improves deep representation learning on long tailed data. Extensive experimental evaluations on person re-identification and face recognition tasks confirm the effectiveness of our method.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.10826,10.1109/CVPR42600.2020.00304,https://arxiv.org/pdf/2002.10826.pdf
c6be86fed2dc5e75be946c72b4c705fbe76bfc81,0,1,0,Learning a Compact Vein Discrimination Model With GANerated Samples,"Despite the great success achieved by convolutional neural networks (CNNs) in various image understanding tasks, it is still difficult for CNNs to be applied to vein recognition tasks due to the problems of insufficient training datasets, intra-class variations, and inter-class similarities. Besides, due to the essential requirement on the storage of millions of parameters for CNN, it is challenging to use a CNN for designing a vein-based embedded person identification system. In this paper, these two problems are addressed by learning a discriminative and compact vein recognition model. For the first problem, a hierarchical generative adversarial network (HGAN) consisting of a constrained CNN and a CycleGAN is proposed for data augmentation. Two similarity losses are defined for estimating the self-similarity and inter-class dissimilarity, and a CycleGAN model is properly trained with these two losses for better task-specific training sample generation. After obtaining a baseline vein recognition model fine-tuned on the augmented datasets, the existence of parameter redundancy in the over-parameterized network motivates the proposal of model compression by way of filter pruning and low rank approximation, thus making the compressed model more suitable for deployment on embedded systems. Through the vein recognition experiments with two different datasets and an additional palmprint recognition experiment, the proposed algorithms are shown to yield a highly compact model while keeping the accuracy acceptable for application.",2020,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2019.2924553,
c6c8c865b92cbd0428f9bcaa5cc55fc99ef68da8,1,0,0,Discriminative Feature Learning With Foreground Attention for Person Re-Identification,"The performance of person re-identification (Re-ID) has been seriously affected by the large cross-view appearance variations caused by mutual occlusions and background clutter. Hence, learning a feature representation that can adaptively emphasize the foreground persons becomes very critical to solve the person Re-ID problem. In this paper, we propose a simple yet effective foreground attentive neural network (FANN) to learn a discriminative feature representation for person Re-ID, which can adaptively enhance the positive side of foreground and weaken the negative side of background. Specifically, a novel foreground attentive subnetwork is designed to drive the network’s attention, in which a decoder network is used to reconstruct the binary mask by using a novel local regression loss function, and an encoder network is regularized by the decoder network to focus its attention on the foreground persons. The resulting feature maps of encoder network are further fed into the body part subnetwork and feature fusion subnetwork to learn discriminative features. Besides, a novel symmetric triplet loss function is introduced to supervise feature learning, in which the intra-class distance is minimized and the inter-class distance is maximized in each triplet unit, simultaneously. Training our FANN in a multi-task learning framework, a discriminative feature representation can be learned to find out the matched reference to each probe among various candidates in the gallery. Extensive experimental results on several public benchmark datasets are evaluated, which have shown clear improvements of our method over the state-of-the-art approaches.",2019,IEEE Transactions on Image Processing,1807.01455,10.1109/TIP.2019.2908065,https://arxiv.org/pdf/1807.01455.pdf
c6ede58bd7030b270dba2247a431dabd529c6790,1,0,0,SSKD: Self-Supervised Knowledge Distillation for Cross Domain Adaptive Person Re-Identification,"Domain adaptive person re-identification (re-ID) is a challenging task due to the large discrepancy between the source domain and the target domain. To reduce the domain discrepancy, existing methods mainly attempt to generate pseudo labels for unlabeled target images by clustering algorithms. However, clustering methods tend to bring noisy labels and the rich fine-grained details in unlabeled images are not sufficiently exploited. In this paper, we seek to improve the quality of labels by capturing feature representation from multiple augmented views of unlabeled images. To this end, we propose a Self-Supervised Knowledge Distillation (SSKD) technique containing two modules, the identity learning and the soft label learning. Identity learning explores the relationship between unlabeled samples and predicts their one-hot labels by clustering to give exact information for confidently distinguished images. Soft label learning regards labels as a distribution and induces an image to be associated with several related classes for training peer network in a self-supervised manner, where the slowly evolving network is a core to obtain soft labels as a gentle constraint for reliable images. Finally, the two modules can resist label noise for re-ID by enhancing each other and systematically integrating label information from unlabeled images. Extensive experiments on several adaptation tasks demonstrate that the proposed method outperforms the current state-of-the-art approaches by large margins.",2020,ArXiv,2009.05972,,https://arxiv.org/pdf/2009.05972.pdf
c6f99e3f39b427798729ca7093d4f083746a98ae,0,1,1,Weakly Supervised Person Re-ID: Differentiable Graphical Learning and a New Benchmark.,"Person reidentification (Re-ID) benefits greatly from the accurate annotations of existing data sets (e.g., CUHK03 and Market-1501), which are quite expensive because each image in these data sets has to be assigned with a proper label. In this work, we ease the annotation of Re-ID by replacing the accurate annotation with inaccurate annotation, i.e., we group the images into bags in terms of time and assign a bag-level label for each bag. This greatly reduces the annotation effort and leads to the creation of a large-scale Re-ID benchmark called SYSU-30k. The new benchmark contains 30k individuals, which is about 20 times larger than CUHK03 (1.3k individuals) and Market-1501 (1.5k individuals), and 30 times larger than ImageNet (1k categories). It sums up to 29,606,918 images. Learning a Re-ID model with bag-level annotation is called the weakly supervised Re-ID problem. To solve this problem, we introduce a differentiable graphical model to capture the dependencies from all images in a bag and generate a reliable pseudolabel for each person's image. The pseudolabel is further used to supervise the learning of the Re-ID model. Compared with the fully supervised Re-ID models, our method achieves state-of-the-art performance on SYSU-30k and other data sets. The code, data set, and pretrained model will be available at https://github.com/wanggrun/SYSU-30k.",2020,IEEE transactions on neural networks and learning systems,1904.03845,10.1109/tnnls.2020.2999517,https://arxiv.org/pdf/1904.03845.pdf
c705a3688f815720b185b4bc36cb247d809a2044,0,1,0,Learning Large Margin Multiple Granularity Features with an Improved Siamese Network for Person Re-Identification,"Person re-identification (Re-ID) is a non-overlapping multi-camera retrieval task to match different images of the same person, and it has become a hot research topic in many fields, such as surveillance security, criminal investigation, and video analysis. As one kind of important architecture for person re-identification, Siamese networks usually adopt standard softmax loss function, and they can only obtain the global features of person images, ignoring the local features and the large margin for classification. In this paper, we design a novel symmetric Siamese network model named Siamese Multiple Granularity Network (SMGN), which can jointly learn the large margin multiple granularity features and similarity metrics for person re-identification. Firstly, two branches for global and local feature extraction are designed in the backbone of the proposed SMGN model, and the extracted features are concatenated together as multiple granularity features of person images. Then, to enhance their discriminating ability, the multiple channel weighted fusion (MCWF) loss function is constructed for the SMGN model, which includes the verification loss and identification loss of the training image pair. Extensive comparative experiments on four benchmark datasets (CUHK01, CUHK03, Market-1501 and DukeMTMC-reID) show the effectiveness of our proposed method and its performance outperforms many state-of-the-art methods.",2020,Symmetry,,10.3390/sym12010092,https://pdfs.semanticscholar.org/c705/a3688f815720b185b4bc36cb247d809a2044.pdf
c753521ba6fb06c12369d6fff814bb704c682ef5,0,1,0,Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-Identification,"We propose a novel deep network called Mancs that solves the person re-identification problem from the following aspects: fully utilizing the attention mechanism for the person misalignment problem and properly sampling for the ranking loss to obtain more stable person representation. Technically, we contribute a novel fully attentional block which is deeply supervised and can be plugged into any CNN, and a novel curriculum sampling method which is effective for training ranking losses. The learning tasks are integrated into a unified framework and jointly optimized. Experiments have been carried out on Market1501, CUHK03 and DukeMTMC. All the results show that Mancs can significantly outperform the previous state-of-the-arts. In addition, the effectiveness of the newly proposed ideas has been confirmed by extensive ablation studies.",2018,ECCV,,10.1007/978-3-030-01225-0_23,http://openaccess.thecvf.com/content_ECCV_2018/papers/Cheng_Wang_Mancs_A_Multi-task_ECCV_2018_paper.pdf
c7af4d0e033d02b5d6f70149ca9fd3d3b187e995,0,1,0,GAN-based Semi-supervised Learning On Fewer Labeled Samples,"Semi-supervised learning is recently addressed by means of neural networks in the framework of deep learning. For the semi-supervised tasks where training samples are partially labeled, the generative adversarial networks (GANs) are applicable not only to augmentation of the training samples but also to the end-to-end learning of classifiers exploiting the unlabeled samples. It, however, is found that the previous GAN-based semi-supervised method is less effective on the smaller number of labeled samples, and thus in this paper, we propose a novel GAN-based method to effectively work on fewer labeled samples. In the GAN framework, through analyzing gradients of the discriminator which are fundamental to learn the network via back-propagation, we formulate a discriminator model and accordingly a generator loss to cope with the less discriminative classifier trained on the fewer labeled samples. The proposed model is also mixed with the original one to further improve discriminativity on the semi-supervised learning in an efficient way beyond the simple linear combination. The experimental results on semisupervised classification tasks using MNIST, SVHN and CIFAR-10 datasets show that the proposed method exhibits favorable performance compared to the other methods.",2018,BMVC,,,http://bmvc2018.org/contents/papers/0327.pdf
c7dd7688b7f67e717ff95d5b5f74f3df71c3a80b,1,0,1,Pair-based Uncertainty and Diversity Promoting Early Active Learning for Person Re-identification,"The effective training of supervised Person Re-identification (Re-ID) models requires sufficient pairwise labeled data. However, when there is limited annotation resource, it is difficult to collect pairwise labeled data. We consider a challenging and practical problem called Early Active Learning, which is applied to the early stage of experiments when there is no pre-labeled sample available as references for human annotating. Previous early active learning methods suffer from two limitations for Re-ID. First, these instance-based algorithms select instances rather than pairs, which can result in missing optimal pairs for Re-ID. Second, most of these methods only consider the representativeness of instances, which can result in selecting less diverse and less informative pairs. To overcome these limitations, we propose a novel pair-based active learning for Re-ID. Our algorithm selects pairs instead of instances from the entire dataset for annotation. Besides representativeness, we further take into account the uncertainty and the diversity in terms of pairwise relations. Therefore, our algorithm can produce the most representative, informative, and diverse pairs for Re-ID data annotation. Extensive experimental results on five benchmark Re-ID datasets have demonstrated the superiority of the proposed pair-based early active learning algorithm.",2020,ACM Trans. Intell. Syst. Technol.,,10.1145/3372121,
c81281a31b8bb02b3168492bf9955de48e740866,1,0,0,An Effective Adversarial Attack on Person Re-Identification in Video Surveillance via Dispersion Reduction,"Person re-identification across a network of cameras, with disjoint views, has been studied extensively due to its importance in wide-area video surveillance. This is a challenging task due to several reasons including changes in illumination and target appearance, and variations in camera viewpoint and camera intrinsic parameters. The approaches developed to re-identify a person across different camera views need to address these challenges. More recently, neural network-based methods have been proposed to solve the person re-identification problem across different camera views, achieving state-of-the-art performance. In this paper, we present an effective and generalizable attack model that generates adversarial images of people, and results in very significant drop in the performance of the existing state-of-the-art person re-identification models. The results demonstrate the extreme vulnerability of the existing models to adversarial examples, and draw attention to the potential security risks that might arise due to this in video surveillance. Our proposed attack is developed by decreasing the dispersion of the internal feature map of a neural network to degrade the performance of several different state-of-the-art person re-identification models. We also compare our proposed attack with other state-of-the-art attack models on different person re-identification approaches, and by using four different commonly used benchmark datasets. The experimental results show that our proposed attack outperforms the state-of-art attack models on the best performing person re-identification approaches by a large margin, and results in the most drop in the mean average precision values.",2020,IEEE Access,,10.1109/ACCESS.2020.3024149,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195855.pdf
c81d60bb68c465bab14da845e0e8517710a1bf80,0,1,0,The Improved Joint Bayesian Method for Person Re-identification Across Different Camera,"Due to the view point, illumination, personal gait and other background situation, person re-identification across cameras has been a challenging task in video surveillance area. In order to address the problem, a novel method called Joint Bayesian across different cameras for person re-identification (JBR) is proposed. Motivated by the superior measurement ability of Joint Bayesian, a set of Joint Bayesian matrices is obtained by learning with different camera pairs. With the global Joint Bayesian matrix, the proposed method combines the characteristics of multi-camera shooting and person re-identification. Then this method can improve the calculation precision of the similarity between two individuals by learning the transition between two cameras. For investigating the proposed method, it is implemented on two compare large-scale re-ID datasets, the Market-1501 and DukeMTMC-reID. The RANK-1 accuracy significantly increases about 3% and 4%, and the maximum a posterior (MAP) improves about 1% and 4%, respectively.",2019,J. Inf. Process. Syst.,,,
c835e50a8a6a91beb7e2f7e3d902ac1b3a2d844b,0,1,0,Artificial Semantic Memory with Autonomous Learning Applied to Social Robots,"Semantic memory stores knowledge about the meanings of words and the relationships between these meanings. In recent years, Artificial Intelligence, in particular Deep Learning, has successfully resolved the identification of classes of elements in images, and even instances of a class, providing a basic form of semantic memory. Unfortunately, incorporating new instances of a class requires a complex and long process of labeling and offline training. We are convinced that the combination of convolutional networks and statistical classifiers allows us to create a long-term semantic memory that is capable of learning online. To validate this hypothesis, we have implemented a long-term semantic memory in a social robot. The robot initially only recognizes people, but, after interacting with different people, is able to distinguish them from each other. The advantage of our approach is that the process of long-term memorization is done autonomously without the need for offline processing.",2019,IWINAC,,10.1007/978-3-030-19651-6_39,
c8ac121e9c4eb9964be9c5713f22a95c1c3b57e9,0,1,0,Ensemble Feature for Person Re-Identification,"In person re-identification (re-ID), the key task is feature representation, which is used to compute distance or similarity in prediction. Person re-ID achieves great improvement when deep learning methods are introduced to tackle this problem. The features extracted by convolutional neural networks (CNN) are more effective and discriminative than the hand-crafted features. However, deep feature extracted by a single CNN network is not robust enough in testing stage. To improve the ability of feature representation, we propose a new ensemble network (EnsembleNet) by dividing a single network into multiple end-to-end branches. The ensemble feature is obtained by concatenating each of the branch features to represent a person. EnsembleNet is designed based on ResNet-50 and its backbone shares most of the parameters for saving computation and memory cost. Experimental results show that our EnsembleNet achieves the state-of-the-art performance on the public Market1501, DukeMTMC-reID and CUHK03 person re-ID benchmarks.",2019,ArXiv,1901.05798,,https://arxiv.org/pdf/1901.05798.pdf
c8ca231a21c28c57d15ace1d9057879046a7f08c,1,1,0,EgoReID: Person re-identification in Egocentric Videos Acquired by Mobile Devices with First-Person Point-of-View,"Widespread use of wearable cameras and recording devices such as cellphones have opened the door to a lot of interesting research in first-person Point-of-view (POV) videos (egocentric videos). In recent years, we have seen the performance of video-based person Re-Identification (ReID) methods improve considerably. However, with the influx of varying video domains, such as egocentric videos, it has become apparent that there are still many open challenges to be faced. These challenges are a result of factors such as poor video quality due to ego-motion, blurriness, severe changes in lighting conditions and perspective distortions. To facilitate the research towards conquering these challenges, this paper contributes a new, first-of-its-kind dataset called EgoReID. The dataset is captured using 3 mobile cellphones with non-overlapping field-of-view. It contains 900 IDs and around 10,200 tracks with a total of 176,000 detections. Moreover, for each video we also provide 12-sensor meta data. Directly applying current approaches to our dataset results in poor performance. Considering the unique nature of our dataset, we propose a new framework which takes advantage of both visual and sensor meta data to successfully perform Person ReID. In this paper, we propose to adopt human body region parsing to extract local features from different body regions and then employ 3D convolution to better encode temporal information of each sequence of body parts. In addition, we also employ sensor meta data to determine target's next camera and their estimated time of arrival, such that the search is only performed among tracks present in the predicted next camera around the estimated time. This considerably improves our ReID performance as it significantly reduces our search space.",2018,ArXiv,1812.0957,,https://arxiv.org/pdf/1812.09570.pdf
c8de9f5482d9ee9c8f5422872d02723df39aad66,1,0,0,Spatial-Temporal Relation Networks for Multi-Object Tracking,"Recent progress in multiple object tracking (MOT) has shown that a robust similarity score is a key to the success of trackers. A good similarity score is expected to reflect multiple cues, e.g. appearance, location, and topology, over a long period of time. However, these cues are heterogeneous, making them hard to be combined in a unified network. As a result, existing methods usually encode them in separate networks or require a complex training approach. In this paper, we present a unified framework for similarity measurement based on spatial-temporal relation network which could simultaneously encode various cues and perform reasoning across both spatial and temporal domains. We also study the feature representation of a tracklet-object pair in depth, showing a proper design of the pair features can well empower the trackers. The resulting approach is named spatial-temporal relation networks (STRN). It runs in a feed-forward way and can be trained in an end-to-end manner. The state-of-the-art accuracy was achieved on all of the MOT15$\sim$17 benchmarks using public detection and online settings.",2019,ArXiv,1904.11489,,https://arxiv.org/pdf/1904.11489.pdf
c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3,0,0,1,Generalizing from a Few Examples: A Survey on Few-Shot Learning,"Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",2019,,,,
c9242cdb8d953b17a04d3be068f088eb51d7357e,0,1,0,Learning a Domain-Invariant Embedding for Unsupervised Person Re-identification,"Person re-identification (Re-ID) aims at matching images of the same person where images are captured by non-overlapping camera views distributed at different locations. To solve this problem, most recent works require a large pre-labeled dataset for training a deep model. These methods are not always suitable for real-world applications, because the latter often lack labeled data. In order to tackle this drawback, we proposed a novel Domain-Invariant Embedding Network (DIEN) to learn a domain-invariant embedding (DIE) feature by introducing a multi-loss joint learning with Recurrent Top- Down Attention (RTDA) mechanism. Due to the improvement in traditional triplet loss, our proposed model can benefit from both source-domain (labeled) data and target-domain (unlabeled) data. Furthermore, the resulting DIE feature not only has improved class discrimination but also robustness to domain shift. We compared our method with recent competitive algorithms and also evaluated the effectiveness of the proposed modules.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852248,
c9cc0eb2fb91ebd075e46622d92c832bd3dd1417,1,0,0,Multiple Object Tracking via Feature Pyramid Siamese Networks,"When multiple object tracking (MOT) based on the tracking-by-detection paradigm is implemented, the similarity metric between the current detections and existing tracks plays an essential role. Most of the MOT schemes based on a deep neural network learn the similarity metric using a Siamese architecture, but the plain Siamese architecture might not be enough owing to its structural simplicity and lack of motion information. This paper aims to propose a new MOT scheme to overcome the existing problems in the conventional MOTs. Feature pyramid Siamese network (FPSN) is proposed to address the structural simplicity. The FPSN is inspired by a feature pyramid network (FPN) and it extends the Siamese network by applying FPN to the plain Siamese architecture and by developing a new multi-level discriminative feature. A spatiotemporal motion feature is added to the FPSN to overcome the lack of motion information and to enhance the performance in MOT. Thus, FPSN-MOT considers not only the appearance feature but also motion information. Finally, FPSN-MOT is applied to the public MOT challenge benchmark problems and its performance is compared to that of the other state-of-the-art MOT methods.",2019,IEEE Access,,10.1109/ACCESS.2018.2889442,https://ieeexplore.ieee.org/ielx7/6287639/8600701/08587153.pdf
c9eae2c5db4c2502ca223953851821d931d262a8,1,0,0,DLGAN: Disentangling Label-Specific Fine-Grained Features for Image Manipulation,"Recent studies have shown how disentangling images into content and feature spaces can provide controllable image translation/ manipulation. In this paper, we propose a framework to enable utilizing discrete multi-labels to control which features to be disentangled, i.e., disentangling label-specific fine-grained features for image manipulation (dubbed DLGAN). By mapping the discrete label-specific attribute features into a continuous prior distribution, we leverage the advantages of both discrete labels and reference images to achieve image manipulation in a hybrid fashion. For example, given a face image dataset (e.g., CelebA) with multiple discrete fine-grained labels, we can learn to smoothly interpolate a face image between black hair and blond hair through reference images while immediately controlling the gender and age through discrete input labels. To the best of our knowledge, this is the first work that realizes such a hybrid manipulation within a single model. More importantly, it is the first work to achieve image interpolation between two different domains without requiring continuous labels as the supervision. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.",2019,ArXiv,1911.09943,,https://arxiv.org/pdf/1911.09943.pdf
ca24213986799272302f190248f8cdedaa97c3d1,0,1,0,Good practices on building effective CNN baseline model for person re-identification,"Person re-identification is indeed a challenging visual recognition task due to the critical issues of human pose variation, human body occlusion, camera view variation, etc. To address this, most of the state-of-the-art approaches are proposed based on deep convolutional neural network (CNN), being leveraged by its strong feature learning power and classification boundary fitting capacity. Although the vital role towards person re-identification, how to build effective CNN baseline model has not been well studied yet. To answer this open question, we propose 3 good practices in this paper from the perspectives of adjusting CNN architecture and training procedure. In particular, they are adding batch normalization after the global pooling layer, executing identity categorization directly using only one fully-connected layer, and using Adam as optimizer. The extensive experiments on 3 widely-used benchmark datasets demonstrate that, our propositions essentially facilitate the CNN baseline model to achieve the state-of-the-art performance without any other high-level domain knowledge or low-level technical trick.",2019,International Conference on Graphic and Image Processing,1807.11042,10.1117/12.2524386,https://arxiv.org/pdf/1807.11042.pdf
ca7710fa53cb12e313433985782ab59603a620f4,1,1,0,A Novel Unsupervised Camera-Aware Domain Adaptation Framework for Person Re-Identification,"Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of discriminative information in target domain. From the perspective of representation learning, this paper proposes a novel end-to-end deep domain adaptation framework to address them. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic in person Re-ID, and develop a “camera-aware” domain adaptation method via adversarial learning. With this method, the learned representation reduces distribution discrepancy not only between source and target domains but also across all cameras. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved representation in training process. Together, the above two methods give rise to a new unsupervised domain adaptation framework for person Re-ID. Extensive experiments and ablation studies conducted on benchmark datasets demonstrate its superiority and interesting properties.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.03425,10.1109/ICCV.2019.00817,https://arxiv.org/pdf/1904.03425.pdf
cab6c4898fc6c8f9bec493b7fd60fc9d19a165a2,0,1,0,Multi-task Network Learning Representation Features of Attributes and Identity for Person Re-identification,"Person re-identification (re-ID) has become increasingly popular due to its significance in practical application. In most of the available methods for person re-ID, the solutions focus on verification and recognition of the person identity and pay main attention to the appearance details of person. In this paper, we propose multi-task network architecture to learn powerful representation features of attributes and identity for person re-ID. Firstly, we utilize the semantic descriptor on attributes such as gender, clothing details to effectively learn representation features. Secondly, we employ joint supervision of softmax loss and center loss for person identification to obtain deep features with inter-class dispersion and intra-class compactness. Finally, we use the convolutional neural network (CNN) and multi-task learning strategy to integrate the person attributes and identity to complete classifications tasks for person re-ID. Experiments are conducted on Market1501 and DukeMTMC-reID to verify the efficiency of our method.",2018,CCBR,,10.1007/978-3-319-97909-0_73,
cabcdbefe93dcc74157e6274b43b341a7b4ed022,1,0,0,Domain Adaptive Attention Model for Unsupervised Cross-Domain Person Re-Identification,"Person re-identification (Re-ID) across multiple datasets is a challenging yet important task due to the possibly large distinctions between different datasets and the lack of training samples in practical applications. This work proposes a novel unsupervised domain adaption framework which transfers discriminative representations from the labeled source domain (dataset) to the unlabeled target domain (dataset). We propose to formulate the domain adaption task as an one-class classification problem with a novel domain similarity loss. Given the feature map of any image from a backbone network, a novel domain adaptive attention model (DAAM) first automatically learns to separate the feature map of an image to a domain-shared feature (DSH) map and a domain-specific feature (DSP) map simultaneously. Specially, the residual attention mechanism is designed to model DSP feature map for avoiding negative transfer. Then, a DSH branch and a DSP branch are introduced to learn DSH and DSP feature maps respectively. To reduce domain divergence caused by that the source and target datasets are collected from different environments, we force to project the DSH feature maps from different domains to a new nominal domain, and a novel domain similarity loss is proposed based on one-class classification. In addition, a novel unsupervised person Re-ID loss is proposed to take full use of unlabeled target data. Extensive experiments on the Market-1501 and DukeMTMC-reID benchmarks demonstrate state-of-the-art performance of the proposed method. Code will be released to facilitate further studies on the cross-domain person re-identification task.",2019,ArXiv,1905.10529,,https://arxiv.org/pdf/1905.10529.pdf
caf29efc020f140adfc29b7a9109cd68650ec021,1,0,0,Monitoring Pedestrian Flow on Campus with Multiple Cameras using Computer Vision and Deep Learning Techniques,"This paper proposes a robust method for multi-camera person re-identification (ReID), which can accurately track pedestrians across non-overlapping cameras. Closed-circuit television (CCTV) are widely used to capture pedestrian movement in different places. By integrating CCTV with computer vision and deep learning techniques, trajectory of individual pedestrian can be efficiently acquired for analyzing pedestrian walking behaviors. Many existing ReID methods aim to extract discriminative human features to distinguish a person from others. Recent state-of-the-art performance is achieved mostly by obtaining fine features from each body part. However, these part-based feature extraction methods did not consider which parts are more useful for person ReID. Therefore, this paper proposes a weighted-parts feature extraction method, such that features of specific body parts are more influential to identity prediction. After comparing the performances of utilizing each part alone, several parts are considered more view-invariant and discriminative. Higher weights are then imposed on these specific parts to extract more useful human features for person ReID. Experimental results with videos on a college campus show that the ReID accuracy of our proposed method notably outperforms many existing ones.",2020,,,10.1007/978-981-15-0802-8_184,
cb0a43e76521c57caad5b6e9b1cce6ce0e81824e,1,1,1,Progressive Learning for Person Re-Identification With One Example,"In this paper, we focus on the one-example person re-identification (re-ID) task, where each identity has only one labeled example along with many unlabeled examples. We propose a progressive framework that gradually exploits the unlabeled data for person re-ID. In this framework, we iteratively: 1) update the convolutional neural network (CNN) model and (2) estimate pseudo labels for the unlabeled data. We split the training data into three parts, i.e., labeled data, pseudo-labeled data, and index-labeled data. Initially, the re-ID model is trained using the labeled data. For the subsequent model training, we update the CNN model by the joint training on the three data parts. The proposed joint training method can optimize the model by both the data with labels (or pseudo labels) and the data without any reliable labels. For the label estimation step, instead of using a static sampling strategy, we propose a progressive sampling strategy to increase the number of the selected pseudo-labeled candidates step by step. We select a few candidates with most reliable pseudo labels from unlabeled examples as the pseudo-labeled data, and keep the rest as index-labeled data by assigning them with the data indexes. During iterations, the index-labeled data are dynamically transferred to pseudo-labeled data. Notably, the rank-1 accuracy of our method outperforms the state-of-the-art method by 21.6 points (absolute, i.e., 62.8% versus 41.2%) on MARS, and 16.6 points on DukeMTMC-VideoReID. Extended to the few-example setting, our approach with only 20% labeled data surprisingly achieves comparable performance to the supervised state-of-the-art method with 100% labeled data.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2891895,https://yu-wu.net/pdf/TIP2019_One-Example-reID.pdf
cb2a8b1cf564fb5af4868648b26d4a57057e0713,1,0,0,Robust joint learning network: improved deep representation learning for person re-identification,"Existing person re-identification methods, which based on deep representation learning, mostly only focus on either global feature or local feature. This obviously ignores the joint advantages and the correlation between global and local features. In this paper, we test and verify the benefits of jointly learning local and global features in a network based on the Convolutional Neural Network (CNN). Specifically, we give distinct weights to global loss and local loss when considering their different influence on our research, then we innovatively combine two losses into one loss. Besides, we propose a novel and strong network to learn part-level features with unified partition. Experimental results on three person ReID data sets, show that our method outperforms existing deep learning methods.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-6998-x,
cbdc7c937f854a31deb9ff83fb4c88d30b68786f,1,1,0,Identity Preserving Generative Adversarial Network for Cross-Domain Person Re-Identification,"In this paper, we study the domain adaptive person re-identification(re-ID) problem: train a re-ID model on the labeled source domain and test it on the unlabeled target domain. It’s known challenging due to the feature distribution bias between the source domain and target domain. The previous methods directly reduce the bias by image-to-image style translation between the source and the target domain in an unsupervised manner. However, these methods only consider the rough bias between the source domain and the target domain but neglect the detailed bias between the source domain and the target camera domains (divided by camera views), which contain critical factors influencing the testing performance of re-ID model. In this work, we particularly focus on the bias between the source domain and the target camera domains. To overcome this problem, a multi-domain image-to-image translation network, termed Identity Preserving Generative Adversarial Network (IPGAN) is proposed to learn the mapping relationship between the source domain and the target camera domains. IPGAN can translate the styles of images from the source domain to the target camera domains and generate many images with styles of target camera domains. Then the re-ID model is trained with the translated images generated by IPGAN. During the training of the re-ID model, we aim to learn the discriminative feature. We design and train a novel re-ID model, termed IBN-reID, in which Instance and Batch Normalization block (IBN-block) are introduced. Experimental results on Market-1501, DukeMTMC-reID and MSMT17 show that the images generated by IPGAN are more suitable for cross-domain re-ID. Very competitive re-ID accuracy is achieved by our method.",2019,IEEE Access,1811.1151,10.1109/ACCESS.2019.2933910,https://arxiv.org/pdf/1811.11510.pdf
cbe8fa1b7d7d602049a186c9340fb46f8b791a23,0,1,0,GENERATIVE ADVERSARIAL NETWORK BASED ACOUSTIC SCENE TRAINING SET AUGMENTATION AND SELECTION USING SVM HYPERPLANE,"Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the crossvalidated experiments on development DB, the usage of the generated features could improve ASC performance.",2017,,,,http://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Mun_215.pdf
cbf5b3469c7216c37733efca6c2cdb94357b14a7,1,1,0,Person Re-identification Based on Feature Fusion and Triplet Loss Function,"The task of Person re-identification (re-ID) is to recognize an individual observed by non-overlapping cameras. Robust feature representation is a crucial problem in re-ID. With the rise of deep learning, most current approaches adopt convolutional neural networks (CNN) to extract features. However, the feature representation learned by CNN is often global and lacks detailed local information. To address this issue, this paper proposes a simple CNN architecture consisting of a re-ID subnetwork and an attribute sub-network. In re-ID sub-network, global feature and semantic feature are extracted and fused in a weighted manner, and triplet loss is adopted to further improve the discriminative ability of the learned fusion feature. On the other hand, attribute sub-network focuses on local aspects of a person and offers local structural information that is helpful for re-ID. The two sub-networks are combined on the loss level and their complementary aspects are leveraged to improve the re-ID accuracy. Comparative evaluations demonstrate that our method outperforms several state-of-the-art ones. On the challenging Market1501 and DukeMTMC datasets, 86.3% rank-1 accuracy and 69.4% mAP, and 72.1% rank-1 accuracy and 53.4% mAP are achieved respectively.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8546082,
cbf63c44422a9a94f349af7ab193ad251ca56ff3,0,1,0,Towards Precise Intra-camera Supervised Person Re-identification,"Intra-camera supervision (ICS) for person re-identification (Re-ID) assumes that identity labels are independently annotated within each camera view and no inter-camera identity association is labeled. It is a new setting proposed recently to reduce the burden of annotation while expect to maintain desirable Re-ID performance. However, the lack of inter-camera labels makes the ICS Re-ID problem much more challenging than the fully supervised counterpart. By investigating the characteristics of ICS, this paper proposes camera-specific non-parametric classifiers, together with a hybrid mining quintuplet loss, to perform intra-camera learning. Then, an inter-camera learning module consisting of a graph-based ID association step and a Re-ID model updating step is conducted. Extensive experiments on three large-scale Re-ID datasets show that our approach outperforms all existing ICS works by a great margin. Our approach performs even comparable to state-of-the-art fully supervised methods in two of the datasets.",2020,ArXiv,2002.04932,,https://arxiv.org/pdf/2002.04932.pdf
cc10e4659cb72ef719414cdfc0e3c38a97f9c650,1,0,0,AANet: Attribute Attention Network for Person Re-Identifications,"This paper proposes Attribute Attention Network (AANet), a new architecture that integrates person attributes and attribute attention maps into a classification framework to solve the person re-identification (re-ID) problem. Many person re-ID models typically employ semantic cues such as body parts or human pose to improve the re-ID performance. Attribute information, however, is often not utilized. The proposed AANet leverages on a baseline model that uses body parts and integrates the key attribute information in an unified learning framework. The AANet consists of a global person ID task, a part detection task and a crucial attribute detection task. By estimating the class responses of individual attributes and combining them to form the attribute attention map (AAM), a very strong discriminatory representation is constructed. The proposed AANet outperforms the best state-of-the-art method \cite{Sun_2018_ECCV} using ResNet-50 by 3.36\% in mAP and 3.12\% in Rank-1 accuracy on DukeMTMC-reID dataset. On Market1501 dataset, AANet achieves 92.38\% mAP and 95.10\% Rank-1 accuracy with re-ranking, outperforming~\cite{kalayeh2018human}, another state of the art method using ResNet-152, by 1.42\% in mAP and 0.47\% in Rank-1 accuracy. In addition, AANet can perform person attribute prediction (e.g., gender, hair length, clothing length etc.), and localize the attributes in the query image.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1912.09021,10.1109/CVPR.2019.00730,https://arxiv.org/pdf/1912.09021.pdf
cc1cc334897af613e9103de9afe23e1882e0c153,1,1,0,Hyperbolic Image Embeddings,"Computer vision tasks such as image classification, image retrieval, and few-shot learning are currently dominated by Euclidean and spherical embeddings so that the final decisions about class belongings or the degree of similarity are made using linear hyperplanes, Euclidean distances, or spherical geodesic distances (cosine similarity). In this work, we demonstrate that in many practical scenarios, hyperbolic embeddings provide a better alternative.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.02239,10.1109/cvpr42600.2020.00645,https://arxiv.org/pdf/1904.02239.pdf
cc2433902036de4004dec9be5e4099544741e500,0,1,0,Pose Guided Gated Fusion for Person Re-identification,"Person re-identification is an important yet challenging problem in visual recognition. Despite the recent advances with deep learning (DL) models for spatio-temporal and multi-modal fusion, re-identification approaches often fail to leverage the contextual information (e.g., pose and illumination) to dynamically select the most discriminant con-volutional filters (i.e., appearance features) for feature representation and inference. State-of-the-art techniques for gated fusion employ complex dedicated part- or attention-based architectures for late fusion, and do not incorporate pose and appearance information to train the backbone network. In this paper, a new DL model is proposed for pose-guided re-identification, comprised of a deep backbone, pose estimation, and gated fusion network. Given a query image of an individual, the backbone convolutional NN produces a feature embedding required for pair-wise matching with embeddings for reference images, where feature maps from the pose network and from mid-level CNN layers are combined by the gated fusion network to generate pose-guided gating. The proposed framework allows to dynamically activate the most discriminant CNN filters based on pose information in order to perform a finer grained recognition. Extensive experiments on three challenging benchmark datasets indicate that integrating the pose-guided gated fusion into the state-of-the-art re-identification backbone architecture allows to improve their recognition accuracy. Experimental results also support our intuition on the advantages of gating backbone appearance information using the pose feature maps at mid-level CNN layers.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093370,http://openaccess.thecvf.com/content_WACV_2020/papers/Bhuiyan_Pose_Guided_Gated_Fusion_for_Person_Re-identification_WACV_2020_paper.pdf
cc2c68333a847dc758893a48941142781ad31291,0,1,0,Domain Adaptive Person Re-Identification via Coupling Optimization,"Domain adaptive person Re-Identification (ReID) is challenging owing to the domain gap and shortage of annotations on target scenarios. To handle those two challenges, this paper proposes a coupling optimization method including the Domain-Invariant Mapping (DIM) method and the Global-Local distance Optimization (GLO), respectively. Different from previous methods that transfer knowledge in two stages, the DIM achieves a more efficient one-stage knowledge transfer by mapping images in labeled and unlabeled datasets to a shared feature space. GLO is designed to train the ReID model with unsupervised setting on the target domain. Instead of relying on existing optimization strategies designed for supervised training, GLO involves more images in distance optimization, and achieves better robustness to noisy label prediction. GLO also integrates distance optimizations in both the global dataset and local training batch, thus exhibits better training efficiency. Extensive experiments on three large-scale datasets,i.e., Market-1501, DukeMTMC-reID, andMSMT17, show that our coupling optimization outperforms state-of-the-art methods by a large margin. Our method also works well in unsupervised training, and even outperforms several recent domain adaptive methods.",2020,ACM Multimedia,2011.03363,10.1145/3394171.3413904,https://arxiv.org/pdf/2011.03363.pdf
cc59e72f564b80d6bdf1fff8ce0799ba9f59873f,0,1,0,BCaR: Beginner Classifier as Regularization Towards Generalizable Re-ID,"In recent years, the performance of person re-identification has been dramatically improved by virtue of sophisticated training methods. However, most of the existing methods are based on the assumption that the statistics of a target domain can be utilized during training. This inevitably introduces huge costs for data collection each time a person re-identification system is deployed, which hinders the applicability to real-world scenarios. To mitigate this issue, we expand upon the concept of domain generalization. Typical person re-identification datasets are composed of a large amount of identities. However, examples for each identity are rather scarce. It is widely known that if examples are highly biased, over-fitting is likely to occur and degrade the performance. To alleviate this problem, we propose a novel soft-label regularization method that combines an expert feature extractor with a beginner classifier for generating soft labels. From a representation learning perspective, a convolutional neural network-based feature extractor is thought to prioritize common patterns. Therefore, the subsequent classifier typically fits common examples first, followed by rare ones. On the basis of this observation, we force the beginner classifier to remain uncertain towards rare examples by means of periodic initialization. Accordingly, the beginner classifier assigns highly confident labels to common examples and ambiguous labels to rare ones, thus enabling soft labels to mitigate over-fitting to biased examples (e.g., highly occluded ones). Extensive analysis shows that our method successfully assigns ambiguous labels to biased examples and thus increases the rank-1 accuracy by 3.4 %, 1.6 %, 0.9 %, and 5.2 % on the VIPeR, PRID, GRID, and i-LIDS datasets, respectively. The source codes are available at https://github.com/hitachi-rd-cv/bcar.",2020,,,,https://www.bmvc2020-conference.com/assets/papers/0303.pdf
cc6378854c605e604d40d48410edbfa59bf37e5a,0,1,0,Uncertainty-optimized deep learning model for small-scale person re-identification,"In recent years, deep learning has developed rapidly and is widely used in various fields, such as computer vision, speech recognition, and natural language processing. For end-to-end person re-identification, most deep learning methods rely on large-scale datasets. Relatively few methods work with small-scale datasets. Insufficient training samples will affect neural network accuracy significantly. This problem limits the practical application of person re-identification. For small-scale person re-identification, the uncertainty of person representation and the overfitting problem associated with deep learning remain to be solved. Quantifying the uncertainty is difficult owing to complex network structures and the large number of hyperparameters. In this study, we consider the uncertainty of pedestrian representation for small-scale person re-identification. To reduce the impact of uncertain person representations, we transform parameters into distributions and conduct multiple sampling by using multilevel dropout in a testing process. We design an improved Monte Carlo strategy that considers both the average distance and shortest distance for matching and ranking. When compared with state-of-the-art methods, the proposed method significantly improve accuracy on two small-scale person re-identification datasets and is robust on four large-scale datasets.",2019,Science China Information Sciences,,10.1007/s11432-019-2675-3,
cca002c219413b1152a85e8d88a6245725d9a1c2,0,1,0,Multi-level Similarity Perception Network for Person Re-identification,"In this article, we propose a novel deep Siamese architecture based on a convolutional neural network (CNN) and multi-level similarity perception for the person re-identification (re-ID) problem. According to the distinct characteristics of diverse feature maps, we effectively apply different similarity constraints to both low-level and high-level feature maps during training stage. Due to the introduction of appropriate similarity comparison mechanisms at different levels, the proposed approach can adaptively learn discriminative local and global feature representations, respectively, while the former is more sensitive in localizing part-level prominent patterns relevant to re-identifying people across cameras. Meanwhile, a novel strong activation pooling strategy is utilized on the last convolutional layer for abstract local-feature aggregation to pursue more representative feature representations. Based on this, we propose final feature embedding by simultaneously encoding original global features and discriminative local features. In addition, our framework has two other benefits: First, classification constraints can be easily incorporated into the framework, forming a unified multi-task network with similarity constraints. Second, as similarity-comparable information has been encoded in the network’s learning parameters via back-propagation, pairwise input is not necessary at test time. That means we can extract features of each gallery image and build an index in an off-line manner, which is essential for large-scale real-world applications. Experimental results on multiple challenging benchmarks demonstrate that our method achieves splendid performance compared with the current state-of-the-art approaches.",2019,ACM Trans. Multim. Comput. Commun. Appl.,,10.1145/3309881,
cca9d893b9ce6f21e2565442680690b99c74fa8f,0,1,0,Interaction-And-Aggregation Network for Person Re-Identification,"Person re-identification (reID) benefits greatly from deep convolutional neural networks (CNNs) which learn robust feature embeddings. However, CNNs are inherently limited in modeling the large variations in person pose and scale due to their fixed geometric structures. In this paper, we propose a novel network structure, Interaction-and-Aggregation (IA), to enhance the feature representation capability of CNNs. Firstly, Spatial IA (SIA) module is introduced. It models the interdependencies between spatial features and then aggregates the correlated features corresponding to the same body parts. Unlike CNNs which extract features from fixed rectangle regions, SIA can adaptively determine the receptive fields according to the input person pose and scale. Secondly, we introduce Channel IA (CIA) module which selectively aggregates channel features to enhance the feature representation, especially for small-scale visual cues. Further, IA network can be constructed by inserting IA blocks into CNNs at any depth. We validate the effectiveness of our model for person reID by demonstrating its superiority over state-of-the-art methods on three benchmark datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1907.08435,10.1109/CVPR.2019.00954,https://arxiv.org/pdf/1907.08435.pdf
ccc8203baf3fa4b6410c6fe2c5cc9b46a476f04d,0,1,0,Disassembling the Dataset: A Camera Alignment Mechanism for Multiple Tasks in Person Re-identification,"In person re-identification (ReID), one of the main challenges is the distribution inconsistency among different datasets. Previous researchers have defined several seemingly individual topics, such as fully supervised learning, direct transfer, domain adaptation, and incremental learning, each with different settings of training and testing scenarios. These topics are designed in a dataset-wise manner, i.e., images from the same dataset, even from disjoint cameras, are presumed to follow the same distribution. However, such distribution is coarse and training-set-specific, and the ReID knowledge learned in such manner works well only on the corresponding scenarios. To address this issue, we propose a fine-grained distribution alignment formulation, which disassembles the dataset and aligns all training and testing cameras. It connects all topics above and guarantees that ReID knowledge is always learned, accumulated, and verified in the aligned distributions. In practice, we devise the Camera-based Batch Normalization, which is easy for integration and nearly cost-free for existing ReID methods. Extensive experiments on the above four ReID tasks demonstrate the superiority of our approach. The code will be publicly available.",2020,ArXiv,,,
cd79213a5e10dbd039328232dbb51eb4a80f3b5f,1,0,1,Video Person Re-Identification using Learned Clip Similarity Aggregation,"We address the challenging task of video-based person re-identification. Recent works have shown that splitting the video sequences into clips and then aggregating clip-based similarity is appropriate for the task. We show that using a learned clip similarity aggregation function allows filtering out hard clip pairs, e.g. where the person is not clearly visible, is in a challenging pose, or where the poses in the two clips are too different to be informative. This allows the method to focus on clip-pairs which are more informative for the task. We also introduce the use of 3D CNNs for video-based re-identification and show their effectiveness by performing equivalent to previous works, which use optical flow in addition to RGB, while using RGB inputs only. We give quantitative results on three challenging public benchmarks and show better or competitive performance. We also validate our method qualitatively.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1910.08055,10.1109/WACV45572.2020.9093510,https://arxiv.org/pdf/1910.08055.pdf
cdacb6df332da90135e7f4a54908c4c506e69e05,0,1,0,Consistency-Preserving deep hashing for fast person re-identification,"Abstract Numerous methods have been proposed for person re-identification (Re-ID) with promising performances. While most of them neglect the matching efficiency which is crucial in real-world applications. Recently, several hashing based approaches have been developed, which consider the importance of matching speed in large-scale datasets. Despite the considerable efficiency of these traditional and deep learning based hashing methods, the concomitant matching accuracy reduction is unacceptable in practical application. Towards this end, we propose a novel deep hashing framework, namely Consistency-Preserving Deep Hashing (CPDH), aiming to bridge the gap between the effective high-dimensional feature and low-dimensional binary vector by focusing on the consistency preservation of hash code. First, CPDH designs a new hash structure to extract the hash code. Next, a noise consistency cost is proposed to improve robustness of both hash code and high-dimensional feature. Finally, a topology consistency cost is provided to maintain the ordinal relation between the high-dimensional feature space and Hamming space. Comprehensive experimental results on three widely-used benchmark datasets demonstrate the superior performance of proposed method as compared with existing state-of-the-art approaches.",2019,Pattern Recognit.,,10.1016/J.PATCOG.2019.05.036,
cdc324bd97f735a5031537509afe525801e432f6,1,0,0,BIRDSAI: A Dataset for Detection and Tracking in Aerial Thermal Infrared Videos,"Monitoring of protected areas to curb illegal activities like poaching and animal trafficking is a monumental task. To augment existing manual patrolling efforts, unmanned aerial surveillance using visible and thermal infrared (TIR) cameras is increasingly being adopted. Automated data acquisition has become easier with advances in unmanned aerial vehicles (UAVs) and sensors like TIR cameras, which allow surveillance at night when poaching typically occurs. However, it is still a challenge to accurately and quickly process large amounts of the resulting TIR data. In this paper, we present the first large dataset collected using a TIR camera mounted on a fixed-wing UAV in multiple African protected areas. This dataset includes TIR videos of humans and animals with several challenging scenarios like scale variations, background clutter due to thermal reflections, large camera rotations, and motion blur. Additionally, we provide another dataset with videos synthetically generated with the publicly available Microsoft AirSim simulation platform using a 3D model of an African savanna and a TIR camera model. Through our benchmarking experiments on state-of-the-art detectors, we demonstrate that leveraging the synthetic data in a domain adaptive setting can significantly improve detection performance. We also evaluate various recent approaches for single and multi-object tracking. With the increasing popularity of aerial imagery for monitoring and surveillance purposes, we anticipate this unique dataset to be used to develop and evaluate techniques for object detection, tracking, and domain adaptation for aerial, TIR videos.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093284,https://projects.iq.harvard.edu/files/teamcore/files/2020_07_teamcore_wacv_birdsai.pdf
cdcd5cfc25793b861b1f8db79f3bbeab45533ace,0,1,0,SaADB: A Self-attention Guided ADB Network for Person Re-identification,"Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on person image representation and re-ID task via feature erasing. However, BDB drops the features randomly which may lead to sub-optimal results. In this paper, we propose a novel Self-attention guided Adaptive DropBlock network (SaADB) for person re-ID which can adaptively erase the most discriminative regions. Specifically, SaADB first obtains a self-attention map by channel-wise pooling and returns a drop mask by thresholding the self-attention map. Then, the input features and self-attention guided drop mask are multiplied to generate the dropped feature maps. Meanwhile, we utilize the spatial and channel attention to learn a better feature map and iteratively train with the feature dropping module for person re-ID. Experiments on several benchmark datasets demonstrate that the proposed SaADB significantly beats the prevalent competitors in person re-ID.",2020,ArXiv,2007.03584,,https://arxiv.org/pdf/2007.03584.pdf
cde97d98e8840050b05faafa477b7f349dc00092,1,1,0,ESA-ReID: Entropy-Based Semantic Feature Alignment for Person re-ID,"Person re-identification (re-ID) is a challenging task in real-world. Besides the typical application in surveillance system, re-ID also has significant values to improve the recall rate of people identification in content video (TV or Movies). However, the occlusion, shot angle variations and complicated background make it far away from application, especially in content video. In this paper we propose an entropy based semantic feature alignment model, which takes advantages of the detailed information of the human semantic feature. Considering the uncertainty of semantic segmentation, we introduce a semantic alignment with an entropy-based mask which can reduce the negative effects of mask segmentation errors. We construct a new re-ID dataset based on content videos with many cases of occlusion and body part missing, which will be released in future. Extensive studies on both existing datasets and the new dataset demonstrate the superior performance of the proposed model.",2020,ArXiv,2007.04644,,https://arxiv.org/pdf/2007.04644.pdf
cf4184300a6453134af723cc598b620b336aec29,0,1,0,Partial Person Re-identification with Alignment and Hallucination,"Partial person re-identification involves matching pedestrian frames where only a part of a body is visible in corresponding images. This reflects practical CCTV surveillance scenario, where full person views are often not available. Missing body parts make the comparison very challenging due to significant misalignment and varying scale of the views. We propose Partial Matching Net (PMN) that detects body joints, aligns partial views and hallucinates the missing parts based on the information present in the frame and a learned model of a person. The aligned and reconstructed views are then combined into a joint representation and used for matching images. We evaluate our approach and compare to other methods on three different datasets, demonstrating significant improvements.",2018,ACCV,1807.09162,10.1007/978-3-030-20876-9_7,https://arxiv.org/pdf/1807.09162.pdf
cf430a05fffa6f2b2f07985504bdccba78ed09b9,1,0,0,Long-Term Cloth-Changing Person Re-identification,,2020,ArXiv,2005.12633,,https://arxiv.org/pdf/2005.12633.pdf
cf6fd00f2f82a4240073e095da08cbb3fca8d101,1,0,0,Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID,,2020,ArXiv,2006.02713,,https://arxiv.org/pdf/2006.02713.pdf
cf869f393b9f9d019aa23cc26777659bde93e2d9,1,0,0,Unsupervised Domain Adaptation Through Synthesis For Person Re-Identification,"Person re-identification is a hot topic because of its widespread applications in video surveillance and public security. However, it remains a challenging task because of drastic variations in illumination or background across surveillance cameras, which causes the current methods can not work well in real-world scenarios. In addition, due to the scarce dataset, many methods suffer from over-fitting to a different extent. To remedy the above two problems, firstly, we develop a data collector and labeler, which can generate the synthetic random scenes and simultaneously annotate them without any manpower. Based on it, we build a large-scale, diverse synthetic dataset. Secondly, we propose a novel unsupervised Re-ID method via domain adaptation, which can exploit the synthetic data to boost the performance of re-identification in a completely unsupervised way, and free humans from heavy data annotations. Extensive experiments show that our proposed method achieves the state-of-the-art performance on two benchmark datasets, and is very competitive with current cross-domain Re-ID method.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/icme46284.2020.9102822,
cf8adfd3d6ab5f2a2f24fae821bd056e996663e5,1,0,0,Self-balance Motion and Appearance Model for Multi-object Tracking in UAV,"Under the tracking-by-detection framework, multi-object tracking methods try to connect object detections with target trajectories by reasonable policy. Most methods represent objects by the appearance and motion. The inference of the association is mostly judged by a fusion of appearance similarity and motion consistency. However, the fusion ratio between appearance and motion are often determined by subjective setting. In this paper, we propose a novel self-balance method fusing appearance similarity and motion consistency. Extensive experimental results on public benchmarks demonstrate the effectiveness of the proposed method with comparisons to several state-of-the-art trackers.",2019,MMAsia,,10.1145/3338533.3366561,
cfe9065ce9f22822607e82234551e0c5262ae394,1,1,0,Feature preserving GAN and multi-scale feature enhancement for domain adaption person Re-identification,"Abstract The performance of person Re-identification (Re-ID) model depends much on its training dataset, and drops significantly when the detector is applied to a new scene due to the large variations between the source training dataset and the target scene. In this paper, we proposed multi-scale Feature Enhancement(MFE) Re-ID model and Feature Preserving Generative Adversarial Network (FPGAN) for cross-domain person Re-ID task. Here, MFE Re-ID model provides a strong baseline model for cross-domain person Re-ID task, and FPGAN bridges the domain gap to improve the performance of Re-ID on target scene. In the MFE Re-ID model, person semantic feaure maps, extracted from backbone of segmentation model, enhance person body region’s multi-scale feature responce. This operation could capture multi-scale robust discriminative visual factors related to person. In FPGAN, we translate the labeled images from source to target domain in an unsupervised manner, and learn a transfer function to preserve the person perceptual information of source images and ensure the transferred person images show similar styles with the target dataset. Extensive experiments demonstrate that combining FPGAN and MFE Re-ID model could achieve state-of-the-art results in cross-domain Re-ID task on DukeMTMC-reID and Market-1501 datasets. Besides, MFE Re-ID model could achieve state-of-the-art results in supervised Re-ID task. All source codes and models will be released for comparative study.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.07.063,
d00a0fbdb45fe19bd17bf874e0ffd0b00352d1ba,1,0,0,Marine Vessel Re-Identification: A Large-Scale Dataset and Global-and-Local Fusion-Based Discriminative Feature Learning,"A marine vessel re-identification system has to determine whether or not different images represent the same vessel. Accurate vessel re-identification improves onshore closed-circuit television monitoring in a vessel traffic services system as well as onboard surveillance of surrounding vessels. However, because ships are rigid bodies and the marine environment is harsh, the accurate re-identification of vessels at sea can be very difficult. We describe a marine vessel-re-identification framework, Global-and-Local Fusion-based Multi-view Feature Learning (GLF-MVFL), which is based on a combination of global and fine-grained local features. GLF-MVFL combines cross-entropy loss with our newly-developed orientation-guided quintuplet loss. We exploit intrinsic features of marine vessels to optimize multi-view representation learning for re-identification. GLF-MVFL uses ResNet-50 as the backbone network to extract features for simultaneous quintuple input. It detects and discriminates between features and estimates viewpoints to form a comprehensive re-identification framework. We created an annotated large-scale vessel retrieval dataset, VesselID-539, which contains images from viewpoints similar to those of an autonomous surface vessel, to use in evaluating the performance of the model. Extensive experiments and analysis of the results obtained from using VesselID-539 demonstrate that our approach significantly increases the accuracy of vessel re-identification and is more effective and robust for images from different viewpoints than other approaches.",2020,IEEE Access,,10.1109/ACCESS.2020.2969231,
d01dda7383329d7f2420546ec936c1a0bc40f05b,0,1,0,Improving Person Re-identification by Background Subtraction Using Two-Stream Convolutional Networks,"The field of person re-identification is facing problems related to the variation of illumination and background scenes. In order to reduce the impact of those variations, we propose in this work a two-stream re-identification system based on a siamese network (S-CNN). The proposed system takes as input a pair of person images: the original image and the image without background. In the background subtraction step, a segmentation network (SEG-CNN) is used to detect the person body part and capture a complementary information. We experimentally prove that the combination of the two streams (images with and without background) improves the recognition rates. In the rank-1, the improvement is respectively of \(2\%\) and \(4\%\) for Market-1501 and DukeMTMC-reID datasets.",2019,ICIAR,,10.1007/978-3-030-27202-9_31,
d0d63012c572875f5e29258ddb02b8fe3f8324f1,1,0,0,Improving Person Re-Identification Performance Using Body Mask Via Cross-Learning Strategy,"The task of person re-identification (re-id) is to find the same pedestrian across non-overlapping cameras. Normally, the performance of person re-id can be affected by background clutters. However, existing segmentation algorithms are hard to obtain perfect foreground person images. To effectively leverage the body (foreground) cue, and in the meantime pay attention to discriminative information in the background (e.g., companion or vehicle), we propose to use a cross-learning strategy to take both foreground and other discriminative information into account. In addition, since currently existing foreground segmentation result always involves noise, we use Label Smoothing Regularization (LSR) to strengthen the generalization capability during our learning process. In experiments, we pick up two state-of-the-art person re-id methods to verify the effectiveness of our proposed cross-learning strategy. Our experiments are carried out on two publicly available person re-id datasets. Obvious performance improvements can be observed on both datasets.",2019,2019 IEEE Visual Communications and Image Processing (VCIP),,10.1109/VCIP47243.2019.8965655,
d0ddfc2db6fb5a3a24eb4de588b4dd991f42b2c8,1,1,0,A Survey on Deep Learning-Based Person Re-Identification Systems,"Person re-identification systems (person Re-ID) have recently gained more attention between computer vision researchers. They are playing a key role in intelligent visual surveillance systems and have widespread applications like applications for public security. The person Re-ID systems can identify if a person has been seen by a non-overlapping camera over large camera network in an unconstrained environment. It is a challenging issue since a person appears differently under different camera views and faces many challenges such as pose variation, occlusion and illumination changes. Many methods had been introduced for generating handcrafted features aimed to handle the person Re-ID problem. In recent years, many studies have started to apply deep learning methods to enhance the person Re-ID performance due the deep learning yielded significant results in computer vision issues. Therefore, this paper is a survey of the recent studies that proposed to improve the person Re-ID systems using deep learning. The public datasets that are used for evaluating these systems are discussed. Finally, the paper addresses future directions and current issues that must be considered toward improving the person Re-ID systems.",2019,IEEE Access,,10.1109/ACCESS.2019.2957336,
d1e09c88785d8587f75988bd94865f7fd96b7026,1,0,0,Deviation based clustering for unsupervised person re-identification,"Abstract Recently, unsupervised person re-identification (re-ID) has gained a lot of attention, since it does not depend on intensive manual annotation and is more practical to deploy in the real world directly. An inspiring method, the Bottom-up Clustering (BUC), achieves the state-of-the-art among unsupervised re-ID methods and outperforms most semi-supervised and transfer learning algorithms. The BUC utilizes the minimum-distance between samples in different clusters as the merging criterion, and the number of samples as the penalization term. However, the minimum-distance criterion only considers one pair of samples between two clusters, and cannot exploit the information of all samples in clusters. The penalization, intuitively, is unsuitable for the dataset with irregular sample quantity. To relieve this problem, we propose a deviation based clustering re-ID approach, which takes the inter- and intra-cluster deviation into consideration. The inter-cluster deviation denotes the increase of deviation after merging two clusters, considering all samples between the two clusters to merge. The intra-cluster deviation, working as penalization, denotes the distance between samples and the center in each cluster, and thus it can help to mitigate the side-effect of irregular datasets. The two criterions can reflect the inter- and intra- dispersion precisely. Based on these criterions, we group similar samples into clusters and utilize the cluster identities as a pseudo annotation to train our model. To evaluate our proposed approach, we implement abundant experiments on two popular re-ID datasets, where one has irregular sample quality (i.e., Market-1501) and the other has regular sample quality (i.e., DukeMTMC-reID). Evaluations show that our method outperforms BUC by 1.8% on Rank-1 (i.e., 68.0% accuracy) and 2.1% on mAP (i.e., 40.4% accuracy) for the Market-1501 dataset, and well maintains the benefit of BUC on the DukeMIMC-reID dataset with the accuracy of 47.8% in Rank-1 and 27.0% in mAP.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.04.039,
d1e8a08aafc64661206c495e963e0b244dfe59cd,1,0,0,R 4-A . 1 : Dynamics-Based Video Analytics,"Faculty/Staff Name Title Institution Email Octavia Camps Co-PI Northeastern University camps@coe.neu.edu Mario Sznaier Co-PI Northeastern University msznaier@coe.neu.edu Graduate, Undergraduate and REU Students Name Degree Pursued Institution Month/Year of Graduation Mengran Gou PhD Northeastern University 5/2018 Sadjad Esfeden Asghari PhD Northeastern University 12/2020 Wenqian Liu PhD Northeastern University 12/2020 Bengizu Ozbay PhD Northeastern University 12/2019 Dong Yin PhD Northeastern University 12/2020 Xikang Zhang PhD Northeastern University 5/2018 Yuexi Zhang PhD Northeastern University 12/2021 Abhishek Sharma MS Northeastern University 6/2018 Can Uner MS Northeastern University 6/2019",2015,,,,https://pdfs.semanticscholar.org/aa42/0d32c48a3fd526a91285673cd55ca9fe2447.pdf
d1f1c1b60b8e963bf4023c8282a0c316ab06a35a,1,0,0,A Signal Processing Perspective on Human Gait: Decoupling Walking Oscillations and Gestures,"This study focuses on gesture recognition in mobile interaction settings, i.e. when the interacting partners are walking. This kind of interaction requires a particular coordination, e.g. by staying in the field of view of the partner, avoiding obstacles without disrupting group composition and sustaining joint attention during motion. In literature, various studies have proven that gestures are in close relation in achieving such goals.",2019,ICR,,10.1007/978-3-030-26118-4_8,
d2345f8decf8f942256a0987ebd4d8573a9ff399,1,0,0,Occlusion-robust Online Multi-object Visual Tracking using a GM-PHD Filter with a CNN-based Re-identification,"We propose a novel online multi-object visual tracking algorithm via a tracking-by-detection paradigm using a Gaussian mixture Probability Hypothesis Density (GM-PHD) filter and deep Convolutional Neural Network (CNN) appearance representations learning. The GM-PHD filter has a linear complexity with the number of objects and observations while estimating the states and cardinality of unknown and time-varying number of objects in the scene. Though it handles object birth, death and clutter in a unified framework, it is susceptible to miss-detections and does not include the identity of objects. We use visual-spatio-temporal information obtained from object bounding boxes and deeply learned appearance representations to perform estimates-to-tracks data association for labelling of each target. We learn the deep CNN appearance representations by training an identification network (IdNet) on large-scale person re-identification data sets. We also employ additional unassigned tracks prediction after the update step to overcome the susceptibility of the GM-PHD filter towards miss-detections caused by occlusion. Our tracker which runs in real-time is applied to track multiple objects in video sequences acquired under varying environmental conditions and objects density. Lastly, we make extensive evaluations on Multiple Object Tracking 2016 (MOT16) and 2017 (MOT17) benchmark data sets and find out that our online tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy and identification.",2019,ArXiv,1912.05949,,https://arxiv.org/pdf/1912.05949.pdf
d2b86b6dc93631990e21a12278e77f002fb4b116,1,0,0,Aalborg Universitet Attention in Multimodal Neural Networks for Person Re-identification,"In spite of increasing interest from the research community, person re-identification remains an unsolved problem. Correctly deciding on a true match by comparing images of a person, captured by several cameras, requires extraction of discriminative features to counter challenges such as changes in lighting, viewpoint and occlusion. Besides devising novel feature descriptors, the setup can be changed to capture persons from an overhead viewpoint rather than a horizontal. Furthermore, additional modalities can be considered that are not affected by similar environmental changes as RGB images. In this work, we present a Multimodal ATtention network (MAT) based on RGB and depth modalities. We combine a Convolution Neural Network with an attention module to extract local and discriminative features that are fused with globally extracted features. Attention is based on correlation between the two modalities and we finally also fuse RGB and depth features to generate a joint multilevel RGB-D feature. Experiments conducted on three datasets captured from an overhead view show the importance of attention, increasing accuracies by 3.43%, 2.01% and 2.13% on OPR, DPI-T and TVPR, respectively.",2018,,,,http://vbn.aau.dk/files/274076313/attention_multimodal.pdf
d36ec4906c9457a43f66f1e1f4c0b6342709541c,0,1,0,Deep Metric Learning with Online Hard and Soft Selection for Person Re-identification,"Deep metric learning has been widely used for image retrieval and verification tasks. Traditional contrastive loss and triplet loss depend highly on the selection of pair/triplet images. It makes the training process unstable and uncomplete. In this paper, we propose a novel global level loss function that considers histograms for intra distances within class and inter distances between different classes. We compared two forms of global level loss (hard selection based loss and soft selection based loss) and both achieved better result than traditional triplet loss, multi-class N pair loss and other related works. The experiment is conducted on the person re-identification dataset Market 1501 and DukeMTMC-reID.",2018,"2018 Joint 7th International Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",,10.1109/ICIEV.2018.8641037,
d374bed85c3d788fdb1a752954c6f34839004d33,1,0,0,Vision Meets Wireless Positioning: Effective Person Re-identification with Recurrent Context Propagation,,2020,ArXiv,2008.04146,,https://arxiv.org/pdf/2008.04146.pdf
d39599b5d6af5d55690361564522c3b366e3abc8,0,1,0,Parallel Connected Generative Adversarial Network with Quadratic Operation for SAR Image Generation and Application for Classification,"Thanks to the availability of large-scale data, deep Convolutional Neural Networks (CNNs) have witnessed success in various applications of computer vision. However, the performance of CNNs on Synthetic Aperture Radar (SAR) image classification is unsatisfactory due to the lack of well-labeled SAR data, as well as the differences in imaging mechanisms between SAR images and optical images. Therefore, this paper addresses the problem of SAR image classification by employing the Generative Adversarial Network (GAN) to produce more labeled SAR data. We propose special GANs for generating SAR images to be used in the training process. First, we incorporate the quadratic operation into the GAN, extending the convolution to make the discriminator better represent the SAR data; second, the statistical characteristics of SAR images are integrated into the GAN to make its value function more reasonable; finally, two types of parallel connected GANs are designed, one of which we call PWGAN, combining the Deep Convolutional GAN (DCGAN) and Wasserstein GAN with Gradient Penalty (WGAN-GP) together in the structure, and the other, which we call CNN-PGAN, applying a pre-trained CNN as a discriminator to the parallel GAN. Both PWGAN and CNN-PGAN consist of a number of discriminators and generators according to the number of target categories. Experimental results on the TerraSAR-X single polarization dataset demonstrate the effectiveness of the proposed method.",2019,Sensors,,10.3390/s19040871,https://pdfs.semanticscholar.org/d395/99b5d6af5d55690361564522c3b366e3abc8.pdf
d3b8b0e39e1e3b31f643f040d2b866fa2b5fad6f,0,0,1,Fine-Grained Fusion With Distractor Suppression for Video-Based Person Re-Identification,"Video based person re-identification aims to associate video clips with the same identity by designing discriminative and representative features. Existing approaches simply compute representations for video clips via frame-level or region-level feature aggregation, where fine-grained local information is inaccessible. To address this issue, we propose a novel module called fine-grained fusion with distractor suppression (short as FFDS) to fully exploit the local features towards better representation of a specific video clip. Concretely, in the proposed FFDS module, the importance of each local feature of an anchor image is calculated by pixel-wise correlation mining with other intra-sequence frames. In this way, ’good’ local features co-exist across the video frames are enhanced in the attention map, while sparse ’distractors’ can be suppressed. Moreover, to maintain the high-level semantic information of deep CNN features as well as enjoying the fine-grained local information, we adopt the feature mimicking scheme during the training process. Extensive experiments on two challenging large-scale datasets demonstrate effectiveness of the proposed method.",2019,IEEE Access,,10.1109/ACCESS.2019.2932102,
d4a5c9b2197b6bc476aa296b8d59515c9684e97d,1,1,0,CA3Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification,"Person re-identification aims to identify the same pedestrian across non-overlapping camera views. Deep learning techniques have been applied for person re-identification recently, towards learning representation of pedestrian appearance. This paper presents a novel Contextual-Attentional Attribute-Appearance Network ($\rm CA^3Net$) for person re-identification. The $\rm CA^3Net$ simultaneously exploits the complementarity between semantic attributes and visual appearance, the semantic context among attributes, visual attention on attributes as well as spatial dependencies among body parts, leading to discriminative and robust pedestrian representation. Specifically, an attribute network within $\rm CA^3Net$ is designed with an Attention-LSTM module. It concentrates the network on latent image regions related to each attribute as well as exploits the semantic context among attributes by a LSTM module. An appearance network is developed to learn appearance features from the full body, horizontal and vertical body parts of pedestrians with spatial dependencies among body parts. The $\rm CA^3Net$ jointly learns the attribute and appearance features in a multi-task learning manner, generating comprehensive representation of pedestrians. Extensive experiments on two challenging benchmarks, i.e., Market-1501 and DukeMTMC-reID datasets, have demonstrated the effectiveness of the proposed approach.",2018,ACM Multimedia,1811.07544,10.1145/3240508.3240585,https://arxiv.org/pdf/1811.07544.pdf
d5227420a84c89898641b828d54c9d4f9eac09e0,1,0,0,TVV: Real-Time Visual Identity and Tracking with Edge Computing,"Video surveillance today has become pervasive, making visual identification and tracking technology attractive to a broad class of applications like traffic counting, crime tracking, and Blockchain. However, visual tracking is also a victim of the ubiquity of surveillance camera: a huge amount of data that generated by the cameras leads to severe congestion problem, which decreases the frame rate and in turn affects the tracking accuracy. In this paper, we present TVV, a real-time visual tracking system that leverages edge computing to support accurate and continuous tracking in large scale areas. The design of TVV is based on a insight that almost 80% of frames in a video stream exhibit high quality, and such frames can be processed on the edge nodes using a lightweight filtering method named KCF. Based on this insight, TVV adaptively load the visual tacking task on the edge or the server, based on the quality of the currently generated frame. In this way, the traffic load is largely decreased, without sacrificing the tracking accuracy. Our experimental result show that the average frame rate of TVV achieves 45.75 fps, outperforming most state-of-the-art visual tracking approaches.",2019,EWSN,,,
d52a7e12cd38c24058212d78ba89797520d4dcef,1,1,0,Deep Multi-Index Hashing for Person Re-Identification,"Traditional person re-identification (ReID) methods typically represent person images as real-valued features, which makes ReID inefficient when the gallery set is extremely large. Recently, some hashing methods have been proposed to make ReID more efficient. However, these hashing methods will deteriorate the accuracy in general, and the efficiency of them is still not high enough. In this paper, we propose a novel hashing method, called deep multi-index hashing (DMIH), to improve both efficiency and accuracy for ReID. DMIH seamlessly integrates multi-index hashing and multi-branch based networks into the same framework. Furthermore, a novel block-wise multi-index hashing table construction approach and a search-aware multi-index (SAMI) loss are proposed in DMIH to improve the search efficiency. Experiments on three widely used datasets show that DMIH can outperform other state-of-the-art baselines, including both hashing methods and real-valued methods, in terms of both efficiency and accuracy.",2019,ArXiv,1905.1098,,https://arxiv.org/pdf/1905.10980.pdf
d52c105b47a7f2139c3ed3e51a5e0a755b723835,0,1,0,Improving deep ensemble vehicle classification by using selected adversarial samples,"Abstract Most image classification algorithms aim to maximize the percentage of class labels that are predicted correctly. These algorithms often missclassify images from minority categories as into the dominant categories. To overcome the issue of unbalanced data for classifying vehicles from traffic surveillance images, we propose a semi supervised pipeline focused on integrating deep neural networks with data augmentation based on generative adversarial nets (GANs). The proposed approach consists of three main stages. In the first stage, we trained several GANs on the original dataset to generate adversarial samples for the rare classes. In the second stage, an ensemble of CNN models with different architectures are trained on the original imbalanced data set, and then a sample selection step is performed to filter out the low-quality adversarial samples. In the final stage, the aforementioned ensemble model is refined on the augmented dataset by adding the selected adversarial samples. Experiments on the highly imbalanced large benchmark “MIOvision Traffic Camera Dataset (MIO-TCD)” classification challenge dataset demonstrate that the proposed framework is able to increase the mean performance of some categories to some extent, while maintaining a high overall accuracy, compared with the baseline.",2018,Knowl. Based Syst.,,10.1016/j.knosys.2018.06.035,
d5382a96feac7bc7248845fe9396c99fb9847c05,1,1,0,Unsupervised Disentanglement GAN for Domain Adaptive Person Re-Identification,"While recent person re-identification (ReID) methods achieve high accuracy in a supervised setting, their generalization to an unlabelled domain is still an open problem. In this paper, we introduce a novel unsupervised disentanglement generative adversarial network (UD-GAN) to address the domain adaptation issue of supervised person ReID. Our framework jointly trains a ReID network for discriminative features extraction in a source labelled domain using identity annotation, and adapts the ReID model to an unlabelled target domain by learning disentangled latent representations on the domain. Identity-unrelated features in the target domain are distilled from the latent features. As a result, the ReID features better encompass the identity of a person in the unsupervised domain. We conducted experiments on the Market1501, DukeMTMC and MSMT17 datasets. Results show that the unsupervised domain adaptation problem in ReID is very challenging. Nevertheless, our method shows improvement in half of the domain transfers and achieve state-of-the-art performance for one of them.",2020,ArXiv,2007.1556,,https://arxiv.org/pdf/2007.15560.pdf
d549188111f78af46ccb04ea58961d4503b881f1,0,1,0,Pedestrian Retrieval Using Valuable Absence Augmentation,"In this paper, we propose a novel data augmentation method named valuable absence augmentation (VAA) in order to alleviate the overfitting and evaluate the influence of the pedestrian valuable parts for the network performance. Specifically, we first train a base convolutional neural network model and obtain the attention map of the pedestrian. Then, we use the attention map to generate new samples. Finally, original samples and new samples are combined to fine-tune the base network model. We conduct experiments on a large-scale pedestrian retrieval database, i.e., Market-1501. Experimental results show that the pedestrian valuable part has a crucial influence for the network performance and that the proposed method achieves better performance than other state-of-the-art methods.",2020,,,10.1007/978-981-15-0187-6_29,
d575c947215b4c8da2598637f7c6ff1d8f0700f6,0,1,0,A plug-and-play LSTM-based attention module for person re-identification,"Spatial attention mechanism is widely used to extract local feature in person re-identification. However, some existing multi-stage spatial attention structures lack flexibility and require complicated training process. In this paper, a plug-and-play LSTM-based Attention Module(LAM) is proposed to enhance flexibility of the multi-attention mechanism. First, we employ the single-stage multi-attention structure to replace the traditional multi-stage multi-attention structure. Our structure encapsulates multiple attention machines in single module and thus the module can be added to any backbone networks without any modification directly. Then, correlation is introduced to spatial attention machines through LSTM. Correlation between different attention machines preserves diversity of the local feature and exploit the capacity of multi-attention mechanism. Moreover, the LAM is added to the backbone network in the form of residual, which enables the LAM to be trained with the backbone network synchronously. Therefore, the training process is simplified effectively. Experiments on CUHK03, Market-1501 and DukeMTMC-ReID datasets demonstrate the advantage of the proposed method.",2020,International Conference on Graphic and Image Processing,,10.1117/12.2557241,
d61dba7a4ac023c08fc5e2bda15774673013b1d1,0,1,0,Multi-Discriminator Generative Adversarial Network for Semi-Supervised SAR Target Recognition,"Convolutional neural network (CNN) has shown powerful potential on synthetic aperture radar (SAR) automatic target recognition (ATR). However, the training of deep structure of CNN has high requirement for sufficient labeled sample images while the existing SAR images are limited and difficult to obtain. In this paper, an improved recognition architecture which combines CNN with generative adversarial network (GAN) is proposed. We generate unlabeled images from the training dataset by GAN and use them together with the original images for SAR. Then the label prediction is implemented by the CNN based semi-supervised learning. In order to address the instability issue in training caused by the adversarial principal of GAN, the multi-discriminator GAN (MGAN) architecture is introduced in the proposed framework. Meanwhile, the label smoothing is utilized to regularize the semi-supervised model of CNN classification. Experiment results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset indicate that the proposed method can effectively improve the recognition accuracy and robustness of CNN system.",2019,2019 IEEE Radar Conference (RadarConf),,10.1109/radar.2019.8835801,
d6569507bcc8066a6afcbabbd65d0d6562e896d3,0,1,0,Multi-camera transfer GAN for person re-identification,"Abstract Person re-identification is a cross-camera retrieval task. Person re-identification performance in a single dataset has been significantly improved, but person re-identification model trained in one dataset usually can't work well in another dataset. To solve this problem, this paper proposes a method of image-to-image translation, CTGAN (Multi-Camera Transfer GAN), which can be performed on multiple camera domains of pedestrian dataset by using one single model. The marked training images are transferred to each camera of the target dataset. At the same time, for the feature learning model, this paper adopts the MSCDA (Mixed Selective Convolution Descriptor Aggregation) method, which can locate the main pedestrian objects in the image, filter out the background noise, and keep the useful depth descriptor. In the paper, experiments show that the method is effective.",2019,J. Vis. Commun. Image Represent.,,10.1016/J.JVCIR.2019.01.029,
d67dd32a6bb38e530642f97fd763aff9a24e5f69,1,0,0,SMOT: Single-Shot Multi Object Tracking,"We present single-shot multi-object tracker (SMOT), a new tracking framework that converts any single-shot detector (SSD) model into an online multiple object tracker, which emphasizes simultaneously detecting and tracking of the object paths. Contrary to the existing tracking by detection approaches [37,43,17] which suffer from errors made by the object detectors, SMOT adopts the recently proposed scheme of tracking by re-detection. We combine this scheme with SSD detectors by proposing a novel tracking anchor assignment module. With this design SMOTis able to generate tracklets with a constant per-frame runtime. A light-weighted linkage algorithm is then used for online tracklet linking. On three benchmarks of object tracking: Hannah, Music Videos, and MOT17, the proposed SMOT achieves state-of-the-art performance.",2020,ArXiv,2010.16031,,https://arxiv.org/pdf/2010.16031.pdf
d6d96b6405c1636bb7080ee809c8ec7fa1ada98f,1,1,0,Efficient Pipelines for Vision-Based Context Sensing,"Context awareness is an essential part of mobile and ubiquitous computing. Its goal is to unveil situational information about mobile users like locations and activities. The sensed context can enable many services like navigation, AR, and smarting shopping. Such context can be sensed in different ways including visual sensors. There is an emergence of vision sources deployed worldwide. The cameras could be installed on roadside, in-house, and on mobile platforms. This trend provides huge amount of vision data that could be used for context sensing. However, the vision data collection and analytics are still highly manual today. It is hard to deploy cameras at large scale for data collection. Organizing and labeling context from the data are also labor intensive. In recent years, advanced vision algorithms and deep neural networks are used to help analyze vision data. But this approach is limited by data quality, labeling effort, and dependency on hardware resources. In summary, there are three major challenges for today’s vision-based context sensing systems: data collection and labeling at large scale, process large data volumes efficiently with limited hardware resources, and extract accurate context out of vision data. The thesis explores the design space that consists of three dimensions: sensing task, sensor types, and task locations. Our prior work explores several points in this design space. Specifically, we develop Gnome [227] for accurate outdoor localization. It leverages 2D and 3D information from Google Street View to mitigate GPS signal’s error in different cities, and uses offloading and caching to work efficiently on stock phones in real time. For vision-only outdoor sensing, we present ALPS [199] that applies optimized object detection algorithms to detects and localizes roadside objects captured in Google Street View. It optimizes the processing speed by adaptively downloading and processing images. For indoor scenarios, we designed TAR [225] and Grab [226] for tracking people and detecting their interactions with objects. Both projects fuse camera outputs with those of other sensors to achieve state-of-the-art performance. For outdoor behavior sensing, we develop Caesar [224] to abstract complex activities as graphs, and define customized activities using extensible vocabulary. Caesar also uses lazy evaluation optimizations to reduce GPU processing overhead and mobile energy consumption. The thesis makes contributions by (1) developing efficient and scalable solutions for different points in the design space of vision-based sensing tasks; (2) achieving state-of-the-art accuracy in those applications; (3) and developing guidelines for designing such sensing systems.",2020,ArXiv,2011.00427,,https://arxiv.org/pdf/2011.00427.pdf
d6e1fd34daf96536534a56f663b23bf58c99cf62,0,1,0,Geometry Guided Pose-Invariant Facial Expression Recognition,"Driven by recent advances in human-centered computing, Facial Expression Recognition (FER) has attracted significant attention in many applications. However, most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifier for each pose. Different from existing methods, this paper proposes an end-to-end deep learning model that allows to simultaneous facial image synthesis and pose-invariant facial expression recognition by exploiting shape geometry of the face image. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, given an input face and a target pose and expression designated by a set of facial landmarks, an identity-preserving face can be generated through guiding by the target pose and expression. Second, the identity representation is explicitly disentangled from both expression and pose variations through the shape geometry delivered by facial landmarks. Third, our model can automatically generate face images with different expressions and poses in a continuous way to enlarge and enrich the training set for the FER task. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on both controlled and in-the-wild benchmark datasets including Multi-PIE, BU-3DFE, and SFEW. The code is included in the supplementary material.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2972114,
d7bf5a50975e33eeae4277c0ea5de3505f46bdcf,1,0,0,Customized Multi-person Tracker,"This work addresses the task of multi-person tracking in crowded street scenes, where long-term occlusions pose a major challenge. One popular way to address this challenge is to re-identify people before and after occlusions using Convolutional Neural Networks (CNNs). To achieve good performance, CNNs require a large amount of training data, which is not available for multi-person tracking scenarios. Instead of annotating large training sequences, we introduce a customized multi-person tracker that automatically adapts its person re-identification CNNs to capture the discriminative appearance patterns in a test sequence. We show that a few high-quality training examples that are automatically mined from the test sequence can be used to fine-tune pre-trained CNNs, thereby teaching them to recognize the uniqueness of people’s appearance in the test sequence. To that end, we introduce a hierarchical correlation clustering (HCC) framework, in which we utilize an existing robust correlation clustering tracking model, but with different graph structures to generate local, reliable tracklets as well as globally associated tracks. We deploy intuitive physical constraints on the local tracklets to generate the high-quality training examples for customizing the person re-identification CNNs. Our customized multi-person tracker achieves state-of-the-art performance on the challenging MOT16 tracking benchmark.",2018,ACCV,,10.1007/978-3-030-20890-5_39,http://is.tuebingen.mpg.de/uploads_file/attachment/attachment/469/0509.pdf
d7c6b3628725638188dee085b58752f754697d99,1,1,0,Learning an Evolutionary Embedding via Massive Knowledge Distillation,"Knowledge distillation methods aim at transferring knowledge from a large powerful teacher network to a small compact student one. These methods often focus on close-set classification problems and matching features between teacher and student networks from a single sample. However, many real-world classification problems are open-set. This paper proposes an Evolutionary Embedding Learning (EEL) framework to learn a fast and accurate student network for open-set problems via massive knowledge distillation. First, we revisit the formulation of canonical knowledge distillation and make it suitable for the open-set problems with massive classes. Second, by introducing an angular constraint, a novel correlated embedding loss (CEL) is proposed to match embedding spaces between the teacher and student network from a global perspective. Lastly, we propose a simple yet effective paradigm towards a fast and accurate student network development for knowledge distillation. We show the possibility to implement an accelerated student network without sacrificing accuracy, compared with its teacher network. The experimental results are quite encouraging. EEL achieves better performance with other state-of-the-art methods for various large-scale open-set problems, including face recognition, vehicle re-identification and person re-identification.",2020,International Journal of Computer Vision,,10.1007/s11263-019-01286-x,
d8050d377c4390c31d4cf98638fb838446a7ee24,1,0,0,A Hybrid of Hard and Soft Attention for Person Re-Identification,"Existing pedestrian re-identification methods based on deep learning have achieved good results under constrained conditions. However, there exist some challenges including large human pose variations, viewpoint changes, severe occlusions and imprecise detection of persons. So we present a Hard$/$Soft hybrid Attention Network (HSAN) that combines pose information and attention mechanism to deal with the challenges. Our model includes two main parts: Pose-guided Hard Attention (PHA) and Regional Soft Attention (RSA). PHA uses the keypoints generated by pose estimation to enhance the foreground information, and RSA is learned to eliminate the background clutter. We extract reliable features and locate discriminative regions by using these two modules to handle occlusions, pose changes and background noises. We conduct a lot of experiments on public datasets including DukeMTMC-ReID, Market-1501, and CUHK03, and the results show that our method achieves state-of-the-art performance.",2019,2019 Chinese Automation Congress (CAC),,10.1109/CAC48633.2019.8997406,
d80a0f4a98d6c91c31677d87b30ae77dd355266b,0,1,0,Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark,"Person re-identification (ReID) benefits greatly from the accurate annotations of existing datasets (e.g., CUHK03 \cite{li2014deepreid} and Market-1501 \cite{zheng2015scalable}), which are quite expensive because each image in these datasets has to be assigned with a proper label. In this work, we explore to ease the annotation of ReID by replacing the accurate annotation with inaccurate annotation, i.e., we group the images into bags in terms of time and assign a bag-level label for each bag. This greatly reduces the annotation effort and leads to the creation of a large-scale ReID benchmark called SYSU-30$k$. The new benchmark contains $30k$ categories of persons, which is about $20$ times larger than CUHK03 ($1.3k$ categories) and Market-1501 ($1.5k$ categories), and $30$ times larger the ImageNet ($1k$ categories). It totally sums up to 29,606,918 images. Learning a ReID model with bag-level annotation is called the weakly supervised ReID problem. To solve this problem, we introduce conditional random fields (CRFs) to capture the dependencies from all images in a bag and generate a reliable pseudo label for each person image. The pseudo label is further used to supervise the learning of the ReID model. When compared with the fully supervised ReID models, our method achieves the state-of-the-art performance on SYSU-30$k$ and other datasets. The code, dataset, and pretrained model will be available online.",2019,ArXiv,,,
d84ff9149d7b4dd8ee1b4549b9c52c9832ef0f8d,0,1,0,Color inference from semantic labeling for person search in videos,"We propose an explainable model to generate semantic color labels for person search. In this context, persons are described from their semantic parts, such as hat, shirt, etc. Person search consists in looking for people based on these descriptions. In this work, we aim to improve the accuracy of color labels for people. Our goal is to handle the high variability of human perception. Existing solutions are based on hand-crafted features or learnt features that are not explainable. Moreover most of them only focus on a limited set of colors. We propose a method based on binary search trees and a large peer-labelled color name dataset. This allows us to synthesize the human perception of colors. Using semantic segmentation and our color labeling method, we label segments of pedestrians with their associated colors. We evaluate our solution on person search on datasets such as PCN, and show a precision as high as 80.4%.",2020,ICIAR,1911.13114,10.1007/978-3-030-50347-5_13,https://arxiv.org/pdf/1911.13114.pdf
d8ab3b54fe007d3716c6885b3a95d874cbff792e,1,1,0,Memory-based Jitter: Improving Visual Recognition on Long-tailed Data with Diversity In Memory,"This paper considers deep visual recognition on long-tailed data, with the majority categories only occupying relatively few samples. The tail categories are prone to lack of within-class diversity, which compromises the representative ability of the learned visual concepts. A radical solution is to augment the tail categories with higher diversity. To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ) to gain extra diversity for the tail data. We observe that the deep model keeps on jittering from one historical edition to another, even when it already approaches convergence. The ``jitter'' means the small variations between historical models. We argue that such jitter largely originates from the within-class diversity of the overall data and thus encodes the within-class distribution pattern. To utilize such jitter for tail data augmentation, we store the jitter among historical models into a memory bank and get the so-called Memory-based Jitter. With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, \emph{i.e.}, image classification and deep metric learning (on long-tailed data). On image classification, MBJ collects the historical embeddings to learn an accurate classifier. In contrast, on deep metric learning, it collects the historical prototypes of each class to learn a robust deep embedding. Under both scenarios, MBJ enforces higher concentration on tail classes, so as to compensate for their lack of diversity. Extensive experiments on three long-tailed classification benchmarks and two deep metric learning benchmarks (person re-identification, in particular) demonstrate the significant improvement. Moreover, the achieved performance are on par with the state-of-the-art on both tasks.",2020,ArXiv,2008.09809,,https://arxiv.org/pdf/2008.09809.pdf
d8be272d7981e5b8d0a6d0eb2c02b9f73ae84ce2,0,1,0,Person Re-identification with Joint-Loss,"Person re-identification is a technique that search the given target in the video surveillance network. This technique has been widely pplied to security and surveillance system, and also become a esearch hotspot in computer vision. Person re-identification has been challenging due to the large number of cameras in the network and ariation in camera angles, illumination, occlusion and poses. In this paper, we proposed a person re-id approach that can resist occlusions and variations based on a human pose guided convolution neural network framework with joint loss functions. We extract local features from body parts localized by landmarks, merge it with global features to learn the similarity metric. Identification loss and pose-constrained triplet loss function are jointly employed to train the model. Our approach outperforms most state-of-the-art methods on three large-scale datasets, with an accuracy of 83.31%, 86.1% and 72.6% on Cuhk03, Market1501 and Duke MTMC-reID respectively.",2017,2017 International Conference on Virtual Reality and Visualization (ICVRV),,10.1109/ICVRV.2017.00010,
d8c68f098626136cd40b735f26eada6d7a0bd439,0,1,0,Person Re-Identification Using Attribute Priori Distribution,"In order to improve the recognition accuracy of pedestrian recognition based on deep learning and attribute learning, a new neural network model is proposed to identify pedestrian attributes and IDs. Compared with the existing methods, this model has three advantages, one is to increase an all connected layer in the network to guarantee the model′s migration which improves the network′s discriminant ability after fine-tuning; Second, based on the number of samples of each attribute, we normalized processing the loss of each attribute in the loss function, to avoid the number of disequilibrium effect between classes among the data set which attributes to identify results; third, use the data of each attribute distribution in the prior knowledge, through a number of ratios to adjust each attribute′s weight to avoid the influence of positive and negative samples′ disequilibrium. Experimental results show that the proposed algorithm has high recognition rate, the rank-1 accuracy reached 86.90% on Market 1501 dataset, 72.83% on DukeMTMC dataset and 75.68% on PETA dataset, it also shows that the proposed algorithm has good robustness on illumination changes, pedestrians posture changes, perspective changes and occlusions.",2019,,,,http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2019/5/PDF/zdhxb-45-5-953.pdf
d8dd0aff09922823b9fb8b3b78f66c558ea7c7d0,1,0,0,Intra-Frame Object Tracking by Deblatting,"Objects moving at high speed along complex trajectories often appear in videos, especially videos of sports. Such objects elapse non-negligible distance during exposure time of a single frame and therefore their position in the frame is not well defined. They appear as semi-transparent streaks due to the motion blur and cannot be reliably tracked by standard trackers. We propose a novel approach called Tracking by Deblatting based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. The trajectory is then estimated by fitting a piecewise quadratic curve, which models physically justifiable trajectories. As a result, tracked objects are precisely localized with higher temporal resolution than by conventional trackers. The proposed TbD tracker was evaluated on a newly created dataset of videos with ground truth obtained by a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms baseline both in recall and trajectory accuracy.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),1905.03633,10.1109/ICCVW.2019.00283,https://arxiv.org/pdf/1905.03633.pdf
d90a998e11dc26a540ca4aeed7bb375311b45932,1,1,0,Person Re-Identification Via Pose-Aware Multi-Semantic Learning,"Person re-identification (ReID) remains an open-ended research topic, with its variety of substantial applications such as tracking, searching, etc. Existing methods mostly explore the highest-semantic feature embedding, ignoring the insights hidden among the earlier layers. Moreover, owing to the misalignment and pose variations, pose-related information is of great significance and needs to be comprehensively utilized. In this paper, we present a novel person ReID framework called Pose-aware Multi-semantic Fusion Network (PMFN). First, taking into account multiple semantics, we propose Multi-semantic Fusion Network (MFN) as the backbone, employing several shortcuts to reserve bypass feature maps for subsequent fusion. Second, to learn a pose-sensitive embedding, pose-aware clues are considered, forming the complete PMFN and investigating the well-aligned global and local body regions. Finally, the center loss is introduced for enhancing the feature discriminability. Exhaustive experiments on two large-scale person ReID benchmarks demonstrate the strengths of our approach over recent state-of-the-art works.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/icme46284.2020.9102719,
d9216cc2a3c03659cb2392b7cc8509feb7829579,0,1,0,Adaptation and Re-identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-identification,"Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID methods which require fully supervised data for training.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1804.09347,10.1109/CVPRW.2018.00054,https://arxiv.org/pdf/1804.09347.pdf
d947c0a7ab0d800d0237360606b6e2796d3907eb,0,1,0,Exploring deep learning representations for biometric multimodal systems.,"Biometrics is an important area of research today. A complete biometric system comprises sensors, feature extraction, pattern matching algorithms, and decision making. Biometric systems demand high accuracy and robustness, and researchers are using a combination of several biometric sources, two or more algorithms for pattern matching and di↵erent decision-making systems. These systems are called multimodal biometric systems and today represent state-of-the-art for biometrics. However, the process of extracting features in multimodal biometric systems poses a major challenge today. Deep learning has been used by researchers in the machine learning field to automatize the feature extraction process and several advances were achieved, such as the case of face recognition problem. However, deep learning based methods require a large amount of data and with the exception of facial recognition, there are no databases large enough for the other biometric modalities, hindering the application of deep learning in multimodal methods. In this thesis, we propose a set of contributions to favor the use of deep learning in multimodal biometric systems. First of all, we explore data augmentation and transfer learning techniques for training deep convolution networks, in restricted biometric databases in terms of labeled images. Second, we propose a simple protocol, aiming at reproducibility, for the creation and evaluation of multimodal (or synthetic) multimodal databases. This protocol allows the investigation of multiple biometric modalities combination, even for less common and novel modalities. Finally, we investigate the impact of merging multimodal biometric systems in which all modalities are represented by means of deep descriptors.",2019,,,,https://pdfs.semanticscholar.org/d947/c0a7ab0d800d0237360606b6e2796d3907eb.pdf
d95d9fe8f32a6b328436799a06a7c7de5da14c79,1,0,0,A Discriminative Person Re-Identification Model With Global-Local Attention and Adaptive Weighted Rank List Loss,"At present, occlusion and appearance similarity pose severe challenges to person re-identification tasks. Although many robust deep convolutional neural networks alleviate these problems, convolutional layers with limited receptive fields cannot model global semantic information well. In addition, in the person re-identification model, many metric losses ignore or destroy the intra-class structure of the sample, which makes the model difficult to be optimized. Therefore, we design a discriminative Re-identification model with global-local attention and adaptive weighted rank list loss (GLWR). Specifically, our global-local attention (GL-Attention) learns the semantic context in the channel and spatial dimensions. By learning the dependencies between features, GL-Attention integrates global semantic information into local features to extract discriminative features. Unlike rank list loss, our adaptive weighted rank list loss (WRLL) adaptively assigns weights according to the metric distance between the negative sample and the input image, which further improves the performance of the model. Experimental studies on three public datasets (Market-1501, DukeMTMC-ReID and CUHK03) indicate that the performance of our GLWR is significantly superior to many of the latest algorithms.",2020,IEEE Access,,10.1109/ACCESS.2020.3036985,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09252959.pdf
d978f926af597072857321d76c18666bd938a042,0,1,0,Generating artificial images of plant seedlings using generative adversarial networks,"Plants seedlings are a part of a domain with low inter-class and relatively high intra-class variance with respect to visual appearance. This paper presents an approach for generating artificial image samples of plant seedlings using generative adversarial networks (GAN) to alleviate for the lack of training data for deep learning systems in this domain. We show that it is possible to use GAN to produce samples that are visually distinct across nine different plants species and maintain a high amount variance within each species. The generated samples resemble the intended species with an average recognition accuracy of 58.9 ± 9.2 % , evaluated using a state-of-the-art classification model. The observed errors are related to samples representing species which are relatively anonymous at the dicotyledonous growth stage and to the model's incapability to reproduce small shape details. The artificial plant samples are also used for pretraining a classification model, which is finetuned using real data. The pretrained model achieves 62.0 ± 5.3 % accuracy on classifying real plant seedlings prior to any finetuning, thus providing a strong basis for further training. However, finetuning the pretrained models show no performance increase compared to models trained without finetuning, as both approaches are capable of achieving near perfect classification on the dataset applied in this work.",2019,,,10.1016/j.biosystemseng.2019.09.005,
d99866f3576e4375b79ba2c703e3cd6192be2994,1,1,0,An End-to-End Foreground-Aware Network for Person Re-Identification,"Person re-identification is a crucial task of identifying pedestrians of interest across multiple surveillance camera views. In person re-identification, a pedestrian is usually represented with features extracted from a rectangular image region that inevitably contains the scene background, which incurs ambiguity to distinguish different pedestrians and degrades the accuracy. To this end, we propose an end-to-end foreground-aware network to discriminate foreground from background by learning a soft mask for person re-identification. In our method, in addition to the pedestrian ID as supervision for foreground, we introduce the camera ID of each pedestrian image for background modeling. The foreground branch and the background branch are optimized collaboratively. By presenting a target attention loss, the pedestrian features extracted from the foreground branch become more insensitive to the backgrounds, which greatly reduces the negative impacts of changing backgrounds on matching an identical across different camera views. Notably, in contrast to existing methods, our approach does not require any additional dataset to train a human landmark detector or a segmentation model for locating the background regions. The experimental results conducted on three challenging datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17, demonstrate the effectiveness of our approach.",2019,ArXiv,1910.11547,,https://arxiv.org/pdf/1910.11547.pdf
da1e7d442d2e5d6b39a0015e241a2b3fe4036a13,1,0,0,"MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection","We present the Multiview Extended Video with Activities (MEVA) dataset, a new and very-large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. Our dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity. We have annotated 144 hours for 37 activity types, marking bounding boxes of actors and props. Our collection observed approximately 100 actors performing scripted scenarios and spontaneous background activity over a three-week period at an access-controlled venue, collecting in multiple modalities with overlapping and non-overlapping indoor and outdoor viewpoints. The resulting data includes video from 38 RGB and thermal IR cameras, 42 hours of UAV footage, as well as GPS locations for the actors. 122 hours of annotation are sequestered in support of the NIST Activity in Extended Video (ActEV) challenge; the other 22 hours of annotation and the corresponding video are available on our website, along with an additional 306 hours of ground camera data, 4.6 hours of UAV data, and 9.6 hours of GPS logs. Additional derived data includes camera models geo-registering the outdoor cameras and a dense 3D point cloud model of the outdoor scene. The data was collected with IRB oversight and approval and released under a CC-BY-4.0 license.",2020,ArXiv,2012.00914,,https://arxiv.org/pdf/2012.00914.pdf
da3b4a098573798891da2cc1cc25f06310ed83b8,0,1,0,MT-IVSN: a novel model for vehicle re-identification,"Image re-identification is usually used to find specific images from image libraries or video sequences. In recent years, convolutional neural networks have gradually become the dominant method in this field. In this paper, a Multitask Identification-Verification Siamese Network for vehicles is proposed, which combines color feature vectors with the inherent structure, category and other attributes of the vehicle. A vehicle 2D structural model and a method of image viewpoint normalization are also presented to ensure the high consistency of features in the expression of images from different angles. In addition, based on the vehicle 2D structural model, a dynamic annular non-uniform partition color super-pixel sampling strategy for vehicle face is investigated to construct a color feature vector. In the experiments, the proposed model and method are evaluated on the public vehicles and VeRi datasets. The experimental results show that the proposed method and model have made great progress.",2020,,,10.1007/s12652-020-01988-y,
da646a89396309d4af2a52336f994ba828470e53,0,1,0,Open-World Person Re-Identification With Deep Hash Feature Embedding,"Most existing person re-identification (re-id) methods are designed based on the artificial closed-set assumption that the probe and gallery identities are exactly overlapped with a small search pool. This leads to poor scalability in real-world applications where the task is often to re-id a small set of target people (i.e., watch-list) among a large search pool with unknown ID overlap, namely, an open-set deployment setting. In this paper, we firstly propose a new person re-id setting called Watch-List based Open-Set (WLOS) person re-id, which is characterised by the above open-set deployment and a watch-list available at the training stage. Then, we address such a under-studied WLOS problem by formulating a novel Task Dedicated Deep Hashing (TDDH) approach which learning a purpose-specific deep hash model particularly for the given target people in an efficient end-to-end manner. Extensive experiments on three large-scale re-id benchmarks are conducted to demonstrate the advantages and superiority of the TDDH over a wide range of the state-of-the-art hashing and re-id methods under the more realistic open-set setting.",2019,IEEE Signal Processing Letters,,10.1109/LSP.2019.2946965,
dacf90db5fe8de3aec17b5edbb6c630f57cbe5bc,1,1,0,Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians,"In the conventional person Re-ID setting, it is widely assumed that cropped person images are for each individual. However, in a crowded scene, off-shelf-detectors may generate bounding boxes involving multiple people, where the large proportion of background pedestrians or human occlusion exists. The representation extracted from such cropped images, which contain both the target and the interference pedestrians, might include distractive information. This will lead to wrong retrieval results. To address this problem, this paper presents a novel deep network termed Pedestrian-Interference Suppression Network (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to enhance the feature of the target in the gallery, under the guidance of the query. Furthermore, the involving Guidance Reversed Attention Module and the Multi-Person Separation Loss promote QGAB to suppress the interference of other pedestrians. Our method is evaluated on two new pedestrian-interference datasets and the results show that the proposed method performs favorably against existing Re-ID methods.",2020,ECCV,2008.06963,10.1007/978-3-030-58539-6_39,https://arxiv.org/pdf/2008.06963.pdf
dad39f0e10285242ae7c825146c552a4844c741a,1,0,0,Person Re-Identification Using Hybrid Representation Reinforced by Metric Learning,"Person Re-Identification (Re-Id) is among the main constituents of an automated visual surveillance system. It aims at finding out true matches of a given query person from a large repository of non-overlapping camera images/videos. In this paper, we have proposed an efficient Re-Id approach that is based on a highly discriminative hybrid person representation which combines the low-level hand-crafted appearance based features together with the mid-level attributes and semantic based deep features. The low-level hand crafted features are extracted by using hierarchical Gaussian and local histogram distributions in different color spaces. These features incorporate discriminative texture, shape and color information which is invariant to distractors, e.g., variations in pose, viewpoint and illumination, and so on. The mid-level attribute based deep features are extracted to incorporate contextual- and semantic-based information. The feature space is optimized and self-learned using cross-view quadratic discriminant analysis and multiple metric learning, with the aim to reduce the intra-class differences and increase the inter-class variations for robust person matching. The proposed framework is evaluated on publicly available small scale (VIPeR, PRID450s, and GRID) and large scale (CUHK01, Market1501, and DukeMTMC-ReID) person Re-Id datasets. The experimental results show that the hybrid hand-crafted and deep features outperformed the existing state-of-the-art in approaches in the unsupervised paradigm.",2018,IEEE Access,,10.1109/ACCESS.2018.2882254,
daf5a4a48f9f007e41ff54b4aa2016449ad9c22d,1,0,0,Tracking-by-Counting: Using Network Flows on Crowd Density Maps for Tracking Multiple Targets,"State-of-the-art multi-object tracking~(MOT) methods follow the tracking-by-detection paradigm, where object trajectories are obtained by associating per-frame outputs of object detectors. In crowded scenes, however, detectors often fail to obtain accurate detections due to heavy occlusions and high crowd density. In this paper, we propose a new MOT paradigm, tracking-by-counting, tailored for crowded scenes. Using crowd density maps, we jointly model detection, counting, and tracking of multiple targets as a network flow program, which simultaneously finds the global optimal detections and trajectories of multiple targets over the whole video. This is in contrast to prior MOT methods that either ignore the crowd density and thus are prone to errors in crowded scenes, or rely on a suboptimal two-step process using heuristic density-aware point-tracks for matching targets.Our approach yields promising results on public benchmarks of various domains including people tracking, cell tracking, and fish tracking.",2020,ArXiv,2007.09509,,https://arxiv.org/pdf/2007.09509.pdf
db05d6d1b9938d3aa246a6f5e03412ec2191bd0f,1,0,0,Hierarchical attributes learning for pedestrian re-identification via parallel stochastic gradient descent combined with momentum correction and adaptive learning rate,"Convolutional neural networks (CNNs) have obtained high accuracy results for pedestrian re-identification in the past few years. There is always a trade-off between high accuracy and computational time in CNNs. Training CNN is always very difficult as it may take a long time to produce high accuracy results. To overcome this limitation, a novel method parallel stochastic gradient descent (PSGD) is proposed to train a five-hierarchical parallel CNNs that is designed according to pedestrian attributes. Moreover, the momentum correction and adaptive adjustment of learning rate are applied during training process and the time interval for updating parameters is inspected during optimization of parameters selection. The results of this paper prove the effectiveness of proposed PSGD that successfully decreases the training process by five times and surpasses the state-of-the-art methods of pedestrian re-identification in terms of both accuracy and time. The minimum reported running time of the proposed method is 8.7 s which is minimum among all other state-of-the-art methods. These promising results show the efficiency and performance of the proposed model.",2019,Neural Computing and Applications,,10.1007/s00521-019-04485-2,
db1936e48d2cf2da1af46ddedf3b4d285fe031c6,1,1,0,Deep Multi-Task Network for Learning Person Identity and Attributes,"Person re-identification (re-ID) has been gaining in popularity in the research community owing to its numerous applications and growing importance in the surveillance industry. Recent methods often employ partial features for person re-ID and offer fine-grained information beneficial for person retrieval. In this paper, we focus on learning improved partial discriminative features using a deep convolutional neural architecture, which includes a pyramid spatial pooling module for efficient person feature representation. Furthermore, we propose a multi-task convolutional network that learns both personal attributes and identities in an end-to-end framework. Our approach incorporates partial features and global features for identity and attribute prediction, respectively. Experiments on several large-scale person re-ID benchmark data sets demonstrate the accuracy of our approach. For example, we report rank-1 accuracies of 85.37% (+3.47 %) and 92.81% (+0.51 %) on the DukeMTMC re-ID and Market-1501 data sets, respectively. The proposed method shows encouraging improvements compared with the state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2875783,
db1d4acc0334c1eb013dd7959b87303b4958a336,1,0,0,Classification accuracy of basketball simulation training system based on sensor fusion and Bayesian algorithm,,2020,J. Intell. Fuzzy Syst.,,10.3233/jifs-189070,
db9810031d9e20051904f7591d06f6f58b03924f,1,1,0,Unsupervised Person Re-Identification by Soft Multilabel Learning,"Although unsupervised person re-identification (RE-ID) has drawn increasing research attentions due to its potential to address the scalability problem of supervised RE-ID models, it is very challenging to learn discriminative information in the absence of pairwise labels across disjoint camera views. To overcome this problem, we propose a deep model for the soft multilabel learning for unsupervised RE-ID. The idea is to learn a soft multilabel (real-valued label likelihood vector) for each unlabeled person by comparing the unlabeled person with a set of known reference persons from an auxiliary domain. We propose the soft multilabel-guided hard negative mining to learn a discriminative embedding for the unlabeled target domain by exploring the similarity consistency of the visual features and the soft multilabels of unlabeled target pairs. Since most target pairs are cross-view pairs, we develop the cross-view consistent soft multilabel learning to achieve the learning goal that the soft multilabels are consistently good across different camera views. To enable effecient soft multilabel learning, we introduce the reference agent learning to represent each reference person by a reference agent in a joint embedding. We evaluate our unified deep model on Market-1501 and DukeMTMC-reID. Our model outperforms the state-of-the-art unsupervised RE-ID methods by clear margins. Code is available at https://github.com/KovenYu/MAR.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.06325,10.1109/CVPR.2019.00225,https://arxiv.org/pdf/1903.06325.pdf
dbb7b563e84903dad4953a8e9f23e3c54c6d7e78,1,0,0,Joint Person Re-identification and Camera Network Topology Inference in Multiple Cameras,"Person re-identification is the task of recognizing or identifying a person across multiple views in multi-camera networks. Although there has been much progress in person re-identification, person re-identification in large-scale multi-camera networks still remains a challenging task because of the large spatio-temporal uncertainty and high complexity due to a large number of cameras and people. To handle these difficulties, additional information such as camera network topology should be provided, which is also difficult to automatically estimate, unfortunately. In this study, we propose a unified framework which jointly solves both person re-identification and camera network topology inference problems with minimal prior knowledge about the environments. The proposed framework takes general multi-camera network environments into account and can be applied to online person re-identification in large-scale multi-camera networks. In addition, to effectively show the superiority of the proposed framework, we provide a new person re-identification dataset with full annotations, named SLP, captured in the multi-camera network consisting of nine non-overlapping cameras. Experimental results using our person re-identification and public datasets show that the proposed methods are promising for both person re-identification and camera topology inference tasks.",2019,Comput. Vis. Image Underst.,1710.00983,10.1016/j.cviu.2019.01.003,https://arxiv.org/pdf/1710.00983.pdf
dbd8d908a66c8a3070b693b29b23becde383bdc0,1,0,0,Learning Domain-Specific Features From General Features for Person Re-Identification,"Person re-identification (re-id) plays a vital role in surveillance and forensics application. Since the labeled images for person re-id task is limited, the generalization ability of existed person re-id models is poor. On the other hand, images of different classes (pedestrian and non-pedestrian images) share some general features. To this end, this paper aims to improve the performance of person re-id by designing a relearning network which can learn domain-specific features and general features simultaneously. The proposed relearning network consists of a pretrained backbone network which provides the general features, and several attention-based subnetworks that learn domain-specific features from general features of different levels. Besides, we propose a coarse-fine loss to improve the generalization of person re-id model by making full use of the massive labeled non-pedestrian images. Experimental results on the publicly available Market-1501, DukeMTMC-reID and CUHK03 pedestrian re-id datasets demonstrate the effectiveness of the proposed relearning network and coarse-fine loss.",2020,IEEE Access,,10.1109/ACCESS.2020.3018627,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09173660.pdf
dc76f0d58965f3258c0267dde4d7a25034cf3db7,1,1,0,Characterizing the Utility of Surveillance Video for Person Re-Identification,"Surveillance videos have many applications which can now be accomplished with automated systems. However, the performance of an automated system under realistic circumstances usually does not match the performance produced with a benchmark dataset, due to quality degradations and content variations. In this paper, we discuss several quality and non-quality factors that impact algorithm performance, and design a metric that measures the utility of a video for performing a specific task. Having such measurement allows a user to compress and/or select videos prior to implementing video analytics, reducing both computing power and storage. Here, we choose to study one of the most common applications for video analytics-person re-identification-and we call the corresponding quality measurement “identifiability”.",2019,2019 IEEE International Symposium on Technologies for Homeland Security (HST),,10.1109/HST47167.2019.9032998,https://engineering.purdue.edu/VADL/publications/Haoyu_HST19.pdf
dc8135c8925eed81c2ced5157df3a7f1b47c4769,1,0,0,Amur Tiger Re-identification in the Wild,"Monitoring the population and movements of endangered species is an important task to wildlife conversation. Traditional tagging methods do not scale to large populations, while applying computer vision methods to camera sensor data requires re-identification (re-ID) algorithms to obtain accurate counts and moving trajectory of wildlife. However, existing re-ID methods are largely targeted at persons and cars, which have limited pose variations and constrained capture environments. This paper tries to fill the gap by introducing a novel large-scale dataset, the Amur Tiger Re-identification in the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. In contrast to typical re-ID datasets, the tigers are captured in a diverse set of unconstrained poses and lighting conditions. We demonstrate with a set of baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we propose a novel method for tiger re-identification, which introduces precise pose parts modeling in deep neural networks to handle large pose variation of tigers, and reaches notable performance improvement over existing re-ID methods. The dataset will be public available at this https URL .",2019,ArXiv,1906.05586,,https://arxiv.org/pdf/1906.05586.pdf
dc996940ae50b3eae54f62758ff9c29497115d46,1,1,0,Weakly Supervised Tracklet Person Re-Identification by Deep Feature-wise Mutual Learning,"The scalability problem caused by the difficulty in annotating Person Re-identification(Re-ID) datasets has become a crucial bottleneck in the development of this http URL address this problem, many unsupervised Re-ID methods have recently been proposed.Nevertheless, most of these models require transfer from another auxiliary fully supervised dataset, which is still expensive to this http URL this work, we propose a Re-ID model based on Weakly Supervised Tracklets(WST) data from various camera views, which can be inexpensively acquired by combining the fragmented tracklets of the same person in the same camera view over a period of time.We formulate our weakly supervised tracklets Re-ID model by a novel method, named deep feature-wise mutual learning(DFML), which consists of Mutual Learning on Feature Extractors (MLFE) and Mutual Learning on Feature Classifiers (MLFC).We propose MLFE by leveraging two feature extractors to learn from each other to extract more robust and discriminative features.On the other hand, we propose MLFC by adapting discriminative features from various camera views to each classifier. Extensive experiments demonstrate the superiority of our proposed DFML over the state-of-the-art unsupervised models and even some supervised models on three Re-ID benchmark datasets.",2019,ArXiv,1910.14333,,https://arxiv.org/pdf/1910.14333.pdf
dd4451f90830fd3cf6a3a6989495431bcbecfd4b,1,1,0,Multi-camera Vehicle Tracking from End-to-end based on Spatial-Temporal Information and Visual Features,"In large-scale traffic video analysis, continuous tracking of vehicles across cameras overcomes the time and space limitations of a single camera, and is conducive to transportation design and traffic flow optimization. In this work, we propose an end-to-end framework for multi-camera vehicle detection, tracking and re-identification in complex traffic environments with urban multi-junctions, which integrates visual features and temporal-spatial information of the trajectories for optimization. Based on detection and tracking of multi-vehicles in a single camera, our method distinguishes and marks the vehicle trajectories from different intersections where they enter and exit. Then, the visual features of the same vehicle keyframes are extracted to match between the cameras of the specific matching link, while taking into account the constraint of the trajectory time. In the end, our algorithm shortens vehicle trajectories' average matching time in two cameras to 2 seconds, and the accuracy is 81.59% in the test scenarios, which greatly improves the efficiency and accuracy of vehicle re-identification.",2019,CSAI,,10.1145/3374587.3374629,
dd929617cd9554a974620a8e052e3c53de725481,1,0,0,FLAT MANIFOLD VAES,"Latent-variable models represent observed data by mapping a prior distribution over some latent space to an observed space. Often, the prior distribution is specified by the user to be very simple, effectively shifting the burden of a learning algorithm to the estimation of a highly non-linear likelihood function. This poses a problem for the calculation of a popular distance function, the geodesic between data points in the latent space, as this is often solved iteratively via numerical methods. These are less effective if the problem at hand is not well captured by first or secondorder approximations. In this work, we propose less complex likelihood functions by allowing complex distributions and explicitly penalising the curvature of the decoder. This results in geodesics which are approximated well by the Euclidean distance in latent space, decreasing the runtime by a factor of 1,000 with little loss in accuracy.",2019,,,,https://pdfs.semanticscholar.org/dd92/9617cd9554a974620a8e052e3c53de725481.pdf
dda70f6e730ecdf56da7ae1ceb448f2426ebfb13,1,1,0,Occluded Person Re-Identification,"Person re-identification (re-id) suffers from a serious occlusion problem when applied to crowded public places. In this paper, we propose to retrieve a full-body person image by using a person image with occlusions. This differs significantly from the conventional person re-id problem where it is assumed that person images are detected without any occlusion. We thus call this new problem the occluded person re-identitification. To address this new problem, we propose a novel Attention Framework of Person Body (AFPB) based on deep learning, consisting of 1) an Occlusion Simulator (OS) which automatically generates artificial occlusions for full-body person images, and 2) multi-task losses that force the neural network not only to discriminate a person's identity but also to determine whether a sample is from the occluded data distribution or the full-body data distribution. Experiments on a new occluded person re-id dataset and three existing benchmarks modified to include full-body person images and occluded person images show the superiority of the proposed method.",2018,2018 IEEE International Conference on Multimedia and Expo (ICME),1804.02792,10.1109/ICME.2018.8486568,https://arxiv.org/pdf/1804.02792.pdf
de16794c64d178315e743267998ea030328f55c6,1,0,0,Decouple co-adaptation: Classifier randomization for person re-identification,"Abstract The Person Re-identification (ReID) task aims to match persons across cameras in a surveillance system. In the past few years, many researches are devoted to ReID and its performance has gained significant improvement. ReID models are usually trained as a joint framework comprising a person feature extractor and a classifier. However, there exists co-adaptation between the feature extractor and the classifier, which prevents the feature extractor from making effective and sufficient optimization and results in inferior retrieval performance. In this paper, we propose a very simple and effective training method, called DeAda, to decouple this co-adaptation. Our main motivation is to construct a series of weak classifiers during training by randomization of parameters, so that optimization on the feature extractor could be strengthened in the training stage. DeAda is easy, effective, and efficient, and could serve as a plug-and-play optimization tool for ReID models, without additional memory and time cost. We also analyze the theoretical property of DeAda and show that it could produce identical features for the same person under some simple assumptions. We demonstrate its effectiveness on three public ReID datasets: Market1501, DukeMTMC-reID and CUHK03 over different ReID models. With DeAda optimization, we finally obtain state-of-the-art results on all the three datasets.",2020,Neurocomputing,,10.1016/j.neucom.2019.11.093,
de574be7c1373f116849eb6a2fa18d64476b4839,0,1,0,Model compression via pruning and knowledge distillation for person re-identification,"Person re-identification (ReID) is an important problem in intelligent monitoring. Recently, with the development of deep learning, convolutional neural networks have achieved state-of-the-art performance on person ReID problems. However, the deep neural network models used by these methods tend to have large number of parameters and high computational cost, thereby hindering their deployment on resource-constraint devices or real-time applications. In this study, we propose a method that distills the knowledge to a pruned model to reduce the parameters, which can be divided into two stages: one is to apply unstructured pruning method on over-parameterized models, whereas the other is to carry out representation and metric learning-based knowledge distillation on the model after pruning to improve performance. Finally, the proposed method can effectively reduce the total number of parameters by 8.4 with only 0.1% drop of rank-1 accuracy on the Market1501 dataset and no drop of rank-1 accuracy on the DukeMTMC-reID dataset.",2020,,,10.1007/s12652-020-02312-4,
de59500cec60c37bb04f342259882360475b1e1e,1,1,0,Occluded person re-identification based on feature fusion and sparse reconstruction,"Person Re-identification is one of the hotspots in the field of computer vision, especially for occluded person re-identification, which is still a challenge. In this paper, a feature fusion and sparse reconstruction based method of occluded person re-identification is proposed, which is suitable for person re-identification in various occlusion situations and where pose estimation is employed to obtain the occlusion body parts. A Full Occlusion Re-identification Network(FORN) is developed, where the obstruction is blackened. In the FORN, partial feature extraction and sparse feature reconstruction is combined through tree connections. The fusion features are facilitated in the FORN for occluded person similarity matching so that the matching rate of person re-identification under various occlusion situations is improved. On the occluded person re-identification datasets Partial-REID and Partial-iLIDS, the FORN method has obtained the experimental results of R-1 index 62.75% and 64.26%, and R-3 index 79.43% and 73.10%, respectively. Experiments are also conducted on conventional person re-identification datasets and the experimental results have verified the effectiveness and advancement of the proposed method.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09361-z,
de5a73fd6615e4c1844aeb9b77c6a6a909feeda8,1,0,0,Dual L1-Normalized Context Aware Tensor Power Iteration and Its Applications to Multi-object Tracking and Multi-graph Matching,"The multi-dimensional assignment problem is universal for data association analysis such as data association-based visual multi-object tracking and multi-graph matching. In this paper, multi-dimensional assignment is formulated as a rank-1 tensor approximation problem. A dual L1-normalized context/hyper-context aware tensor power iteration optimization method is proposed. The method is applied to multi-object tracking and multi-graph matching. In the optimization method, tensor power iteration with the dual unit norm enables the capture of information across multiple sample sets. Interactions between sample associations are modeled as contexts or hyper-contexts which are combined with the global affinity into a unified optimization. The optimization is flexible for accommodating various types of contextual models. In multi-object tracking, the global affinity is defined according to the appearance similarity between objects detected in different frames. Interactions between objects are modeled as motion contexts which are encoded into the global association optimization. The tracking method integrates high order motion information and high order appearance variation. The multi-graph matching method carries out matching over graph vertices and structure matching over graph edges simultaneously. The matching consistency across multi-graphs is based on the high-order tensor optimization. Various types of vertex affinities and edge/hyper-edge affinities are flexibly integrated. Experiments on several public datasets, such as the MOT16 challenge benchmark, validate the effectiveness of the proposed methods.",2019,International Journal of Computer Vision,,10.1007/s11263-019-01231-y,https://link.springer.com/content/pdf/10.1007/s11263-019-01231-y.pdf
de743a94f80c03c17b01f407e8405cbd386357f9,1,1,0,Deep Multi-Task Transfer Network for Cross Domain Person Re-Identification,"As a prominent application of surveillance video analysis, person re-identification attracts much more research attention recently. Existing person re-identification models often focus on supervision by the pedestrian identity annotation, while it has limited scalability in realistic. Though several unsupervised person re-identification researches pay attention to solve this problem, they are either clustering based or cross domain based approaches, where a conventional assumption of them is the identity number of the target dataset is acknowledged. To relax this hypothesis, we propose a Deep Multi-task Transfer Network (DMTNet) for cross domain person re-identification, which conduct classification, attribute attention and identification task between source and target domains. There are three main novelties in DMTNet, including clustering number estimating algorithm to learn prior knowledge from source data to estimate the identity number, attribute attention importance learning rather than directly utilizing attribute information, and a multi-task transfer learning mechanism to transfer specific tasks cross domains. To prove the superiority of our DMTNet, we implement several compared experiments on DukeMTMC-reID and Market-1501 datasets, which results show the advancement of our network. Moreover, the discussions for different modules also point out the significance of the specific tasks.",2020,IEEE Access,,10.1109/ACCESS.2019.2962581,
df13e81d51d7372851b4c8c17677ff09df55086a,1,0,0,Structured Domain Adaptation for Unsupervised Person Re-identification,"Unsupervised domain adaptation (UDA) aims at adapting the model trained on a labeled source-domain dataset to another target-domain dataset without any annotation. The task of UDA for the open-set person re-identification (re-ID) is even more challenging as the identities (classes) have no overlap between the two domains. Existing UDA methods for person re-ID have the following limitations. 1) Pseudo-label-based methods achieve state-of-the-art performances but ignore the complex relations between two domains' images, along with the valuable source-domain annotations. 2) Domain translation-based methods cannot achieve competitive performances as the domain translation is not properly regularized to generate informative enough training samples that well maintain inter-sample relations. To tackle the above challenges, we propose an end-to-end structured domain adaptation framework that consists of a novel structured domain-translation network and two domain-specific person image encoders. The structured domain-translation network can effectively transform the source-domain images into the target domain while well preserving the original intra- and inter-identity relations. The target-domain encoder could then be trained using both source-to-target translated images with valuable ground-truth labels and target-domain images with pseudo labels. Importantly, the domain-translation network and target-domain encoder are jointly optimized, improving each other towards the overall objective, i.e. to achieve optimal re-ID performances on the target domain. Our proposed framework outperforms state-of-the-art methods on multiple UDA tasks of person re-ID.",2020,ArXiv,2003.0665,,https://arxiv.org/pdf/2003.06650.pdf
df32921dbc13a65a5dae101db58a8d9d9d92ca16,0,1,0,Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation,"Supervised learning in large discriminative models is a mainstay for modern computer vision. Such an approach necessitates investing in large-scale human-annotated datasets for achieving state-of-the-art results. In turn, the efficacy of supervised learning may be limited by the size of the human annotated dataset. This limitation is particularly notable for image segmentation tasks, where the expense of human annotation is especially large, yet large amounts of unlabeled data may exist. In this work, we ask if we may leverage semi-supervised learning in unlabeled video sequences and extra images to improve the performance on urban scene segmentation, simultaneously tackling semantic, instance, and panoptic segmentation. The goal of this work is to avoid the construction of sophisticated, learned architectures specific to label propagation (e.g., patch matching and optical flow). Instead, we simply predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. The procedure is iterated for several times. As a result, our Naive-Student model, trained with such simple yet effective iterative semi-supervised learning, attains state-of-the-art results at all three Cityscapes benchmarks, reaching the performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable step towards building a simple procedure to harness unlabeled video sequences and extra images to surpass state-of-the-art performance on core computer vision tasks.",2020,ECCV,2005.10266,10.1007/978-3-030-58545-7_40,https://arxiv.org/pdf/2005.10266.pdf
df392f57d24f3d2a89126fbbdfea0aaa86d01a90,1,0,0,Depth occlusion perception feature analysis for person re-identification,"Abstract Person re-identification (ReID) has achieved significant improvement under the setting of matching two holistic person images. However, persons are easily occluded by the various objects and other persons in real-world scenarios, making Person ReID a challenging task. In this paper, we propose a novel method named Pose-Driven Visibility Model (PDVM) to effectively solve the degradation of recognition performance caused by occlusion. Firstly, we extract non-occluded human body features through pose estimation, pay attention to the salient features of non-human parts through self-attention mechanism, and obtains the final feature representation after the combination. Secondly, we more accurately locate person body parts by utilizing the detected human keypoints in different occlusion situations, effectively reducing the impact of unalignment and realizing better matching for persons. We implement extensive experiments on Occluded-DukeMTMC and Partial-REID. Our proposed method achieves state of the art performances which reaches 53.0% Rank-1 accuracy on Occluded-DukeMTMC dataset and ablation analysis also verify the effectiveness of our method. 2020 Elsevier Ltd. All rights reserved",2020,Pattern Recognit. Lett.,,10.1016/J.PATREC.2020.09.009,
df4ed9983f7114ca4f0ab71f1476c0bf7521e317,1,1,0,Pose Transferrable Person Re-identification,"Person re-identification (ReID) is an important task in the field of intelligent security. A key challenge is how to capture human pose variations, while existing benchmarks (i.e., Market1501, DukeMTMC-reID, CUHK03, etc.) do NOT provide sufficient pose coverage to train a robust ReID system. To address this issue, we propose a pose-transferrable person ReID framework which utilizes posetransferred sample augmentations (i.e., with ID supervision) to enhance ReID model training. On one hand, novel training samples with rich pose variations are generated via transferring pose instances from MARS dataset, and they are added into the target dataset to facilitate robust training. On the other hand, in addition to the conventional discriminator of GAN (i.e., to distinguish between REAL/FAKE samples), we propose a novel guider sub-network which encourages the generated sample (i.e., with novel pose) towards better satisfying the ReID loss (i.e., cross-entropy ReID loss, triplet ReID loss). In the meantime, an alternative optimization procedure is proposed to train the proposed Generator-Guider-Discriminator network. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 show that our method achieves great performance improvement, and outperforms most state-of-the-art methods without elaborate designing the ReID model.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00431,http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf
df527756b33d1c2722bd005b246f7df75ea0520a,0,1,0,Selective deep ensemble for instance retrieval,"In public security systems, visual instance retrieval has an explosive growing requirement, especially for large-scale image or video databases. Due to its wide range of applications in surveillance scenario, this paper aims at the retrieval tasks centered around ‘vehicle’ and ‘pedestrian’ targets. Many previous CNN-based methods have not exploited the ensemble abilities of different models, which achieve limited accuracy since a certain kind of deep architecture is not comprehensive. On the other hand, some features in the original deep representation are useless for retrieval tasks, while the attention-aware compact representation will be much more efficient and effective. To address the above problems, we propose a Selective Deep Ensemble (SDE) framework to combine various models and features in a complementary way, inspired by the attention mechanism. It is demonstrated that a large improvement can be acquired with slight increase on computation cost. Finally, we evaluate the performance on three public instance-retrieval datasets, VehicleID, VeRi and Market-1501, outperforming state-of-the-art methods by a large margin.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-5967-8,
df6a1687ec9d4bc9ce6c08b53765b3c6447200c4,1,1,0,Human Machine Joint Decision Making in Distorted Surveillance Scenario,"There is plenty of human-machine joint decision-making scenarios in the real world applications, such as driving assistant, suspect identification, medical diagnosis, etc. Existing algorithms propose that machine should give a rejection option when having a high risk or uncertainty score so that the input can be passed to human to make the decision. This is an interesting algorithmic model of human-machine collaboration, but implicitly assumes that humans are more trustworthy than machines. Such an assumption ignores the bias and inconsistency of human, especially in scenarios where machines have superior recognition ability than humans. In this work, we investigate the human-machine joint decision-making problem in distorted surveillance videos, where machines experimentally prove to be comparable to human beings in tolerance to distortion, sometimes even stronger. We propose a new human-machine joint decision-making framework by considering both the confidences of machine and human. To obtain the confidence of human, we build a real-life human decision-making database and propose a deep neural network to estimate human's confidence. Then, confidence alignment method and decision rule are proposed to further output the final decision. Experiments demonstrate that the proposed framework can make less human intervention and more accurate decisions in several human-machine joint decision-making scenarios.",2019,2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI),,10.1109/CCHI.2019.8901918,
df6c27ae97e42816344fde669140ab608c300438,1,1,0,EANet: Enhancing Alignment for Cross-Domain Person Re-identification,"Person re-identification (ReID) has achieved significant improvement under the single-domain setting. However, directly exploiting a model to new domains is always faced with huge performance drop, and adapting the model to new domains without target-domain identity labels is still challenging. In this paper, we address cross-domain ReID and make contributions for both model generalization and adaptation. First, we propose Part Aligned Pooling (PAP) that brings significant improvement for cross-domain testing. Second, we design a Part Segmentation (PS) constraint over ReID feature to enhance alignment and improve model generalization. Finally, we show that applying our PS constraint to unlabeled target domain images serves as effective domain adaptation. We conduct extensive experiments between three large datasets, Market1501, CUHK03 and DukeMTMC-reID. Our model achieves state-of-the-art performance under both source-domain and cross-domain settings. For completeness, we also demonstrate the complementarity of our model to existing domain adaptation methods. The code is available at this https URL.",2018,ArXiv,1812.11369,,https://arxiv.org/pdf/1812.11369.pdf
dfa94a084203c90aa128056b30969b50421befe7,0,1,0,Multi-level Supervised Network for Person Re-identification,"Most existing methods for person re-identification (re-id) only utilize last layer features of deep convolution neural networks (CNNs), which may lose the useful lower level information and decreases the performance. To alleviate this problem, we propose a novel multi-level feature learning framework, Multi-Level Supervised Network (MLSN), for re-id. Specifically, MLSN is consist of a backbone deep CNN and several part-wise sub-networks. The backbone CNN extracts multi-level semantic feature maps at each middle layer and the part-wise sub-networks transform the feature maps to part-based pedestrian descriptors through a designed supervised learning. By fusing these descriptors, MLSN can utilize both low level information and high level information for re-id. Moreover, an effective optimizing strategy is presented to further improve the performance. Experimental results on three large scale datasets Market-1501, DukeMTMC-reid and CUHK03 show that our proposed approach outperforms several state-of-the-art methods.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683858,
e00b81d7326d47729a84c606ee5a7fe5811b03bc,0,1,0,A Weighted Center Graph Fusion Method for Person Re-Identification,"Feature fusion is widely used in person re-identification (re-ID) and has been proven effective. However, it is difficult to know which features are effective to identify a specific person and how to fuse features to explore complementary information and apply the advantages of each feature. Motivated by these problems, this paper proposes a new method of person re-ID to fuse the recognition results of multiple features at the rank level. Three innovations are included in this method: first, multiple metric spaces are constructed based on the correlation of different features to generate multiple rank results; second, the most similar candidates in each corresponding rank list is converted into a graph structure by our proposed weighted center graph (WCG), and we use an adaptive value  $K$  to automatically seek the most similar images of each query, thus improving the accuracy of candidate targets. Finally, to evaluate the effect of each WCG, a discriminative power coefficient is designed and used to assign a proper coefficient for each WCG according to the discriminative power of corresponding features. The result can be obtained by re-ranking the fused WCG. The extensive experiments on five datasets demonstrate the matching rate of our proposed method by comparing with several state-of-the-art methods. Our code is available at https://github.com/gengshuze/WCG.git.",2019,IEEE Access,,10.1109/ACCESS.2019.2898729,
e0291bfa521158761b6ee8a70f868f88610b0f58,1,1,0,Enabling Open-Set Person Re-Identification for Real-World Scenarios,"Person re-identification (re-ID) is a significant problem of computer vision with increasing scientific attention. To date, numerous studies have been conducted to improve the accuracy and robustness of person re-ID to meet the practical demands. However, most of the previous efforts concentrated on solving the closed-set variant of the problem, where a query is assumed to always have a correct match within the set of known people (the gallery set). However, this assumption is usually not valid for the industrial re-ID use cases. In this study, we focus on the open-set person re-ID problem, where, in addition to the similarity ranking, the solution is expected to detect the presence or absence of a given query identity within the gallery set. To determine good practices and to assess the practicality of the person re-ID in industrial applications, first, we convert popular closed-set person re-ID datasets into the open-set scenario. Second, we compare performance of eight state-of-the-art closed-set person re-ID methods under the open-set conditions. Third, we experimentally determine the efficiency of using different loss function combinations for the open-set problem. Finally, we investigate the impact of a statistics-driven gallery refinement approach on the open-set person re-ID performance in the low false-acceptance rate (FAR) region, while simultaneously reducing the computational demands of retrieval. Results show an average detection and identification rate increase of 8.38% and 3.39% on the DukeMTMC-reID and Market1501 datasets, respectively,",2020,,,10.18178/joig.8.2.26-36,
e0333869f6d12ae6396c7ee914750bb613ff4014,1,0,0,Long-Term Action Dependence-Based Hierarchical Deep Association for Multi-Athlete Tracking in Sports Videos,"Tracking multiple athletes in sports videos is a very challenging Multi-Object Tracking (MOT) task, as athletes generally share high similarity in appearance with large deformations. In this paper, unlike the existing hand-crafted solutions, we propose a novel and effective approach to this issue, which hierarchically associates detections of the same identity through discriminative and robust deep features. First, in detection association, we make use of athlete appearances and poses instead of traditional position cues to generate short tracklets for better initialization. Second, in tracklet association, a new deep architecture, namely Siamese Tracklet Affinity Networks (STAN), is presented, which is able to bi-directionally simulate the unseen dynamics of actions, comprehensively models the long-term action dependences, and sequentially estimates their affinity. Such hierarchical association is finally solved as a minimum-cost network flow problem. We extensively evaluate the proposed approach on the APIDIS, NCAA Basketball and VolleyTrack (newly collected) databases, and the experimental results show its advantages.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3009034,
e04898b25857e2f232c82011ce95e974d6354d78,1,0,0,Spatial-Temporal Omni-Scale Feature Learning for Person Re-Identification,"State-of-the-art person re-identification (ReID) models use Convolutional Neural Networks (CNN) for feature extraction and comparison. Often these models fail to recognize all the intra- and inter-class variations that emerge in person ReID, making it harder to discriminate between data subjects. In this paper we seek to reduce these problems and improve performance by combining two state-of-the-art models. We use the Omni-Scale Network (OSNet) as our CNN to test the Market1501 and DukeMTMC-ReID datasets for person ReID. To fully utilize the potential of these datasets, we apply the spatialtemporal constraint which extracts the camera ID and timestamp from each image to form a distribution. We combine these two methods to create a hybrid model titled Spatial-Temporal OmniScale Network (st-OSNet). Our model attains a Rank-1 (R1) accuracy of 98.2% and mean average precision (mAP) of 92.7% for the Market1501 dataset. For the DukeMTMC-reID dataset our model achieves 94.3% R1 and 86.1% mAP, hereby surpassing the results of OSNet by a large margin for both datasets (94.3%, 86.4%, 88.4%, 76.1%, respectively).",2020,2020 8th International Workshop on Biometrics and Forensics (IWBF),,10.1109/IWBF49977.2020.9107966,
e070d25182ab02fa937543e99dd1e4ac812719f3,0,1,0,FM2u-Net: Face Morphological Multi-Branch Network for Makeup-Invariant Face Verification,"It is challenging in learning a makeup-invariant face verification model, due to (1) insufficient makeup/non-makeup face training pairs, (2) the lack of diverse makeup faces, and (3) the significant appearance changes caused by cosmetics. To address these challenges, we propose a unified Face Morphological Multi-branch Network (FMMu-Net) for makeup-invariant face verification, which can simultaneously synthesize many diverse makeup faces through face morphology network (FM-Net) and effectively learn cosmetics-robust face representations using attention-based multi-branch learning network (AttM-Net). For challenges (1) and (2), FM-Net (two stacked auto-encoders) can synthesize realistic makeup face images by transferring specific regions of cosmetics via cycle consistent loss. For challenge (3), AttM-Net, consisting of one global and three local (task-driven on two eyes and mouth) branches, can effectively capture the complementary holistic and detailed information. Unlike DeepID2 which uses simple concatenation fusion, we introduce a heuristic method AttM-FM, attached to AttM-Net, to adaptively weight the features of different branches guided by the holistic information. We conduct extensive experiments on makeup face verification benchmarks (M-501, M-203, and FAM) and general face recognition datasets (LFW and IJB-A). Our framework FMMu-Net achieves state-of-the-art performances.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00577,https://pdfs.semanticscholar.org/e070/d25182ab02fa937543e99dd1e4ac812719f3.pdf
e07360c751636a8beec13d9662e81f3f87caf5e7,1,1,0,Feature mask network for person re-identification,"Abstract Person re-identification aims at establishing the identity of a pedestrian from a gallery that contains images of people obtained from a multi-camera system, which has many applications in video surveillance for public security and safety. Many challenges such as occlusions, drastic lighting and pose variations across the camera views, and noise make this task highly challenging. While most approaches focus on learning features and metrics to derive better representations, we hypothesize that both local and global contextual cues are crucial for an accurate identity matching. To this end, we propose a Feature Mask Network (FMN) that takes advantage of ResNet high-level features to predict a feature map mask and then imposes it on the low-level features to dynamically re-weight different object parts for a complementary feature representation. This serves as an attention mechanism by allowing the network to focus on local details selectively. We frame the network training as a multi-task objective optimization, which further improves the learned feature descriptions. We conduct experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach respectively achieves significant improvements and competitive results when compared to the state-of-the-art.",2020,Pattern Recognit. Lett.,,10.1016/J.PATREC.2019.02.015,
e0747adcb8853e5deea4c093da5d1c59b8f7b04e,1,0,0,Improved Mutual Mean-Teaching for Unsupervised Domain Adaptive Re-ID,"In this technical report, we present our submission to the VisDA Challenge in ECCV 2020 and we achieved one of the top-performing results on the leaderboard. Our solution is based on Structured Domain Adaptation (SDA) and Mutual Mean-Teaching (MMT) frameworks. SDA, a domain-translation-based framework, focuses on carefully translating the source-domain images to the target domain. MMT, a pseudo-label-based framework, focuses on conducting pseudo label refinery with robust soft labels. Specifically, there are three main steps in our training pipeline. (i) We adopt SDA to generate source-to-target translated images, and (ii) such images serve as informative training samples to pre-train the network. (iii) The pre-trained network is further fine-tuned by MMT on the target domain. Note that we design an improved MMT (dubbed MMT+) to further mitigate the label noise by modeling inter-sample relations across two domains and maintaining the instance discrimination. Our proposed method achieved 74.78% accuracies in terms of mAP, ranked the 2nd place out of 153 teams.",2020,ArXiv,2008.10313,,https://arxiv.org/pdf/2008.10313.pdf
e095b62847324a4b48c4457f990a07ce6b85f641,1,1,1,Self-Similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-Identification,"Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC→Market1501) and 4.4% (Market1501→DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00621,
e0c5f3e40c8b6ab4c0ca70c1cdd4518a78a9fc63,0,0,1,Consistent Cross-view Matching for Unsupervised Person Re-identification,,2019,ArXiv,,,
e0db189e68e3b008836940d7e6971b3f718d2a10,0,1,0,Generating Person Images with Appearance-aware Pose Stylizer,"Generation of high-quality person images is challenging, due to the sophisticated entanglements among image factors, e.g., appearance, pose, foreground, background, local details, global structures, etc. In this paper, we present a novel end-to-end framework to generate realistic person images based on given person poses and appearances. The core of our framework is a novel generator called Appearance-aware Pose Stylizer (APS) which generates human images by coupling the target pose with the conditioned person appearance progressively. The framework is highly flexible and controllable by effectively decoupling various complex person image factors in the encoding phase, followed by re-coupling them in the decoding phase. In addition, we present a new normalization method named adaptive patch normalization, which enables region-specific normalization and shows a good performance when adopted in person image generation model. Experiments on two benchmark datasets show that our method is capable of generating visually appealing and realistic-looking results using arbitrary image and pose inputs.",2020,IJCAI,2007.09077,10.24963/ijcai.2020/87,https://arxiv.org/pdf/2007.09077.pdf
e14432ee3fe3ae41c8724349f88f28f59dc9a69c,0,1,0,Beyond Scalar Neuron: Adopting Vector-Neuron Capsules for Long-Term Person Re-Identification,"Current person re-identification (re-ID) works mainly focus on the short-term scenario where a person is less likely to change clothes. However, in the long-term re-ID scenario, a person has a great chance to change clothes. A sophisticated re-ID system should take such changes into account. To facilitate the study of long-term re-ID, this paper introduces a large-scale re-ID dataset called “Celeb-reID” to the community. Unlike previous datasets, the same person can change clothes in the proposed Celeb-reID dataset. Images of Celeb-reID are acquired from the Internet using street snap-shots of celebrities. There is a total of 1,052 IDs with 34,186 images making Celeb-reID being the largest long-term re-ID dataset so far. To tackle the challenge of cloth changes, we propose to use vector-neuron (VN) capsules instead of the traditional scalar neurons (SN) to design our network. Compared with SN, one extra-dimensional information in VN can perceive cloth changes of the same person. We introduce a well-designed ReIDCaps network and integrate capsules to deal with the person re-ID task. Soft Embedding Attention (SEA) and Feature Sparse Representation (FSR) mechanisms are adopted in our network for performance boosting. Experiments are conducted on the proposed long-term re-ID dataset and two common short-term re-ID datasets. Comprehensive analyses are given to demonstrate the challenge exposed in our datasets. Experimental results show that our ReIDCaps can outperform existing state-of-the-art methods by a large margin in the long-term scenario. The new dataset and code will be released to facilitate future researches.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2948093,
e1af55ad7bb26e5e1acde3ec6c5c43cffe884b04,1,1,0,Person Re-identification by Mid-level Attribute and Part-based Identity Learning,"Existing deep models using attributes usually take global features for identity classification and attribute recognition. However, some attributes exist in local position, such as a hat and shoes, therefore global feature alone is insufficient for person representation. In this work, we propose to use the attribute recognition as an auxiliary task for person re-identification. The attributes are recognised from the local regions of mid-level layers. Besides, we extract local features and global features from a high-level layer for identity classification. The mid-level attribute learning improves the discrimination of high-level features, and the local feature is complementary to the global feature. We report competitive results on two large-scale person re-identification benchmarks, Market-1501 and DukeMTMC-reID datasets, which demonstrate the effectiveness of the proposed method.",2018,ACML,,,https://pdfs.semanticscholar.org/e1af/55ad7bb26e5e1acde3ec6c5c43cffe884b04.pdf
e1afb6a7f1e3be34a8fbc5b060f34c3946c678ad,1,1,0,MHSA-Net: Multi-Head Self-Attention Network for Occluded Person Re-Identification,"This paper presents a novel person re-identification model, named Multi-Head Self-Attention Network (MHSA-Net), to prune unimportant information and capture key local information from person images. MHSA-Net contains two main novel components: Multi-Head Self-Attention Branch (MHSAB) and Attention Competition Mechanism (ACM). The MHSAM adaptively captures key local person information, and then produces effective diversity embeddings of an image for the person matching. The ACM further helps filter out attention noise and non-key information. Through extensive ablation studies, we verified that the Structured Self-Attention Branch and Attention Competition Mechanism both contribute to the performance improvement of the MHSA-Net. Our MHSA-Net achieves state-of-the-art performance especially on images with occlusions. We have released our models (and will release the source codes after the paper is accepted) on this https URL.",2020,ArXiv,2008.04015,,https://arxiv.org/pdf/2008.04015.pdf
e207f7d2f08286465c7f6ea51483d12ea8195fb9,0,0,1,Unsupervised Person Re-Identification Using Multi-Branch Feature Compensation Network and Link-Based Cluster Dissimilarity Metric,"Feature extraction and label estimation are critical in unsupervised person re-identification (re-ID). Most previous works focus on acquiring high-layer semantic features and reckon without the lower-layer details lost in the learning process, which causes the extracted features to be less comprehensive and may degrade re-ID performance. Therefore in this paper, a Multi-branch Feature Compensation Network (MFC-Net) is developed in which the significant parts of lower-layer features are learned and fused with high-layer feature as compensation. Moreover, to accurately conduct label estimation through applying hierarchical clustering on individual samples, a Link-based Cluster Dissimilarity Metric (LCDM) is proposed to discover the inner correlation between clusters. These two methods are later integrated as a MFC-LCDM scheme to improve unsupervised re-ID performance. Extensive experiments on large-scale image-based and video-based datasets ulteriorly demonstrate the superiority and effectiveness of MFC-LCDM.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053719,
e278218ba1ff1b85d06680e99b08e817d0962dab,1,0,0,Tracking Persons-of-Interest via Unsupervised Representation Adaptation,"Multi-face tracking in unconstrained videos is a challenging problem as faces of one person often can appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Existing multi-target tracking methods often use low-level features which are not sufficiently discriminative for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face representations using convolutional neural networks (CNNs). Unlike existing CNN-based approaches which are only trained on large-scale face image datasets offline, we automatically generate a large number of training samples using the contextual constraints for a given video, and further adapt the pre-trained face CNN to the characters in the specific videos using discovered training samples. The embedding feature space is fine-tuned so that the Euclidean distance in the space corresponds to the semantic face similarity. To this end, we devise a symmetric triplet loss function which optimizes the network more effectively than the conventional triplet loss. With the learned discriminative features, we apply an EM clustering algorithm to link tracklets across multiple shots to generate the final trajectories. We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques.",2019,International Journal of Computer Vision,1710.02139,10.1007/s11263-019-01212-1,https://arxiv.org/pdf/1710.02139.pdf
e2a687602d288dddcafc8240aae72d64288e56c4,1,1,0,Camera viewpoint learning for person reidentification,"Abstract. Camera viewpoint has a great influence on reidentification (re-ID) for its diversity. However, there are few methods in response to it, which limits the development of the re-ID community. We analyze the influence of camera viewpoint on re-ID to obtain both visualization and quantitative results, indicating the potential to solve this problem. On this basis, we propose mean and variance loss function (MVL) and camera ID classifier (CIC). MVL reduces the gap among the mean and variance of the feature vectors of images acquired by different cameras, so that the distribution of image feature vectors under different cameras is similar. CIC filters out the images, which have the same camera id with the query to avoid recognition interference. The experiments are conducted on Market1501 and DukeMTMC-reID datasets. We show that MVL and CIC can mitigate the negative impact of the camera viewpoint for re-ID and further improve the accuracy of re-ID. In the Market1501 dataset, rank-1 [mean average precision score (mAP)] accuracy is improved from 88.1 (71.1) % to 90.0 (73.9) % for ResNet-50. In the DukeMTMC-reID dataset, rank-1 (mAP) accuracy is improved from 77.0 (58.9) % to 83.9 (64.1) % for ResNet-50.",2019,J. Electronic Imaging,,10.1117/1.JEI.28.6.063016,
e2ab6439b46334b9e5a99a271f641ef4b9f05eb8,0,1,0,Pedestrian re-Identification Based on Tree Branch Network with Local and Global Learning,"Deep part-based methods in recent literature have revealed the great potential of learning local part-level representation for pedestrian image in the task of person re-identification. However, global features that capture discriminative holistic information of human body are usually ignored or not well exploited. This motivates us to investigate joint learning global and local features from pedestrian images. Specifically, in this work, we propose a novel framework termed tree branch network (TBN) for person re-identification. Given a pedestrain image, the feature maps generated by the backbone CNN, are partitioned recursively into serveral pieces, each of which is followed by a bottleneck structure that learns finer-grained features for each level in the hierarchical tree-like framework. In this way, represenations are learned in a coarse-to-fine manner and finally assembled to produce more discriminative image descriptions. Experimental results demonstrate the effectiveness of the global and local feature learning method in the proposed TBN framework. We also show significant improvement in performance over state-of-the-art methods on three public benchmarks: Market-1501, CUHK-03 and DukeMTMC.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),1904.00355,10.1109/ICME.2019.00125,http://jultika.oulu.fi/files/nbnfi-fe202002195917.pdf
e2e5236d6feb07556c4910cfabee4c371c3e2788,1,0,0,FGAGT: Flow-Guided Adaptive Graph Tracking,"Multi-object tracking (MOT) has always been a very important research direction in computer vision and has great applications in autonomous driving, video object behavior prediction, traffic management, and accident prevention. Recently, some methods have made great progress on MOT, such as CenterTrack, which predicts the trajectory position based on optical flow then tracks it, and FairMOT, which uses higher resolution feature maps to extract Re-id features. In this article, we propose the FGAGT tracker. Different from FairMOT, we use Pyramid Lucas Kanade optical flow method to predict the position of the historical objects in the current frame, and use ROI Pooling\cite{He2015} and fully connected layers to extract the historical objects' appearance feature vectors on the feature maps of the current frame. Next, input them and new objects' feature vectors into the adaptive graph neural network to update the feature vectors. The adaptive graph network can update the feature vectors of the objects by combining historical global position and appearance information. Because the historical information is preserved, it can also re-identify the occluded objects. In the training phase, we propose the Balanced MSE LOSS to balance the sample distribution. In the Inference phase, we use the Hungarian algorithm for data association. Our method reaches the level of state-of-the-art, where the MOTA index exceeds FairMOT by 2.5 points, and CenterTrack by 8.4 points on the MOT17 dataset, exceeds FairMOT by 7.2 points on the MOT16 dataset.",2020,ArXiv,,,
e2e833dc94a34c75c629cbb292c790e55bbab295,0,1,0,Deep adversarial data augmentation with attribute guided for person re-identification,"Person re-identification (Re-ID) is aimed at matching the identity class of pedestrian image across multiple different camera views. Most existing Re-ID methods rely on learning model from labeled pairwise training data. This leads to poor scalability and usability due to the lack of mass identity labeling of images for every camera pairs. In this paper, we address this problem by proposing a deep adversarial learning approach capable of generating images for person Re-ID. Specifically, we propose a deep adversarial data augmentation method with attribute (DADAA) which generates various person images by generative adversarial augmentation. The mid-level attribute information is integrated into the proposed DADAA, which is formulated as learning a one-to-many mapping from labeled source dataset to a large-scale target dataset for increasing data diversity against overfitting. Extensive comparative evaluations show that the DADAA method significantly improves the performance of person Re-ID and validate the superiority of this DADAA method over some state-of-the-art methods on Market-1501 and DukeMTMC-ReID.",2019,,,10.1007/S11760-019-01523-3,
e2e9b1224786aac104916979f47c26d7010ed9cc,0,1,0,Unsupervised Person Re-identification Based on Clustering and Domain-Invariant Network,"Person re-identification (Re-ID) is a task which aims to determine whether a pedestrian in a camera has emerged in other cameras. Earlier works stress importance of the supervised learning methods, however, creating labels by hand is too slow and expensive. Hence, supervised methods are always limited in real-world applications. To address the problem, we propose a novel domain adaptation framework for unsupervised person Re-ID. First, target data are clustered and selected to add relative reliable supervised information for target domain. Second, a novel domain adaptive network is designed to decompose the representations to person-related and domain-related part. The former aims at learning domain-invariant and discriminative representation by a adversarial loss and a Re-ID loss with the label smoothing regularization. And the latter further improve a model’s ability of extracting domain-invariant features by separating the domain unique features. What’s more, during learning representation for target domain, a labeled source data not only is utilized to initialize the model but also participate in the training as a beneficial supervision information to generalize the Re-ID model. Experimental results on Market-1501 and DukeMTMC-reID evidence the superior performance of the proposed model over state-of-the-art methods.",2019,ICIG,,10.1007/978-3-030-34113-8_43,
e2ea99701ca3fed6c4b118ef9c24ec33a7e116a6,0,0,1,A Temporal Attentive Approach for Video-Based Pedestrian Attribute Recognition,"In this paper, we first tackle the problem of pedestrian attribute recognition by video-based approach. The challenge mainly lies in spatial and temporal modeling and how to integrating them for effective and dynamic pedestrian representation. To solve this problem, a novel multi-task model based on the conventional neural network and temporal attention strategy is proposed. Since publicly available dataset is rare, two new large-scale video datasets with expanded attribute definition are presented, on which the effectiveness of both video-based pedestrian attribute recognition methods and the proposed new network architecture is well demonstrated. The two datasets are published on this http URL.",2019,PRCV,,10.1007/978-3-030-31723-2_18,
e307c6635472d3d1e512af6e20f2e56c95937bb7,0,1,0,Semi-Supervised Bayesian Attribute Learning for Person Re-Identification,"Person re-identification (re-ID) tasks aim to identify the same person in multiple images captured from non-overlapping camera views. Most previous re-ID studies have attempted to solve this problem through either representation learning or metric learning, or by combining both techniques. Representation learning relies on the latent factors or attributes of the data. In most of these works, the dimensionality of the factors/attributes has to be manually determined for each new dataset. Thus, this approach is not robust. Metric learning optimizes a metric across the dataset to measure similarity according to distance. However, choosing the optimal method for computing these distances is data dependent, and learning the appropriate metric relies on a sufficient number of pair-wise labels. To overcome these limitations, we propose a novel algorithm for person re-ID, called semi-supervised Bayesian attribute learning. We introduce an Indian Buffet Process to identify the priors of the latent attributes. The dimensionality of attributes factors is then automatically determined by nonparametric Bayesian learning. Meanwhile, unlike traditional distance metric learning, we propose a reidentification probability distribution to describe how likely it is that a pair of images contains the same person. This technique relies solely on the latent attributes of both images. Moreover, pair-wise labels that are not known can be estimated from pair-wise labels that are known, making this a robust approach for semi-supervised learning. Extensive experiments demonstrate the superior performance of our algorithm over several state-of-the-art algorithms on small-scale datasets and comparable performance on large-scale re-ID datasets.",2018,AAAI,,,
e3b5e25196973a6487fe4de6c69fad762d2fb74e,0,1,0,Regularization in deep neural networks,"Regularization in Deep Neural Networks by Guoliang Kang Recent years have witnessed the great success of deep learning. As the deep architecture becomes larger and deeper, it is easy to overfit to relatively small amount of data. Regularization has proved to be an effective way to reduce overfitting in traditional statistical learning area. In the context of deep learning, some special design is required to regularize their training process. Generally, we firstly proposed a new regularization technique named “Shakeout” to improve the generalization ability of deep neural networks beyond Dropout, via introducing a combination of L0, L1, and L2 regularization effect into the network training. Then we considered the unsupervised domain adaptation setting where the source domain data is labeled and the target domain data is unlabeled. We proposed “deep adversarial attention alignment” to regularize the behavior of the convolutional layers. Such regularization reduces the domain shift existing at the start in the convolutional layers which has been ignored by previous works and leads to superior adaptation results. Dissertation directed by Professor Yi Yang Center of AI, School of Software",2019,,,,https://opus.lib.uts.edu.au/bitstream/10453/133295/2/02Whole.pdf
e3cb1c45e41b19d5d4de1f7e2fc97c85e48ae85f,1,1,0,Generative Model for Person Re-Identification: A Review,"Person re-identification (re-ID) could automatically match the same pedestrian across multiple cameras. In this paper, we review three kinds of person re-ID methods with the generative model and comprehensively analyze the applications of the generative model. We perform comparison experiments to verify the performance of the generative model on DukeMTMC-reID, and reveal the generative model could produce meaningful training samples and learn more discriminative features for person re-ID.",2019,CSPS,,10.1007/978-981-13-9409-6_174,
e3cc5d86b2032d01c1b40de0da3b7f4458c9c0ee,1,0,0,Globally Consistent Multi-People Tracking using Motion Patterns,"Many state-of-the-art approaches to people tracking rely on detecting them in each frame independently, grouping detections into short but reliable trajectory segments, and then further grouping them into full trajectories. This grouping typically relies on imposing local smoothness constraints but almost never on enforcing more global constraints on the trajectories. In this paper, we propose an approach to imposing global consistency by first inferring behavioral patterns from the ground truth and then using them to guide the tracking algorithm. When used in conjunction with several state-of-the-art algorithms, this further increases their already good performance. Furthermore, we propose an unsupervised scheme that yields almost similar improvements without the need for ground truth.",2016,ArXiv,1612.00604,,https://arxiv.org/pdf/1612.00604.pdf
e3f522413a7ba6d249eaec0f806eacda61f16253,0,1,0,Multi-scale Spatial-temporal Network for Person Re-identification,"Video-based person re-identification (ReID) is an important task, which has received much attention in recent years due to its efficiency in the field of surveillance. Researchers have employed many effective approaches for video-based person ReID, but there are still two problems. Firstly, the same pedestrian in the video sequences differs in size. Secondly, traditional RNNs can only process one-dimension features, which are not suitable for dealing with video sequences. To solve above problems, we propose a new network called Multi-scale Spatial-Temporal Network (MSTN), which combines multi-scale feature extractor and CLSTM together to tackle the discrepant sizes of pedestrians and extract more representative temporal information for the video sequences. We conduct the experiments on the iLIDS-VID, PRID-2011 and MARS datasets, and our approach outperforms state-of-the-art methods by a large margin.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683716,https://sigport.org/sites/default/files/docs/last_version1_1.pdf
e403977752c3da58f5f534180df0f5e3690b5510,1,1,0,Multi-task network based pedestrian re-identification,"Person re-identification is widely regarded as an image retrieval problem. Given a pedestrian-of-interest (query) image in one camera, the person re-identification system aims to identify the pictures of the same person from an image pool (gallery). Due to the differences in camera pixels, pose, illumination, occlusion, and intra-class variations across different cameras, the task of person re-identification remained challenging to the community of computer vision scientists. In this paper, we propose a multi-task network, based on a uniform partition network, which computes the identification loss and verification loss of two input images simultaneously. Given a pair of images as input, the system predicts the identities of the two input images and outputs a similarity score at the same time, to indicate whether they belong to the same identity or not. To get more fine-grained part-level features, we adopted the part-based convolutional baseline network for feature extraction of each input image and output a convolutional descriptor consisting of six local features. Our model achieved 81.19% mAP and 93.34% rank-1 accuracy on Market-1501 datasets. It also achieved 72.12% mAP and 85.59% rank-1 accuracy on DukeMTMC-reID. Comparing them with those of state-of-the-art, our model outperformed the state-of-the-art by a margin of 3.79% mAP, 1.03% rank-1, and 6.02% mAP, 3.79% rank-1 on Market-1501 and DukeMTMC-reID, respectively.",2019,2019 6th International Conference on Systems and Informatics (ICSAI),,10.1109/ICSAI48974.2019.9010442,
e42e7735f94a8f498ef0bf790ab43a668f904848,1,0,0,Low-Latency Detec on and Tracking of Aircra in Very High-Resolu on Video Feeds,"Applying machine learning techniques for real-time detection and tracking of objects in very high-resolution video is a problem that has not been extensively studied. In this thesis, the practical uses of object detection for airport remote towers are explored. We present a Kalman filter-based tracking framework for low-latency aircraft tracking in very high-resolution video streams. The object detector is trained and tested on a dataset containing 3000 labelled images of aircraft taken at Swedish airports, reaching an mAP of 90.91% with an average IoU of 89.05% on the test set. The tracker is benchmarked on remote tower video footage from Örnsköldsvik and Sundsvall using slightly modified variants of the MOT-CLEAR and ID metrics for multiple object trackers, obtaining an IDF1 score of 91.9%, and a MOTA score of 83.3%. The prototype runs the tracking pipeline on seven high resolution cameras simultaneously at 10 Hz on a single thread, suggesting large potential speed gains being attainable through parallelization.",2018,,,,http://liu.diva-portal.org/smash/get/diva2:1222527/FULLTEXT01.pdf
e444727b8daa0040ae4eb59f1d059123547e52c7,0,1,0,Part-Based Enhanced Super Resolution Network for Low-Resolution Person Re-Identification,"Person re-identification (REID) is an important task in video surveillance and forensics applications. Many previous works often build models on the assumption that they have same resolution cross different camera views, while it is divorced from reality. To increase the adaptability of person REID models, this paper focuses on the low-resolution person REID task to relax the impractical assumption when traditional low-resolution person REID models are under pixel-to-pixel supervision in low and high resolution pedestrian image pairs. In addition, they are easily influenced by the global background, illumination or pose variations across camera views. Therefore, we propose a Part-based Enhanced Super Resolution (PESR) network by employing a part division strategy and an enhanced generative adversarial network to boost the unpaired pedestrian image super resolution process. Specifically, the part-based super resolution network transforms low resolution image in probe into high resolution without any pixel-to-pixel supervision and the part-based synthetic feature extractor module can learn discriminative pedestrian feature representation for the generated high resolution images, which employ a part feature connection loss as constraint to conduct matching for person re-identification. Furthermore, evaluations on four public person REID datasets demonstrate the advantages of our method over the state-of-the-art ones.",2020,IEEE Access,,10.1109/ACCESS.2020.2971612,https://ieeexplore.ieee.org/ielx7/6287639/8948470/08984370.pdf
e533a0a5d8d6d2a717048370d279fd63d027604c,1,0,1,A Flow-Guided Mutual Attention Network for Video-Based Person Re-Identification,"Person Re-Identification (ReID) is a challenging problem in many video analytics and surveillance applications, where a person's identity must be associated across a distributed non-overlapping network of cameras. Video-based person ReID has recently gained much interest because it allows capturing discriminant spatio-temporal information from video clips that is unavailable for image-based ReID. Despite recent advances, deep learning (DL) models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, a flow-guided Mutual Attention network is proposed for fusion of image and optical flow sequences using any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information. Our Mutual Attention network relies on the joint spatial attention between image and optical flow features maps to activate a common set of salient features across them. In addition to flow-guided attention, we introduce a method to aggregate features from longer input streams for better video sequence-level representation. Our extensive experiments on three challenging video ReID datasets indicate that using the proposed Mutual Attention network allows to improve recognition accuracy considerably with respect to conventional gated-attention networks, and state-of-the-art methods for video-based person ReID.",2020,,2008.03788,,https://arxiv.org/pdf/2008.03788.pdf
e542a578af706862dd99af178f31a9e0ea260dc9,0,1,0,THE PURDUE UNIVERSITY GRADUATE SCHOOL STATEMENT OF DISSERTATION APPROVAL,"Kim, Daesung PhD, Purdue University, August 2019. Stability for Functional and Geometric Inequalities and a Stochastic Representation of Fractional Integrals and Nonlocal Operators. Major Professor: Rodrigo Bañuelos. The dissertation consists of two research topics. The first research direction is to study stability of functional and geometric inequalities. Stability problem is to estimate the deficit of a functional or geometric inequality in terms of the distance from the class of optimizers or a functional that identifies the optimizers. In particular, we investigate the logarithmic Sobolev inequality, the Beckner–Hirschman inequality (the entropic uncertainty principle), and isoperimetric type inequalities for the expected lifetime of Brownian motion. In Chapter 3, we derive several types of stability estimates of the logarithmic Sobolev inequality in terms of the Wasserstein distance, L distances, and the Kolmogorov distance. We consider the spaces of probability measures satisfying different conditions on the second moments, the lower bounds of the density, and some integrability of the density. To obtain these results, we employ the optimal transport technique, Fourier analysis, and probability theoretic approach. In Chapter 4, we construct an example to understand the conditions on the space and the distance under which stability of the logarithmic Sobolev inequality does not hold. As an application, we show that stability of the Beckner–Hirschman inequality does not hold for the normalized L distance with some weighted measures in Chapter 5. In Chapter 6, we study quantitative improvements of the inequalities for the expected lifetime of Brownian motion, which state that the L-norms of the expected lifetime in a bounded domain for 1 ≤ p ≤ ∞, are maximized when the region is a ball with the same volume. Since the inequalities also hold for a general class of Lévy",2017,,,,https://www.cerias.purdue.edu/assets/pdf/bibtex_archive/2019-4.pdf
e597d7b3a23f9a3d9a7798406503feda1711ad2b,1,0,0,Multi-Target Multi-Camera Tracking by Tracklet-to-Target Assignment,"This paper focuses on the Multi-Target Multi-Camera Tracking task (MTMCT), which aims at tracking multiple targets within a multi-camera network. As the trajectory of each target is inherently split into multiple sub-trajectories (namely local tracklets) in a multi-camera network, a major challenge of MTMCT is how to accurately match the local tracklets generated within each camera across different cameras and generate a complete global trajectory for each target, i.e., the cross-camera tracklet matching problem. We solve the cross-camera tracklet matching problem by TRACklet-to-Target Assignment (TRACTA), and propose the Restricted Non-negative Matrix Factorization (RNMF) algorithm to compute the optimal assignment solution that meets a set of constraints, which should be in force in practice. TRACTA can correct the tracking errors caused by occlusions and missed detections in local tracklets, and produce a complete global trajectory for each target across all the cameras. Moreover, we also develop an analytical way of estimating the total number of targets in the camera network, which plays an important role to compute the tracklet-to-target assignment. Experimental evaluations and ablation studies on four MTMCT benchmark datasets show the superiority of the proposed TRACTA method.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2980070,
e5d74fa6fb97eb73bd804163906a965659c53f71,0,1,0,Curriculum Enhanced Supervised Attention Network for Person Re-Identification,"Though deep learning methods in Person Re-ID have increased its performance, the field is still confronted with great challenges such as pedestrian misalignment due to the inferior pedestrian detector. To solve such problems, we propose Curriculum Enhanced Supervised Attention Network (CE-SAN): Firstly, an attention module is trained under supervision, which helps the network further emphasize the key information and associate the local and global branch together, leading to a better exploitation of the discriminative features. Secondly, a curriculum design is adopted to divide the dataset into subsets according to the distribution density of the training samples, enabling the network to learn gradually to increase the capability. Moreover, The CE-SAN is easy to be plugged in most of the backbones with high generalization ability. To prove the CE-SAN's superiority, experiments are conducted on three datasets, CE-SAN achieves competitive performance with the state-of-the-arts on Market-1501 and DukeMTMC-reID. Particularly, on the most challenging MSMT17, it outperforms the state-of-the-art methods by 5.2<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> in Rank-1 and 6.5<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> in mAP.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.3024794,
e6872f20a16fe4f07793cc458884d9378b3a82f5,0,1,0,Pose-Based View Synthesis for Vehicles: A Perspective Aware Method,"In this paper, we focus on the problem of novel view synthesis for vehicles. Some previous works solve the problem of novel view synthesis in a controlled 3D environment by exploiting additional 3D details (i.e., camera viewpoints and underlying 3D models). However, in real scenarios, the 3D details are difficult to obtain. In this case, we find that introducing vehicle pose to represent the views of vehicles is an alternative paradigm to solve the lack of 3D details. In novel view synthesis, preserving local details is one of the most challenging problems. To address this problem, we propose a perspective-aware generative model (PAGM). We are motivated by the prior that vehicles are made of quadrilateral planes. Preserving these rigid planes during image generation ensures that image details are kept. To this end, a classic image transformation method is leveraged, i.e., perspective transformation. In our GAN-based system, the perspective transformation is applied to the encoder feature maps, and the resulting maps are regarded as new conditions for the decoder. This strategy preserves the quadrilateral planes all the way through the network, thus shuttling the texture details from the input image to the generated image. In the experiments, we show that PAGM can generate high-quality vehicle images with fine details. Quantitatively, our method is superior to several competing approaches employing either GAN or the perspective transformation. Code is available at: https://github.com/ilvkai/view-synthesis-for-vehicles",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2980130,https://ieeexplore.ieee.org/ielx7/83/8835130/09042874.pdf
e68bce8de2946e3f1b0b7899e76c37fbe0e8a42c,0,1,0,Pseudo-positive regularization for deep person re-identification,"An intrinsic challenge of person re-identification (re-ID) is the annotation difficulty. This typically means (1) few training samples per identity and (2) thus the lack of diversity among the training samples. Consequently, we face high risk of over-fitting when training the convolutional neural network (CNN), a state-of-the-art method in person re-ID. To reduce the risk of over-fitting, this paper proposes a Pseudo-Positive Regularization method to enrich the diversity of the training data. Specifically, unlabeled data from an independent pedestrian database are retrieved using the target training data as query. A small proportion of these retrieved samples are randomly selected as the Pseudo-Positive samples and added to the target training set for the supervised CNN training. The addition of Pseudo-Positive samples is therefore a Data Augmentation method to reduce the risk of over-fitting during CNN training. We implement our idea in the identification CNN models (i.e., CaffeNet, VGGNet-16 and ResNet-50). On CUHK03 and Market-1501 datasets, experimental results demonstrate that the proposed method consistently improves the baseline and yields competitive performance to the state-of-the-art person re-ID methods.",2017,Multimedia Systems,1711.065,10.1007/s00530-017-0571-8,https://arxiv.org/pdf/1711.06500.pdf
e6ace3e178d6277675f7c48849c21f3f503c6b62,1,1,0,View-Invariant and Similarity Learning for Robust Person Re-Identification,"Person re-identification aims to identify pedestrians across non-overlapping camera views. Deep learning methods have been successfully applied to solving the problem and have achieved impressive results. However, these methods rely either on feature extraction or metric learning alone ignoring the joint benefit and mutual complementary effects of the person view-specific representation. In this paper, we propose a multi-view deep network architecture coupled with n-pair loss (JNPL) to eliminate the complex view discrepancy and learn nonlinear mapping functions that are view-invariant. We show that the problem of the large variation in viewpoints of a pedestrian can be well solved using a multi-view network. We simultaneously exploit the complementary representation shared between views and propose an adaptive similarity loss function to better learn a similarity metric. In detail, we first extract view-invariant feature representation from n-pair of images using multi-stream CNN and then aggregate these features for predictions. Given n-positive pairs and a negative example, the network aggregate the feature map of the n-positive pairs and predicts the identity of the person and at the same time learns features that discriminate positive pairs against the negative sample. Extensive evaluations on three large scale datasets demonstrate the substantial advantages of our method over existing state-of-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2960030,
e6d12e5ff20cead0904a54fb4d3653b35237861b,0,1,0,Exploring Spatial Significance via Hybrid Pyramidal Graph Network for Vehicle Re-identification,"Existing vehicle re-identification methods commonly use spatial pooling operations to aggregate feature maps extracted via off-the-shelf backbone networks. They ignore exploring the spatial significance of feature maps, eventually degrading the vehicle re-identification performance. In this paper, firstly, an innovative spatial graph network (SGN) is proposed to elaborately explore the spatial significance of feature maps. The SGN stacks multiple spatial graphs (SGs). Each SG assigns feature map's elements as nodes and utilizes spatial neighborhood relationships to determine edges among nodes. During the SGN's propagation, each node and its spatial neighbors on an SG are aggregated to the next SG. On the next SG, each aggregated node is re-weighted with a learnable parameter to find the significance at the corresponding location. Secondly, a novel pyramidal graph network (PGN) is designed to comprehensively explore the spatial significance of feature maps at multiple scales. The PGN organizes multiple SGNs in a pyramidal manner and makes each SGN handles feature maps of a specific scale. Finally, a hybrid pyramidal graph network (HPGN) is developed by embedding the PGN behind a ResNet-50 based backbone network. Extensive experiments on three large scale vehicle databases (i.e., VeRi776, VehicleID, and VeRi-Wild) demonstrate that the proposed HPGN is superior to state-of-the-art vehicle re-identification approaches.",2020,ArXiv,2005.14684,,https://arxiv.org/pdf/2005.14684.pdf
e6db63fd196490449e3cc2829e7894092c2580bf,1,0,0,Scaling Video Analytics Systems to Large Camera Deployments,"Driven by advances in computer vision and the falling costs of camera hardware, organizations are deploying video cameras en masse for the spatial monitoring of their physical premises. Scaling video analytics to massive camera deployments, however, presents a new and mounting challenge, as compute cost grows proportionally to the number of camera feeds. This paper is driven by a simple question: can we scale video analytics in such a way that cost grows sublinearly, or even remains constant, as we deploy more cameras, while inference accuracy remains stable, or even improves. We believe the answer is yes. Our key observation is that video feeds from wide-area camera deployments demonstrate significant content correlations (e.g. to other geographically proximate feeds), both in space and over time. These spatio-temporal correlations can be harnessed to dramatically reduce the size of the inference search space, decreasing both workload and false positive rates in multi-camera video analytics. By discussing use-cases and technical challenges, we propose a roadmap for scaling video analytics to large camera networks, and outline a plan for its realization.",2019,HotMobile,1809.02318,10.1145/3301293.3302366,https://arxiv.org/pdf/1809.02318.pdf
e746447afc4898713a0bcf2bb560286eb4d20019,1,1,0,Leveraging Virtual and Real Person for Unsupervised Person Re-Identification,"Person re-identification (re-ID) is a challenging instance retrieval problem, especially when identity annotations are not available for training. Although modern deep re-ID approaches have achieved great improvement, it is still difficult to optimize the deep re-ID model and learn discriminative person representation without annotations in training data. To address this challenge, this study considers the problem of unsupervised person re-ID and introduces a novel approach to solve this problem by leveraging virtual and real data. Our approach includes two components: virtual person generation and training of the deep re-ID model. For virtual person generation, we learn a person generation model and a camera style transfer model using unlabeled real data to generate virtual persons with different poses and camera styles. The virtual data is formed as labeled training data, enabling subsequent training deep re-ID model in supervision. For training of the deep re-ID model, we divide it into three steps: 1) pre-training a coarse re-ID model by using virtual data; 2) collaborative filtering based positive pair mining from the real data; and 3) fine-tuning of the coarse re-ID model by leveraging the mined positive pairs and virtual data. The final re-ID model is achieved by iterating between step 2 and step 3 until convergence. Extensive experiments demonstrate the effectiveness of our method. Experimental results on two large-scale datasets, Market-1501 and DukeMTMC-reID, show the advantages of our method over state-of-the-art approaches in unsupervised person re-ID. Our code is now available online.1",2020,IEEE Transactions on Multimedia,1811.02074,10.1109/TMM.2019.2957928,https://arxiv.org/pdf/1811.02074.pdf
e762ec6b873df5279176949f44fc497fb85ba5ed,1,1,0,Person Re-Identification With Joint Verification and Identification of Identity-Attribute Labels,"One of the major challenges in person Re-Identification (ReID) is the inconsistent visual appearance of a person. Current works on visual feature and distance metric learning have achieved significant achievements, but still suffer from the limited robustness to pose variations, viewpoint changes, etc. This makes person ReID among multiple cameras still challenging. This work is motivated to learn mid-level human attributes which are robust to visual appearance variations and could be used as efficient features for person matching. We propose a supervised multi-task learning framework which considers attribute label information with joint identification-verification network to simultaneously learn an attribute-semantic and identity-discriminative feature representation. Specifically, this framework adopts the part-based deep neural network and learn three different tasks simultaneously: person identification, person verifications and attribute identification, so as to discover and capture concurrently complementary discriminative information about a person image from global and local image features and mid-level attribute features in one deep neural network. With the multi-task learning architecture, we obtain a discriminative model that reaches a synergy in distinguishing different person images, as manifested with the competitive accuracy on three person ReID datasets: Market1501, DukeMTMC-reID and VIPeR.",2019,IEEE Access,,10.1109/ACCESS.2019.2939071,
e76d5224a2de41ead8aace56328053d8e3d43ccb,1,0,0,Learning deep features with adaptive triplet loss for person reidentification,"Person reidentification (re-id) aims to match a specified person across non-overlapping cameras, which remains a very challenging problem. While previous methods mostly focus on feature extraction or metric learning, this paper makes the attempt in jointly learning both the global full-body and local body-parts features of the input persons with a multichannel convolutional neural network (CNN) model, which is trained by an adaptive triplet loss function that serves to minimize the distance between the same person and maximize the distance between different persons. The experimental results show that our approach achieves very promising results on the large-scale Market-1501 and DukeMTMC-reID datasets.",2018,International Symposium on Multispectral Image Processing and Pattern Recognition,,10.1117/12.2283478,
e78265f1ce6f08d003f56586e051b3fa7febcd87,1,1,0,Joint multi-scale discrimination and region segmentation for person re-ID,"Abstract Most existing person re-identification methods are mainly based on human part partition with horizontal stripes or human body semantic segmentation. In this paper, we propose a method called MDRS (Multi-scale Discriminative network with Region Segmentation) to integrate multi-scale discriminative feature learning, horizontal stripe partition and semantic segmentation in a single framework, in which multi-scale horizontal stripe partition and usage of both global and local features make the framework be robust to human pose variation, occlusion and background clutter, and semantic segmentation boosts the performance of person identification via shared multi-scale feature extraction. MDRS is trained end-to-end with a multi-task learning strategy that considers three tasks simultaneously: person identification, triplet prediction and pixel-wise semantic segmentation. Comprehensive experiments confirm that our approach exceeds many methods and robustly achieves excellent performances on mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.08.022,
e799c5c7e169f471950eb76dbb329c2d031347ae,1,0,0,Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers,"Online Multi-Object Tracking (MOT) from videos is a challenging computer vision task which has been extensively studied for decades. Most of the existing MOT algorithms are based on the Tracking-by-Detection (TBD) paradigm combined with popular machine learning approaches which largely reduce the human effort to tune algorithm parameters. However, the commonly used supervised learning approaches require the labeled data (e.g., bounding boxes), which is expensive for videos. Also, the TBD framework is usually suboptimal since it is not end-to-end, i.e., it considers the task as detection and tracking, but not jointly. To achieve both label-free and end-to-end learning of MOT, we propose a Tracking-by-Animation framework, where a differentiable neural model first tracks objects from input frames and then animates these objects into reconstructed frames. Learning is then driven by the reconstruction error through backpropagation. We further propose a Reprioritized Attentive Tracking to improve the robustness of data association. Experiments conducted on both synthetic and real video datasets show the potential of the proposed model. Our project page is publicly available at: https://github.com/zhen-he/tracking-by-animation",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1809.03137,10.1109/CVPR.2019.00141,https://arxiv.org/pdf/1809.03137.pdf
e7df0b3c8befa4263971d1b645545ebecdacec06,0,1,0,Disentangled Person Image Generation,"Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task1.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1712.02621,10.1109/CVPR.2018.00018,https://arxiv.org/pdf/1712.02621.pdf
e7dfe797f2de0b1864f04422d83ef49149500231,1,0,0,DGD: Densifying the Knowledge of Neural Networks with Filter Grafting and Knowledge Distillation,"With a fixed model structure, knowledge distillation and filter grafting are two effective ways to boost single model accuracy. However, the working mechanism and the differences between distillation and grafting have not been fully unveiled. In this paper, we evaluate the effect of distillation and grafting in the filter level, and find that the impacts of the two techniques are surprisingly complementary: distillation mostly enhances the knowledge of valid filters while grafting mostly reactivates invalid filters. This observation guides us to design a unified training framework called DGD, where distillation and grafting are naturally combined to increase the knowledge density inside the filters given a fixed model structure. Through extensive experiments, we show that the knowledge densified network in DGD shares both advantages of distillation and grafting, lifting the model accuracy to a higher level.",2020,ArXiv,2004.12311,,https://arxiv.org/pdf/2004.12311.pdf
e80c21a7eef9030429bb682838555b2d9c6c6392,1,1,0,A Part Power Set Model for Scale-Free Person Retrieval,"Recently, person re-identification (re-ID) has attracted increasing research attention, which has broad application prospects in video surveillance and beyond. To this end, most existing methods highly relied on well-aligned pedestrian images and hand-engineered part-based model on the coarsest feature map. In this paper, to lighten the restriction of such fixed and coarse input alignment, an end-to-end part power set model with multi-scale features is proposed, which captures the discriminative parts of pedestrians from global to local, and from coarse to fine, enabling part-based scale-free person re-ID. In particular, we first factorize the visual appearance by enumerating k-combinations for all k of n body parts to exploit rich global and partial information to learn discriminative feature maps. Then, a combination ranking module is introduced to guide the model training with all combinations of body parts, which alternates between ranking combinations and estimating an appearance model. To enable scale-free input, we further exploit the pyramid architecture of deep networks to construct multi-scale feature maps with a feasible amount of extra cost in term of memory and time. Extensive experiments on the mainstream evaluation datasets, including Market-1501, DukeMTMC-reID and CUHK03, validate that our method achieves the state-of-the-art performance.",2019,IJCAI,,10.24963/ijcai.2019/471,https://pdfs.semanticscholar.org/c6da/34f1c53ffc50e9c723d65edebc0fcaa5ffe6.pdf
e89134b84fdf9efe485ee5348c5b54ab5ef49f08,1,1,0,Re-identifying people in the crowd,"Developing an automated surveillance system is of great interest for various reasons including forensic and security applications. In the case of a network of surveillance cameras with nonoverlapping fields of view, person detection and tracking alone are insufficient to track a subject of interest across the network. In this case, instances of a person captured in one camera view need to be retrieved among a gallery of different people, in other camera views. This vision problem is commonly known as person re-identification (re-id). Cross-view instances of pedestrians exhibit varied levels of illumination, viewpoint, and pose variations which makes the problem very challenging. Despite recent progress towards improving accuracy, existing systems suffer from low applicability to real-world scenarios. This is mainly caused by the need for large amounts of annotated data from pairwise camera views to be available for training. Given the difficulty of obtaining such data and annotating it, this thesis aims to bring the person re-id problem a step closer to real-world deployment. In the first contribution, the single-shot protocol, where each individual is represented by a pair of images that need to be matched, is considered. Following the extensive annotation of four datasets for six attributes, an evaluation of the most widely used feature extraction schemes is conducted. The results reveal two high-performing descriptors among those evaluated, and show illumination variation to have the most impact on re-id accuracy. Motivated by the wide availability of videos from surveillance cameras and the additional visual and temporal information they provide, video-based person re-id is then investigated, and a supervised system is developed. This is achieved by improving and extending the best performing image-based person descriptor into three dimensions and combining it with distance metric learning. The system obtained achieves state-of-the-art results on two widely used datasets. Given the cost and difficulty of obtaining labelled data from pairwise cameras in a network to train the model, an unsupervised video-based person re-id method is also developed. It is based on a set-based distance measure that leverages rank vectors to estimate the similarity scores between person tracklets. The proposed system outperforms other unsupervised methods by a large margin on two datasets while competing with deep learning methods on another large-scale dataset.",2019,,,,http://nrl.northumbria.ac.uk/id/eprint/44174/1/riachy.chirine_phd.pdf
e89b9bdd0f50e0a4107c7d7fb83352d6b6d1f88e,0,1,0,Vehicle re-identification in tunnel scenes via synergistically cascade forests,"Abstract Nowadays, numerous cameras have been equipped in tunnels for monitoring the tunnel safety, such as detecting fire, vehicle stopping, crashes, and so forth. Nevertheless, safety events in tunnels may occur in the blind zones not covered by the multi-camera monitoring systems. Therefore, this paper opens the challenging problem, tunnel vehicle re-identification (abbr. tunnel vehicle Re-ID), to make a between-camera speculation. Different from the open road scenes focused by existing vehicle Re-ID methods, tunnel vehicle Re-ID is more challenging because of poor light condition, low resolution, frequent occlusion, severe motion blur, high between-vehicle similarity, and so on. To be specific, we propose a synergistically cascade forests (SCF) model which aims to gradually construct the linking relation between vehicle samples with an increasing of alternative layers of random forest and extremely randomized forest. Through the modeling of SCF, we can restrict the influence of little inter-variation of different vehicle identities and large intra-variation of the same identities. This paper constructs a new and challenging tunnel vehicle dataset (Tunnel-VReID), consisting of 1000 pairs of tunnel vehicle images. Extensive experiments on our Tunnel-VReID demonstrate that the proposed method can outperform current state-of-the-art methods. Besides, in order to prove the adaptation ability of SCF, we also verify the superiority of SCF on a large-scale vehicle Re-ID dataset, named as VehicleID, collected in open road scenes.",2020,Neurocomputing,,10.1016/j.neucom.2019.11.069,http://crabwq.github.io/pdf/2019%20Vehicle%20Re-identification%20in%20Tunnel%20Scenes%20via%20Synergistically%20Cascade%20Forests.pdf
e89cf011bb543137b961807924e0b765d536aa98,1,0,0,iQIYI-VID: A Large Dataset for Multi-modal Person Identification,"Person identification in the wild is very challenging due to great variation in poses, face quality, clothes, makeup and so on. Traditional research, such as face recognition, person re-identification, and speaker recognition, often focuses on a single modal of information, which is inadequate to handle all the situations in practice. Multi-modal person identification is a more promising way that we can jointly utilize face, head, body, audio features, and so on. In this paper, we introduce iQIYI-VID, the largest video dataset for multi-modal person identification. It is composed of 600K video clips of 5,000 celebrities. These video clips are extracted from 400K hours of online videos of various types, ranging from movies, variety shows, TV series, to news broadcasting. All video clips pass through a careful human annotation process, and the error rate of labels is lower than 0.2%. We evaluated the state-of-art models of face recognition, person re-identification, and speaker recognition on the iQIYI-VID dataset. Experimental results show that these models are still far from being perfect for task of person identification in the wild. We further demonstrate that a simple fusion of multi-modal features can improve person identification considerably. We have released the dataset online to promote multi-modal person identification research.",2018,ArXiv,1811.07548,,https://arxiv.org/pdf/1811.07548.pdf
e8bd19fffcac59f2be259c9ec28e65ce3ce74e41,0,1,0,CD-ABM: Curriculum Design with Attention Branch Model for Person Re-identification,"Person re-identification (re-ID) is a challenging problem due to background clutter, illumination and pose variation, occlusion, and pedestrian misalignment. Current state-of-the-art methods commonly extract discriminative information by deep networks based on one-stage training. Though straightforward, using one-stage learning, the presence of pedestrian misalignment in practical applications may significantly degrade the performance of the learned model. To address this issue, we propose a novel model for person re-ID, called CD-ABM. It adopts a curriculum design to proceed training from easy to hard samples and generates an attention map in a supervised manner to further facilitate discriminative feature extraction. Compared with existing methods, CD-ABM has the following advantages: (1) The curriculum design can gradually improve the model capability through progressive learning. (2) The attention map enables the local branch to be associated with the global branch and better exploits both local and global information. Experiments on three benchmark datasets show that, CD-ABM can achieve competitive performance with the state-of-the-arts. Noteworthily, on the most challenging dataset MSMT17, it surpasses state-of-the-art methods by 15.9% in Rank-1 and 21.0% in mAP.",2019,PRICAI,,10.1007/978-3-030-29894-4_50,
e8dac6b899e2be56b4d8b4b5bfb422eb1fe2cb68,1,1,0,A novel two-stream saliency image fusion CNN architecture for person re-identification,"Background interference, which arises from complex environment, is a critical problem for a robust person re-identification (re-ID) system. The background noise may significantly compromise the feature learning and matching process. To reduce the background interference, this paper proposes a saliency image embedding as a pedestrian descriptor. First, to eliminate the background for each pedestrian image, the saliency image is constructed, which is implemented through an unsupervised manifold ranking-based saliency detection algorithm. Second, to reduce some errors and details missing of pedestrian during the saliency image construction process, a saliency image fusion (SIF) convolutional neural network (CNN) architecture is well designed, in which the original pedestrian image and saliency image are both employed as input. We implement our idea in the identification models based on some state-of-the-art backbone CNN models (i.e., CaffeNet, VGGNet-16, GoogLeNet and ResNet-50). We show that the learned pedestrian descriptor by the proposed SIF CNN architecture provides a significant improvement over the baselines and produces a competitive performance compared with the state-of-the-art person re-ID methods on three large-scale person re-ID benchmarks (i.e., Market-1501, DukeMTMC-reID and MARS).",2017,Multimedia Systems,,10.1007/s00530-017-0583-4,
e8fe28a6b454e906988e18e432fc119d5d3c8ebb,0,1,0,RRGCCAN: Re-Ranking via Graph Convolution Channel Attention Network for Person Re-Identification,"The classical person re-identification methods are mostly focused on employing discriminative features amongst which the distance is measured on Euclidean space, while the effort of re-ranking is constrained as the lack of the utilization of quality context representation in embedding set. In this paper, we incorporate graph models on feature subsets resorting to the initial ranking by adopting the integration of the attention mechanism into graph convolution network. On the one hand, the context information regarding embedding pairs is considered to compute feature group similarity through the aggregation operation by using graph convolution networks. On the other hand, we adopt a channel attention mechanism to enhance the contribution of relevant feature channels, further strengthening the ability of similarity pulling and dissimilarity pushing of the overall network. Experimental study shows that the proposed network structure is superior to the state-of-the-art deep neural networks on three very challenging datasets that are popular in examining person re-identification techniques.",2020,IEEE Access,,10.1109/ACCESS.2020.3009653,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09142224.pdf
e91031f037a7961106ab4cdec7ab78eaa241daa7,1,0,0,Rethinking the competition between detection and ReID in Multi-Object Tracking,"Due to balanced accuracy and speed, joint learning detection and ReID-based one-shot models have drawn great attention in multi-object tracking(MOT). However, the differences between the above two tasks in the one-shot tracking paradigm are unconsciously overlooked, leading to inferior performance than the two-stage methods. In this paper, we dissect the reasoning process of the aforementioned two tasks. Our analysis reveals that the competition of them inevitably hurts the learning of task-dependent representations, which further impedes the tracking performance. To remedy this issue, we propose a novel cross-correlation network that can effectively impel the separate branches to learn task-dependent representations. Furthermore, we introduce a scale-aware attention network that learns discriminative embeddings to improve the ReID capability. We integrate the delicately designed networks into a one-shot online MOT system, dubbed CSTrack. Without bells and whistles, our model achieves new state-of-the-art performances on MOT16 and MOT17. We will release our code to facilitate further work.",2020,ArXiv,2010.12138,,https://arxiv.org/pdf/2010.12138.pdf
e93df00d19d7a04994e89dd0e8bbef10f7e20d25,1,0,0,Navigating the Visual Fog: Analyzing and Managing Visual Data from Edge to Cloud,"Visual data produced at the edge is rich with information, opening a world of analytics opportunities for applications to explore. However, the demanding requirements of visual data on computational resources and bandwidth have hindered effective processing, preventing the data from being used in an economically efficient manner. In order to scale out visual analytics systems, it is necessary to have a framework that works collaboratively between edge and cloud. In this paper, we propose an end-to-end (E2E) visual fog architecture, designed for processing and management of visual data. Using our architecture to extract shopper insights, we are able to achieve application specified real time requirements for extracting and querying visual data, showing the feasibility of our design in a real-world setting. We also discuss the lessons we learned from deploying an edge-to-cloud architecture for video streaming applications.",2019,HotEdge,,,https://pdfs.semanticscholar.org/3a7f/f99758f05fa966f65ea37c6eb398f970b12d.pdf
e975afb0a4dc0bf7a36a8d3862a1e1e3f0c9d2d2,1,0,0,A CRF-based Framework for Tracklet Inactivation in Online Multi-Object Tracking,"Online multi-object tracking (MOT) is an active research topic in the domain of computer vision. In this paper, a CRF-based framework is put forward to tackle the tracklet inactivation issues in online MOT problems. We apply the proposed framework to one of the state-of-the-art online MOT trackers, Tracktor++. The baseline algorithm for online MOT has the drawback of simple strategy on tracklet inactivation, which relies merely on tracking hypotheses’ classification scores partitioned by using a fixed threshold. To overcome such a drawback, a discrete conditional random field (CRF) is developed to exploit the intra-frame relationship between tracking hypotheses. Separate sets of feature functions are designed for the unary and binary terms in the CRF so as to cope with various challenges in practical situations. The hypothesis filtering and dummy nodes techniques are employed to handle the problem of varying CRF nodes in the MOT context. In this paper, the inference of CRF is achieved by using the loopy belief propagation algorithm, and the parameters of the CRF are determined by utilizing the maximum likelihood estimation method. Experimental results demonstrate that the developed tracker with our CRF-based framework outperforms the baseline on the MOT16 and MOT17 datasets. The extensibility of the proposed method is further validated by an extensive experiment.",2020,ArXiv,2011.14594,,https://arxiv.org/pdf/2011.14594.pdf
e99d71d0eae6d74f1ef06458f4eb5d5807989550,1,0,0,Deep Group-Shuffling Dual Random Walks With Label Smoothing for Person Reidentification,"Person reidentification (ReID) is a challenging task of finding a target pedestrian in a gallery set collected from multiple nonoverlapping camera views. Recently, state-of-the-art ReID performance has been achieved via an end-to-end trainable deep neural network framework, which integrates convolution feature extraction, similarity learning and reranking into a joint optimization framework. In such a framework, the similarity is learned via an embedding network, the reranking is conducted with a random walk, and the whole framework is optimized with a cross-entropy-based verification loss. Unfortunately, the embedding net is difficult to train well because their two-dimensional outputs mutually interfere each other when using the conventional random walk. In addition, the supervision information has not been fully exploited during the training phase due to the binary nature of the verification loss. In this paper, we propose a novel approach, called group-shuffling dual random walks with label smoothing (GSDRWLS), in which random walks are performed separately on two channels—one for positive verification and one for negative verification—and the binary verification labels are properly modified with an adaptive label smoothing technique before feeding into the verification loss in order to train the overall network effectively and to avoid the overfitting problem. Extensive experiments conducted on three large benchmark datasets, including CUHK03, Market-1501 and DukeMTMC, confirm the superior performance of our proposal.",2020,IEEE Access,,10.1109/ACCESS.2020.2976849,
e9cc7ed014628d80ea1ddf1d16be0c1051f2bf22,1,0,0,ViTAA: Visual-Textual Attributes Alignment in Person Search by Natural Language,"Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches the given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as the performance boosting by a robust feature learning that the referred identity can be accurately bundled by multiple attribute visual cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into subspaces corresponding to attributes using a light auxiliary attribute segmentation computing branch. It then aligns these visual features with the textual attributes parsed from the sentences by using a novel contrastive learning loss. Upon that, we validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Code will be publicly available upon publication.",2020,ECCV,2005.07327,10.1007/978-3-030-58610-2_24,https://arxiv.org/pdf/2005.07327.pdf
e9d549989926f36abfa5dc7348ae3d79a567bf30,0,1,0,Orientation-Guided Similarity Learning for Person Re-identification,"Person re-identification (re-id) is a promising topic in computer vision, which concentrates on similarity learning of individuals across different camera views. It remains challenging due to the unpredictable orientation variations, the partial occlusions, and the inaccurate detections. To solve these problems, we present an orientation-guided similarity learning architecture to learn discriminative feature representations and define similarity metric for person re-id. Our proposed architecture explicitly leverages pedestrian orientation and body part cues to enhance the generalization ability. In the architecture, an orientation-guided loss function that pulls the positive samples with the same orientations closer is designed to alleviate the orientation variations. Meanwhile, an aligned dense network with pose estimation is presented to extract robust global-local fusion representations, which effectively exploits local features to overcome partial occlusions. In the end, we introduce a two-stage Top-k reranking strategy to optimize initial re-id results by min-hash and weighted distance. Extensive experimental results demonstrate that our proposed approach significantly outperforms state-of-the-art re-id methods on the popular CUHK03, Market1501, and DukeMTMC-reID datasets.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545620,
e9d98198bed5c08f697750f1fd1610029f115f9f,1,1,0,Intra-Camera Supervised Person Re-Identification,"Existing person re-identification (re-id) methods mostly exploit a large set of cross-camera identity labelled training data. This requires a tedious data collection and annotation process, leading to poor scalability in practical re-id applications. On the other hand unsupervised re-id methods do not need identity label information, but they usually suffer from much inferior and insufficient model performance. To overcome these fundamental limitations, we propose a novel person re-identification paradigm based on an idea of independent per-camera identity annotation. This eliminates the most time-consuming and tedious inter-camera identity labelling process, significantly reducing the amount of human annotation efforts. Consequently, it gives rise to a more scalable and more feasible setting, which we call Intra-Camera Supervised (ICS) person re-id, for which we formulate a Multi-tAsk mulTi-labEl (MATE) deep learning method. Specifically, MATE is designed for self-discovering the cross-camera identity correspondence in a per-camera multi-task inference framework. Extensive experiments demonstrate the cost-effectiveness superiority of our method over the alternative approaches on three large person re-id datasets. For example, MATE yields 88.7% rank-1 score on Market-1501 in the proposed ICS person re-id setting, significantly outperforming unsupervised learning models and closely approaching conventional fully supervised learning competitors.",2020,ArXiv,2002.05046,,https://arxiv.org/pdf/2002.05046.pdf
e9fb48c89bf525e2be0e11492b817f88e97359ee,0,1,0,Learning Task-oriented Disentangled Representations for Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to address the mismatch between the distributions of training and testing data, but unfortunately, they ignore the task-oriented information across domains and are inflexible to perform well in complicated open-set scenarios. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance.",2020,ArXiv,2007.13264,,https://arxiv.org/pdf/2007.13264.pdf
ea15e018a30dbaaf76ea7778bf7602b71f4d955b,1,0,0,Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID,"Domain adaptive object re-ID aims to transfer the learned knowledge from the labeled source domain to the unlabeled target domain to tackle the open-class re-identification problems. Although state-of-the-art pseudo-label-based methods have achieved great success, they did not make full use of all valuable information because of the domain gap and unsatisfying clustering performance. To solve these problems, we propose a novel self-paced contrastive learning framework with hybrid memory. The hybrid memory dynamically generates source-domain class-level, target-domain cluster-level and un-clustered instance-level supervisory signals for learning feature representations. Different from the conventional contrastive learning strategy, the proposed framework jointly distinguishes source-domain classes, and target-domain clusters and un-clustered instances. Most importantly, the proposed self-paced method gradually creates more reliable clusters to refine the hybrid memory and learning targets, and is shown to be the key to our outstanding performance. Our method outperforms state-of-the-arts on multiple domain adaptation tasks of object re-ID and even boosts the performance on the source domain without any extra annotations. Our generalized version on unsupervised person re-ID surpasses state-of-the-art algorithms by considerable 16.2% and 14.6% on Market-1501 and DukeMTMC-reID benchmarks. Code is available at this https URL.",2020,ArXiv,2006.02713,,https://arxiv.org/pdf/2006.02713.pdf
ea3ef928b3076bc6c01f0f8b2c868100dc03a746,1,1,0,Learning Shape Representations for Clothing Variations in Person Re-Identification,"Person re-identification (re-ID) aims to recognize instances of the same person contained in multiple images taken across different cameras. Existing methods for re-ID tend to rely heavily on the assumption that both query and gallery images of the same person have the same clothing. Unfortunately, this assumption may not hold for datasets captured over long periods of time (e.g., weeks, months or years). To tackle the re-ID problem in the context of clothing changes, we propose a novel representation learning model which is able to generate a body shape feature representation without being affected by clothing color or patterns. We call our model the Color Agnostic Shape Extraction Network (CASE-Net). CASE-Net learns a representation of identity that depends only on body shape via adversarial learning and feature disentanglement. Due to the lack of large-scale re-ID datasets which contain clothing changes for the same person, we propose two synthetic datasets for evaluation. We create a rendered dataset SMPL-reID with different clothes patterns and a synthesized dataset Div-Market with different clothing color to simulate two types of clothing changes. The quantitative and qualitative results across 5 datasets (SMPL-reID, Div-Market, two benchmark re-ID datasets, a cross-modality re-ID dataset) confirm the robustness and superiority of our approach against several state-of-the-art approaches",2020,ArXiv,2003.0734,,https://arxiv.org/pdf/2003.07340.pdf
ea40b7c5c08616a6e1f62b68b5ea959d3405d290,0,1,0,Rethinking Person Re-Identification with Confidence,"A common challenge in person re-identification systems is to differentiate people with very similar appearances. The current learning frameworks based on cross-entropy minimization are not suited for this challenge. To tackle this issue, we propose to modify the cross-entropy loss and model confidence in the representation learning framework using three methods: label smoothing, confidence penalty, and deep variational information bottleneck. A key property of our approach is the fact that we do not make use of any hand-crafted human characteristics but rather focus our attention on the learning supervision. Although methods modeling confidence did not show significant improvements on other computer vision tasks such as object classification, we are able to show their notable effect on the task of re-identifying people outperforming state-of-the-art methods on 3 publicly available datasets. Our analysis and experiments not only offer insights into the problems that person re-id suffers from, but also provide a simple and straightforward recipe to tackle this issue.",2019,ArXiv,1906.04692,,https://www.epfl.ch/labs/vita/wp-content/uploads/2019/06/confidence_reID_iccv2019_5June2019.pdf
ea7b7bf40533a5c6871c32effb3bc3d91207532b,0,0,1,Semi-supervised person re-identification by similarity-embedded cycle GANs,"Recently, person re-identification (PR-ID) has attracted numerous of research interest because of its broad applications. However, most of the existing PR-ID models always follow the supervised framework, which requires substantial labeled data. In fact, it is often very hard to get enough labeled training samples in many practical application scenarios. To overcome this limitation, some semi-supervised PR-ID methods have been presented more recently. Although some of these semi-supervised models achieve satisfied results, there is still much space to improve. In this paper, we propose a novel semi-supervised PR-ID by similarity-embedded cycle GANs (SECGAN). Our SECGAN model can learn cross-view features with limited labeled data by using cycle GANs. Simultaneously, to further enhance the ability of cycle GANs so that it can extract more discriminative and robust features, similarity metric subnet and specific features extracting subnet are embedded into cycle GANs. Extensive experiments have been conducted on three public PR-ID benchmark datasets, and the experimental results show that our proposed SECGAN approach outperforms several typical supervised methods and the existing state-of-the-art semi-supervised methods including traditional and deep learning semi-supervised methods.",2020,Neural Computing and Applications,,10.1007/s00521-020-04809-7,
eab2fd2277eee55bcbc5ba24d4dba0afe3f6b403,1,1,1,Tracklet Self-Supervised Learning for Unsupervised Person Re-Identification,"Existing unsupervised person re-identification (re-id) methods mainly focus on cross-domain adaptation or one-shot learning. Although they are more scalable than the supervised learning counterparts, relying on a relevant labelled source domain or one labelled tracklet per person initialisation still restricts their scalability in real-world deployments. To alleviate these problems, some recent studies develop unsupervised tracklet association and bottom-up image clustering methods, but they still rely on explicit camera annotation or merely utilise suboptimal global clustering. In this work, we formulate a novel tracklet self-supervised learning (TSSL) method, which is capable of capitalising directly from abundant unlabelled tracklet data, to optimise a feature embedding space for both video and image unsupervised re-id. This is achieved by designing a comprehensive unsupervised learning objective that accounts for tracklet frame coherence, tracklet neighbourhood compactness, and tracklet cluster structure in a unified formulation. As a pure unsupervised learning re-id model, TSSL is end-to-end trainable at the absence of source data annotation, person identity labels, and camera prior knowledge. Extensive experiments demonstrate the superiority of TSSL over a wide variety of the state-of-the-art alternative methods on four large-scale person re-id benchmarks, including Market-1501, DukeMTMC-ReID, MARS and DukeMTMC-VideoReID.",2020,AAAI,,10.1609/AAAI.V34I07.6921,https://pdfs.semanticscholar.org/eab2/fd2277eee55bcbc5ba24d4dba0afe3f6b403.pdf
eb0593aee32f29a300e25628bfae7f2e260c6e9f,1,0,0,A computationally efficient pipeline for camera-based indoor person tracking,"Multi-camera person tracking requires the combination of high performance computation and efficient communication approaches in order to satisfy both the required accuracy and real-time processing requirements. In this paper, we present the initial results of our ongoing research project for a multi-target multi-camera tracking system. We propose a modular image processing pipeline comprised of background estimation, person detection, feature extraction, feature matching, and position estimation to track people between video frames, conscious of maintaining lower computation times and efficient interaction between multiple cameras. We present a weighted sequential k-means clustering approach to address the key challenge of feature matching for identifying/re-identifying individuals in an indoor environment. This approach is a form of computationally efficient online unsupervised learning suitable for meeting real-world requirements. Our results show that our approach has comparable accuracy in terms of assigning labels for person tracking, while achieving real-time computational requirements in an unsupervised manner.",2017,2017 International Conference on Image and Vision Computing New Zealand (IVCNZ),,10.1109/IVCNZ.2017.8402479,
eb1cd4d3a94e44e7af8e7a0d564d497bda548389,0,1,0,Transductive semi-supervised metric learning for person re-identification,"Abstract Semi-supervised learning is important and has become more widespread because obtaining labeled data is expensive and labor-intensive. In this paper, we focus on the challenging semi-supervised person Re-identification (ReID) task, which is a metric learning problem based on the assumption that unlabeled data is open-set. To address this problem, we propose the Transductive Semi-Supervised Metric Learning (TSSML) framework. In TSSML, we propose a graph-based transductive hard mining method for deeply mining hard triplets in unlabeled data and a degree-based relationship confidence scoring method for further reducing incorrect triplets. Moreover, we investigate the feature consistency loss (FCL) and adopt the curriculum learning strategy to improve the representation learning for semi-supervised ReID. Extensive experiments have been conducted on three large-scale ReID datasets and demonstrate the effectiveness of our TSSML framework.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107569,
eb47a8f5d9dc2020fc7312f07dce4346b128be53,0,1,0,Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based Object Re-Identification,"Object re-identification (re-id) aims to identify a specific object across times or camera views, with the person re-id and vehicle re-id as the most widely studied applications. Re-id is challenging because of the variations in viewpoints, (human) poses, and occlusions. Multi-shots of the same object can cover diverse viewpoints/poses and thus provide more comprehensive information. In this paper, we propose exploiting the multi-shots of the same identity to guide the feature learning of each individual image. Specifically, we design an Uncertainty-aware Multi-shot Teacher-Student (UMTS) Network. It consists of a teacher network (T-net) that learns the comprehensive features from multiple images of the same object, and a student network (S-net) that takes a single image as input. In particular, we take into account the data dependent heteroscedastic uncertainty for effectively transferring the knowledge from the T-net to S-net. To the best of our knowledge, we are the first to make use of multi-shots of an object in a teacher-student learning manner for effectively boosting the single image based re-id. We validate the effectiveness of our approach on the popular vehicle re-id and person re-id datasets. In inference, the S-net alone significantly outperforms the baselines and achieves the state-of-the-art performance.",2020,AAAI,2001.05197,10.1609/AAAI.V34I07.6774,https://arxiv.org/pdf/2001.05197.pdf
eb7a3d294158a26ae4af9bfc2c2e0419ea691a8e,1,0,0,Deep Motion Model for Pedestrian Tracking in 360 Degrees Videos,This paper proposes a deep convolutional neural network (CNN) for pedestrian tracking in 360\(^{\circ }\) videos based on the target’s motion.,2019,ICIAP,,10.1007/978-3-030-30642-7_4,
eb7bee63337d6219bc4391ada57b1bae03717872,1,0,0,The WILDTRACK Multi-Camera Person Dataset,"People detection methods are highly sensitive to the perpetual occlusions among the targets. As multi-camera set-ups become more frequently encountered, joint exploitation of the across views information would allow for improved detection performances. We provide a large-scale HD dataset named WILDTRACK which finally makes advanced deep learning methods applicable to this problem. The seven-static-camera set-up captures realistic and challenging scenarios of walking people.  Notably, its camera calibration with jointly high-precision projection widens the range of algorithms which may make use of this dataset. In aim to help accelerate the research on automatic camera calibration, such annotations also accompany this dataset.  Furthermore, the rich-in-appearance visual context of the pedestrian class makes this dataset attractive for monocular pedestrian detection as well, since: the HD cameras are placed relatively close to the people, and the size of the dataset further increases seven-fold.  In summary, we overview existing multi-camera datasets and detection methods, enumerate details of our dataset, and we benchmark multi-camera state of the art detectors on this new dataset.",2017,ArXiv,1707.09299,,https://arxiv.org/pdf/1707.09299.pdf
eb7e871e42d1bbdbad11762e9b26e9e1a9b866e9,1,0,0,Filtering Point Targets via Online Learning of Motion Models,"Filtering point targets in highly cluttered and noisy data frames can be very challenging, especially for complex target motions. Fixed motion models can fail to provide accurate predictions, while learning based algorithm can be difficult to design (due to the variable number of targets), slow to train and dependent on separate train/test steps. To address these issues, this paper proposes a multi-target filtering algorithm which learns the motion models, on the fly, using a recurrent neural network with a long short-term memory architecture, as a regression block. The target state predictions are then corrected using a novel data association algorithm, with a low computational complexity. The proposed algorithm is evaluated over synthetic and real point target filtering scenarios, demonstrating a remarkable performance over highly cluttered data sequences.",2019,ArXiv,1902.0763,,https://arxiv.org/pdf/1902.07630.pdf
eb7fcbe39b8017c24cfb0edb5f62e2b2a7ef2c5d,1,0,0,Multi-Target Tracking with Trajectory Prediction and Re-Identification,"Due to the complexity and clutter of real-world scenes, occlusion becomes a long-lasting difficulty in object tracking. Most existing tracking methods cannot effectively handle occlusion. In this paper, we propose a novel tracking framework that combines trajectory prediction and multi-cue appearance modeling to deal with the occlusion difficulty. When a target is completely occluded by background or other targets, it is unable to observe the target position. Therefore, we propose a Long Short-Term Memory (LSTM) model that merges attention mechanism and interaction module to predict the locations of all targets in the next frame. Considering that partial occlusion and inaccuracy of object bounding boxes often take place, we propose a multi-branch deep network architecture combining global and local features to realize accurate tracking and person re-identification (ReID). According to the experimental results on multiple benchmark datasets, our method achieves state-of the-art performance and outperforms many existing approaches.",2019,2019 Chinese Automation Congress (CAC),,10.1109/CAC48633.2019.8996811,
eba8cd66299f289631dfdb3235afac66b4b1b366,0,1,0,Exploiting multigranular salient features with hierarchical multi-mode attention network for pedestrian re-IDentification,"Abstract In this paper, we propose an end-to-end hierarchical-based multi-mode attention network and adaptive fusion (HMAN-HAF) strategy to learn different-level salient features for re-ID tasks. First, according to each layer’s characteristics, a hierarchical multi-mode attention network (HMAN) is designed to adopt different attention models for different-level salient feature learning. Specifically, refined channel-wise attention (CA) is adopted to capture high-level valuable semantic information, an attentive region model (AR) is used to detect salient regions in the low layer, and fused attention (FA) is designed to capture the salient regions of valuable channels in the middle layer. Second, a hierarchical adaptive fusion (HAF) is constructed to fulfill the complementary strengths of different-level salient features. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods on the following challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03.",2020,,,10.1016/J.JVCIR.2020.102914,
ebb65806ec0ebcb2ff8a22f143bfd714e72c1c76,1,0,0,Object Re-Identification Based on Deep Learning,"With the explosive growth of video data and the rapid development of computer vision technology, more and more relevant technologies are applied in our real life, one of which is object re-identification (Re-ID) technology. Object Re-ID is currently concentrated in the field of person Re-ID and vehicle Re-ID, which is mainly used to realize the cross-vision tracking of person/vehicle and trajectory prediction. This chapter combines theory and practice to explain why the deep network can reidentify the object. To introduce the main technical route of object Re-ID, the examples of person/vehicle Re-ID are given, and the improvement points of existing object Re-ID research are described separately.",2019,,,10.5772/INTECHOPEN.86564,https://pdfs.semanticscholar.org/6883/d6268be574206ed094e3807dd2167d1bf8c8.pdf
ebd136934af4b164e2165baadede310cdff5ede7,0,1,0,Adversarial Generation of Training Examples: Applications to Moving Vehicle License Plate Recognition,"Generative Adversarial Networks (GAN) have attracted much research attention recently, leading to impressive results for natural image generation. However, to date little success was observed in using GAN generated images for improving classification tasks. Here we attempt to explore, in the context of car license plate recognition, whether it is possible to generate synthetic training data using GAN to improve recognition accuracy. With a carefully-designed pipeline, we show that the answer is affirmative. First, a large-scale image set is generated using the generator of GAN, without manual annotation. Then, these images are fed to a deep convolutional neural network (DCNN) followed by a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM), which performs the feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experimental results on a few data sets demonstrate the effectiveness of using GAN images: an improvement of 7.5% over a strong baseline with moderate-sized real data being available. We show that the proposed framework achieves competitive recognition accuracy on challenging test datasets. We also leverage the depthwise separate convolution to construct a lightweight convolutional RNN, which is about half size and 2x faster on CPU. Combining this framework and the proposed pipeline, we make progress in performing accurate recognition on mobile and embedded devices.",2017,,,,
ec0f586a489a5c6ce83e8f39487b541d1bb8b19e,0,1,0,Dual Distribution Alignment Network for Generalizable Person Re-Identification,"Domain generalization (DG) serves as a promising solution to handle person Re-Identification (Re-ID), which trains the model using labels from the source domain alone, and then directly adopts the trained model to the target domain without model updating. However, existing DG approaches are usually disturbed by serious domain variations due to significant dataset variations. Subsequently, DG highly relies on designing domain-invariant features, which is however not well exploited, since most existing approaches directly mix multiple datasets to train DG based models without considering the local dataset similarities, i.e., examples that are very similar but from different domains. In this paper, we present a Dual Distribution Alignment Network (DDAN), which handles this challenge by mapping images into a domain-invariant feature space by selectively aligning distributions of multiple source domains. Such an alignment is conducted by dual-level constraints, i.e., the domain-wise adversarial feature learning and the identity-wise similarity enhancement. We evaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID) benchmark. Quantitative results demonstrate that the proposed DDAN can well align the distributions of various source domains, and significantly outperforms all existing domain generalization approaches.",2020,ArXiv,2007.13249,,https://arxiv.org/pdf/2007.13249.pdf
ec2a4c3715aa145c7fd1b7eea47f00d524550d29,0,1,0,Conditional multichannel generative adversarial networks with an application to traffic signs representation learning,"Generative adversarial networks (GANs) are known to produce photorealistic representations. However, we show in this study that this is only valid when the input channels come from a regular RGB camera sensor. In order to alleviate this shortcoming, we propose a general solution to which we refer to as multichannel GANs (MCGANs). In contrast to the existing approaches, MCGANs can process multiple channels with different textures and resolutions. This is achieved by using known concepts in deep learning such as weight sharing and specially separated convolutions. The proposed pipeline enables particular kernels to learn low-level characteristics from the different channels without the need for exhaustive hyper-parameter tuning. We demonstrate the improved representational ability of the framework on traffic sign samples that are captured by a camera with a so-called red-clear-clear-clear pixel topology. Furthermore, we extend our solution by applying the concept of conditions, that offers a whole spectrum of new features, especially for the generation of traffic signs. Throughout this paper, we further discuss relevant applications for the generated synthetic data.",2018,Progress in Artificial Intelligence,,10.1007/s13748-018-0149-5,
ec60c1335ed0741526bb9cb55ca5715a47584115,0,1,0,Triplet Online Instance Matching Loss for Person Re-identification,"Mining the shared features of same identity in different scene, and the unique features of different identity in same scene, are most significant challenges in the field of person re-identification (ReID). Online Instance Matching (OIM) loss function and Triplet loss function are main methods for person ReID. Unfortunately, both of them have drawbacks. OIM loss treats all samples equally and puts no emphasis on hard samples. Triplet loss processes batch construction in a complicated and fussy way and converges slowly. For these problems, we propose a Triplet Online Instance Matching (TOIM) loss function, which lays emphasis on the hard samples and improves the accuracy of person ReID effectively. It combines the advantages of OIM loss and Triplet loss and simplifies the process of batch construction, which leads to a more rapid convergence. It can be trained on-line when handle the joint detection and identification task. To validate our loss function, we collect and annotate a large-scale benchmark dataset (UESTC-PR) based on images taken from surveillance cameras, which contains 499 identities and 60,437 images. We evaluated our proposed loss function on Duke, Marker-1501 and UESTC-PR using ResNet-50, and the result shows that our proposed loss function outperforms the baseline methods by a maximum of 21.7%, including Softmax loss, OIM loss and Triplet loss.",2020,ArXiv,2002.1056,,https://arxiv.org/pdf/2002.10560.pdf
ec721b4b280ce593428499d013bc01ca19dbcac3,1,1,0,Joint Discriminative and Generative Learning for Person Re-Identification,"Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes. The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.07223,10.1109/CVPR.2019.00224,https://arxiv.org/pdf/1904.07223.pdf
ec85c3f57594afa77a8b0e516035550ef6f79db4,0,1,0,Support Neighbor Loss for Person Re-Identification,"Person re-identification (re-ID) has recently been tremendously boosted due to the advancement of deep convolutional neural networks (CNN). The majority of deep re-ID methods focus on designing new CNN architectures, while less attention is paid on investigating the loss functions. Verification loss and identification loss are two types of losses widely used to train various deep re-ID models, both of which however have limitations. Verification loss guides the networks to generate feature embeddings of which the intra-class variance is decreased while the inter-class ones is enlarged. However, training networks with verification loss tends to be of slow convergence and unstable performance when the number of training samples is large. On the other hand, identification loss has good separating and scalable property. But its neglect to explicitly reduce the intra-class variance limits its performance on re-ID, because the same person may have significant appearance disparity across different camera views. To avoid the limitations of the two types of losses, we propose a new loss, called support neighbor (SN) loss. Rather than being derived from data sample pairs or triplets, SN loss is calculated based on the positive and negative support neighbor sets of each anchor sample, which contain more valuable contextual information and neighborhood structure that are beneficial for more stable performance. To ensure scalability and separability, a softmax-like function is formulated to push apart the positive and negative support sets. To reduce intra-class variance, the distance between the anchor's nearest positive neighbor and furthest positive sample is penalized. Integrating SN loss on top of Resnet50, superior re-ID results to the state-of-the-art ones are obtained on several widely used datasets.",2018,ACM Multimedia,1808.0603,10.1145/3240508.3240674,https://arxiv.org/pdf/1808.06030.pdf
ec9c20ed6cce15e9b63ac96bb5a6d55e69661e0b,1,1,0,Robust Pedestrian Detection for Semi-automatic Construction of a Crowded Person Re-Identification Dataset,"The problem of re-identification of people in a crowd commonly arises in real application scenarios, yet it has received less attention than it deserves. To facilitate research focusing on this problem, we have embarked on constructing a new person re-identification dataset with many instances of crowded indoor and outdoor scenes. This paper proposes a two-stage robust method for pedestrian detection in a complex crowded background to provide bounding box annotations. The first stage is to generate pedestrian proposals using Faster R-CNN and locate each pedestrian using Non-maximum Suppression (NMS). Candidates in dense proposal regions are merged to identify crowd patches. We then apply a bottom-up human pose estimation method to detect individual pedestrians in the crowd patches. The locations of all subjects are achieved based on the bounding boxes from the two stages. The identity of the detected subjects throughout each video is then automatically annotated using multiple features and spatial-temporal clues. The experimental results on a crowded pedestrians dataset demonstrate the effectiveness and efficiency of the proposed method.",2018,AMDO,,10.1007/978-3-319-94544-6_7,http://epubs.surrey.ac.uk/846990/1/2018_AMDO.pdf
eca9511fa49679a9a8dab45552f8c83e4d96700d,1,1,0,Learning to Generate Novel Domains for Domain Generalization,"This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model's ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.",2020,ECCV,2007.03304,10.1007/978-3-030-58517-4_33,https://arxiv.org/pdf/2007.03304.pdf
ecc8eb75d2d6909120a31d03cb11b92a43441d53,0,1,0,Unsupervised Cross-Domain Person Re-identification Based on Style Transfer,"Person re-identification (person Re-ID) is mostly viewed as an image retrieval problem across different cameras. Most of existing models have achieved significant improvement on single-domain setting but failed to generalize to new domain due to the image style variations between domains. In this work, we aim to adapt a model trained on source domain to unlabeled target domain without significant performance drop. Thus, a novel style transfer framework that allows us to separate image style from content is proposed in Re-ID. Firstly, the source domain image is transferred to the target style while retaining the ID information. Then the source domain images and transferred images are combined to train a style-independent Re-ID model. Experiments show that we achieved higher performance of unsupervised cross-domain person Re-ID on the Market-1501 and DukeMTMC-reID datasets.",2019,ICIC,,10.1007/978-3-030-26763-6_6,
ecd1508bc0c30c2c8dc88a9fc516d344bae9f6fa,0,1,0,Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation,"Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement ""image inpainting"" on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.",2018,ECCV,1711.09554,10.1007/978-3-030-01246-5_47,https://arxiv.org/pdf/1711.09554.pdf
ed16d32d0c40c08620d882df9045cc720442aa7e,1,1,0,Deep Fusion Feature Representation Learning With Hard Mining Center-Triplet Loss for Person Re-Identification,"Person re-identification (Re-ID) is a challenging task in the field of computer vision and focuses on matching people across images from different cameras. The extraction of robust feature representations from pedestrian images through CNNs with a single deterministic pooling operation is problematic as the features in real pedestrian images are complex and diverse. To address this problem, we propose a novel center-triplet (CT) model that combines the learning of robust feature representation and the optimization of metric loss function. Firstly, we design a fusion feature learning network (FFLN) with a novel fusion strategy consisting of max pooling and average pooling. Instead of adopting a single deterministic pooling operation, the FFLN combines two pooling operations that can learn high response values, bright features, and low response values, discriminative features simultaneously. Our model obtains more discriminative fusion features by adaptively learning the weights of the features learned by the corresponding pooling operations. In addition, we design a hard mining center-triplet loss (HCTL), a novel improved triplet loss, which effectively optimizes the intra/inter-class distance and reduces the cost of computing and mining hard training samples simultaneously, thereby enhancing the learning of robust feature representation. Finally, we proved our method can learn robust and discriminative feature representations for complex pedestrian images in real scenes. The experimental results also illustrate that our method achieves an 81.8% mAP and a 93.8% rank-1 accuracy on Market1501, a 68.2% mAP and an 83.3% rank-1 accuracy on DukeMTMC-ReID, and a 43.6% mAP and a 74.3% rank-1 accuracy on MSMT17, outperforming most state-of-the-art methods and achieving better performance for person re-identification.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2020.2972125,
ed27bc6e3c0e099d02150fcaef140f96f17f1e39,1,1,0,Reasonably Assign Label Distributions to GAN Images in Person Re-Identification Baseline*,"Nowadays, amount of computer vision tasks achieve better performance by using Generative Adversarial Nets(GAN), the main and basic purpose of GAN is to generate unlabeled images so that training deep learning models with these images can hopefully boost system performance significantly. In this paper, we propose a specific method of properly making use of unlabeled images generated by GAN for person re-identification(re-ID) baseline training, we propose label boosting regularization for outliers(LBRO) algorithm, which assigns reasonable label distributions to different numbers of unlabeled images when computing loss. We verify that LBRO can reduce overfitting as well as underfitting under various circumstances, also greatly enhances the performance in person re- ID baseline. Experiments on three mainstream datasets: Market-1501, CUHK03, DukeMTMC-reID show effective results of our method with well-tuned hyperparameters. We can conclude that adding more unlabeled images into training set as a form of regularization can combat overfitting but one must consider careful label distributions of different sizes of GAN images when computing loss.",2018,2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM),,10.1109/BigMM.2018.8499058,
eda6747742caef5ac70d0bcd66a8895a4c92153a,0,1,0,Semi-Supervised SAR ATR via Multi-Discriminator Generative Adversarial Network,"As a supervised deep learning algorithm well-suited for image processing, convolutional neural network (CNN) has shown great potential on synthetic aperture radar (SAR) automatic target recognition (ATR) and achieved superior performance in recent years. However, the training of the deep convolution network depends heavily on sufficient labeled samples while the SAR images are scarce and difficult to obtain, and it is time-consuming to artificially annotate labels for raw images. In this paper, a semi-supervised recognition method combining generative adversarial network (GAN) with CNN is proposed. We generated unlabeled images with GAN and set them as the input of CNN together with original labeled images, so as to implement the effective training and recognition with limited training samples. In order to address the instability training issue caused by the adversarial principal of GAN, a dynamic adjustable multi-discriminator GAN (MGAN) architecture is introduced in the proposed framework. Meanwhile, the label smoothing regularization (LSR) is applied to regularize the semi-supervised recognition model of the CNN. Experiments carried out on the moving and stationary target acquisition and recognition (MSTAR) dataset have indicated that the proposed method possesses the ability to improves the accuracy and robustness of CNN system, especially when the training dataset is limited.",2019,IEEE Sensors Journal,,10.1109/JSEN.2019.2915379,
eddb1a126eafecad2cead01c6c3bb4b88120d78a,1,0,0,Applications of a Graph Theoretic Based Clustering Framework in Computer Vision and Pattern Recognition,"Recently, several clustering algorithms have been used to solve variety of problems from different discipline. This dissertation aims to address different challenging tasks in computer vision and pattern recognition by casting the problems as a clustering problem. We proposed novel approaches to solve multi-target tracking, visual geo-localization and outlier detection problems using a unified underlining clustering framework, i.e., dominant set clustering and its extensions, and presented a superior result over several state-of-the-art approaches.",2018,ArXiv,1802.02181,,https://arxiv.org/pdf/1802.02181.pdf
edf9df3f6fac8892dad184a04303e873c1f123ab,1,0,0,Person Search by Text Attribute Query As Zero-Shot Learning,"Existing person search methods predominantly assume the availability of at least one-shot imagery sample of the queried person. This assumption is limited in circumstances where only a brief textual (or verbal) description of the target person is available. In this work, we present a deep learning method for attribute text description based person search without any query imagery. Whilst conventional cross-modality matching methods, such as global visual-textual embedding based zero-shot learning and local individual attribute recognition, are functionally applicable, they are limited by several assumptions invalid to person search in deployment scale, data quality, and/or category name semantics. We overcome these issues by formulating an Attribute-Image Hierarchical Matching (AIHM) model. It is able to more reliably match text attribute descriptions with noisy surveillance person images by jointly learning global category-level and local attribute-level textual-visual embedding as well as matching. Extensive evaluations demonstrate the superiority of our AIHM model over a wide variety of state-of-the-art methods on three publicly available attribute labelled surveillance person search benchmarks: Market-1501, DukeMTMC, and PA100K.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00375,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/64519/2/Gong%20Person%20search%20by%202020%20Accepted.pdf
ee537a3805ba36eddfd8b1585f0fa99e493c4c48,1,1,0,Person Re-identification with Bias-controlled Adversarial Training,"Inspired by the effectiveness of adversarial training in the area of Generative Adversarial Networks we present a new approach for learning feature representations in person re-identification. We investigate different types of bias that typically occur in re-ID scenarios, i.e., pose, body part and camera view, and propose a general approach to address them. We introduce an adversarial strategy for controlling bias, named Bias-controlled Adversarial framework (BCA), with two complementary branches to reduce or to enhance bias-related features. The results and comparison to the state of the art on different benchmarks show that our framework is an effective strategy for person re-identification. The performance improvements are in both full and partial views of persons.",2019,ArXiv,1904.00244,,https://arxiv.org/pdf/1904.00244.pdf
ee53e93b60bd3573deeb80eb91f139d3a67afcc5,1,1,0,Unsupervised Domain Adaptation with Noise Resistible Mutual-Training for Person Re-identification,"Unsupervised domain adaptation (UDA) in the task of person re-identification (re-ID) is highly challenging due to large domain divergence and no class overlap between domains. Pseudo-label based selftraining is one of the representative techniques to address UDA. However, label noise caused by unsupervised clustering is always a trouble to selftraining methods. To depress noises in pseudo-labels, this paper proposes a Noise Resistible Mutual-Training (NRMT) method, which maintains two networks during training to perform collaborative clustering and mutual instance selection. On one hand, collaborative clustering eases the fitting to noisy instances by allowing the two networks to use pseudolabels provided by each other as an additional supervision. On the other hand, mutual instance selection further selects reliable and informative instances for training according to the peer-confidence and relationship disagreement of the networks. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art UDA methods for person re-ID.",2020,ECCV,,10.1007/978-3-030-58621-8_31,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560511.pdf
eee511c13e0119a080e14656d2e1545b18d95ff5,0,1,0,Pedestrian Re-identification Model based on Joint learning Method,A pedestrian re-identification(Re-ID) model based on joint learning method is proposed. This paper proposed a two-branch model including global feature branch and global feature branch. The global feature branch uses the spatial pyramid pooling model to average the local feature informations of the image and increase the global information of different receptive fields. The local feature branch uses the dynamical distance optimization algorithm without additional information. Experimental results on public datasets show that the proposed model can effectively learn global and local features for Re-ID and outperforms existing basic person re-identification methods.,2020,2020 Chinese Control And Decision Conference (CCDC),,10.1109/CCDC49329.2020.9164515,
ef00c7264c8e5020f44c96a69c4a0f4fbb3da8e5,1,1,0,Mining Hard Samples Globally and Efficiently for Person Reidentification,"Person reidentification (ReID) is an important application of Internet of Things (IoT). ReID recognizes pedestrians across camera views at different locations and time, which is usually treated as a ranking task. An essential part of this task is the hard sample mining. Technically, two strategies could be employed, i.e., global hard mining and local hard mining. For the former, hard samples are mined within the entire training set, while for the latter, it is done in mini-batches. In literature, most existing methods operate locally. Examples include batch-hard sample mining and semihard sample mining. The reason for the rare use of global hard mining is the high computational complexity. In this article, we argue that global mining helps to find harder samples that benefit model training. To this end, this article introduces a new system to: 1) efficiently mine hard samples (positive and negative) from the entire training set and 2) effectively use them in training. Specifically, a ranking list network coupled with a multiplet loss is proposed. On the one hand, the multiplet loss makes the ranking list progressively created to avoid the time-consuming initialization. On the other hand, the multiplet loss aims to make effective use of the hard and easy samples during training. In addition, the ranking list makes it possible to globally and effectively mine hard positive and negative samples. In the experiments, we explore the performance of the global and local sample mining methods, and the effects of the semihard, the hardest, and the randomly selected samples. Finally, we demonstrate the validity of our theories using various public data sets and achieve competitive results via a quantitative evaluation.",2020,IEEE Internet of Things Journal,,10.1109/JIOT.2020.2980549,https://ieeexplore.ieee.org/ielx7/6488907/9219268/09035458.pdf
ef25939ba5f83465395275160a7f8a8e36475a62,1,0,0,Visual Object Tracking Robust to Illumination Variation Based on Hyperline Clustering,"Color histogram-based trackers have obtained excellent performance against many challenging situations. However, since the appearance of color is sensitive to illumination, they tend to achieve lower accuracy when illumination is severely variant throughout a sequence. To overcome this limitation, we propose a novel hyperline clustering based discriminant model, an illumination invariant model that is able to distinguish the object from its surrounding background. Furthermore, we exploit this model and propose an anchor based scale estimation to cope with shape deformation and scale variation. Numerous experiments on recent online tracking benchmark datasets demonstrate that our approach achieve favorable performance compared with several state-of-the-art tracking algorithms. In particular, our approach achieves higher accuracy than comparative methods in the illumination variant and shape deformation challenging situations.",2019,Inf.,,10.3390/info10010026,https://pdfs.semanticscholar.org/ef25/939ba5f83465395275160a7f8a8e36475a62.pdf
ef320b3918d80f2e0abf944dd96bf67167d10c96,1,0,0,Person Reidentification by Deep Structured Prediction—A Fully Parameterized Approach,"Existing efforts on person reidentification (re-ID) either ignore the structural interactions among person images or require a highly crafted re-ID structure as a priori information. In contrast, our approach formulates person re-ID as a deep structured prediction problem that outperforms the state-of-the-art methods by utilizing neural-style-transfer-based structure sampling and fully parameterized energy networks.",2019,IEEE MultiMedia,,10.1109/MMUL.2019.2897678,
ef467d614fadb7a89fd126c360a09d2c8e0672c3,0,1,0,Person Re-identification in Identity Regression Space,"Most existing person re-identification (re-id) methods are unsuitable for real-world deployment due to two reasons: Unscalability to large population size, and Inadaptability over time. In this work, we present a unified solution to address both problems. Specifically, we propose to construct an identity regression space (IRS) based on embedding different training person identities (classes) and formulate re-id as a regression problem solved by identity regression in the IRS. The IRS approach is characterised by a closed-form solution with high learning efficiency and an inherent incremental learning capability with human-in-the-loop. Extensive experiments on four benchmarking datasets (VIPeR, CUHK01, CUHK03 and Market-1501) show that the IRS model not only outperforms state-of-the-art re-id methods, but also is more scalable to large re-id population size by rapidly updating model and actively selecting informative samples with reduced human labelling effort.",2018,International Journal of Computer Vision,1806.09695,10.1007/s11263-018-1105-3,https://link.springer.com/content/pdf/10.1007%2Fs11263-018-1105-3.pdf
efa4a7b4a2eb60ebf02de0669f82d1e99fbe07e4,0,1,0,Attention-based Fusion for Multi-source Human Image Generation,"We present a generalization of the person-image generation task, in which a human image is generated conditioned on a target pose and a set X of source appearance images. In this way, we can exploit multiple, possibly complementary images of the same person which are usually available at training and at testing time. The solution we propose is mainly based on a local attention mechanism which selects relevant information from different source image regions, avoiding the necessity to build specific generators for each specific cardinality of X. The empirical evaluation of our method shows the practical interest of addressing the person-image generation problem in a multi-source setting.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1905.02655,10.1109/WACV45572.2020.9093602,https://arxiv.org/pdf/1905.02655.pdf
efbe862642a3612a3a30de81985d35bb3bf41333,1,1,0,Semantics-Guided Clustering with Deep Progressive Learning for Semi-Supervised Person Re-identification,"Person re-identification (re-ID) requires one to match images of the same person across camera views. As a more challenging task, semi-supervised re-ID tackles the problem that only a number of identities in training data are fully labeled, while the remaining are unlabeled. Assuming that such labeled and unlabeled training data share disjoint identity labels, we propose a novel framework of Semantics-Guided Clustering with Deep Progressive Learning (SGC-DPL) to jointly exploit the above data. By advancing the proposed Semantics-Guided Affinity Propagation (SG-AP), we are able to assign pseudo-labels to selected unlabeled data in a progressive fashion, under the semantics guidance from the labeled ones. As a result, our approach is able to augment the labeled training data in the semi-supervised setting. Our experiments on two large-scale person re-ID benchmarks demonstrate the superiority of our SGC-DPL over state-of-the-art methods across different degrees of supervision. In extension, the generalization ability of our SGC-DPL is also verified in other tasks like vehicle re-ID or image retrieval with the semi-supervised setting.",2020,ArXiv,2010.01148,,https://arxiv.org/pdf/2010.01148.pdf
efd8eb3ade5ed99833df31740017ff61ae525b75,1,0,0,Self-Supervised Small Soccer Player Detection and Tracking,"In a soccer game, the information provided by detecting and tracking brings crucial clues to further analyze and understand some tactical aspects of the game, including individual and team actions. State-of-the-art tracking algorithms achieve impressive results in scenarios on which they have been trained for, but they fail in challenging ones such as soccer games. This is frequently due to the player small relative size and the similar appearance among players of the same team. Although a straightforward solution would be to retrain these models by using a more specific dataset, the lack of such publicly available annotated datasets entails searching for other effective solutions. In this work, we propose a self-supervised pipeline which is able to detect and track low-resolution soccer players under different recording conditions without any need of ground-truth data. Extensive quantitative and qualitative experimental results are presented evaluating its performance. We also present a comparison to several state-of-the-art methods showing that both the proposed detector and the proposed tracker achieve top-tier results, in particular in the presence of small players. Code available at ""https://github.com/samuro95/Self-Supervised-Small-Soccer-Player-Detection-Tracking"".",2020,ArXiv,2011.10336,10.1145/3422844.3423054,https://arxiv.org/pdf/2011.10336.pdf
efdc83256186c2a641c3df7b83202f7011eac81a,0,1,0,Adversarial Open-World Person Re-Identification,"In a typical real-world application of re-id, a watch-list (gallery set) of a handful of target people (e.g. suspects) to track around a large volume of non-target people are demanded across camera views, and this is called the open-world person re-id. Different from conventional (closed-world) person re-id, a large portion of probe samples are not from target people in the open-world setting. And, it always happens that a non-target person would look similar to a target one and therefore would seriously challenge a re-id system. In this work, we introduce a deep open-world group-based person re-id model based on adversarial learning to alleviate the attack problem caused by similar non-target people. The main idea is learning to attack feature extractor on the target people by using GAN to generate very target-like images (imposters), and in the meantime the model will make the feature extractor learn to tolerate the attack by discriminative learning so as to realize group-based verification. The framework we proposed is called the adversarial open-world person re-identification, and this is realized by our Adversarial PersonNet (APN) that jointly learns a generator, a person discriminator, a target discriminator and a feature extractor, where the feature extractor and target discriminator share the same weights so as to makes the feature extractor learn to tolerate the attack by imposters for better group-based verification. While open-world person re-id is challenging, we show for the first time that the adversarial-based approach helps stabilize person re-id system under imposter attack more effectively.",2018,ECCV,1807.10482,10.1007/978-3-030-01216-8_18,https://arxiv.org/pdf/1807.10482.pdf
f00edcf26f0931529a53dbda5a0ce2333e0c694e,1,1,1,Temporal Aggregation with Clip-level Attention for Video-based Person Re-identification,"Video-based person re-identification (Re-ID) methods can extract richer features than image-based ones from short video clips. The existing methods usually apply simple strategies, such as average/max pooling, to obtain the tracklet-level features, which has been proved hard to aggregate the information from all video frames. In this paper, we propose a simple yet effective Temporal Aggregation with Clip-level Attention Network (TACAN) to solve the temporal aggregation problem in a hierarchal way. Specifically, a tracklet is firstly broken into different numbers of clips, through a two-stage temporal aggregation network we can get the tracklet-level feature representation. A novel min-max loss is introduced to learn both a clip-level attention extractor and a clip-level feature representer in the training process. Afterwards, the resulting clip-level weights are further taken to average the clip-level features, which can generate a robust tracklet-level feature representation at the testing stage. Experimental results on four benchmark datasets, including the MARS, iLIDS-VID, PRID-2011 and DukeMTMC-VideoReID, show that our TACAN has achieved significant improvements as compared with the state-of-the-art approaches.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093413,http://openaccess.thecvf.com/content_WACV_2020/papers/Li_Temporal_Aggregation_with_Clip-level_Attention_for_Video-based_Person_Re-identification_WACV_2020_paper.pdf
f0169abf130c8eaa79d89b9a227abe870d4602de,0,1,0,A segmentation-based human alignment network for person re-identification with frequency weighting re-ranking,"Pedestrian re-identification (re-ID) is a computer vision technology to recognize an individual in non-overlapping multi-camera scenes. However, this process suffers from misalignment due to the influence of dramatic changes in person poses and views, and background interference. To address this issue, we focus on the discriminative feature representations by proposing a Segmentation-based Human Alignment Network, named SegHAN. It is exploited based on the segmentation of human body to alleviate the misalignment issue, which not only guarantees the robustness to pedestrian spatial locations, but also reduces background interference. In addition, a Frequency Weighting Re-ranking model (FWR) is designed to further enhance the performance of the proposed SegHAN. In this process, the frequency information obtained from the construction of similarity set is considered into the re-ranking model, which ranks the related images with high frequency in front. Experiments show that the proposed SegHAN and re-ranking model are beneficial to enhance re-ID performance and they both achieve a competitive performance when compared with state-of-the-art methods on three challenging re-ID datasets.",2019,,,,https://www.academiapublishing.org/print/Geng%20et%20al.pdf
f03234e32b87ac4b08655253cf206b1c0bb3a369,1,0,0,MFBN: An Efficient Base Model For Person Re-Identification,"Person Re-IDentification (Re-ID) has developed rapidly with deep learning methods, as for these methods, the base mod- els used in most of them are not customized for Re-ID task. Although some studies have carefully designed the models special for Re-ID task, these models are always not easy to be the base model and expand with new methods due to their great complexity. In this paper, we propose a novel efficient base model named as Multi-granularity Feature Boosting Network (MFBN). MFBN consists of branches with information in different granularities. MFBN combines these branches into one whole, so MFBN is easy to be ex- tended as a base model. Moreover, MFBN applies feature boosting technique to boost fine granularitiy branch features with coarse granularity branch features, and applies channel- wise attention to increase diversities between features in multiple granularities. With these improves, MFBN has surpassed popular base models and got state-of-the-art results on mainstream Re-ID datasets including Market-1501, DukeMTMC-reID and CUHK-03. MFBN achieves results of rank-1/mAP=95.2%/93.2% on Market-1501 dataset and rank-1/mAP=90.3%/88.3% on DukeMTMC-reID dataset. Code and pretrained models are available at https://github.com/hsyi/Multi-granularity-Feature-Boosting-Network",2019,ICMAI 2019,,10.1145/3325730.3325764,
f035d95a80e59e2d71330b6c661750b13f3664d4,0,1,0,A benchmark for clothes variation in person re‐identification,"Person re‐identification (re‐ID) has drawn attention significantly in the computer vision society due to its application and research significance. It aims to retrieve a person of interest across different camera views. However, there are still several factors that hinder the applications of person re‐ID. In fact, most common data sets either assume that pedestrians do not change their clothing across different camera views or are taken under constrained environments. Those constraints simplify the person re‐ID task and contribute to early development of person re‐ID, yet a person has a great possibility to change clothes in real life. To facilitate the research toward conquering those issues, this paper mainly introduces a new benchmark data set for person re‐identification. To the best of our knowledge, this data set is currently the most diverse for person re‐identification. It contains 107 persons with 9,738 images, captured in 15 indoor/outdoor scenes from September 2019 to December 2019, varying according to viewpoints, lighting, resolutions, human pose, seasons, backgrounds, and clothes especially. We hope that this benchmark data set will encourage further research on person re‐identification with clothes variation. Moreover, we also perform extensive analyses on this data set using several state‐of‐the‐art methods. Our dataset is available at https://github.com/nkicsl/NKUP-dataset.",2020,Int. J. Intell. Syst.,,10.1002/int.22276,
f06a12928307e17b1aff2b9f4a6c11791f19b6a7,0,1,0,Deep Mutual Learning,"Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary - mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1706.00384,10.1109/CVPR.2018.00454,https://www.pure.ed.ac.uk/ws/files/58941044/zhang2018DeepMutualLearning.pdf
f07c4112063a624597cdc40c93a9e8d3f175f177,1,1,0,UNSUPERVISED DEEP TRANSFER LEARNING APPROACH TO PERSON RE-IDENTIFICATION 1,"Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline ReID methods which require fully supervised data for training.",2018,,,,https://pdfs.semanticscholar.org/f07c/4112063a624597cdc40c93a9e8d3f175f177.pdf
f0a15d4a339dac731533bcb5169405997d3e0e00,1,0,0,3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset,"In this work we present a novel publicly available stereo based 3D RGB dataset for multi-object zebrafish tracking, called 3D-ZeF. Zebrafish is an increasingly popular model organism used for studying neurological disorders, drug addiction, and more. Behavioral analysis is often a critical part of such research. However, visual similarity, occlusion, and erratic movement of the zebrafish makes robust 3D tracking a challenging and unsolved problem. The proposed dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes. Furthermore, we present a complexity score and a novel open-source modular baseline system for 3D tracking of zebrafish. The performance of the system is measured with respect to two detectors: a naive approach and a Faster R-CNN based fish head detector. The system reaches a MOTA of up to 77.6%. Links to the code and dataset is available at the project page http://vap.aau.dk/3d-zef",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2006.08466,10.1109/CVPR42600.2020.00250,https://arxiv.org/pdf/2006.08466.pdf
f0ec043ce0533ed240faac4ea48444cbf37e90f8,1,0,0,Context-aware intelligent video analysis for the management of smart buildings,"Les systemes de vision artificielle sont aujourd'hui limites a l'extraction de donnees issues de ce que les cameras « voient ». Cependant, la comprehension de ce qu'elles voient peut etre enrichie en associant la connaissance du contexte et la connaissance d'interpretation d'un humain.Dans ces travaux de these, nous proposons une approche associant des algorithmes de vision atificielle a une modelisation semantique du contexte d'acquisition.Cette approche permet de realiser un raisonnement sur la connaissance extraite des images par les cameras en temps reel. Ce raisonnement offre une reponse aux problemes d'occlusion et d'erreurs de detections inherents aux algorithmes de vision artificielle. Le systeme complet permet d'offrir un ensemble de services intelligents (guidage, comptage...) tout en respectant la vie privee des personnes observees. Ces travaux forment la premiere etape du developpement d'un bâtiment intelligent qui peut automatiquement reagir et evoluer en observant l'activite de ces usagers, i.e., un bâtiment intelligent qui prend en compte les informations contextuelles.Le resultat, nomme WiseNET, est une intelligence artificielle en charge des decisions au niveau du bâtiment (qui pourrait etre etendu a un groupe de bâtiments ou meme a l'echelle d'un ville intelligente). Elle est aussi capable de dialoguer avec l'utilisateur ou l'administrateur humain de maniere explicite.",2019,,,,https://pdfs.semanticscholar.org/a050/8726c6d3aab6d48d691ad91a79581354d4de.pdf
f1038725d775f890a7b9e295cb55755061663d9b,1,1,0,Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification,"Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. Such a setting severely limits their scalability in real-world applications where no labelled samples are available during the training phase. To overcome this limitation, we develop a novel unsupervised Multi-task Mid-level Feature Alignment (MMFA) network for the unsupervised cross-dataset person re-identification task. Under the assumption that the source and target datasets share the same set of mid-level semantic attributes, our proposed model can be jointly optimised under the person's identity classification and the attribute learning task with a cross-dataset mid-level feature alignment regularisation term. In this way, the learned feature representation can be better generalised from one dataset to another which further improve the person re-identification accuracy. Experimental results on four benchmark datasets demonstrate that our proposed method outperforms the state-of-the-art baselines.",2018,BMVC,1807.0144,,https://arxiv.org/pdf/1807.01440.pdf
f12e2888e6db23433166db72ff77c448cb6845e8,1,1,0,GLAD: Global–Local-Alignment Descriptor for Scalable Person Re-Identification,"The huge variance of human pose and the misalign-ment of detected human images significantly increase the difficulty of pedestrian image matching in person Re-Identification (Re-ID). Moreover, the massive visual data being produced by surveillance video cameras requires highly efficient person Re-ID systems. Targeting to solve the first problem, this work proposes a robust and discriminative pedestrian image descriptor, namely, the Global–Local-Alignment Descriptor (GLAD). For the second problem, this work treats person Re-ID as image retrieval and proposes an efficient indexing and retrieval framework. GLAD explicitly leverages the local and global cues in the human body to generate a discriminative and robust representation. It consists of part extraction and descriptor learning modules, where several part regions are first detected and then deep neural networks are designed for representation learning on both the local and global regions. A hierarchical indexing and retrieval framework is designed to perform offline relevance mining to eliminate the huge person ID redundancy in the gallery set, and accelerate the online Re-ID procedure. Extensive experimental results on widely used public benchmark datasets show GLAD achieves competitive accuracy compared to the state-of-the-art methods. On a large-scale person, with the Re-ID dataset containing more than 520 K images, our retrieval framework significantly accelerates the online Re-ID procedure while also improving Re-ID accuracy. Therefore, this work has the potential to work better on person Re-ID tasks in real scenarios.",2019,IEEE Transactions on Multimedia,,10.1109/TMM.2018.2870522,
f14af0ad856120d84072a9f5ed68d12dcf07f071,1,1,0,Person Re-identification Based on Pose-guided Generative Adversarial Network,"Person re-identification (re-id) is a challenging and valuable research topic in the field of computer vision. It needs to match person images with the same identity in multiple camera systems. The change of pose is one of the key factors that affect the network to extract robust features. In order to mitigate the influence of pose variations on person re-id, the paper proposes a person re-id method based on a pose-guided generative adversarial network (PG-GAN), which can be used to learn identity-sensitive and pose-insensitive features. The algorithm is composed of a Siamese convolutional neural network (SCNN) and generative adversarial networks (GANs). SCNN is a symmetric structure with ResNet-50. GANs contain multiple pose discriminators and identity discriminators, as well as incorporate pose loss, which requires appearance of the generated image with same identity to be similar. The proposed method has been well performed on pedestrian reidentification datasets.",2020,ACM TUR-C,,10.1145/3393527.3393560,
f1821b905cec8acac9f81f030260aa614ff1d7a2,0,1,0,Attention Driven Person Re-identification,"Person re-identification (ReID) is a challenging task due to arbitrary human pose variations, background clutters, etc. It has been studied extensively in recent years, but the multifarious local and global features are still not fully exploited by either ignoring the interplay between whole-body images and body-part images or missing in-depth examination of specific body-part images. In this paper, we propose a novel attention-driven multi-branch network that learns robust and discriminative human representation from global whole-body images and local body-part images simultaneously. Within each branch, an intra-attention network is designed to search for informative and discriminative regions within the whole-body or body-part images, where attention is elegantly decomposed into spatial-wise attention and channel-wise attention for effective and efficient learning. In addition, a novel inter-attention module is designed which fuses the output of intra-attention networks adaptively for optimal person ReID. The proposed technique has been evaluated over three widely used datasets CUHK03, Market-1501 and DukeMTMC-ReID, and experiments demonstrate its superior robustness and effectiveness as compared with the state of the arts.",2019,Pattern Recognit.,1810.05866,10.1016/j.patcog.2018.08.015,https://arxiv.org/pdf/1810.05866.pdf
f1c305484114a9b630e00648141806cee9eda43d,0,1,0,Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation: the Benefit of Target Expectation Maximization,"In this paper we make two contributions to unsupervised domain adaptation in the convolutional neural network. First, our approach transfers knowledge in the deep side of neural networks for all convolutional layers. Previous methods usually do so by directly aligning higher-level representations, e.g., aligning the activations of fully-connected layers. In this case, although the convolutional layers can be modified through gradient back-propagation, but not significantly. Our approach takes advantage of the natural image correspondence built by CycleGAN. Departing from previous methods, we use every convolutional layer of the target network to uncover the knowledge shared by the source domain through an attention alignment mechanism. The discriminative part of an image is relatively insensitive to the change of image style, ensuring our attention alignment particularly suitable for robust knowledge adaptation. Second, we estimate the posterior label distribution of the unlabeled data to train the target network. Previous methods, which iteratively update the pseudo labels by the target network and refine the target network by the updated pseudo labels, are straightforward but vulnerable to noisy labels. Instead, our approach uses category distribution to calculate the cross-entropy loss for training, thereby ameliorating deviation accumulation. The two contributions make our approach outperform the state-of-theart methods by +2.6% in all the six transfer tasks on Office- 31 on average. Notably, our approach yields +5.1% improvement for the challenging $\textbf{D}$ ${\rightarrow}$ $\textbf{A}$ task.",2018,ECCV,1801.10068,10.1007/978-3-030-01252-6_25,https://arxiv.org/pdf/1801.10068.pdf
f2213f6039a091117b8fb947e9cef51d9f3fb6f4,1,0,0,Vulnerability of Person Re-Identification Models to Metric Adversarial Attacks,"Person re-identification (re-ID) is a key problem in smart supervision of camera networks. Over the past years, models using deep learning have become state of the art. However, it has been shown that deep neural networks are flawed with adversarial examples, i.e. human-imperceptible perturbations. Extensively studied for the task of image closed- set classification, this problem can also appear in the case of open-set retrieval tasks. Indeed, recent work has shown that we can also generate adversarial examples for metric learning systems such as re-ID ones. These models remain vulnerable: when faced with adversarial examples, they fail to correctly recognize a person, which represents a security breach. These attacks are all the more dangerous as they are impossible to detect for a human operator. Attacking a metric consists in altering the distances between the feature of an attacked image and those of reference images, i.e. guides. In this article, we investigate different possible attacks depending on the number and type of guides available. From this metric attack family, two particularly effective attacks stand out. The first one, called Self Metric Attack, is a strong attack that does not need any image apart from the attacked image. The second one, called FurthestNegative Attack, makes full use of a set of images. Attacks are evaluated on commonly used datasets: Market1501 and DukeMTMC. Finally, we propose an efficient extension of adversarial training protocol adapted to metric learning as a defense that increases the robustness of re-ID models.1",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00405,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Bouniot_Vulnerability_of_Person_Re-Identification_Models_to_Metric_Adversarial_Attacks_CVPRW_2020_paper.pdf
f23582b0d665b598c206999d1a6bafa4d4d36dc3,0,1,0,Pedestrian-Aligned Multiscale Features Network for Person Re-identification,"Person re-identification intends to seek out and match a target pedestrian from different cameras. In practice, images captured by different cameras typically have variances in complex scenes. In addition, large-scale datasets generally adopt automatic detectors to obtain pedestrian detection boxes, and detection errors can lead to image misalignment problems. Both complex scenes and detection errors reduce the robustness of pedestrian characteristics and exasperate the capability of pedestrian alignment. To tackle the above conundrums, we design a novel end-to-end method called Pedestrian-Aligned Multiscale Features Network (PA-MFN), integrating pedestrian alignment and multiscale features network through spatial transformer networks (STN). We employ pedestrian alignment network to work out image misalignment problems satisfactorily. Additionally, we carefully design multiscale features network to extract both global and local features. Comprehensive experiments indicate that our approach gains superior performance on Market-1501, DukeMTMC-reID as well as CUHK03 datasets.",2019,2019 Chinese Automation Congress (CAC),,10.1109/CAC48633.2019.8996826,
f275eba91217b0ab0093654a7f5f2fb8c2ae68ce,0,1,1,Adaptive Exploration for Unsupervised Person Re-identification,"Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.",2020,ACM Trans. Multim. Comput. Commun. Appl.,1907.04194,10.1145/3369393,https://arxiv.org/pdf/1907.04194.pdf
f2f4c0b674a30046f17e97423f38fabc6b43601e,0,1,0,Data augmentation via photo-to-sketch translation for sketch-based image retrieval,"Sketch-based image retrieval (SBIR) technique has progressed by deep learning to learn cross-modal distance metrics that relate sketches and photos from a large number of sketch-photo pairs. However, datasets of sketch-photo pairs are small, as acquisition of a large number of such pairs is expensive. To alleviate the issue, data augmentation via image transformation such as scaling, flipping, rotation, and deformation has been widely adopted. Still, insufficiency in training set seems to have impeded deep learning from achieving its full potential for SBIR. In this paper, we propose a novel data augmentation approach dedicated for SBIR. A deep neural network called Photo2Sketch (P2S) converts photos into line drawings that are visually similar to those sketched by human. An artificially augmented training dataset of sketch-photo pairs is generated at low cost by feeding photos from a large image corpus into the P2S. Experiments evaluate quality of sketch-like images generated by the P2S as well as efficacy of the proposed data augmentation algorithm under SBIR scenario. In particular, retrieval accuracy is significantly improved when the proposed algorithm is combined with the data augmentation by image transformation",2019,International Conference on Graphic and Image Processing,,10.1117/12.2524230,
f316facd96bd91732614f07c202def16926e494f,1,0,0,Intra-Camera Supervised Person Re-Identification,,2020,ArXiv,2002.05046,,https://arxiv.org/pdf/2002.05046.pdf
f38242446a9cf11caa57b796203a048ea9d83eb5,0,1,0,Research of Pedestrian Re-identification Method Based on Video Surveillance,"In order to solve the problem of person recognition in cross-view video sequences of non-overlapping camera, most of the current person re-identification models based on deep learning either need to manually label features as their attributes, or learn the overall single semantic level of feature representation. This paper proposes a person re-identification method based on DNN with multi-level feature fusion, it can automatically learn multi-level discriminative visual factors that are insensitive to viewing condition changes, and identify and utilize them when matching images. Firstly, this paper uses the HOG feature to perform person detection on the video of the two cameras respectively. The person images detected of the camera1 are used as the prob, the person images detected in the camera2 are used as the gallery, and then the two parts are put into the person re-ID model and completed by the training. Finally, the cross-view tracking is implemented for the re-identified persons in combination with the KCF algorithm. The experimental results confirm the accuracy and efficiency of the method.",2019,2019 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),,10.1109/ICIIBMS46890.2019.8991439,
f3b32098030f6b6e4d4582fde736bebfa7f0a875,0,1,0,Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification,"Deep learning has been successful for many computer vision tasks due to the availability of shared and centralised large-scale training data. However, increasing awareness of privacy concerns poses new challenges to deep learning, especially for human subject related recognition such as person re-identification (Re-ID). In this work, we solve the Re-ID problem by decentralised learning from non-shared private training data distributed at multiple user sites of independent multi-domain label spaces. We propose a novel paradigm called Federated Person Re-Identification (FedReID) to construct a generalisable global model (a central server) by simultaneously learning with multiple privacy-preserved local models (local clients). Specifically, each local client receives global model updates from the server and trains a local model using its local data independent from all the other clients. Then, the central server aggregates transferrable local model updates to construct a generalisable global feature embedding model without accessing local data so to preserve local privacy. This client-server collaborative learning process is iteratively performed under privacy control, enabling FedReID to realise decentralised learning without sharing distributed data nor collecting any centralised data. Extensive experiments on ten Re-ID benchmarks show that FedReID achieves compelling generalisation performance beyond any locally trained models without using shared training data, whilst inherently protects the privacy of each local client. This is uniquely advantageous over contemporary Re-ID methods.",2020,ArXiv,2006.0415,,https://arxiv.org/pdf/2006.04150.pdf
f45c568e40e08ee1d75e4f063db3a96a392c0032,1,1,0,SVDNet for Pedestrian Retrieval,"This paper proposes the SVDNet for retrieval problems, with focus on the application of person re-identification (reID). We view each weight vector within a fully connected (FC) layer in a convolutional neuron network (CNN) as a projection basis. It is observed that the weight vectors are usually highly correlated. This problem leads to correlations among entries of the FC descriptor, and compromises the retrieval performance based on the Euclidean distance. To address the problem, this paper proposes to optimize the deep representation learning process with Singular Vector Decomposition (SVD). Specifically, with the restraint and relaxation iteration (RRI) training scheme, we are able to iteratively integrate the orthogonality constraint in CNN training, yielding the so-called SVDNet. We conduct experiments on the Market-1501, CUHK03, and DukeMTMC-reID datasets, and show that RRI effectively reduces the correlation among the projection vectors, produces more discriminative FC descriptors, and significantly improves the re-ID accuracy. On the Market-1501 dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for CaffeNet, and from 73.8% to 82.3% for ResNet-50.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1703.05693,10.1109/ICCV.2017.410,https://arxiv.org/pdf/1703.05693.pdf
f483ddc9a640220f89d8a7327382e9d6e86da33d,1,0,0,Multi-layer attention for person re-identification,,2019,,,10.1051/MATECCONF/201927702025,https://www.matec-conferences.org/articles/matecconf/pdf/2019/26/matecconf_jcmme2018_02025.pdf
f4a44b94d4b184e4638cb1019526170590f99ac4,1,1,0,AM-LFS: AutoML for Loss Function Search,"Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1905.07375,10.1109/ICCV.2019.00850,https://arxiv.org/pdf/1905.07375.pdf
f4e65ab81a0f4ffa50d0c9bc308d7365e012cc75,1,1,1,Deep Active Learning for Video-based Person Re-identification,"It is prohibitively expensive to annotate a large-scale video-based person re-identification (re-ID) dataset, which makes fully supervised methods inapplicable to real-world deployment. How to maximally reduce the annotation cost while retaining the re-ID performance becomes an interesting problem. In this paper, we address this problem by integrating an active learning scheme into a deep learning framework. Noticing that the truly matched tracklet-pairs, also denoted as true positives (TP), are the most informative samples for our re-ID model, we propose a sampling criterion to choose the most TP-likely tracklet-pairs for annotation. A view-aware sampling strategy considering view-specific biases is designed to facilitate candidate selection, followed by an adaptive resampling step to leave out the selected candidates that are unnecessary to annotate. Our method learns the re-ID model and updates the annotation set iteratively. The re-ID model is supervised by the tracklets' pesudo labels that are initialized by treating each tracklet as a distinct class. With the gained annotations of the actively selected candidates, the tracklets' pesudo labels are updated by label merging and further used to re-train our re-ID model. While being simple, the proposed method demonstrates its effectiveness on three video-based person re-ID datasets. Experimental results show that less than 3\% pairwise annotations are needed for our method to reach comparable performance with the fully-supervised setting.",2018,ArXiv,1812.05785,,https://arxiv.org/pdf/1812.05785.pdf
f54e47a89cbbedb17f9e7e734b8c91612325e2ba,1,1,0,Leader-Based Multi-Scale Attention Deep Architecture for Person Re-Identification,"Person re-identification (re-id) aims to match people across non-overlapping camera views in a public space. This is a challenging problem because the people captured in surveillance videos often wear similar clothing. Consequently, the differences in their appearance are typically subtle and only detectable at particular locations and scales. In this paper, we propose a deep re-id network (MuDeep) that is composed of two novel types of layers – a multi-scale deep learning layer, and a leader-based attention learning layer. Specifically, the former learns deep discriminative feature representations at different scales, while the latter utilizes the information from multiple scales to lead and determine the optimal weightings for each scale. The importance of different spatial locations for extracting discriminative features is learned explicitly via our leader-based attention learning layer. Extensive experiments are carried out to demonstrate that the proposed MuDeep outperforms the state-of-the-art on a number of benchmarks and has a better generalization ability under a domain generalization setting.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2019.2928294,http://epubs.surrey.ac.uk/852875/1/final_version.pdf
f571725ffc18c6249702ab457b287495302a4e68,0,1,0,A Survey of Generative Adversarial Networks,"Generative adversarial networks(GANs) coming from the game theory allow machines to learn deep representations without extra training data. By training two adversarial networks, including a generator and a discriminator, GANs could get the distribution of the real samples. This capability makes it a prospect learning method in image synthesis, image recognition, image translation etc. In this paper, we survey the state of the art of GANs by categorizing the GANs into four classifications on the basis of GANs' functions and list two application domains: vision computing & natural language processing(NLP) regarding to GANs' applications.",2018,2018 Chinese Automation Congress (CAC),,10.1109/CAC.2018.8623645,
f5bce258fc6f899e9c9f6658db5c6eccc74975b0,1,1,0,Energy Clustering for Unsupervised Person Re-identification,"Due to the high cost of data annotation in supervised learning for person re-identification (Re-ID) methods, unsupervised learning becomes more attractive in the real world. The Bottom-up Clustering (BUC) approach based on hierarchical clustering serves as one promising unsupervised clustering method. One key factor of BUC is the distance measurement strategy. Ideally, the distance measurement should consider both inter-cluster and intra-cluster distance of all samples. However, BUC uses the minimum distance, only considers a pair of the nearest sample between two clusters and ignores the diversity of other samples in clusters. To solve this problem, we propose to use the energy distance to evaluate both the inter-cluster and intra-cluster distance in hierarchical clustering(E-cluster), and use the sum of squares of deviations(SSD) as a regularization term to further balance the diversity and similarity of energy distance evaluation. We evaluate our method on large scale re-ID datasets, including Market-1501, DukeMTMC-reID and MARS. Extensive experiments show that our method obtains significant improvements over the state-of-the-art unsupervised methods, and even better than some transfer learning methods.",2020,Image Vis. Comput.,1909.00112,10.1016/j.imavis.2020.103913,https://arxiv.org/pdf/1909.00112.pdf
f5c1ec9a2b65a976e5e56a9d8ee2f0059ce8d3d8,0,1,0,Attention Cropping: A Novel Data Augmentation Method for Real-world Plant Species Identification,"This paper investigates the issue of realistic plant species identification. The recognition is close to the condition of a real-world scenario and the dataset is at a large-scale level. A novel data augmentation method is proposed. The image is cropped in terms with visual attention. Different from general way, the cropping is implemented before the image is resized and fed to convolutional neural networks in our proposed method. To deal with the challenge of distinguishing target from complicated background, a method of multiple saliency detections is introduced. Extensive experiments are conducted on both traditional and specific datasets for real-world identification. We introduce the concept of complexity of image background to describe the background complicated rate. Experiments demonstrate that multiple saliency detections can generate corresponding coordinates of the interesting regions well and attention cropping is an efficient data augmentation method. Results show that our method can provide superior performance on different types of datasets. Compared with the precision of methods without attention cropping, the results with attention cropping data augmentation achieve substantial improvement.",2018,ArXiv,,,
f5c2cc0dfa79974d74cedad2ba48c1cbf65ab386,0,1,0,Channel convolution residual block for person re-identification,"In previous works, the channel attention mechanism has been widely used in person re-identification. However, the channel attention mechanism completely compresses the spatial dimension during calculation, which harms the diversity of the channel information over different pixels. In this paper, a channel convolution residual block is proposed for more detailed inter-channel correlation modeling. First, we preserve spatial context information when introducing the channel dependency, which enables pixel-wise inter-channel correlation modeling. At the same time, a bottleneck strategy is used to reduce parameters in the spatial dimension. Second, the channel convolution instead of the fully connected layer is employed to reduce the parameters in the channel dimension. In addition, the inter-channel correlation is merged into the backbone network directly in the form of residual, and thus the block can be embedded in any deep neural networks. Experiments on Market1501 and DukeMTMC-ReID datasets demonstrate that the channel convolution residual block improves the accuracy of person re-identification task effectively.",2020,International Conference on Graphic and Image Processing,,10.1117/12.2557238,
f64f236814080dea063a00166d9cb2afd0247f07,0,1,0,A Mask Based Deep Ranking Neural Network for Person Retrieval,"Person retrieval faces many challenges including cluttered background, appearance variations (e.g., illumination, pose, occlusion) among different camera views and the similarity among different person's images. To address these issues, we put forward a novel mask based deep ranking neural network with a skipped fusing layer. Firstly, to alleviate the problem of cluttered background, masked images with only the foreground regions are incorporated as input in the proposed neural network. Secondly, to reduce the impact of the appearance variations, the multi-layer fusion scheme is developed to obtain more discriminative fine-grained information. Lastly, considering person retrieval is a special image retrieval task, we propose a novel ranking loss to optimize the whole network. The proposed ranking loss can further mitigate the interference problem of similar negative samples when producing ranking results. The extensive experiments validate the superiority of the proposed method compared with the state-of-the-art methods on many benchmark datasets.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME.2019.00092,
f6560353e5ad41923bb5b87d350cdd4b51f5bb53,1,0,0,Automatic multiple human tracking using an adaptive hybrid GMM based detection in a crowd,"For a visual surveillance in a crowd, multiple human tracking is essential and of course a challenging task. Real-world applications require multiple cameras to capture crowd scenes so that a keen tracking is observed. Automatic tracking in a crowded environment is very important criteria for the surveillance. Accurate and real-time tracking in a crowd, the number of people present in the public places and shopping mall are some of the vital information for monitoring traffic violations. To provide human safety and security, surveillance like theft prevention and automated checkout provides the necessary consumer information to the managers. The conventional tracking algorithm does not handle the complex background, multi-view points, various illumination changes and severe occlusion occurring in a crowd. The above problem can be effectively handled by using the proposed Adaptive Hybrid Multiple Human Tracking (AHMHT) method. The proposed work utilizes the Adaptive Hybrid Gaussian Mixture Model (AHGMM) (Karpagavalli and Ramprasad, International Journal of Multimedia Tools and Application 76(12):14129–14149, 12) detected output, so that, the proposed algorithm tracks all the blobs in each frame on the basis of motion information along with the width and height information of exact blob. The experimental results demonstrate that the proposed method performs well compared to other methods. The multiple human tracking rates are improved with maximum of 91% using the proposed frame work compared with other methods. The proposed method is efficient in terms of computational time (CT) using an adaptive hybrid tracking.",2020,Multimedia Tools and Applications,,10.1007/s11042-019-08181-0,
f67da6d51d8aa038ad5993b2c921170053858f7f,0,1,0,Semantic Constraint GAN for Person Re-Identification in Camera Sensor Networks,"In this paper, we propose a novel data augmentation method named Semantic Constraint Generative Adversarial Network (SCGAN) for person re-identification (Re-ID) in camera sensor networks. The proposed SCGAN can generate multiple style pedestrian images with high-level semantic information. To this end, we design two types of semantic constraints, i.e., attention constraint and identity constraint. The attention constraint aims to restrict the significant areas in the attention map to be consistent before and after image transformation. The identity constraint focuses on keeping the identity of the generated pedestrian image to be the same as that of the real one. After generating pedestrian images using SCGAN, we combine them with the real pedestrian images to train the person Re-ID model. Since the proposed SCGAN increases the diversity of training samples, the generalization of Re-ID model is enhanced. We evaluate the proposed SCGAN on three large-scale person Re-ID databases, i.e., Market1501, CUHK03 and DukeMTMC-reID, and experimental results reveal that the proposed SCGAN yields consistent improvements over other methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2958126,
f6bc035a92e279199885193f13de63c8a342ad74,0,1,0,Cross-Modality Paired-Images Generation for RGB-Infrared Person Re-Identification,"RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. The key solution is to learn aligned features to the bridge RGB and IR modalities. However, due to the lack of correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing the distance between the entire RGB and IR sets. However, this set-level alignment may lead to misalignment of some instances, which limits the performance for RGB-IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross-modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged images. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Extensive experimental results on two standard benchmarks demonstrate that the proposed model favourably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve a gain of 9.2% and 7.7% in terms of Rank-1 and mAP. Code is available at https://github.com/wangguanan/JSIA-ReID.",2020,AAAI,2002.04114,10.1609/AAAI.V34I07.6894,https://arxiv.org/pdf/2002.04114.pdf
f6c1122a787b840df0a56b4a839c91d2310eaaeb,0,1,0,Cross-Camera Person Re-Identification With Body-Guided Attention Network,"Various challenges exist throughout person re-identification (ReID) process, including background clutters, illumination variation, pose variation, occlusion, etc. Addressing these problems, this paper explores the incorporation of human attention mechanism in person ReID and proposes an attention-aware model named Body-guided Attention Network (BANet). The proposed attention is based on the body masked images which are obtained by a reliable pixel-level segmentation strategy. To optimize the feature representation learning so as to pay more attention to the discriminative details of human body, BANet is built. It is composed of three attention branches. In order to guide attention learning layer by layer, these branches are applied to the convolution features of different levels. The proposed BANet aims to fully utilize fine-grained information of body region to guide the final process of feature extraction. Extensive experiments on benchmarks including CUHK03, Market1501 and DukeMTMC-reID show that BANet can achieve state-of-the-art performance, which validates the importance of attention mechanism in person ReID.",2020,IEEE Sensors Journal,,10.1109/JSEN.2019.2942106,
f706b43109520169bf165bca98aca349458a09ff,0,1,0,Multi-Domain Adversarial Feature Generalization for Person Re-Identification,"With the assistance of sophisticated training methods applied to single labeled datasets, the performance of fullysupervised person re-identification (Person Re-ID) has been improved significantly in recent years. However, these models trained on a single dataset usually suffer from considerable performance degradation when applied to videos of a different camera network. To make Person Re-ID systems more practical and scalable, several cross-dataset domain adaptation methods have been proposed, which achieve high performance without the labeled data from the target domain. However, these approaches still require the unlabeled data of the target domain during the training process, making them impractical. A practical Person Re-ID system pre-trained on other datasets should start running immediately after deployment on a new site without having to wait until sufficient images or videos are collected and the pretrained model is tuned. To serve this purpose, in this paper, we reformulate person re-identification as a multi-dataset domain generalization problem. We propose a multi-dataset feature generalization network (MMFA-AAE), which is capable of learning a universal domain-invariant feature representation from multiple labeled datasets and generalizing it to ‘unseen’ camera systems. The network is based on an adversarial auto-encoder to learn a generalized domain-invariant latent feature representation with the Maximum Mean Discrepancy (MMD) measure to align the distributions across multiple domains. Extensive experiments demonstrate the effectiveness of the proposed method. Our MMFA-AAE approach not only outperforms most of the domain generalization Person Re-ID methods, but also surpasses many state-of-the-art supervised methods and unsupervised domain adaptation methods by a large margin.",2020,ArXiv,2011.12563,,https://arxiv.org/pdf/2011.12563.pdf
f81f69570113e5171203ac121d1ec1d8b91df4a4,0,1,0,Local Convolutional Neural Networks for Person Re-Identification,"Recent works have shown that person re-identification can be substantially improved by introducing attention mechanisms, which allow learning both global and local representations. However, all these works learn global and local features in separate branches. As a consequence, the interaction/boosting of global and local information are not allowed, except in the final feature embedding layer. In this paper, we propose local operations as a generic family of building blocks for synthesizing global and local information in any layer. This building block can be inserted into any convolutional networks with only a small amount of prior knowledge about the approximate locations of local parts. For the task of person re-identification, even with only one local block inserted, our local convolutional neural networks (Local CNN) can outperform state-of-the-art methods consistently on three large-scale benchmarks, including Market-1501, CUHK03, and DukeMTMC-ReID.",2018,ACM Multimedia,,10.1145/3240508.3240645,
f8c4959ca67846d0c08f371ee884bb8a0845af1e,1,0,0,Enhancing Model Performance of Person Re-Indentification on Unknown Target Domain,"Person re-identification(ReID) is the task that aims at retrieving the same person from the images taken across different cameras. Benefiting from the improvement of deep learning algorithms and the appearance of large datasets, the performance of ReID models has been greatly improved. However, most ReID models focus on a single dataset and their performance will drop dramatically when the train-set and test-set are from different datasets. To improve the generalization ability of the ReID model, this paper proposes a method that takes the advantage of triplet loss and multi-dataset training. And the experiment results show that this method can enhance the model performace in cross dataset usage.",2018,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),,10.1109/ICSESS.2018.8663745,
f8d30281a099111cfb3b3d771545ec27092f9ae4,1,1,0,An end-to-end exemplar association for unsupervised person Re-identification,"Tracklet association methods learn the cross camera retrieval ability though associating underlying cross camera positive samples, which have proven to be successful in unsupervised person re-identification task. However, most of them use poor-efficiency association strategies which costs long training hours but gains the low performance. To solve this, we propose an effective end-to-end exemplar associations (EEA) framework in this work. EEA mainly adapts three strategies to improve efficiency: (1) end-to-end exemplar-based training, (2) exemplar association and (3) dynamic selection threshold. The first one is to accelerate the training process, while the others aim to improve the tracklet association precision. Compared with existing tracklet associating methods, EEA obviously reduces the training cost and achieves the higher performance. Extensive experiments and ablation studies on seven RE-ID datasets demonstrate the superiority of the proposed EEA over most state-of-the-art unsupervised and domain adaptation RE-ID methods.",2020,Neural Networks,,10.1016/j.neunet.2020.05.015,
f8e3868368aab4845c24150491875425bcd36780,1,0,0,Integrating Coarse Granularity Part-level Features with Supervised Global-level Features for Person Re-identification,"Holistic person re-identification (Re-ID) and partial person re-identification have achieved great progress respectively in recent years. However, scenarios in reality often include both holistic and partial pedestrian images, which makes single holistic or partial person Re-ID hard to work. In this paper, we propose a robust coarse granularity part-level person Re-ID network (CGPN), which not only extracts robust regional level body features, but also integrates supervised global features for both holistic and partial person images. CGPN gains two-fold benefit toward higher accuracy for person Re-ID. On one hand, CGPN learns to extract effective body part features for both holistic and partial person images. On the other hand, compared with extracting global features directly by backbone network, CGPN learns to extract more accurate global features with a supervision strategy. The single model trained on three Re-ID datasets including Market-1501, DukeMTMC-reID and CUHK03 achieves state-of-the-art performances and outperforms any existing approaches. Especially on CUHK03, which is the most challenging dataset for person Re-ID, in single query mode, we obtain a top result of Rank-1/mAP=87.1\%/83.6\% with this method without re-ranking, outperforming the current best method by +7.0\%/+6.7\%.",2020,ArXiv,2010.07675,,https://arxiv.org/pdf/2010.07675.pdf
f8e4ff53dfa0f36c3f0e6ca8da2f2fdeea1b3902,0,1,0,Deep leaf-bootstrapping generative adversarial network for structural image data augmentation,,2019,Comput. Aided Civ. Infrastructure Eng.,,10.1111/MICE.12458,
f8f8f612c5c5b422ff8e3b31258e29dee530c79c,1,0,0,Investigating fast re-identification for multi-camera indoor person tracking,"Abstract In person tracking applications involving multiple cameras, person re-identification is an important step for ensuring accurate tracking of individuals as they move between camera views. However, changes in camera parameters and environmental conditions can make re-identification challenging. This is especially difficult in resource-constrained environments, as is often the case in many real-world intelligent applications. In this paper, we explore dimensionality reduction, metric learning, and classification for achieving re-identification in a computationally efficient way. We report that the covariance metric transformation is a sufficient distance metric for achieving good linear separability between identity classes, and produces better results than more complex approaches across two re-identification datasets. We also explore one-shot learning methods for performing classification, show that our Sequential k-Means algorithm outperforms other fast one-shot learning approaches, and discuss parameter tuning to improve accuracy.",2019,Comput. Electr. Eng.,,10.1016/J.COMPELECENG.2019.06.009,
f8f92624c8794d54e08b3a8f94910952ae03cade,1,1,1,CamStyle: A Novel Data Augmentation Method for Person Re-Identification,"Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2018.2874313,
f9ae37ec9329c57dac897b479308ef26e3521be5,1,1,0,Learning Disentangled Representation for Robust Person Re-identification,"We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. The key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose) and this requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g.,clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, largely outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models will be available online at the time of the publication.",2019,NeurIPS,1910.12003,,https://arxiv.org/pdf/1910.12003.pdf
f9ae38e2dce0205c5cb9884837016cf3e73f7584,1,1,0,Person Re-Identification Based on DropEasy Method,"Currently, majority of person re-identification (reID) technologies are network-constrained by Dropout regularization, which relies on the random zeroing out of some features to make these features more independent. However, such random zeroing regularization methods are not very effective for improving network performance, because they neglect the unique contribution of different features to the network performance. To improve the value of indiscriminative features in network training, a DropEasy-based person reID method is proposed in this paper. Features are classified into discriminative and indiscriminative ones, according to the distance between the feature vectors of positive or negative sample pairs wherein the discriminative features are zeroed out, while the indiscriminative features are reserved, and the network only learns through indiscriminative features. Furthermore, because networks are always inclined to make up for incomplete information by drawing on the surrounding features in the feature maps, Dropout loses its effectiveness for the network-constraints. To solve this challenge, the DropEasy2d method that can be effectively applied to convolutional layers is further proposed in this paper. DropEasy2d searches discriminative feature areas in the feature maps by sliding and zeroing windows while reserving the indiscriminative features areas to constrain network learning. The effectiveness of the proposed method is demonstrated using the Market-1501, DuckMTMC-reID, and CUHK03 datasets. For example, in the Market-1501 dataset, DropEasy can improve the mean average precision (mAP) and Rank-1 accuracy of the ID-discriminative embedding (IDE) to 72.7%(+8.8%) and 90.5%(+6.8%), respectively, while DropEasy2d can raise them to 68.5%(+4.6%) and 88.7%(+5.0%), respectively. Based on the results, the proposed method can improve the network performance during the extraction and generalization of the discriminative features.",2019,IEEE Access,,10.1109/ACCESS.2019.2929523,
f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df,1,0,0,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro,"The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID_GAN.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1701.07717,10.1109/ICCV.2017.405,https://opus.lib.uts.edu.au/bitstream/10453/118067/4/FF67E427-6528-4081-B0B7-C3EB797E0421.pdf
f9b87773c250616202c32f07ea92b2fd13cc680a,0,1,1,PoseTrackReID: Dataset Description,"Current datasets for video-based person re-identification (re-ID) do not include structural knowledge in form of human pose annotations for the persons of interest. Nonetheless, pose information is very helpful to disentangle useful feature information from background or occlusion noise. Especially real-world scenarios, such as surveillance, contain a lot of occlusions in human crowds or by obstacles. On the other hand, video-based person re-ID can benefit other tasks such as multi-person pose tracking in terms of robust feature matching. For that reason, we present PoseTrackReID, a largescale dataset for multi-person pose tracking and video-based person re-ID. With PoseTrackReID, we want to bridge the gap between person re-ID and multi-person pose tracking. Additionally, this dataset provides a good benchmark for current state-of-the-art methods on multi-frame person re-ID.",2020,ArXiv,2011.06243,,https://arxiv.org/pdf/2011.06243.pdf
f9f3160404c328364adb1bcd1d61f3796d869bb5,0,1,0,Frontal Face Generation from Multiple Pose-Variant Faces with CGAN in Real-World Surveillance Scene,"It is well known that frontal face is much easier to be recognized than pose-variant face for both human and machine perception. However, it is not easy to acquire a frontal face in real-world video surveillance. This paper proposes a method to synthetize a frontal face for recognition in video surveillance scene, which is based on Conditional Generative Adversarial Networks (cGAN) with input of multiple pose-variant faces from a video. Experimental results show that the proposed approach can generate suitable frontal faces and improve face recognition by around 20% on a dataset of 43276 face images from 19 persons, collected from the real-world video surveillance scene. The effectiveness of multiple frames against single frame as input is demonstrated. Moreover, we investigate the generator with different depth for synthetizing frontal faces, in which an up-down sampling trick is designed for synthetizing higher quality frontal face images and boosts the performance of the generator.",2018,"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2018.8462648,http://mirlab.org/conference_papers/International_Conference/ICASSP%202018/pdfs/0001308.pdf
fa00da0a2e9b8029cf42133f13e01fa79860a7ef,0,1,0,Autoencoder Ensemble for Person Re-Identification,"Person re-identification (re-id) aims to match people from non-overlapping multi-camera networks. Recently, with the advancement of deep learning techniques, the performance of re-id has been improved swiftly. However, most of the existing re-id methods need large number of samples for training due to which the models do not generalize well on smaller datasets and suffers from small sample size problem. Additionally, they focus on single scale appearance information while ignoring rich information that can be exploited from other scales. In this paper, we propose a simple yet effective autoencoder, comprising of an encoder and a sequential decoder. The goal of the network is two fold. First, the network learns features by introducing a generative task to the embedding layer so that it can make features more generalizable to the unknown test data to prevent from overfitting. Second, the encoder feature embedding is used as an input to decoder to reconstruct the input image with various scales to achieve robustness against scale variations. The effectiveness of our proposed method is validated on three public person re-identification datasets, Market-1501, DukeMTMC-reID and CUHK03.",2019,2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM),,10.1109/BigMM.2019.00-15,
fa0411b67f2579ca9144540988c23aad6f2fce6d,1,0,0,Unsupervised Domain Adaptation for Person Re-Identification through Source-Guided Pseudo-Labeling,"Person Re-Identification (re-ID) aims at retrieving images of the same person taken by different cameras. A challenge for re-ID is the performance preservation when a model is used on data of interest (target data) which belong to a different domain from the training data domain (source data). Unsupervised Domain Adaptation (UDA) is an interesting research direction for this challenge as it avoids a costly annotation of the target data. Pseudo-labeling methods achieve the best results in UDA-based re-ID. Surprisingly, labeled source data are discarded after this initialization step. However, we believe that pseudo-labeling could further leverage the labeled source data in order to improve the post-initialization training steps. In order to improve robustness against erroneous pseudo-labels, we advocate the exploitation of both labeled source data and pseudo-labeled target data during all training iterations. To support our guideline, we introduce a framework which relies on a two-branch architecture optimizing classification and triplet loss based metric learning in source and target domains, respectively, in order to allow \emph{adaptability to the target domain} while ensuring \emph{robustness to noisy pseudo-labels}. Indeed, shared low and mid-level parameters benefit from the source classification and triplet loss signal while high-level parameters of the target branch learn domain-specific features. Our method is simple enough to be easily combined with existing pseudo-labeling UDA approaches. We show experimentally that it is efficient and improves performance when the base method has no mechanism to deal with pseudo-label noise or for hard adaptation tasks. Our approach reaches state-of-the-art performance when evaluated on commonly used datasets, Market-1501 and DukeMTMC-reID, and outperforms the state of the art when targeting the bigger and more challenging dataset MSMT.",2020,ArXiv,2009.09445,,https://arxiv.org/pdf/2009.09445.pdf
fa26702ef27402ca77a88cf8807350a3ad38bf03,0,1,0,Siamese Cosine Network Embedding for Person Re-identification,"In person re-identification, feature embedding is the key point for new coming identities. Most state-of-the-art models adopt the features learned by convolutional neural networks (CNNs) to do similarity comparison. However, the learned features are not good enough for new identities because CNNs are designed for classification of class-known objects, not for similarity comparison of any two identities. To improve feature embedding, we propose a pairwise cosine loss based on cosine similarity measurement. Subsequently, we design a Siamese cosine network embedding (SCNE) to learn deep features for person re-identification. It is based on the Siamese architecture, with intra-class input pairs and joint supervision by the softmax loss and the pairwise cosine loss. Experimental results show that our SCNE achieves the state-of-the-art performance on the public Market1501 and CUHK03 person re-ID benchmarks.",2017,CCCV,,10.1007/978-981-10-7305-2_31,
fa7308328df7575b5ad0bbd26cdd8d75230cedf4,0,1,0,Deepagent: An Algorithm Integration Approach for Person Re-Identification,"Person re-identification(RE-ID) has played a significant role in the fields of image processing and computer vision because of its potential value in practical applications. Researchers are striving to design new algorithms to improve the performance of RE-ID but ignore the advantages of existing approaches. In this paper, motivated by deep reinforcement learning, we propose a Deep Agent which can integrate existing algorithms and enable them to complement each other. Two Deep Agents are designed to integrate algorithms for data augmentation and feature extraction parts separately for RE-ID. Experiment results demonstrate that the integrated algorithms can achieve a better accuracy than using each one of them alone.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451521,https://www.vislab.ucr.edu/PUBLICATIONS/pubs/Journal%20and%20Conference%20Papers/after10-1-1997/Conference/2018/DEEPAGENT-AN%20ALGORITHM%20INTEGRATION%20APPROACH%20FOR%20PERSON.pdf
fa90b3232a74066463b9a2bbde1f6bd9d5612c5e,0,1,0,Can Virtual Samples Solve Small Sample Size Problem of KISSME in Pedestrian Re-Identification of Smart Transportation?,"This work investigates whether virtual samples can solve the small sample size (S3) problem of Keep-It-Simple-and-Straightforward Metric Learning (KISSME) in Pedestrian re-identification (Re-ID). Re-ID is a very challenging and important problem in the field of multi-camera surveillance in smart transportation. Among many hand-crafted ways (not deeply-learned ones) to solve it, KISSME has received great attention. Although it has achieved convincing performance in some applications, it encounters an S3 problem in calculating various classes of covariance matrices whose eigenvalues become too small. Such small eigenvalues cause an instability issue when computing the inverse of covariance matrices, thus resulting in poor Re-ID performance. If we can increase the number of samples, then an S3 problem is alleviated or eliminated. This work makes a hypothesis that virtual samples can do so, and proposes a new algorithm to generate them. It adopts a Genetic Algorithm to generate virtual features (corresponding to virtual samples) based on the dimension-reduced sample features, which eliminates the process of re-extracting features of newly generated virtual samples and save time. It can clearly increase the magnitude of otherwise small eigenvalues, helps one perform the accurate estimation of the inverse of various covariance matrices and finally alleviates the S3 problem. Experimental results based on a commonly-used database confirm that the proposed method can significantly improve the matching rate of pedestrian Re-ID, which fully shows that virtual samples are indeed effective for alleviating the S3 problem in pedestrian Re-ID.",2020,IEEE Transactions on Intelligent Transportation Systems,,10.1109/TITS.2019.2933509,
fae3ff37995414fb9c5f1cac19301b7dff2f2bc8,1,0,0,The 2019 AI City Challenge,"The AI City Challenge has been created to accelerate intelligent video analysis that helps make cities smarter and safer. With millions of traffic video cameras acting as sensors around the world, there is a significant opportunity for real-time and batch analysis of these videos to provide actionable insights. These insights will benefit a wide variety of agencies, from traffic control to public safety. The 2019 AI City Challenge is the third annual edition in the AI City Challenge series with significant growing attention and participation. AI City Challenge 2019 enabled 334 academic and industrial research teams from 44 countries to solve real-world problems using real city-scale traffic camera video data. The Challenge was launched with three tracks. Track 1 addressed city-scale multi-camera vehicle tracking, Track 2 addressed city-scale vehicle re-identification, and Track 3 addressed traffic anomaly detection. Each track was chosen in consultation with departments of transportation focusing on problems of greatest public value. With the largest available dataset for such tasks, and ground truth for each track, the 2019 AI City Challenge received 129 submissions from 96 individuals teams (there were 22, 84, 23 team submissions from Tracks 1, 2, and 3 respectively). Participation in this challenge has grown five-fold this year as tasks have become more relevant to traffic optimization and challenging to the computer vision community. Results observed strongly underline the value AI brings to city-scale video analysis for traffic optimization.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/87c0/1e88e58fa9059c6c3b4d8f3ebdecc8be0541.pdf
fb05beff04ddf51f4a5f5d225f00e7b83da58b1b,1,0,0,Vehicle Tracking and Monitoring in Surveillance Video,"A video surveillance system has become an integral part of the smart city infrastructure. However, it still lacks its utilization to the full potential. In this work, we present a case study in which a storage center surveillance data has been used to extract useful information automatically. A surveillance camera records the movement of trucks passing through a storage center gate where a clerk registers the truck-related data. This process is prone to suffer from inaccuracy, fraud, and loss. The challenge is to automate the data entry process through video such that the warehouse achieves a seamless and errorfree record-keeping. In this paper, we present a framework to use the surveillance video to extract useful information such as detection of trucks, their registration number/ownership identification, count of incoming and outgoing trucks, and count of loaded or empty trucks. We tested the work presented in this paper at the paddy storage centers in Chhattisgarh, India, and the results were very encouraging.",2019,2019 IEEE Conference on Information and Communication Technology,,10.1109/CICT48419.2019.9066256,
fb4b700ba023c08e64c13f8030f40dcc901ac518,1,1,0,Joint Detection and Identification Feature Learning for Person Search,"Existing person re-identification benchmarks and methods mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be searched from a gallery of whole scene images. To close the gap, we propose a new deep learning framework for person search. Instead of breaking it down into two separate tasks&#x2014;pedestrian detection and person re-identification, we jointly handle both aspects in a single convolutional neural network. An Online Instance Matching (OIM) loss function is proposed to train the network effectively, which is scalable to datasets with numerous identities. To validate our approach, we collect and annotate a large-scale benchmark dataset for person search. It contains 18,184 images, 8,432 identities, and 96,143 pedestrian bounding boxes. Experiments show that our framework outperforms other separate approaches, and the proposed OIM loss function converges much faster and better than the conventional Softmax loss.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),1604.0185,10.1109/CVPR.2017.360,https://arxiv.org/pdf/1604.01850.pdf
fba470daf569021d528361b11a9f5b52d6bea493,1,0,0,Patch-Based Discriminative Feature Learning for Unsupervised Person Re-Identification,"While discriminative local features have been shown effective in solving the person re-identification problem, they are limited to be trained on fully pairwise labelled data which is expensive to obtain. In this work, we overcome this problem by proposing a patch-based unsupervised learning framework in order to learn discriminative feature from patches instead of the whole images. The patch-based learning leverages similarity between patches to learn a discriminative model. Specifically, we develop a PatchNet to select patches from the feature map and learn discriminative features for these patches. To provide effective guidance for the PatchNet to learn discriminative patch feature on unlabeled datasets, we propose an unsupervised patch-based discriminative feature learning loss. In addition, we design an image-level feature learning loss to leverage all the patch features of the same image to serve as an image-level guidance for the PatchNet. Extensive experiments validate the superiority of our method for unsupervised person re-id. Our code is available at https://github.com/QizeYang/PAUL.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00375,http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Patch-Based_Discriminative_Feature_Learning_for_Unsupervised_Person_Re-Identification_CVPR_2019_paper.pdf
fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea,0,1,0,"A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications","Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.",2020,ArXiv,2001.06937,,https://arxiv.org/pdf/2001.06937.pdf
fc8b1413bbaf78d7b6fd2559342de8180c566d31,1,1,1,Generalizing a Person Retrieval Model Hetero- and Homogeneously,"Person re-identification (re-ID) poses unique challenges for unsupervised domain adaptation (UDA) in that classes in the source and target sets (domains) are entirely different and that image variations are largely caused by cameras. Given a labeled source training set and an unlabeled target training set, we aim to improve the generalization ability of re-ID models on the target testing set. To this end, we introduce a Hetero-Homogeneous Learning (HHL) method. Our method enforces two properties simultaneously: (1) camera invariance, learned via positive pairs formed by unlabeled target images and their camera style transferred counterparts; (2) domain connectedness, by regarding source/target images as negative matching pairs to the target/source images. The first property is implemented by homogeneous learning because training pairs are collected from the same domain. The second property is achieved by heterogeneous learning because we sample training pairs from both the source and target domains. On Market-1501, DukeMTMC-reID and CUHK03, we show that the two properties contribute indispensably and that very competitive re-ID UDA accuracy is achieved. Code is available at: https://github.com/zhunzhong07/HHL.",2018,ECCV,,10.1007/978-3-030-01261-8_11,https://opus.lib.uts.edu.au/bitstream/10453/131487/1/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.pdf
fce6eac39672564587abd9133be8d91757a8902c,1,1,1,Bayesian query expansion for multi-camera person re-identification,"Abstract Person re-identification (re-ID) is challenging because pedestrians may exhibit distinct appearance under different cameras. Given a query image, previous methods usually output the person retrieval results directly, which may perform badly due to the limited information provided by the single query image. To mine more query information, we add an expansion step to post-process the initial ranking list. The intuition is that a true match in the gallery may be difficult to be found by the query alone, but it can be easily retrieved by other true matches in the initial ranking list. In this paper, we propose the Bayesian Query Expansion (BQE) method to generate a new query with information from the initial ranking list. The Bayesian model is used to predict true matches in the gallery. We apply pooling on the features of these “true matches” to get a single vector, i.e., the expanded new query, with which the retrieval process is performed again to obtain the final results. We evaluate BQE with various feature extraction methods and distance metric learning methods on four large-scale re-ID datasets. We observe consistent improvement over all the baselines and report competitive performances compared with the state-of-the-art results.",2020,Pattern Recognit. Lett.,,10.1016/J.PATREC.2018.06.009,
fcec633bbdeaab2d61fcc6d86f74383ccc3621f9,1,0,0,Robust video editing detection using Scalable Color and Color Layout Descriptors,"Nowadays, recorded videos from surveillance cameras are important evidence for legal investigation in the field of forensic science. Videos may be modified to deviate contents by a person involves in a crime. In this paper, a video editing detection based on Scalable Color Descriptor (SCD) and Color Layout Descriptor (CLD) is proposed. The detection method is composed of two components: (1) generating video identifier and signature and (2) video verification. The experimental results show that applying SCD and CLD to design the detection method outperforms the other descriptors in terms of false acceptance rate and false rejection rate. It is concluded that our method accurately classifies whether or not an incoming video is forged.",2017,2017 14th International Joint Conference on Computer Science and Software Engineering (JCSSE),,10.1109/JCSSE.2017.8025923,
fcede9b4b7645070a709a325f3a66fbe3eb14465,0,1,0,Performance Optimization of Federated Person Re-identification via Benchmark Analysis,"Federated learning is a privacy-preserving machine learning technique that learns a shared model across decentralized clients. It can alleviate privacy concerns of personal re-identification, an important computer vision task. In this work, we implement federated learning to person re-identification (FedReID) and optimize its performance affected by statistical heterogeneity in the real-world scenario. We first construct a new benchmark to investigate the performance of FedReID. This benchmark consists of (1) nine datasets with different volumes sourced from different domains to simulate the heterogeneous situation in reality, (2) two federated scenarios, and (3) an enhanced federated algorithm for FedReID. The benchmark analysis shows that the client-edge-cloud architecture, represented by the federated-by-dataset scenario, has better performance than client-server architecture in FedReID. It also reveals the bottlenecks of FedReID under the real-world scenario, including poor performance of large datasets caused by unbalanced weights in model aggregation and challenges in convergence. Then we propose two optimization methods: (1) To address the unbalanced weight problem, we propose a new method to dynamically change the weights according to the scale of model changes in clients in each training round; (2) To facilitate convergence, we adopt knowledge distillation to refine the server model with knowledge generated from client models on a public dataset. Experiment results demonstrate that our strategies can achieve much better convergence with superior performance on all datasets. We believe that our work will inspire the community to further explore the implementation of federated learning on more computer vision tasks in real-world scenarios.",2020,ACM Multimedia,2008.1156,10.1145/3394171.3413814,https://arxiv.org/pdf/2008.11560.pdf
fd2bc4833c19a60d3646368952dcf35dbda007f3,1,1,0,Improving Person Re-Identification by Adaptive Hard Sample Mining,"The field of person reidentification has made significant advances riding on the wave of deep learning. However, owing to the fact that there are much more easy examples than those meaningful hard examples in dataset, the training tends to stagnate quickly and the model may suffer from over-fitting. Therefore, the hard sample mining method is fateful to optimize the model and improve the learning efficiency. In this paper, an Adaptive Hard Sample Mining algorithm is proposed for training a robust person re-identification model. No need for hand-picking the images in the batch or designing the loss function for both positive and negative pairs, we can briefly calculate the hard level by comparing the prediction result with the true label of the sample. Meanwhile, an adaptive threshold of hard level can make the algorithm not only stay in step with training process harmoniously but also alleviate the under-fitting and over-fitting problem simultaneously. Besides, the designed network to implement the approach has good generalization performance that can be combined with various of existing models readily. Experimental results on Market-1501 and DukeMTMC-reID datasets clearly demonstrate the effectiveness of the proposed algorithm.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451129,
fd5061e8f6314f68e321eae2c374e29168f23c15,0,1,0,Receptive Multi-Granularity Representation for Person Re-Identification,"A key for person re-identification is achieving consistent local details for discriminative representation across variable environments. Current stripe-based feature learning approaches have delivered impressive accuracy, but do not make a proper trade-off between diversity, locality, and robustness, which easily suffers from part semantic inconsistency for the conflict between rigid partition and misalignment. This paper proposes a receptive multi-granularity learning approach to facilitate stripe-based feature learning. This approach performs local partition on the intermediate representations to operate receptive region ranges, rather than current approaches on input images or output features, thus can enhance the representation of locality while remaining proper local association. Toward this end, the local partitions are adaptively pooled by using significance-balanced activations for uniform stripes. Random shifting augmentation is further introduced for a higher variance of person appearing regions within bounding boxes to ease misalignment. By two-branch network architecture, different scales of discriminative identity representation can be learned. In this way, our model can provide a more comprehensive and efficient feature representation without larger model storage costs. Extensive experiments on intra-dataset and cross-dataset evaluations demonstrate the effectiveness of the proposed approach. Especially, our approach achieves a state-of-the-art accuracy of 96.2%@Rank-1 or 90.0%@mAP on the challenging Market-1501 benchmark.",2020,IEEE Transactions on Image Processing,2008.1345,10.1109/TIP.2020.2986878,https://arxiv.org/pdf/2008.13450.pdf
fdaf6546ce8920639b24325bebdf51e92fa0e39a,0,1,0,Group-Group Loss-Based Global-Regional Feature Learning for Vehicle Re-Identification,"Vehicle Re-Identification (Re-ID) is challenging because vehicles of the same model commonly show similar appearance. We tackle this challenge by proposing a Global-Regional Feature (GRF) that depicts extra local details to enhance discrimination power in addition to the global context. It is motivated by the observation that, vehicles of same color, maker, and model can be distinguished by their regional difference, e.g., the decorations on the windshields. To accelerate the GRF learning and promote its discrimination power, we propose a Group-Group Loss (GGL) to optimize the distance within and across vehicle image groups. Different from the siamese or triplet loss, GGL is directly computed on image groups rather than individual sample pairs or triplets. By avoiding traversing numerous sample combinations, GGL makes the model training easier and more efficient. Those two contributions highlight this work from previous methods on vehicle Re-ID task, which commonly learn global features with triplet loss or its variants. We evaluate our methods on two large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental results show our methods achieve promising performance in comparison with recent works.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2950796,
fde900199b2aa6d8a5364342885bdb2f5f7073c2,1,1,0,Deep Structured Prediction: A New Formulation for Person Re-Identification,"Person re-identification (re-ID) based on visual appearance has been an intensively researched area in computer vision and forensic multimedia analysis. Its goal is to associate person detections under different spatial-temporal scenarios across different camera views. Existing efforts on person re-ID can generally be categorized into two approaches: conventional image retrieval and highly-crafted re-ID structures. In this paper, we formulate person re-ID, for the very first time, as an energy-based deep structured prediction problem without the need of explicitly specifying the graph topology of the re-ID structure in advance. We also integrate a structure sampling mechanism, Randomized Dropout Structure Sampling (RDSS), into structured prediction while all the existing works assume that structure samples are readily available for learning. Experiment results show that our new formulation outperforms conventional image retrieval and highly crafted re-ID structures.",2018,2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),,10.1109/MIPR.2018.00014,
fe0596fa9e2766fc92d6469574849e7fee9e871c,1,1,0,Recurrent matching networks of spatial alignment learning for person re-identification,"Person re-identification (re-id) usually refers to matching people across disjoint camera views. Many existing methods focus on extracting discriminative features or learning distance metrics to make the intraclass distance smaller than interclass distances. These methods subconsciously assume that pedestrian images are well aligned. However, one major challenge in person re-id is the unconstrained spatial misalignment between image pairs due to view angle changes and pedestrian pose variations. To address this problem, in this paper, we propose Recurrent Matching Network of Spatial Alignment Learning (RMN-SAL) to simulate the human vision perception. Reinforcement learning is introduced to locate attention regions, since it provides a flexible learning strategy for sequential decision-making. A linear mapping is employed to convert the environment state into spatial constraint, comprising spatial alignment into feature learning. And recurrent models are used to extract information from a sequence of corresponding regions. Finally, person re-id is performed based on the global features and the features from the learned alignment regions. Our contributions are: 1) the recurrent matching network, which can subtly combine local feature learning and sequential spatial correspondence learning into an end-to-end framework; 2) the design of a location network, which is based on reinforcement learning and aims to learn task-specific sequential spatial correspondences for different image pairs through the local pairwise internal representation interactions. The proposed model is evaluated on three benchmarks, including Market-1501, DukeMTMC-reID and CUHK03, and achieves better performances than other methods.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-08364-9,
fe084e481481641df92b166fc81f0edb4a3fbc0c,0,1,0,Identity-sensitive loss guided and instance feature boosted deep embedding for person search,"Abstract Person search aims at detecting and re-identifying pedestrians from whole monitoring images, which is vital for intelligent surveillance. However, this task is still challenging due to the extremely few instances per training identity and inherent fine-grained differences among different identities. To this end, this work proposes an identity-sensitive loss guided and instance feature boosted pipeline to extract deep discriminative feature embedding for person search. First, a prior anchor pre-trained network (PAPN) is designed to obtain proper initial state for the whole deep person search training baseline. Second, a new loss function called instance enhancing loss (IEL) is proposed to learn identity-sensitive features by introducing unlabeled identity information. Specifically, the proposed IEL can selectively utilize unlabeled identities with similar appearances to labeled identities to train the person search network. Third, considering the intra-class compactness of features learned by center loss and contextual inter-class relations, two instance boosting strategies (Boosting) are used to learn more discriminative features. Extensive experiments on two benchmark datasets, namely CUHK-SYSU and PRW, demonstrate the effectiveness of our approach.",2020,Neurocomputing,,10.1016/j.neucom.2020.07.062,
fe3f8826f615cc5ada33b01777b9f9dc93e0023c,1,1,0,Exploring Uncertainty in Conditional Multi-Modal Retrieval Systems,"We cast visual retrieval as a regression problem by posing triplet loss as a regression loss. This enables epistemic uncertainty estimation using dropout as a Bayesian approximation framework in retrieval. Accordingly, Monte Carlo (MC) sampling is leveraged to boost retrieval performance. Our approach is evaluated on two applications: person re-identification and autonomous car driving. Comparable state-of-the-art results are achieved on multiple datasets for the former application.  We leverage the Honda driving dataset (HDD) for autonomous car driving application. It provides multiple modalities and similarity notions for ego-motion action understanding. Hence, we present a multi-modal conditional retrieval network. It disentangles embeddings into separate representations to encode different similarities. This form of joint learning eliminates the need to train multiple independent networks without any performance degradation. Quantitative evaluation highlights our approach competence, achieving 6% improvement in a highly uncertain environment.",2019,ArXiv,1901.07702,,https://arxiv.org/pdf/1901.07702.pdf
fe52056cd1ca3fd72e0f5b5a01b4a8d30e7df24b,1,0,0,Geometry-Aware Video Object Detection for Static Cameras,"In this paper we propose a geometry-aware model for video object detection. Specifically, we consider the setting that cameras can be well approximated as static, e.g. in video surveillance scenarios, and scene pseudo depth maps can therefore be inferred easily from the object scale on the image plane. We make the following contributions: First, we extend the recent anchor-free detector (CornerNet [17]) to video object detections. In order to exploit the spatial-temporal information while maintaining high efficiency, the proposed model accepts video clips as input, and only makes predictions for the starting and the ending frames, i.e. heatmaps of object bounding box corners and the corresponding embeddings for grouping. Second, to tackle the challenge from scale variations in object detection, scene geometry information, e.g. derived depth maps, is explicitly incorporated into deep networks for multi-scale feature selection and for the network prediction. Third, we validate the proposed architectures on an autonomous driving dataset generated from the Carla simulator [5], and on a real dataset for human detection (DukeMTMC dataset [28]). When comparing with the existing competitive single-stage or two-stage detectors, the proposed geometry-aware spatio-temporal network achieves significantly better results.",2019,BMVC,1909.0314,,https://arxiv.org/pdf/1909.03140.pdf
fea0895326b663bf72be89151a751362db8ae881,1,1,0,Homocentric Hypersphere Feature Embedding for Person Re-Identification,"Triplet loss and softmax loss are two widely used loss functions in Person Re-Identification (Person ReID). However, previous works that try to apply these two loss functions have measure inconsistency during training and testing stage and among different parts of the total loss function, which would cause inferior performance of models. To address this issue, we propose a novel homocentric hypersphere embedding scheme to decouple magnitude and orientation information for both feature and weight vectors, and reformulate the triplet loss and the softmax loss to their angular versions and combine them into an angular discriminative loss. We evaluate our proposed method extensively on the widely used Person ReID benchmarks. Our method demonstrates leading performance on all datasets.",2019,2019 IEEE International Conference on Image Processing (ICIP),1804.08866,10.1109/ICIP.2019.8803735,https://arxiv.org/pdf/1804.08866.pdf
febbe18c8e8b7e5568e0ff36fe82ef582692f5f6,0,1,0,Fine-Grained Spatial Alignment Model for Person Re-Identification With Focal Triplet Loss,"Recent advances of person re-identification have well advocated the usage of human body cues to boost performance. However, most existing methods still retain on exploiting a relatively coarse-grained local information. Such information may include redundant backgrounds that are sensitive to the apparently similar persons when facing challenging scenarios like complex poses, inaccurate detection, occlusion and misalignment. In this paper we propose a novel Fine-Grained Spatial Alignment Model (FGSAM) to mine fine-grained local information to handle the aforementioned challenge effectively. In particular, we first design a pose resolve net with channel parse blocks (CPB) to extract pose information in pixel-level. This network allows the proposed model to be robust to complex pose variations while suppressing the redundant backgrounds caused by inaccurate detection and occlusion. Given the extracted pose information, a locally reinforced alignment mode is further proposed to address the misalignment problem between different local parts by considering different local parts along with attribute information in a fine-grained way. Finally, a focal triplet loss is designed to effectively train the entire model, which imposes a constraint on the intra-class and an adaptively weight adjustment mechanism to handle the hard sample problem. Extensive evaluations and analysis on Market1501, DukeMTMC-reid and PETA datasets demonstrate the effectiveness of FGSAM in coping with the problems of misalignment, occlusion and complex poses.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3004267,http://ir.sia.cn//bitstream/173321/27367/1/Fine-Grained%20Spatial%20Alignment%20Model%20for%20Person%20Re-Identification%20with%20Focal%20Triplet%20Loss.pdf
fed7b62c0dfc91a0393b2f3473ba09db75fd60dc,1,0,0,Group Re-identification via Transferred Single and Couple Representation Learning,"Group re-identification (G-ReID) is an important yet less-studied task. Its challenges not only lie in appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also derive from group layout and membership changes. So the key task of G-ReID is to learn representations robust to such changes. To address this issue, we propose a Transferred Single and Couple Representation Learning Network (TSCN). Its merits are two aspects: 1) Due to the lack of labelled training samples, existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To gain the superiority of deep learning models, we treat a group as multiple persons and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style to learn single representations. 2) Taking into account the neighborhood relationship in a group, we further propose learning a novel couple representation between two group members, that achieves more discriminative power in G-ReID tasks. In addition, an unsupervised weight learning method is exploited to adaptively fuse the results of different views together according to result patterns. Extensive experimental results demonstrate the effectiveness of our approach that significantly outperforms state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by 39.0\% CMC-1 on the DukeMCMT dataset.",2019,ArXiv,1905.04854,,https://arxiv.org/pdf/1905.04854.pdf
feee543cc5350a3983031cae8559f126124ffa2e,1,0,0,Multi-camera Vehicle Tracking and Re-identification on AI City Challenge 2019,"In this work, we present our solutions to the image-based vehicle re-identification (ReID) track and multi-camera vehicle tracking (MVT) tracks on AI City Challenge 2019 (AIC2019). For the ReID track, we propose an enhanced multi-granularity network with multiple branches to extract visual features for vehicles with different levels of grains. With the help of these multi-grained features, the proposed framework outperforms the current state-of-the-art vehicle ReID method by 16.3% on Veri dataset. For the MVT track, we first generate tracklets by Kernighan-Lin graph partitioning algorithm with feature and motion correlation, then combine tracklets to trajectories by proposed progressive connection strategy, finally match trajectories under different camera views based on the annotated road boundaries. Our MVT and ReID algorithms are ranked the 10 and 23 in MVT and ReID tracks respectively at the NVIDIA AI City Challenge 2019.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/9201/d0e518eb6ef32b170388efa8716751f57a93.pdf
fefa8f07d998f8f4a6c85a7da781b19bf6b78d7d,1,0,0,Online Multi-Object Tracking with Dual Matching Attention Networks,"In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits of single object tracking and data association methods in a unified framework to handle noisy detections and frequent interactions between targets. Specifically, for applying single object tracking in MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, which encourages the model to focus on hard negative distractors during online learning. For data association, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention mechanisms. The spatial attention module generates dual attention maps which enable the network to focus on the matching patterns of the input image pair, while the temporal attention module adaptively allocates different levels of attention to different samples in the tracklet to suppress noisy observations. Experimental results on the MOT benchmark datasets show that the proposed algorithm performs favorably against both online and offline trackers in terms of identity-preserving metrics.",2018,ECCV,1902.00749,10.1007/978-3-030-01228-1_23,https://arxiv.org/pdf/1902.00749.pdf
ff6e5a58625f5fa330f5a869252bae9102922cb6,1,0,0,Learning Diverse Features with Part-Level Resolution for Person Re-Identification,"Learning diverse features is key to the success of person re-identification. Various part-based methods have been extensively proposed for learning local representations, which, however, are still inferior to the best-performing methods for person re-identification. This paper proposes to construct a strong lightweight network architecture, termed PLR-OSNet, based on the idea of Part-Level feature Resolution over the Omni-Scale Network (OSNet) for achieving feature diversity. The proposed PLR-OSNet has two branches, one branch for global feature representation and the other branch for local feature representation. The local branch employs a uniform partition strategy for part-level feature resolution but produces only a single identity-prediction loss, which is in sharp contrast to the existing part-based methods. Empirical evidence demonstrates that the proposed PLR-OSNet achieves state-of-the-art performance on popular person Re-ID datasets, including Market1501, DukeMTMC-reID and CUHK03, despite its small model size.",2020,PRCV,2001.07442,10.1007/978-3-030-60636-7_2,https://arxiv.org/pdf/2001.07442.pdf
ffbe733a352c1d995f6f5c99ac0c7f01567165dc,1,0,0,Track-Clustering Error Evaluation for Track-Based Multi-camera Tracking System Employing Human Re-identification,"In this study, we present a set of new evaluation measures for the track-based multi-camera tracking (T-MCT) task leveraging the clustering measurements. We demonstrate that the proposed evaluation measures provide notable advantages over previous ones. Moreover, a distributed and online T-MCT framework is proposed, where re-identification (Re-id) is embedded in T-MCT, to confirm the validity of the proposed evaluation measures. Experimental results reveal that with the proposed evaluation measures, the performance of T-MCT can be accurately measured, which is highly correlated to the performance of Re-id. Furthermore, it is also noted that our T-MCT framework achieves competitive score on the DukeMTMC dataset when compared to the previous work that used global optimization algorithms. Both the evaluation measures and the inter-camera tracking framework are proven to be the stepping stone for multi-camera tracking.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.184,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w17/papers/Wu_Track-Clustering_Error_Evaluation_CVPR_2017_paper.pdf
ffc3a021699c9d40d72c6e1fe547e2bd9706b878,0,1,0,Fast and accurate person re-identification with Xception Conv-Net and C 2 F,"Person re-identification (re-id) is the task of identifying a person of interest across disjoint camera views in a multi-camera system. This is a challenging problem due to the different poses, viewpoints and lighting conditions. Deeply learned systems have become prevalent in the person re-identification field as they are capable to deal with the these obstacles. Conv-Net using a coarse-to-fine search framework (Conv-Net+C2F) is such a deeply learned system, which has been developed with both a high-retrieval accuracy as a fast query time in mind. We propose three contributions to improve Conv-Net+C2F: 1) training with an improved optimizer, 2) constructing Conv-Net using a different Convolutional Neural Network (CNN) not yet used for person re-id and 3) coarse descriptors having fewer dimensions for improved speed as well as increased accuracy. With these adaptations Xception ConvNet+C2F achieves state-of-the-art results on Market-1501 (single-query, 72.4% mAP) and the new, challenging data split of CUHK03 (detected, 42.6% mAP).",2019,,,,http://home.kpn.nl/henri.bouma/research/rooijen_2018_ciarp_person_reid.pdf
ffc82c0a385f9eec23a3249cac877b959d34e367,1,1,0,Self Attention based multi branch Network for Person Re-Identification,"Recent progress in the field of person re-identification have shown promising improvement by designing neural networks to learn most discriminative features representations. Some efforts utilize similar parts from different locations to learn better representation with the help of soft attention, while others search for part based learning methods to enhance consecutive regions relationships in the learned features. However, only few attempts have been made to learn non-local similar parts directly for the person re-identification problem. In this paper, we propose a novel self attention based multi branch(classifier) network to directly model long-range dependencies in the learned features. Multi classifiers assist the model to learn discriminative features while self attention module encourages the learning to be independent of the feature map locations. Spectral normalization is applied in the whole network to improve the training dynamics and for the better convergence of the model. Experimental results on two benchmark datasets have shown the robustness of the proposed work.",2020,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),,10.23919/SpliTech49282.2020.9243741,
ffcfad72dff3e7441a4daeec95c0108833a5b38a,1,0,0,Universal Adversarial Perturbations Against Person Re-Identification,"Person re-identification (re-ID) has made great progress and achieved high performance in recent years with the development of deep learning. However, as an application related to security issues, there are few researches considering the safety of person re-ID systems. In this paper, we attempt to explore the robustness of current person re-ID models against adversarial samples. Specifically, we attack the re-ID models using universal adversarial perturbations (UAPs), which are especially dangerous to the surveillance systems because it could fool most pedestrian images with a little overhead. Existing methods for UAPs mainly consider classification models, while the tasks in open set scenarios like re-ID are rarely explored. Re-ID attack is different from classification ones in the sense that the former discards decision boundary during test and cares more about the ranking list. Therefore, we propose an effective method to train UAPs against person re-ID models from the global list-wise perspective. Furthermore, to increase the impact of attack to different models and datasets, we propose a novel UAPs learning method based on total variation minimization. Extensive experiments validate the effectiveness of our proposed method.",2019,ArXiv,1910.14184,,https://arxiv.org/pdf/1910.14184.pdf
ffe4cf3720e378a67e5d43d0af044080ae286e12,0,0,1,One-Shot Learning on Attributed Sequences,"One-shot learning has become an important research topic in the last decade with many real-world applications. The goal of one-shot learning is to classify unlabeled instances when there is only one labeled example per class. Conventional problem setting of one-shot learning mainly focuses on the data that is already in a feature space (such as images). However, the data instances in real-world applications are often more complex and feature vectors may not be available. In this paper, we study the problem of one-shot learning on attributed sequences, where each instance is composed of a set of attributes (e.g., user profile) and a sequence of categorical items (e.g., clickstream). This problem is important for a variety of real-world applications ranging from fraud prevention to network intrusion detection. This problem is more challenging than the conventional one-shot learning since there are dependencies between attributes and sequences. We design a deep learning framework OLAS to tackle this problem. The proposed OLAS utilizes a twin network to generalize the features from pairwise attributed sequence examples. Empirical results on real-world datasets demonstrate the proposed OLAS can outperform the state-of-the-art methods under a rich variety of parameter settings.",2018,2018 IEEE International Conference on Big Data (Big Data),,10.1109/BigData.2018.8622257,https://web.cs.wpi.edu/~xkong/publications/papers/bigdata18.pdf
fff4d6cfc9f1090b7bcdb756985b3df8860ddb5d,1,1,0,Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration,"Autonomous underwater robots working with teams of human divers may need to distinguish between different divers, e.g., to recognize a lead diver or to follow a specific team member. This paper describes a technique that enables autonomous underwater robots to track divers in real time as well as to reidentify them. The approach is an extension of Simple Online Realtime Tracking (SORT) with an appearance metric (deep SORT). Initial diver detection is performed with a custom CNN designed for realtime diver detection, and appearance features are subsequently extracted for each detected diver. Next, realtime tracking-by-detection is performed with an extension of the deep SORT algorithm. We evaluate this technique on a series of videos of divers performing human-robot collaborative tasks and show that our methods result in more divers being accurately identified during tracking. We also discuss the practical considerations of applying multi-person tracking to on-board autonomous robot operations, and we consider how failure cases can be addressed during on-board tracking.",2020,2020 IEEE International Conference on Robotics and Automation (ICRA),,10.1109/ICRA40945.2020.9197308,