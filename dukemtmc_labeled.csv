paperId,uses dataset or derivative,dataset(s) / model(s) used,unable to disambiguate,cites D1,cites D2,cites D3,title,abstract,year,venue,arxivId,doi,pdfUrl
00b9079c3320d93832a23441d3519463cf0e84e9,1,[D2],,1,0,0,Long/Short-Term Appearance Modeling and Two-Step Association for Multi-Object Tracking,"Vision-based multi-object tracking has many potential applications in intelligent transportation systems and intelligent vehicles. Tracking by detection, as a popular approach to multi-object tracking, first obtains detection responses from video sequence and then associates them into tracks for every object. Existing tracking-by-detection methods can work well in constrained scenarios. However, in those complicated scenarios with occlusion and adverse illumination conditions, the detection stage is deteriorated and thus makes it difficult to track objects accurately. In this paper, we present a robust tracker that represents object appearance using stable temporal features and associates the detection responses through a two-step association process. We propose to use Bi-LSTM (Bidirectional Long Short-Term Memory) to model object appearance and obtain reliable temporal features. Then, we estimate the affinity between tracks and detections based on multiple cues including appearance, motion and shape, and integrate the affinity into a two-step association procedure. Our method is verified on MOT datasets and the experimental results are promising as compared to the state-of-the-art.",2019,2019 IEEE Intelligent Transportation Systems Conference (ITSC),,10.1109/ITSC.2019.8916882,
01f3f0293c2a13bc122088e837b513a123f37c70,0,,,1,0,0,Multi-object Tracking with Neural Gating Using Bilinear LSTM,"In recent deep online and near-online multi-object tracking approaches, a difficulty has been to incorporate long-term appearance models to efficiently score object tracks under severe occlusion and multiple missing detections. In this paper, we propose a novel recurrent network model, the Bilinear LSTM, in order to improve the learning of long-term appearance models via a recurrent network. Based on intuitions drawn from recursive least squares, Bilinear LSTM stores building blocks of a linear predictor in its memory, which is then coupled with the input in a multiplicative manner, instead of the additive coupling in conventional LSTM approaches. Such coupling resembles an online learned classifier/regressor at each time step, which we have found to improve performances in using LSTM for appearance modeling. We also propose novel data augmentation approaches to efficiently train recurrent models that score object tracks on both appearance and motion. We train an LSTM that can score object tracks based on both appearance and motion and utilize it in a multiple hypothesis tracking framework. In experiments, we show that with our novel LSTM model, we achieved state-of-the-art performance on near-online multiple object tracking on the MOT 2016 and MOT 2017 benchmarks.",2018,ECCV,,10.1007/978-3-030-01237-3_13,http://web.engr.oregonstate.edu/~lif/1925.pdf
020dc284dd062ef6fed4659df539d79eac3043c2,0,,,0,0,1,Joint graph regularized dictionary learning and sparse ranking for multi-modal multi-shot person re-identification,"Abstract The promising achievement of sparse ranking in image-based recognition gives rise to a number of development on person re-identification (Re-ID) which aims to reconstruct the probe as a linear combination of few atoms/images from an over-complete dictionary/gallery. However, most of the existing sparse ranking based Re-ID methods lack considering the geometric relationships between probe, gallery, and cross-modal images of the same person in multi-shot Re-ID. In this paper, we propose a novel joint graph regularized dictionary learning and sparse ranking method for multi-modal multi-shot person Re-ID. First, we explore the probe-based geometrical structure by enforcing the smoothness between the codings/coefficients, which refers to the multi-shot images from the same person in probe. Second, we explore the gallery-based geometrical structure among gallery images, which encourages the multi-shot images from the same person in the gallery making similar contributions while reconstructing a certain probe image. Third, we explore the cross-modal geometrical structure by enforcing the smoothness between the cross-modal images and thus extend our model for the multi-modal case. Finally, we design an APG based optimization to solve the problem. Comprehensive experiments on benchmark datasets demonstrate the superior performance of the proposed model. The code is available at https://github.com/ttaalle/Lhc .",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107352,
041b96260d58b4604d6509e9e70d69e205062136,1,[D2],,1,0,0,Progressive Multi-stage Feature Mix for Person Re-Identification.,"Image features from a small local region often give strong evidence in person re-identification task. However, CNN suffers from paying too much attention on the most salient local areas, thus ignoring other discriminative clues, e.g., hair, shoes or logos on clothes. %BDB proposes to randomly drop one block in a batch to enlarge the high response areas. Although BDB has achieved remarkable results, there still room for improvement. In this work, we propose a Progressive Multi-stage feature Mix network (PMM), which enables the model to find out the more precise and diverse features in a progressive manner. Specifically, 1. to enforce the model to look for different clues in the image, we adopt a multi-stage classifier and expect that the model is able to focus on a complementary region in each stage. 2. we propose an Attentive feature Hard-Mix (A-Hard-Mix) to replace the salient feature blocks by the negative example in the current batch, whose label is different from the current sample. 3. extensive experiments have been carried out on reID datasets such as the Market-1501, DukeMTMC-reID and CUHK03, showing that the proposed method can boost the re-identification performance significantly.",2020,,2007.08779,,https://arxiv.org/pdf/2007.08779.pdf
056adbf0b00ff13dae94c0a5b02bb7cb180794c0,0,,,1,0,0,Learning Non-Uniform Hypergraph for Multi-Object Tracking,"The majority of Multi-Object Tracking (MOT) algorithms based on the tracking-by-detection scheme do not use higher order dependencies among objects or tracklets, which makes them less effective in handling complex scenarios. In this work, we present a new near-online MOT algorithm based on non-uniform hypergraph, which can model different degrees of dependencies among tracklets in a unified objective. The nodes in the hypergraph correspond to the tracklets and the hyperedges with different degrees encode various kinds of dependencies among them. Specifically, instead of setting the weights of hyperedges with different degrees empirically, they are learned automatically using the structural support vector machine algorithm (SSVM). Several experiments are carried out on various challenging datasets (i.e., PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark), to demonstrate that our method achieves favorable performance against the state-of-the-art MOT methods.",2019,AAAI,1812.03621,10.1609/aaai.v33i01.33018981,https://arxiv.org/pdf/1812.03621.pdf
057dfb33380f8cadea486bbc8c26b8331eac2e2b,1,,1,1,0,0,Learning a Neural Solver for Multiple Object Tracking,"Graphs offer a natural way to formulate Multiple Object Tracking (MOT) within the tracking-by-detection paradigm. However, they also introduce a major challenge for learning methods, as defining a model that can operate on such structured domain is not trivial. As a consequence, most learning-based work has been devoted to learning better features for MOT and then using these with well-established optimization frameworks. In this work, we exploit the classical network flow formulation of MOT to define a fully differentiable framework based on Message Passing Networks (MPNs). By operating directly on the graph domain, our method can reason globally over an entire set of detections and predict final solutions. Hence, we show that learning in MOT does not need to be restricted to feature extraction, but it can also be applied to the data association step. We show a significant improvement in both MOTA and IDF1 on three publicly available benchmarks. Our code is available at https://bit.ly/motsolv.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1912.07515,10.1109/cvpr42600.2020.00628,https://arxiv.org/pdf/1912.07515.pdf
05a1269f7d8b0e739edf9b84ca0cc3d1110f923e,0,,,1,0,0,SAFER-LC Report on risk evaluation system and use cases for pilot test,"No part of this document may be copied, reproduced, disclosed or distributed by any means whatsoever, including electronic without the express permission of the International Union of Railways (UIC), Coordinator of the EU SAFER-LC Project. The same applies for translation, adaptation or transformation, arrangement or reproduction by any method or procedure whatsoever. The document reflects only the author’s views and neither INEA nor the Commission is liable of any use that may be made of the information contained therein. The use of the content provided is at the sole risk of the user. Deliverable D3.4",2019,,,,https://pdfs.semanticscholar.org/05a1/269f7d8b0e739edf9b84ca0cc3d1110f923e.pdf
07f4ba45b771ed123b08261d88acda19406a7987,0,,,1,0,0,Real-Time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification,"Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on large-scale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.",2018,2018 IEEE International Conference on Multimedia and Expo (ICME),1809.04427,10.1109/ICME.2018.8486597,https://arxiv.org/pdf/1809.04427.pdf
08b28a8f2699501d46d87956cbaa37255000daa3,1,[D2],,1,1,0,MaskReID: A Mask Based Deep Ranking Neural Network for Person Re-identification,"Person retrieval faces many challenges including cluttered background, appearance variations (e.g., illumination, pose, occlusion) among different camera views and the similarity among different person's images. To address these issues, we put forward a novel mask based deep ranking neural network with a skipped fusing layer. Firstly, to alleviate the problem of cluttered background, masked images with only the foreground regions are incorporated as input in the proposed neural network. Secondly, to reduce the impact of the appearance variations, the multi-layer fusion scheme is developed to obtain more discriminative fine-grained information. Lastly, considering person retrieval is a special image retrieval task, we propose a novel ranking loss to optimize the whole network. The proposed ranking loss can further mitigate the interference problem of similar negative samples when producing ranking results. The extensive experiments validate the superiority of the proposed method compared with the state-of-the-art methods on many benchmark datasets.",2018,ArXiv,1804.03864,,https://arxiv.org/pdf/1804.03864.pdf
0bf90fd8239bbaed46df6c525160e999c7e6c5e7,0,,,1,0,0,Occlusion Handling in Tracking Multiple People Using RNN,"In tracking-by-detection of multiple targets in video sequences, ID-switch is an undesirable error due to long (short) occlusion among targets. In this paper, we propose an occlusion handling method based on Recurrent Neural Network (RNN) to remedy this issue. The method reconstructs missed detection boxes in order to preserve the ID number of targets after occlusion by predicting the detections in next frames. The prediction is accomplished by learning the motion of targets using a novel RNN. Applying this technique on tracking results of several state-of-the-arts shows that their ID-switch error is reduced.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451140,
0fbdaad9fe987dfecb187597675276073bc679c0,1,[D2],,1,1,0,Bag of Negatives for Siamese Architectures,"Training a Siamese architecture for re-identification with a large number of identities is a challenging task due to the difficulty of finding relevant negative samples efficiently. In this work we present Bag of Negatives (BoN), a method for accelerated and improved training of Siamese networks that scales well on datasets with a very large number of identities. BoN is an efficient and loss-independent method, able to select a bag of high quality negatives, based on a novel online hashing strategy.",2019,BMVC,1908.02391,,https://arxiv.org/pdf/1908.02391.pdf
11cb49d8f19f0491e1930d9471988a3c07b70bb4,1,[D2],,1,1,0,Person Re-Identification With Triplet Focal Loss,"Person re-identification (ReID), which aims at matching individuals across non-overlapping cameras, has attracted much attention in the field of computer vision due to its research significance and potential applications. Triplet loss-based CNN models have been very successful for person ReID, which aims to optimize the feature embedding space such that the distances between samples with the same identity are much shorter than those of samples with different identities. Researchers have found that hard triplets’ mining is crucial for the success of the triplet loss. In this paper, motivated by focal loss designed for the classification model, we propose the triplet focal loss for person ReID. Triplet focal loss can up-weight the hard triplets’ training samples and relatively down-weight the easy triplets adaptively via simply projecting the original distance in the Euclidean space to an exponential kernel space. We conduct experiments on three largest benchmark datasets currently available for person ReID, namely, Market-1501, DukeMTMC-ReID, and CUHK03, and the experimental results verify that the proposed triplet focal loss can greatly outperform the traditional triplet loss and achieve competitive performances with the representative state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2884743,
12d62f1360587fdecee728e6c509acc378f38dc9,1,[D2],,1,1,0,Feature Affinity-Based Pseudo Labeling for Semi-Supervised Person Re-Identification,"Vision-based person re-identification aims to match a person's identity across multiple images, which is a fundamental task in multimedia content analysis and retrieval. Deep neural networks have recently manifested great potential in this task. However, a major bottleneck of existing supervised deep networks is their reliance on a large amount of annotated training data. Manual labeling for person identities in large-scale surveillance camera systems is quite challenging and incurs significant costs. Some recent studies adopt generative model outputs as training data augmentation. To more effectively use these synthetic data for an improved feature learning and re-identification performance, this paper proposes a novel feature affinity-based pseudo labeling method with two possible label encodings. To the best of our knowledge, this is the first study that employs pseudo-labeling by measuring the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. We propose training the network with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. We show that both label encodings can be learned in a unified manner and help improve the overall performance. Our extensive experiments on three person re-identification datasets: Market-1501, DukeMTMC-reID, and CUHK03, demonstrate significant performance boost over the state-of-the-art person re-identification approaches.",2019,IEEE Transactions on Multimedia,1805.06118,10.1109/TMM.2019.2916456,https://arxiv.org/pdf/1805.06118.pdf
13b0348342a5c2e7c87b5eb5156c0bcf45c3c951,1,[D3],,1,0,1,Co-Segmentation Inspired Attention Networks for Video-Based Person Re-Identification,"Person re-identification (Re-ID) is an important real-world surveillance problem that entails associating a person’s identity over a network of cameras. Video-based Re-ID approaches have gained significant attention recently since a video, and not just an image, is often available. In this work, we propose a novel Co-segmentation inspired video Re-ID deep architecture and formulate a Co-segmentation based Attention Module (COSAM) that activates a common set of salient features across multiple frames of a video via mutual consensus in an unsupervised manner. As opposed to most of the prior work, our approach is able to attend to person accessories along with the person. Our plug-and-play and interpretable COSAM module applied on two deep architectures (ResNet50, SE-ResNet50) outperform the state-of-the-art methods on three benchmark datasets.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00065,http://innovarul.github.io/docs/iccv2019_vidreid_cosegmentation.pdf
141133811391cf0f4ba324a056f09f72ef362ad6,0,,,0,1,0,Weakly Supervised Segmentation Framework with Uncertainty: A Study on Pneumothorax Segmentation in Chest X-ray,"Pneumothorax is a critical abnormality that shall be treated with higher priority, and hence a computerized triage scheme is needed. A deep-learning-based framework to automatically segment the pneumothorax in chest X-rays is developed to support the realization of a triage system. Since a large number of pixel-level annotations is commonly needed but difficult to obtain for deep learning model, we propose a weakly supervised framework that allows partial training data to be weakly annotated with only image-level labels. We employ the attention masks derived from an image-level classification model as the pixel-level masks for those weakly-annotated data. Because the attention masks are rough and may have errors, we further develop a spatial label smoothing regularization technique to explore the uncertainty for the incorrectness of the attention masks in the training of segmentation model. Experimental results show that the proposed weakly supervised segmentation algorithm relieves the need of well-annotated data and yield satisfactory performance on the pneumothorax segmentation.",2019,MICCAI,,10.1007/978-3-030-32226-7_68,
14ea09da023d015263e239c1876f547adcd2974c,1,[D2],,1,1,0,Centralized embedding hypersphere feature learning for person re-identification,"ABSTRACT Deep metric learning has become a general method for person re-identification (ReID) recently. Existing methods train ReID model with various loss functions to learn feature representation and identify pedestrian. However, the interaction between person features and classification vectors in the training process is rarely concerned. Distribution of pedestrian features will greatly affect convergence of the model and the pedestrian similarity computing in the test phase. In this paper, we formulate improved softmax function to learn pedestrian features and classification vectors. Our method applies pedestrian feature representation to be scattered across the coordinate space and embedding hypersphere to solve the classification problem. Then, we propose an end-to-end convolutional neural network (CNN) framework with improved softmax function to improve the performance of pedestrian features. Finally, experiments are performed on four challenging datasets. The results demonstrate that our work is competitive compared to the state-of-the-art.",2019,,,10.1080/13682199.2019.1647947,
156382f40f9396a7a3d1fa444fa0769fc467f958,1,[D2],,1,1,0,Scalable Person Re-Identification by Harmonious Attention,"Existing person re-identification (re-id) deep learning methods rely heavily on the utilisation of large and computationally expensive convolutional neural networks. They are therefore not scalable to large scale re-id deployment scenarios with the need of processing a large amount of surveillance video data, due to the lengthy inference process with high computing costs. In this work, we address this limitation via jointly learning re-id attention selection. Specifically, we formulate a novel harmonious attention network (HAN) framework to jointly learn soft pixel attention and hard region attention alongside simultaneous deep feature representation learning, particularly enabling more discriminative re-id matching by efficient networks with more scalable model inference and feature matching. Extensive evaluations validate the cost-effectiveness superiority of the proposed HAN approach for person re-id against a wide variety of state-of-the-art methods on four large benchmark datasets: CUHK03, Market-1501, DukeMTMC, and MSMT17.",2019,International Journal of Computer Vision,,10.1007/s11263-019-01274-1,https://link.springer.com/content/pdf/10.1007/s11263-019-01274-1.pdf
16681e571a927c30c607b781a608796cc1f15ca9,1,[D2],,0,1,0,Adversarially Erased Learning for Person Re-identification by Fully Convolutional Networks,"The generalization ability of deep person re-identification networks is subject to inadequate person data and occlusions. To relieve this dilemma, we propose a feature-level augmentation strategy, Adversarially Erased Learning Module (AELM), using two adversarial classifiers. Specifically, we utilize a classifier to identify discriminative regions and erase them to increase the variant of features. Meanwhile, we input the erased feature maps to another classifier to discover new body regions, which effectively resist occlusion of key parts. To easily perform end-to-end training for AELM, we propose a novel Identity model based on Fully Convolutional Networks (IFCN) to directly obtain body response heatmap during the forward pass by selecting corresponding class-specific feature map. Thus, the discriminative regions can be identified and erased in a convenient way. Moreover, to capture discriminative region for AELM, we present a Complementary Attention Module (CoAM) combined with channel and spatial attention to automatically focus on which feature types and positions are meaningful in the feature maps. In this paper, CoAM and AELM are cascaded into one module which is applied to the outputs of different convolutional layers to integrate mid- and high-level semantic features. Experimental results on three challenging benchmarks demonstrate the effectiveness of the proposed method.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852283,
1866c0b9ea12ba23797dc3afd6f2d01f7dc997ac,1,[D2],,0,1,0,Learning Bias-Free Representation for Large-Scale Person Re-Identification,"Person re-identification (re-ID) aims to match the same person across disjointed cameras. In practice, misalignment is one of the key problems that limits the re-ID accuracy. There are many causes, such as detection errors from automatic detectors, human pose changes and relative movement between people and cameras. However, most current automatic re-ID solutions are still limited in mitigating such misalignments. In this paper, we propose that the misalignment scenarios in re-ID can be divided into three basic types. To further study their effects, we visualize their negative impacts in the learned embedding space and compare the changes in the re-ID accuracy. To address these specific misalignment problems, we design three subnetworks to correct the corresponding misalignments and discuss the performance gains. Moreover, by integrating all subnetworks into one unified structure, we propose a bias-free representation learning method. Unlike previous approaches that mainly focus on one specific kind of misalignment, our proposed method can eliminate the feature bias that is introduced by multiple misalignment problems. Systematic visualizations and comparisons are conducted to demonstrate that our method can efficiently correct the feature bias during the representation learning step. Evaluations on three large-scale datasets show that our proposed bias-free representation learning method can outperform the state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2937509,
1909d5129fdcb2cad5c65e405889bf346419cd06,1,[D2],,1,1,0,Generated Data With Sparse Regularized Multi-Pseudo Label for Person Re-Identification,"Recently, Generative Adversarial Network (GAN) has been adopted to improve person re-identification (person re-ID) performance through data augmentation. However, directly leveraging generated data to train a re-ID model may easily lead to over-fitting issue on these extra data and decrease the generalisability of model to learn 1 ID-related features from real data. Inspired by the previous approach which assigns multi-pseudo labels on the generated data to reduce the risk of over-fitting, we propose to take sparse regularization into consideration. We attempt to further improve the performance of current re-ID models by using the unlabeled generated data. The proposed Sparse Regularized Multi-Pseudo Label (SRMpL) can effectively prevent the over-fitting issue when some larger weights are assigned to the generated data. Our experiments are carried out on two publicly available person re-ID datasets (e.g., Market-1501 and DukeMTMC-reID). Compared with existing unlabeled generated data re-ID solutions, our approach achieves competitive performance. Two classical re-ID models are used to verify our sparse regularization label on generated data, i.e., an ID-embedding network and a two-stream network.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.2972768,
19ccdbe94c4da8f5b289311a8915085eec921f3a,1,[D2],,1,0,0,AlignedReID++: Dynamically matching local information for person re-identification,"Abstract Person re-identification (ReID) is a challenging problem, where global features of person images are not enough to solve unaligned image pairs. Many previous works used human pose information to acquire aligned local features to boost the performance. However, those methods need extra labeled data to train an available human pose estimation model. In this paper, we propose a novel method named Dynamically Matching Local Information (DMLI) that could dynamically align local information without requiring extra supervision. DMLI could achieve better performance, especially when encountering the human pose misalignment caused by inaccurate person detection boxes. Then, we propose a deep model name AlignedReID++ which is jointly learned with global features and local feature based on DMLI. AlignedReID++ improves the performance of global features, and could use DMLI to further increase accuracy in the inference phase. Experiments show effectiveness of our proposed method in comparison with several state-of-the-art person ReID approaches. Additionally, it achieves rank-1 accuracy of 92.8% on Market1501 and 86.2% on DukeMTMCReID with ResNet50. The code and models have been released 2 .",2019,Pattern Recognit.,,10.1016/J.PATCOG.2019.05.028,
19eabf03074557ca266fa818e99ac88bd58178e5,1,[D2],,0,1,0,DeepPFCN: Deep Parallel Feature Consensus Network For Person Re-Identification,"Person re-identification aims to associate images of the same person over multiple non-overlapping camera views at different times. Depending on the human operator, manual re-identification in large camera networks is highly time consuming and erroneous. Automated person re-identification is required due to the extensive quantity of visual data produced by rapid inflation of large scale distributed multi-camera systems. The state-of-the-art works focus on learning and factorize person appearance features into latent discriminative factors at multiple semantic levels. We propose Deep Parallel Feature Consensus Network (DeepPFCN), a novel network architecture that learns multi-scale person appearance features using convolutional neural networks. This model factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels. Finally consensus is built. The feature representations learned by DeepPFCN are more robust for the person re-identification task, as we learn discriminative scale-specific features and maximize multi-scale feature fusion selections in multi-scale image inputs. We further exploit average and max pooling in separate scale for person-specific task to discriminate features globally and locally. We demonstrate the re-identification advantages of the proposed DeepPFCN model over the state-of-the-art re-identification methods on three benchmark datasets: Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%, 64.3%, and 52.6% respectively on these benchmark datasets.",2019,ArXiv,1911.07776,10.1007/978-981-15-8697-2_37,https://arxiv.org/pdf/1911.07776.pdf
1aaf1c043e734321ddbdceb0392fd89c062fb33a,1,[D2],,0,1,0,Pose-Invariant Embedding for Deep Person Re-Identification,"Pedestrian misalignment, which mainly arises from detector errors and pose variations, is a critical problem for a robust person re-identification (re-ID) system. With poor alignment, the feature learning and matching process might be largely compromised. To address this problem, this paper introduces pose-invariant embedding (PIE) as a pedestrian descriptor. First, in order to align pedestrians to a standard pose, the PoseBox structure is introduced, which is generated through pose estimation followed by affine transformations. Second, to reduce the impact of pose estimation errors and information loss during the PoseBox construction, we design a PoseBox fusion (PBF) CNN architecture that takes the original image, the PoseBox, and the pose estimation confidence as input. The proposed PIE descriptor is thus defined as the fully connected layer of the PBF network for the retrieval task. Experiments are conducted on the Market-1501, CUHK03-NP, and DukeMTMC-reID datasets. We show that PoseBox alone yields decent re-ID accuracy and that when integrated in the PBF network, the learned PIE descriptor produces competitive performance compared with state-of-the-art approaches.",2019,IEEE Transactions on Image Processing,1701.07732,10.1109/TIP.2019.2910414,https://arxiv.org/pdf/1701.07732.pdf
1bc9e07a049f2ef6f0ee1ab093e3cf1e90f5b05f,1,[D1],,1,0,0,An online learned hough forest model based on improved multi-feature fusion matching for multi-object tracking,"Object tracking has been one of the most important and active research areas in the field of computer vision. In order to solve low accuracy in object occlusion and deformation for multi-object tracking, an online learned Hough forest model based on improved multi-feature fusion matching for multi-object tracking is proposed in this paper. Firstly, positive and negative samples are selected online according to low-level association among detection responses and construct the feature model of the object with color histogram, histogram of oriented gradient (HOG) and optical flow information. Secondly, longer trajectory associations are generated based on the online learned Hough forest framework. Finally, a trajectory matching algorithm based on multi-feature fusion is proposed, and we introduce two methods of similarity measure in color histogram and feature points matching based on the Gabor filter to generate the probability matrix with the weighted factor. Therefore, it can further form the complete trajectories of the objects by associating them gradually. We evaluate our approach on three public data sets, and show significant improvements compared with state-of-art methods.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-6519-y,
1e3043eac630533f1195e56ef88002f3433409ca,1,[D2],,0,1,0,Backbone Can Not be Trained at Once: Rolling Back to Pre-trained Network for Person Re-Identification,"In person re-identification (ReID) task, because of its shortage of trainable dataset, it is common to utilize fine-tuning method using a classification network pre-trained on a large dataset. However, it is relatively difficult to sufficiently fine-tune the low-level layers of the network due to the gradient vanishing problem. In this work, we propose a novel fine-tuning strategy that allows low-level layers to be sufficiently trained by rolling back the weights of high-level layers to their initial pre-trained weights. Our strategy alleviates the problem of gradient vanishing in low-level layers and robustly trains the low-level layers to fit the ReID dataset, thereby increasing the performance of ReID tasks. The improved performance of the proposed strategy is validated via several experiments. Furthermore, without any add-ons such as pose estimation or segmentation, our strategy exhibits state-of-the-art performance using only vanilla deep convolutional neural network architecture.",2019,AAAI,1901.0614,10.1609/aaai.v33i01.33018859,https://arxiv.org/pdf/1901.06140.pdf
1e7b5039af9d4bf3d7bd70cbd9abbe6430dba8e1,0,,,1,0,0,Object Tracking for Autonomous Driving Systems,Object Tracking for Autonomous Driving Systems,2020,,,,https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-81.pdf
1ea8591721da4c3a3f8b7f1a95c1acd5066aad3e,1,[D2],,0,1,0,Improving Person Re-Identification by Combining Siamese Convolutional Neural Network and Re-Ranking Process,"Person re-identification (re-ID) is an active task with several challenges such as variations of poses, view points, lighting and occlusion. When considering person re-ID as an image retrieval process, measuring the appearance similarity of a pairwise person images is the essential phase. Re-ranking process can improve its accuracy especially when it is based on an other similarity metric. In this paper, we propose a pipeline composed of two methods: A Siamese Convolutional Neural Network (S-CNN) and a k-reciprocal nearest neighbors (k-RNN) re-ranking algorithm. While most existing re-ranking methods ignore the importance of original distance in re-ranking, we jointly combine the S-CNN similarity measure and Jaccard distance to revise the initial ranked list. An experimental study is conducted on two benchmark person re-ID datasets (Market-1501 and Duke-MTMC-reID). The obtained results confirm the effectiveness of our method. A mAP improvement of 11.6% and 15.68% is obtained respectively for the two testing datasets.",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909902,
1eb45acc4f27fc47ed13249273cf5cb86476b154,1,[D3],,1,0,1,A two-stream network with joint spatial-temporal distance for video-based person re-identification,,2020,J. Intell. Fuzzy Syst.,,10.3233/jifs-192067,
205c2889a998f8f1b42c45a2b3a79d78873a4ba7,1,[D2],,1,1,1,Multiple Expert Brainstorming for Domain Adaptive Person Re-identification,"Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEB-Net adopts a mutual learning strategy, where multiple networks with different architectures are pre-trained within a source domain as expert models equipped with specific features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with different architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme about authority of experts. Extensive experiments on large-scale datasets (Market-1501 and DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the state-of-the-arts.",2020,ECCV,2007.01546,10.1007/978-3-030-58571-6_35,https://arxiv.org/pdf/2007.01546.pdf
2077108111f40649c4661be229d892467b45bdef,0,,,0,1,0,A Shape Transformation-based Dataset Augmentation Framework for Pedestrian Detection,"Deep learning-based computer vision is usually data-hungry. Many researchers attempt to augment datasets with synthesized data to improve model robustness. However, the augmentation of popular pedestrian datasets, such as Caltech and Citypersons, can be extremely challenging because real pedestrians are commonly in low quality. Due to the factors like occlusions, blurs, and low-resolution, it is significantly difficult for existing augmentation approaches, which generally synthesize data using 3D engines or generative adversarial networks (GANs), to generate realistic-looking pedestrians. Alternatively, to access much more natural-looking pedestrians, we propose to augment pedestrian detection datasets by transforming real pedestrians from the same dataset into different shapes. Accordingly, we propose the Shape Transformation-based Dataset Augmentation (STDA) framework. The proposed framework is composed of two subsequent modules, i.e. the shape-guided deformation and the environment adaptation. In the first module, we introduce a shape-guided warping field to help deform the shape of a real pedestrian into a different shape. Then, in the second stage, we propose an environment-aware blending map to better adapt the deformed pedestrians into surrounding environments, obtaining more realistic-looking pedestrians and more beneficial augmentation results for pedestrian detection. Extensive empirical studies on different pedestrian detection benchmarks show that the proposed STDA framework consistently produces much better augmentation results than other pedestrian synthesis approaches using low-quality pedestrians. By augmenting the original datasets, our proposed framework also improves the baseline pedestrian detector by up to 38% on the evaluated benchmarks, achieving state-of-the-art performance.",2019,ArXiv,1912.0701,,https://arxiv.org/pdf/1912.07010.pdf
213c7d10e143e21d3e2df90f88eeef10778ca82d,1,[D2],,1,0,0,Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives,"While the depth of modern Convolutional Neural Networks (CNNs) surpasses that of the pioneering networks with a significant margin, the traditional way of appending supervision only over the final classifier and progressively propagating gradient flow upstream remains the training mainstay. Seminal Deeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of optimization arising from gradient flow through a long chain. However, it is still vulnerable to issues including interference to the hierarchical representation generation process and inconsistent optimization objectives, as illustrated theoretically and empirically in this paper. Complementary to previous training strategies, we propose Dynamic Hierarchical Mimicking, a generic feature learning mechanism, to advance CNN training with enhanced generalization ability. Partially inspired by DSN, we fork delicately designed side branches from the intermediate layers of a given neural network. Each branch can emerge from certain locations of the main branch dynamically, which not only retains representation rooted in the backbone network but also generates more diverse representations along its own pathway. We go one step further to promote multi-level interactions among different branches through an optimization formula with probabilistic prediction matching losses, thus guaranteeing a more robust optimization process and better representation ability. Experiments on both category and instance recognition tasks demonstrate the substantial improvements of our proposed method over its corresponding counterparts using diverse state-of-the-art CNN architectures. Code and models are publicly available at https://github.com/d-li14/DHM.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.10739,10.1109/cvpr42600.2020.00766,https://arxiv.org/pdf/2003.10739.pdf
2201de5eb360d7511b7c8f8dbee8702b1faa1bf4,0,,,0,1,0,Weakly Supervised and On-line Machine Learning for Object Tracking and Recognition in Images and Videos,"This manuscript summarises the work that I have been involved in for my post-doctoral research and in the context of my PhD supervision activities during the past 11 years. I have conducted this work partly as a post-doctoral researcher at the Idiap Research Institute in Switzerland, and partly as an associate professor at the LIRIS laboratory and INSA Lyon in France.  The technical section of the manuscript comprises two main parts: the first part being on on-line learning approaches for visual object tracking in dynamic environments, and the second part on similarity metric learning algorithms and Siamese Neural Networks (SNN).  I first present our work on on-line multiple face tracking in a dynamic indoor environment, where we focused on the aspects of track creation and removal for long-term tracking. The automatic detection of the faces to track is challenging in this setting because they may not be detected for long periods of time, and 0 detections may occur frequently. Our proposed algorithm consisted in a recursive Bayesian framework with a separate track creation and removal step based on Hidden Markov Models including observation likelihood functions that are learnt off-line on a set of static and dynamic features related to the tracking behaviour and the objects’ appearance. This approach is very efficient and showed superior performance to the state of the art in on-line multiple object tracking. In the same context, we further developed a new on-line algorithm to estimate the Visual Focus of Attention from videos of persons sitting in a room. This unsupervised on-line learning approach is based on an incremental k-means algorithm and is able to automatically extract, from a video stream, the targets that the persons are looking at in a room.  I further present our research on on-line learnt robust appearance models for single-object tracking. In particular we focused on the problem of model-free, on-line tracking of arbitrary objects, where the state and model of the object to track is initialised in the first frame and updated throughout the rest of the video. Our first approach, called PixelTrack, consists in a combined detection and segmentation framework that robustly learns the appearance of the object to track and avoids drift by an effective on-line co-training algorithm. This method showed excellent tracking performance on public benchmarks, both in terms of robustness and speed, and is particularly suitable for tracking deformable objects. The second tracking approach, called MCT, employs an on-line learnt discriminative classifier that stochastically samples the training instances from a dynamic probability density function that is computed from moving  and possibly distracting image background regions. The use of this motion context showed to be very effective and lead to a significant gain in the overall tracking robustness and performance. We extended this idea by designing a set of features that concisely describe the visual context of the overall scene shown in a video at a given point in time. Then, we applied several complementary tracking algorithms on a set of training videos and computed the corresponding context features for each frame. Finally, we trained a discriminative classifier off-line that estimates the  most suitable tracker for a given context, and applied it on-line in an effective tracker-selection framework. Evaluated on several different “pools” of individual trackers, the combined model lead to an increased performance in terms of accuracy and robustness on challenging public benchmarks.  In the second part of the manuscript, I present several contributions related to SNNs for similarity metric learning. First, we proposed a new objective function and training algorithm called Triangular Similarity Metric Learning that enhances the convergence behaviour and achieved state-of-the-art results on pairwise verification tasks, like face, speaker or kinship verification. Then, I present our work on SNNs for gesture classification from inertial sensor data, where we proposed a new class-balanced learning strategy operating on tuples of training samples and an objective function based on a polar sine formulation. Finally, I present several contributions on SNN with deeper and more complex Convolutional Neural Network models applied to the problem of person re-identification in images. In this context, we proposed different neural architectures and triplet learning methods that include semantic prior knowledge, e.g. on pedestrian attributes, body orientation and surrounding group context, using a combination of  supervised and weakly supervised algorithms. Also, a new learning-to-rank algorithm for SNN, called Rank-Triplet, has been introduced and successfully applied to person re-identification. These recent works achieved state-of-the-art re-identification results on challenging pedestrian image datasets and opened new perspectives for future similarity metric approaches.",2019,,,,https://pdfs.semanticscholar.org/7a79/697510e9a49a5ee63bffd1184fe92b2f4e70.pdf
22895b58f1256cbad680056f785f4eda03d32bc9,1,"[D2], [D4]",,1,1,0,Improving person re-identification by multi-task learning,"We propose a novel Multi-Task Learning Network (MTNET) with four different subtasks for person re-identification mission. At the same time, the attribute recognition mission can be implemented by the same network. We achieve multi-mission by integrating four subtasks, such as identity identification, identity verification, attribute identification, attribute verification. Identity loss and attribute loss can provide complementary information on a different perspective by integrating multi-context information. Identity focuses on the overall contour and appearance, while attribute focuses on local aspects and dresses of one person. Identification loss and verification loss are used to optimize the distance of samples. Identification loss used to construct a robust category space, while verification loss used to optimize the space by minimizing the distance between similar images, and maximizing the distance between dissimilar images. Moreover, an effective verification loss named constraint contrast verification (CCV) is proposed to restrict the distance between feature pair to a foreseeable range that ensures the network has better convergence. The MTNet is an end-to-end deep learning framework, all the parameters and losses can be jointly optimized. We evaluate our approach with the state-of-the-art methods on two famous dataset Market1501 and DukeMTMC-reID. Experiments demonstrate that our MTNet achieves the very competitive results.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.01.027,
231a12de5dedddf1184ae9caafbc4a954ce584c3,0,,,1,0,0,Closed and Open World Multi-shot Person Re-identification. (Ré-identification de personnes à partir de multiples images dans le cadre de bases d'identités fermées et ouvertes),"More than ever, in today’s context of insecurity and terrorism, person re-identification based on video surveillance images has become a hot research topic. Indeed, tracking an individual not only requires to track him within a camera, but also to re-identify him when he re-appears in other cameras. In recent years, remarkable progress has been achieved in person re-identification, notably thanks to the availability of larger datasets composed of thousands of identities captured by several cameras where each camera captures multiple images per identity. Yet, we are still far from being able to automatically re-identify people accurately in real life. Considering the evolution of the available research data and the real applications needs, this thesis has followed one major research axis. How can we tackle the challenging question of open world re-identification in which the person we want to re-identify might not appear in the database of known identities? A secondary research axis consisted in relevantly making use of the multiple images that are available for each identity. The open world re-identification task we consider in this thesis consists in two subtasks: a detection task and a re-identification task. We are given a set of known identities, the gallery identities, but since we are in an open world situation, this set of known identities is supposed not to be overcomplete. Therefore, when presented a query person also referred to as probe person, the detection task aims at determining whether or not the query person is a probable known gallery person. Since the probe person might look similar to several gallery identities, the goal of the re-identification task is to the gallery identities from the most probable match to the least likely one. Our first contribution, COPReV for Closed and Open world Person RE-identification and Verification, is mainly designed for tackling the decision aspect of the problem. We formulate the re-identification task solely as a verification task and aim at determining whether two sets of images represent the same person or two distinct people. With this information, we can find out whether the query person has been identified previously or not and if so, who he is. This is achieved by learning a linear transformation of the features so that the distance between features of the same person are below a threshold and that of distinct people are above that same threshold. The",2017,,,,https://pdfs.semanticscholar.org/d579/8305ee7eee5940478d8e5bcbccd28dc119c3.pdf
242e19b95f1cf77a74f1a99a899a8dc7d34f7c10,0,,,0,0,1,Multi-scale 3D Convolution Network for Video Based Person Re-Identification,"This paper proposes a two-stream convolution network to extract spatial and temporal cues for video based person Re-Identification (ReID). A temporal stream in this network is constructed by inserting several Multi-scale 3D (M3D) convolution layers into a 2D CNN network. The resulting M3D convolution network introduces a fraction of parameters into the 2D CNN, but gains the ability of multi-scale temporal feature learning. With this compact architecture, M3D convolution network is also more efficient and easier to optimize than existing 3D convolution networks. The temporal stream further involves Residual Attention Layers (RAL) to refine the temporal features. By jointly learning spatial-temporal attention masks in a residual manner, RAL identifies the discriminative spatial regions and temporal cues. The other stream in our network is implemented with a 2D CNN for spatial feature extraction. The spatial and temporal features from two streams are finally fused for the video based person ReID. Evaluations on three widely used benchmarks datasets, i.e., MARS, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over existing 3D convolution networks and state-of-art methods.",2019,AAAI,1811.07468,10.1609/aaai.v33i01.33018618,https://arxiv.org/pdf/1811.07468.pdf
2471f190cdce98bc3bf506a3d62cce1c9bbcb09d,1,[D2],,0,1,0,Person ReID: Optimization of Domain Adaption Though Clothing Style Transfer Between Datasets,"It is manifested that when training and testing models on different datasets, the performance of trained models will severely dropped due to the differences in style of the datasets. In person ReID task, the clothing style is a crucial factor existing in different datasets, which has not been considered in the current research. We proposed a novel approach of Optimization of Domain Adaption Though Clothing Style Transfer (ODA-CST), which includes clothing mask extraction and clothing style transfer. Firstly, we generate the clothing mask by jointly locally extracting clothing and globally detecting the person. Meanwhile, we also organize a clothing mask dataset to improve the model. Our ODA-CST can effectively generate photos with the clothing style transferred, which is the first method that tries to solve the clothing style gap in person ReID task to the best knowledge. The importance of clothing style transfer and the effectiveness of our method are verified by the experiment.",2019,PRCV,,10.1007/978-3-030-31726-3_43,
255485196a869c98aacce60a86074fccf07c01eb,0,,,0,0,1,A Survey of Deep Learning Solutions for Multimedia Visual Content Analysis,"The increasing use of social media networks on handheld devices, especially smartphones with powerful built-in cameras, and the widespread availability of fast and high bandwidth broadband connections, added to the popularity of cloud storage, is enabling the generation and distribution of massive volumes of digital media, including images and videos. Such media is full of visual information and holds immense value in today’s world. The volume of data involved calls for automated visual content analysis systems able to meet the demands of practice in terms of efficiency and effectiveness. Deep learning (DL) has recently emerged as a prominent technique for visual content analysis. It is data-driven in nature and provides automatic end-to-end learning solutions without the need to rely explicitly on predefined handcrafted feature extractors. Another appealing characteristic of DL solutions is the performance they can achieve, once the network is trained, under practical constraints. This paper identifies eight problem domains which require analysis of visual artifacts in multimedia. It surveys the recent, authoritative, and the best performing DL solutions and lists the datasets used in the development of these deep methods for the identified types of visual analysis problems. This paper also discusses the challenges that the DL solutions face which can compromise their reliability, robustness, and accuracy for visual content analysis.",2019,IEEE Access,,10.1109/ACCESS.2019.2924733,
25fc32f3b6b4d5f94be61427625444486a709ee2,1,[D2],,0,1,0,Random linear interpolation data augmentation for person re-identification,"Person Re-Identification (person re-ID) is an image retrieval task which identifies the same person in different camera views. Generally, a good person re-ID model requires a large dataset containing over 100000 images to reduce the risk of over-fitting. Most current handcrafted person re-ID datasets, however, are insufficient for training a learning model with high generalization ability. In addition, the lacking of images with various levels of occlusion is still remaining in most existing datasets. Motivated by these two problems, this paper proposes a new data augmentation method called Random Linear Interpolation that can enlarge the sizes of person re-ID datasets and improve the generalization ability of the learning model. The key enabler of our approach is generating fused images by interpolating pairs of original images. In other words, the innovation of the proposed approach is considering data augmentation between two random samples. Plenty of experimental results demonstrates that the proposed method is effective to improve baseline models. On Market1501 and DukeMTMC-reID datasets, our approach can achieve 92.71 % and 82.19 % rank-1 accuracy, respectively.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-7071-5,
261641979088d2aab86fafe867a1af5be52592dc,1,[D2],,0,1,0,Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification,"Most of the existing person re-identification (re-ID) methods achieve promising accuracy in a supervised manner, but they assume the identity labels of the target domain is available. This greatly limits the scalability of person re-ID in real-world scenarios. Therefore, the current person re-ID community focuses on the cross-domain person re-ID that aims to transfer the knowledge from a labeled source domain to an unlabeled target domain and exploits the specific knowledge from the data distribution of the target domain to further improve the performance. To reduce the gap between the source and target domains, we propose a Smoothing Adversarial Domain Attack (SADA) approach that guides the source domain images to align the target domain images by using a trained camera classifier. To stabilize a memory trace of cross-domain knowledge transfer after its initial acquisition from the source domain, we propose a p-Memory Reconsolidation (pMR) method that reconsolidates the source knowledge with a small probability p during the self-training of the target domain. With both SADA and pMR, the proposed method significantly improves the cross-domain person re-ID. Extensive experiments on Market-1501 and DukeMTMC-reID benchmarks show that our pMR-SADA outperforms all of the state-of-the-arts by a large margin.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.01058,http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf
2878d5398a4d45f7c48d97762c2d9df643ebbf68,1,[D3],,1,0,1,Flow-Guided Attention Networks for Video-Based Person Re-Identification,"Person Re-Identification (ReID) is an important problem in many video analytics and surveillance applications,where a person's identity must be associated across a distributed network of cameras. Video-based person ReID has recently gained much interest because it can capture discriminant spatio-temporal information that is unavailable for image-based ReID. Despite recent advances, deep learning models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, two different flow-guided attention networks are proposed for fusion with any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information.Our first proposed network called Gated Attention relies on optical flow to generate gated attention with video-based feature that embed spatially. Hence the proposed framework allows to activate a common set of salient features across multiple frames. In contrast, our second network called Mutual Attention relies on the joint attention between image and optical flow features. This enables spatial attention between both sources of features, across motion and appearance cues. Both methods introduce a feature aggregation method that produce video features by identifying salient spatio-temporal information.Extensive experiments on two challenging video datasets indicate that using the proposed flow-guided spatio-temporal attention networks allows to improve recognition accuracy considerably, outperforming state-of-the-art methods for video-based person ReID. Additionally, our Mutual Attention network is able to process longer frame sequences with a wider range of appearance variations for highly accurate recognition.",2020,ArXiv,2008.03788,,https://arxiv.org/pdf/2008.03788.pdf
29cfc448bf717a760da32128d157a2a510b52ed8,1,[D2],,0,1,0,Cross-level reinforced attention network for person re-identification,"Abstract Attention mechanism is a simple and effective method to enhance discriminative performance of person re-identification (Re-ID). Most of previous attention-based works have difficulty in eliminating the negative effects of meaningless information. In this paper, a universal module, named Cross-level Reinforced Attention (CLRA), is proposed to alleviate this issue. Firstly, we fuse features of different semantic levels using adaptive weights. The fused features, containing richer spatial and semantic information, can better guide the generation of subsequent attention module. Then, we combine hard and soft attention to improve the ability to extract important information in spatial and channel domains. Through the CLRA, the network can aggregate and propagate more discriminative semantic information. Finally, we integrate the CLRA with Harmonious Attention CNN (HA-CNN) and form a novel Cross-level Reinforced Attention CNN (CLRA-CNN) to optimize person Re-ID. Experiment results on several public benchmarks show that the proposed method achieves state-of-the-art performance.",2020,J. Vis. Commun. Image Represent.,,10.1016/j.jvcir.2020.102775,
2c56829c368a4aaa34cc5d24eabac5977057ad98,0,,,0,1,0,Video-based Person Re-identification Method Based on GAN and Pose Estimation,"As the government keeps attaching importance to public security, non-overlapping viewsheds surveillance systems have been deployed widely. It has become especially important to recognize pedestrian target through matching cameras with different viewsheds in nowadays. Deep learning relies on big data to solve overfitting. However, the current video-based person re-identification only has small data volume and homogeneous learning features. To solve this, we put forward a method to improve person re-identification based on the video. This method can increase the sample quantity by generating video frame sequence through generative adversarial network. It also adds the feature information of the pedestrian joints, which can improve the model efficiency. The experiment result shows that the modified method discussed in this paper can improve the recognition rate of public datasets effectively. In the experiments on PRID2011 and iLIDS-VID, Rank 1 attained 80.2% and 66.3%, respectively.",2020,,,,http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2020/3/PDF/zdhxb-46-3-576.pdf
2e192aaed7fe365c26a5e004b0d586370f57080f,1,[D2],,1,1,0,Cluster-level Feature Alignment for Person Re-identification,"Instance-level alignment is widely exploited for person re-identification, e.g. spatial alignment, latent semantic alignment and triplet alignment. This paper probes another feature alignment modality, namely cluster-level feature alignment across whole dataset, where the model can see not only the sampled images in local mini-batch but the global feature distribution of the whole dataset from distilled anchors. Towards this aim, we propose anchor loss and investigate many variants of cluster-level feature alignment, which consists of iterative aggregation and alignment from the overview of dataset. Our extensive experiments have demonstrated that our methods can provide consistent and significant performance improvement with small training efforts after the saturation of traditional training. In both theoretical and experimental aspects, our proposed methods can result in more stable and guided optimization towards better representation and generalization for well-aligned embedding.",2020,ArXiv,2008.0681,,https://arxiv.org/pdf/2008.06810.pdf
2e361f170521a7e3f661a8ed2cc266a32349446a,1,[D2],,1,1,0,Compact Triplet Loss for person re-identification in camera sensor networks,"Abstract The triplet loss in deep learning has achieved promising results for person re-identification (re-ID) in camera sensor networks. However, it neglects the relationship among pedestrian images captured from different sensors, which results in a relatively large intra-class variation. In this paper, we propose a novel loss function named Compact Triplet Loss (CTL) for training Convolutional Neural Networks (CNNs), which not only decreases the intra-class variation but also increases the inter-class variation to improve the generalization ability of person re-ID model. Specifically, CTL simultaneously considers three aspects for pedestrian representations. It pushes the pedestrian images to be closer to their corresponding centers and meanwhile forces different centers are away from each other. In addition, CTL forces the distance between the positive sample pair is smaller than that of the negative sample pair. Finally, we integrate the proposed CTL and the cross-entropy loss to perform multi-task learning. We evaluate the proposed method on Market1501, DukeMTMCreID and CUHK03, and the experimental results reveal our method exceeds other state-of-the-art methods by a large margin.",2019,Ad Hoc Networks,,10.1016/J.ADHOC.2019.101984,
2e4e3d80e0a789dcf45e61401c8af4e3fa96dfea,1,[D2],,1,1,0,Batch DropBlock Network for Person Re-Identification and Beyond,"Since the person re-identification task often suffers from the problem of pose changes and occlusions, some attentive local features are often suppressed when training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network which is a two branch network composed of a conventional ResNet-50 as the global branch and a feature dropping branch.The global branch encodes the global salient representations.Meanwhile, the feature dropping branch consists of an attentive feature learning module called Batch DropBlock, which randomly drops the same region of all input feature maps in a batch to reinforce the attentive feature learning of local regions.The network then concatenates features from both branches and provides a more comprehensive and spatially distributed feature representation. Albeit simple, our method achieves state-of-the-art on person re-identification and it is also applicable to general metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existed works by a large margin (more than 6%).",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00379,
2eecdb103cb8bac86aab6ef8fa19d11ce1e77218,0,,,0,1,0,A novel deep model with multi-loss and efficient training for person re-identification,"Abstract The purpose of Person re-identification (PReID) is to identify the same individual from the non-overlapping cameras, the task has been greatly promoted by the deep learning system. In this study, we review two widely-used CNN frameworks in the PReID community: identification model and triplet model. We provide a comprehensive overview of the advantages and limitations of the two models and present a hybrid model that combines the advantages of both identification and triplet models. Specifically, the proposed model employs triplet loss, identification loss and center loss to simultaneously train the carefully designed network. Furthermore, the dropout scheme is adopted by its identification subnetwork. Given a triplet unit images, the model can output the identities of the three input images and force the Euclidean distance between the mismatched pairs to be larger than those between the matched pairs as well as reduce the variance of the same class at the same time. Extensive comparative experiments on three PReID benchmark datasets (CUHK01, CUHK03, Market-1501) show that our proposed architecture outperforms many state of the art methods in most cases.",2019,Neurocomputing,,10.1016/j.neucom.2018.03.073,https://www.ee.ryerson.ca/~xzhang/publications/NEUCOM2019-PersonReID.pdf
2fe9a1e797bb0a2ba79b1f68c95a1067dbe402fe,1,[D2],,1,0,0,Adaptive L2 Regularization in Person Re-Identification.,"We introduce an adaptive L2 regularization mechanism in the setting of person re-identification. In the literature, it is common practice to utilize hand-picked regularization factors which remain constant throughout the training procedure. Unlike existing approaches, the regularization factors in our proposed method are updated adaptively through backpropagation. This is achieved by incorporating trainable scalar variables as the regularization factors, which are further fed into a scaled hard sigmoid function. Extensive experiments on the Market-1501, DukeMTMC-reID and MSMT17 datasets validate the effectiveness of our framework. Most notably, we obtain state-of-the-art performance on MSMT17, which is the largest dataset for person re-identification. Source code is publicly available at https://github.com/nixingyang/AdaptiveL2Regularization.",2020,,2007.07875,,https://arxiv.org/pdf/2007.07875.pdf
31da1da2d4e7254dd8f2a4578d887c57e0678438,1,[D2],,1,1,1,Unsupervised Person Re-identification,"The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.",2018,ACM Trans. Multim. Comput. Commun. Appl.,1705.10444,10.1145/3243316,https://arxiv.org/pdf/1705.10444.pdf
31e89d17132716fdaf5c749aa7dc14e19e89e725,1,[D2],,1,0,0,Metric Learning with Equidistant and Equidistributed Triplet-based Loss for Product Image Search,"Product image search in E-commerce systems is a challenging task, because of a huge number of product classes, low intra-class similarity and high inter-class similarity. Deep metric learning, based on paired distances independent of the number of classes, aims to minimize intra-class variances and inter-class similarity in feature embedding space. Most existing approaches strictly restrict the distance between samples with fixed values to distinguish different classes of samples. However, the distance of paired samples has various magnitudes during different training stages. Therefore, it is difficult to directly restrict absolute distances with fixed values. In this paper, we propose a novel Equidistant and Equidistributed Triplet-based (EET) loss function to adjust the distance between samples with relative distance constraints. By optimizing the loss function, the algorithm progressively maximizes intra-class similarity and inter-class variances. Specifically, 1) the equidistant loss pulls the matched samples closer by adaptively constraining two samples of the same class to be equally distant from another one of a different class in each triplet, 2) the equidistributed loss pushes the mismatched samples farther away by guiding different classes to be uniformly distributed while keeping intra-class structure compact in embedding space. Extensive experimental results on product search benchmarks verify the improved performance of our method. We also achieve improvements on other retrieval datasets, which show superior generalization capacity of our method in image search.",2020,WWW,,10.1145/3366423.3380094,
332fbbcae6661fe8f035a0c1f49f9c3b9f8350b6,1,[D3],,1,0,1,Multi-Scale Temporal Cues Learning for Video Person Re-Identification,"Temporal cues embedded in videos provide important clues for person Re-Identification (ReID). To efficiently exploit temporal cues with a compact neural network, this work proposes a novel 3D convolution layer called Multi-scale 3D (M3D) convolution layer. The M3D layer is easy to implement and could be inserted into traditional 2D convolution networks to learn multi-scale temporal cues by end-to-end training. According to its inserted location, the M3D layer has two variants, i.e., local M3D layer and global M3D layer, respectively. The local M3D layer is inserted between 2D convolution layers to learn spatial-temporal cues among adjacent 2D feature maps. The global M3D layer is computed on adjacent frame feature vectors to learn their global temporal relations. The local and global M3D layers hence learn complementary temporal cues. Their combination introduces a fraction of parameters to traditional 2D CNN, but leads to the strong multi-scale temporal feature learning capability. The learned temporal feature is fused with a spatial feature to compose the final spatial-temporal representation for video person ReID. Evaluations on four widely used video person ReID datasets, i.e., MARS, DukeMTMC-VideoReID, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over the state-of-the art. For example, it achieves rank1 accuracy of 88.63% on MARS without re-ranking. Our method also achieves a reasonable trade-off between ReID accuracy and model size, e.g., it saves about 40% parameters of I3D CNN.",2020,IEEE Transactions on Image Processing,1908.10049,10.1109/TIP.2020.2972108,https://arxiv.org/pdf/1908.10049.pdf
3337fedaf7ebc6e22aa5a65e548ed9e29891e32d,0,,,1,0,0,Using Panoramic Videos for Multi-Person Localization and Tracking In A 3D Panoramic Coordinate,"3D panoramic multi-person localization and tracking are prominent in many applications, however, conventional methods using LiDAR equipment could be economically expensive and also computationally inefficient due to the processing of point cloud data. In this work, we propose an effective and efficient approach at a low cost. First, we obtain panoramic videos with four normal cameras. Then, we transform human locations from a 2D panoramic image coordinate to a 3D panoramic camera coordinate using camera geometry and human bio-metric property (i.e., height). Finally, we generate 3D tracklets by associating human appearance and 3D trajectory. We verify the effectiveness of our method on three datasets including a new one built by us, in terms of 3D single-view multi-person localization, 3D single-view multi-person tracking, and 3D panoramic multi-person localization and tracking. Our code and dataset are available at https://github.com/fandulu/MPLT.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1911.10535,10.1109/ICASSP40776.2020.9053497,https://arxiv.org/pdf/1911.10535.pdf
334a86ab24ec40b2d084f2e049f4fc1b06b34462,0,,,1,0,0,Deep Learning-based Person Search with Visual Attention Embedding,"In this work, we consider the problem of person search, which is a challenging task that requires both person detection and person re-identification r un c oncurrently. In this context, we propose a person search approach based on deep neural networks that incorporates attention mechanisms to perform retrieval more robustly. Global and local features are extracted for person detection and person identification, respectively, boosted by attention layers that allow the extraction of discriminative feature representations, all in an end-to-end manner. We evaluate our approach on three challenging data sets and show that our proposed method improves the state-of-the-art networks.",2020,2020 13th International Conference on Communications (COMM),,10.1109/COMM48946.2020.9141958,
3367dfef9062681c0631b91b4c8b25f5f87d1187,0,,,1,0,0,"Vision Meets Drones: Past, Present and Future","Drones, or general UAVs, equipped with cameras have been fast deployed with a wide range of applications, including agriculture, aerial photography, and surveillance. Consequently, automatic understanding of visual data collected from drones becomes highly demanding, bringing computer vision and drones more and more closely. To promote and track the evelopments of object detection and tracking algorithms, we have organized two challenge workshops in conjunction with ECCV 2018, and ICCV 2019, attracting more than 100 teams around the world. We provide a large-scale drone captured dataset, VisDrone, which includes four tracks, i.e., (1) image object detection, (2) video object detection, (3) single object tracking, and (4) multi-object tracking. In this paper, we first presents a thorough review of object detection and tracking datasets and benchmarks, and discuss the challenges of collecting large-scale drone-based object detection and tracking datasets with fully manual annotations. After that, we describe our VisDrone dataset, which is captured over various urban/suburban areas of 14 different cities across China from North to South. Being the largest such dataset ever published, VisDrone enables extensive evaluation and investigation of visual analysis algorithms on the drone platform. We provide a detailed analysis of the current state of the field of large-scale object detection and tracking on drones, and conclude the challenge as well as propose future directions. We expect the benchmark largely boost the research and development in video analysis on drone platforms. All the datasets and experimental results can be downloaded from the website: this https URL",2020,ArXiv,2001.06303,,https://arxiv.org/pdf/2001.06303.pdf
36653f8705b56e39642bcd123494eb680cd1636b,0,,,0,1,0,Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,"The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.",2018,ICLR,1711.09325,,https://arxiv.org/pdf/1711.09325.pdf
3724cb3b3db2b33bfd3f4ee8bd8df7bf8e59608a,0,,,0,1,0,Data augmentation in fault diagnosis based on the Wasserstein generative adversarial network with gradient penalty,"Abstract Fault detection and diagnosis in industrial process is an extremely essential part to keep away from undesired events and ensure the safety of operators and facilities. In the last few decades various data based machine learning algorithms have been widely studied to monitor machine condition and detect process faults. However, the faulty datasets in industrial process are hard to acquire. Thus low-data of faulty data or imbalanced data distributions are common to see in industrial processes, resulting in the difficulty to accurately identify different faults for many algorithms. Therefore, in this paper, Wasserstein generative adversarial network with gradient penalty (WGAN-GP) based data augmentation approaches are researched to generate data samples to supplement low-data input set in fault diagnosis field and help improve the fault diagnosis accuracies. To verify its efficient, various classifiers are used and three industrial benchmark datasets are involved to evaluate the performance of GAN based data augmentation ability. The results show the fault diagnosis accuracies for classifiers are increased in all datasets after employing the GAN-based data augmentation techniques.",2020,Neurocomputing,,10.1016/J.NEUCOM.2018.10.109,
37586e83d36006a7e00c1325fc5f9194c94ca055,0,,,0,1,0,Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN,"Studies show that Deep Neural Network (DNN)-based image classification models are vulnerable to maliciously constructed adversarial examples. However, little effort has been made to investigate how DNN-based image retrieval models are affected by such attacks. In this paper, we introduce Unsupervised Adversarial Attacks with Generative Adversarial Networks (UAA-GAN) to attack deep feature-based image retrieval systems. UAA-GAN is an unsupervised learning model that requires only a small amount of unlabeled data for training. Once trained, it produces query-specific perturbations for query images to form adversarial queries. The core idea is to ensure that the attached perturbation is barely perceptible to human yet effective in pushing the query away from its original position in the deep feature space. UAA-GAN works with various application scenarios that are based on deep features, including image retrieval, person Re-ID and face search. Empirical results show that UAA-GAN cripples retrieval performance without significant visual changes in the query images. UAA-GAN generated adversarial examples are less distinguishable because they tend to incorporate subtle perturbations in textured or salient areas of the images, such as key body parts of human, dominant structural patterns/textures or edges, rather than in visually insignificant areas (e.g., background and sky). Such tendency indicates that the model indeed learned how to toy with both image retrieval systems and human eyes.",2019,ArXiv,1907.05793,,https://arxiv.org/pdf/1907.05793.pdf
3db9031ce4465fd8cab23910099d99f483718a81,1,[D2],,0,1,0,Person Re-identification in Aerial Imagery,"Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles (UAVs), visual surveillance by utilizing the UAV platform has been very attractive. Most of the research works for UAV captured visual data are mainly focused on the tasks of object detection and tracking. However, limited attention has been paid to the task of person Re-identification (ReID) which has been widely studied in ordinary surveillance cameras with fixed emplacements. In this paper, to facilitate the research of person ReID in aerial imagery, we collect a large scale airborne person ReID dataset named as Person ReID for Aerial Imagery (PRAI-1581), which consists of 39,461 images of 1581 person identities. The images of the dataset are shot by two DJI consumer UAVs flying at an altitude ranging from 20 to 60 meters above the ground, which covers most of the real UAV surveillance scenarios. In addition, we propose to utilize subspace pooling of convolution feature maps to represent the input person images. Our method can learn a discriminative and compact feature representation for ReID in aerial imagery and can be trained in an end-to-end fashion efficiently. We conduct extensive experiments on the proposed dataset and the experimental results demonstrate that re-identify persons in aerial imagery is a challenging problem, where our method performs favorably against state of the arts. Our dataset can be accessed via \url{this https URL}.",2019,ArXiv,1908.05024,10.1109/TMM.2020.2977528,https://arxiv.org/pdf/1908.05024.pdf
3dd74e372e791e1cb1a17ce40642d00f46d1d6f6,1,[D2],,1,1,1,Unsupervised Person Re-identification by Combining Appearance Features with Spatial-temporal Features,"Most of the existing person re-identification methods usually follow a supervised learning framework and train models based on a large number of labeled pedestrian images. However, directly deploying these trained models in real scenes will lead to poor performances, because the target domain data may be completely different from the training data, thus the model parameters cannot be well fitted. Furthermore, it is very time-consuming and impractical to label a large number of data. In order to solve these problems, we propose a simple and effective strategy for segmentation based on key parts aiming to obtain the discriminative appearance features. Simultaneously, we constructs a hybrid Gaussian model by calculating the time difference of pedestrian groups to acquire spatial-temporal features. Finally, a measure fusion model is used to combine the appearance measure matrix and spatial-temporal distance matrix, which greatly improves the performance of the unsupervised person re-identification. We conduct extensive experiments on the large-scale image datasets, including Market-1501 and DukeMTMC-reID. The experimental results demonstrate that our algorithm is superior to state-of-the-art unsupervised re-identification approaches.",2020,MM 2020,,10.1145/3404716.3404723,
3ef7e092003b66d1e66f2d9570defcb5ac866ba8,1,[D2],,1,0,0,PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks,,2020,ArXiv,2005.02154,,https://arxiv.org/pdf/2005.02154.pdf
3f6139a3017f3948b357eb41d41aee4d7a47eabb,1,[D2],,1,0,0,Bilinear Attention Networks for Person Retrieval,"This paper investigates a novel Bilinear attention (Bi-attention) block, which discovers and uses second order statistical information in an input feature map, for the purpose of person retrieval. The Bi-attention block uses bilinear pooling to model the local pairwise feature interactions along each channel, while preserving the spatial structural information. We propose an Attention in Attention (AiA) mechanism to build inter-dependency among the second order local and global features with the intent to make better use of, or pay more attention to, such higher order statistical relationships. The proposed network, equipped with the proposed Bi-attention is referred to as Bilinear ATtention network (BAT-net). Our approach outperforms current state-of-the-art by a considerable margin across the standard benchmark datasets (e.g., CUHK03, Market-1501, DukeMTMC-reID and MSMT17).",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00812,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.pdf
41c1dd74db853c6256c00df1826290981dac56ec,0,,,1,0,0,Online multi-object tracking with pedestrian re-identification and occlusion processing,"Tracking-by-detection is a common approach for online multi-object tracking problem. At present, the following challenges still exist in the multi-object tracking scenarios: (1) The result of object re-tracking after full occlusion is not ideal; (2) The predicted position of object is not accurate enough in the complicated video scenarios. Aiming at these two problems, this paper proposes a multi-object tracking framework called DROP (Deep Re-identification Occlusion Processing). The framework consists of object detection, fast pedestrian re-identification, and a confidence-based data association algorithm. A lightweight convolutional neural network that can solve the re-tracking problem is constructed by increasing and learning the affinity of appearance features of the same object in different frames. And this paper proposes to judge the occlusion of the object that can solve inaccurate position predicted by Kalman filter by using the data association result of the appearance features of the object, and to reduce the matching error by improving the data association formula. The experimental results on the multi-object tracking datasets MOT15 and MOT16 show that the proposed method can improve the precision while ensure the real-time tracking performance.",2020,The Visual Computer,,10.1007/s00371-020-01854-0,
42dc432f58adfaa7bf6af07e5faf9e75fea29122,1,"[D2], [D4]",,0,1,0,Sequence-based Person Attribute Recognition with Joint CTC-Attention Model,"Attribute recognition has become crucial because of its wide applications in many computer vision tasks, such as person re-identification. Like many object recognition problems, variations in viewpoints, illumination, and recognition at far distance, all make this task challenging. In this work, we propose a joint CTC-Attention model (JCM), which maps attribute labels into sequences to learn the semantic relationship among attributes. Besides, this network uses neural network to encode images into sequences, and employs connectionist temporal classification (CTC) loss to train the network with the aim of improving the encoding performance of the network. At the same time, it adopts the attention model to decode the sequences, which can realize aligning the sequences and better learning the semantic information from attributes. Extensive experiments on three public datasets, i.e., Market-1501 attribute dataset, Duke attribute dataset and PETA dataset, demonstrate the effectiveness of the proposed method.",2018,ArXiv,1811.08115,,https://arxiv.org/pdf/1811.08115.pdf
42f81abff3a2a3c6594f167d51347e4aeb3ee01d,0,,,0,0,1,ASTA-Net: Adaptive Spatio-Temporal Attention Network for Person Re-Identification in Videos,"The attention mechanism has been widely applied to enhance pedestrian representation for person re-identification in videos. However, most existing methods learn the spatial and temporal attention separately, and thus ignore the correlation between them. In this work, we propose a novel Adaptive Spatio-Temporal Attention Network (ASTA-Net) to adaptively aggregate the spatial and temporal attention features into discriminative pedestrian representation for person re-identification in videos. Specifically, multiple Adaptive Spatio-Temporal Fusion modules within ASTA-Net are designed for exploring precise spatio-temporal attention on multi-level feature maps. They first obtain the preliminary spatial and temporal attention features via the spatial semantic relations for each frame and temporal dependencies among inconsecutive frames, then adaptively aggregate the preliminary attention features on the basis of their correlation. Moreover, an Adjacent-Frame Motion module is designed to explicitly extract motion patterns according to the feature-level variation among adjacent frames. Extensive experiments on the three widely-used datasets, i.e., MARS, iLIDS-VID and PRID2011, have demonstrated the effectiveness of the proposed approach.",2020,ACM Multimedia,,10.1145/3394171.3413843,
4327934807725afb7592eb77c264acefc49c9347,1,[D2],,1,1,0,Person Reidentification via Structural Deep Metric Learning,"Despite the promising progress made in recent years, person reidentification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. This paper proposes to tackle this task by jointly learning feature representation and distance metric in an end-to-end manner. Existing deep metric learning-based re-ID methods usually encounter the following two weaknesses: 1) most works based on pairwise or triplet constraints often suffer from slow convergence and poor local optima, partially because they use very limited samples for each update and 2) hard negative sample mining has been widely applied in existing works. However, hard positive samples, which also contribute to the training of network, have not received enough attention. To alleviate these problems, we develop a novel structural metric learning objective for person re-ID, in which each positive pair is allowed to be compared against all negative pairs in a minibatch and each positive pair is adaptively assigned a hardness-aware weight to modulate its contribution. The introduced positive pair weighting strategy enables the algorithm to focus more on the hard positive samples. Furthermore, we propose to enhance the proposed loss function by adding a global loss term to reduce the variances of positive/negative pair distances, which is able to improve the generalization capability of the network model. By this approach, person images can be nonlinearly mapped into a low-dimensional embedding space where similar samples are kept closer and dissimilar samples are pushed farther apart. We implement the proposed algorithm using the inception architecture and evaluate it on three large-scale re-ID data sets. Experiment results demonstrate that our approach is able to outperform most state of the arts while using much lower dimensional deep features.",2019,IEEE Transactions on Neural Networks and Learning Systems,,10.1109/TNNLS.2018.2861991,
448ef67eafb2780a155a677b9da54c92b8f3e540,1,[D2],,0,1,0,Pseudo Label Based on Multiple Clustering for Unsupervised Cross-Domain Person Re-Identification,"Person re-identification (Re-ID) has achieved great improvement with the development of deep learning. However, domain adaptation in unsupervised Re-ID has always been a challenging task. Most existing works based on clustering only cluster once, which may lead to pseudo labels of poor quality. In this letter, we propose a Pseudo Label based on Multiple Clustering (PLMC) approach, which makes full advantage of multiple clustering to obtain more robust pseudo labels. In particular, our PLMC framework consists of two stages, namely, global training stage, and local training stage. We adopt the training strategy that combines the information learned from global features, and local features by training two stages alternately. Extensive experiments are carried out on three standard benchmark datasets (e.g., Maket1501, DukeMTMC-ReID, CUHK03). The results demonstrate that our PLMC method is superior to the previous methods based on single clustering, and achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.3016528,
449d2e25f1da780e9c68feaba9cc630fdf1b7331,1,[D2],,0,1,0,Multiple Back Propagation Network and Metric Fusion for Person Re-identification,"Person re-identification (Re-ID) is a research focus in pattern recognition, which is to identify a person from another camera view. Many researches have studied feature representations and metric distances of person images, which are robust to changes of view angle and illumination. In this paper, we propose a Multiple Back Propagation (MBP) network and Metric Fusion (MF) for person Re-ID. The proposed MBP network is based on DenseNet or ResNet. Each Dense-conv layer or Conv-ID block is linked by a MBP layer. Each MBP layer is divided into two sub-streams. One sub-stream is connected to softmax loss and the other sub-stream is transferred to a convolution layer followed by triplet loss. A Metric Fusion (MF) method with an optimized weighting scheme is proposed for deep feature fusion. Furthermore, we propose a new metric Re-ranking Euclidean distance joining metric fusion. Experiments on three large-scale person Re-ID benchmark datasets, including Market1501, CUHK03 and DukeMTMC-reID, show that the proposed MBPMF method can achieve state-of-the-art performances.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852031,
4794f699a50ae06eb74907674f27cbbe337d9475,1,[D2],,0,1,0,Attention Deep Model with Multi-Scale Deep Supervision for Person Re-Identification,"In recent years, person re-identification (PReID) has become a hot topic in computer vision duo to it is an important part in intelligent surveillance. Many state-of-the-art PReID methods are attention-based or multi-scale feature learning deep models. However, introducing attention mechanism may lead to some important feature information losing issue. Besides, most of the multi-scale models embedding the multi-scale feature learning block into the feature extraction deep network, which reduces the efficiency of inference network. To address these issue, in this study, we introduce an attention deep architecture with multi-scale deep supervision for PReID. Technically, we contribute a reverse attention block to complement the attention block, and a novel multi-scale layer with deep supervision operator for training the backbone network. The proposed block and operator are only used for training, and discard in test phase. Experiments have been performed on Market-1501, DukeMTMC-reID and CUHK03 datasets. All the experiment results show that the proposed model significantly outperforms the other competitive state-of-the-art methods.",2019,ArXiv,1911.10335,,https://arxiv.org/pdf/1911.10335.pdf
4c9112d0bdb0f13938144a3d8a5bf9c2916f4c6e,1,[D2],,1,0,0,Compact Network Training for Person ReID,"The task of person re-identification (ReID) has attracted growing attention in recent years leading to improved performance, albeit with little focus on real-world applications. Most SotA methods are based on heavy pre-trained models, e.g. ResNet50 (~25M parameters), which makes them less practical and more tedious to explore architecture modifications. In this study, we focus on a small-sized randomly initialized model that enables us to easily introduce architecture and training modifications suitable for person ReID. The outcomes of our study are a compact network and a fitting training regime. We show the robustness of the network by outperforming the SotA on both Market1501 and DukeMTMC. Furthermore, we show the representation power of our ReID network via SotA results on a different task of multi-object tracking.",2020,ICMR,1910.07038,10.1145/3372278.3390686,https://arxiv.org/pdf/1910.07038.pdf
4cd07662e23da2bc9592b595d1ba008e621a5173,0,,,1,1,0,Simulating Content Consistent Vehicle Datasets with Attribute Descent,"This paper uses a graphic engine to simulate a large amount of training data with free annotations. Between synthetic and real data, there is a two-level domain gap, i.e., content level and appearance level. While the latter has been widely studied, we focus on reducing the content gap in attributes like illumination and viewpoint. To reduce the problem complexity, we choose a smaller and more controllable application, vehicle re-identification (re-ID). We introduce a large-scale synthetic dataset VehicleX. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes. We propose an attribute descent approach to let VehicleX approximate the attributes in real-world datasets. Specifically, we manipulate each attribute in VehicleX, aiming to minimize the discrepancy between VehicleX and real data in terms of the Frechet Inception Distance (FID). This attribute descent algorithm allows content domain adaptation (DA) orthogonal to existing appearance DA methods. We mix the optimized VehicleX data with real-world vehicle re-ID datasets, and observe consistent improvement. With the augmented datasets, we report competitive accuracy. We make the dataset, engine and our codes available at this https URL.",2020,ECCV,1912.08855,10.1007/978-3-030-58539-6_46,https://arxiv.org/pdf/1912.08855.pdf
4d024deee31516076ff95c807f5d62b8ad11d022,1,[D2],,1,1,0,Unsupervised Region Attention Network for Person Re-Identification,"As supervised person re-identification (Re-Id) requires massive labeled pedestrian data and it is very difficult to collect sufficient labeled data in reality, unsupervised Re-Id approaches attract much more attention than the former. Existing unsupervised person Re-Id models learn global features of pedestrian from whole images or several constant patches. These models ignore the difference of each region in the whole pedestrian images for feature representation, such as occluded and pose invariant regions, and thus reduce the robustness of models for cross-view feature learning. To solve these issues, we propose an Unsupervised Region Attention Network (URAN) that can learn the cross-view region attention features from the cropped pedestrian images, fixed by region importance weights on images. The proposed URAN designs a Pedestrian Region Biased Enhance (PRBE) loss to produce high attention weights for most important regions in pedestrian images. Furthermore, the URAN employs a first neighbor relation grouping algorithm and a First Neighbor Relation Constraint (FNRC) loss to provide the training direction of the unsupervised region attention network, such that the region attention features are discriminant enough for unsupervised person Re-Id task. In experiments, we consider two popular datasets, Market1501 and DukeMTMC-reID, as evaluation of PRBE and FNRC loss, and their balance parameter to demonstrate the effectiveness and efficiency of the proposed URAN, and the experimental results show that the URAN provides better performance than the-state-of-the-arts (higher than existing methods at least 1.1%).",2019,IEEE Access,,10.1109/ACCESS.2019.2953280,
4dcc3f86c0ca68c23e2d0b0f5ebffd5d3fdc8f8a,1,[D2],,1,0,0,PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by Deep Convolutional Neural Networks,"Despite significant progress of applying deep learning methods to the field of content-based image retrieval, there has not been a software library that covers these methods in a unified manner. In order to fill this gap, we introduce PyRetri, an open source library for deep learning based unsupervised image retrieval. The library encapsulates the retrieval process in several stages and provides functionality that covers various prominent methods for each stage. The idea underlying its design is to provide a unified platform for deep learning based image retrieval research, with high usability and extensibility. The project source code, with usage examples, sample data and pre-trained models are available at https://github.com/PyRetri/.",2020,ACM Multimedia,2005.02154,10.1145/3394171.3414537,https://arxiv.org/pdf/2005.02154.pdf
4fb7bd7d68f1f07f787aa2824cc8948a2d6820d8,1,"[D1], [D2]",,1,1,0,Multi-Target Multi-Camera Tracking with Human Body Part Semantic Features,"Recently, Multi-Target Multi-Camera Tracking (MTMCT) has gained more and more attention. It is a challenging task with major problems including occlusion, background clutter, poses and camera point of view variations. Compared to single camera tracking, which takes advantage of location information and strict time constraints, good appearance features are more important to MTMCT. This drives us to extract robust and discriminative features for MTMCT. We propose MTMCT\_HS which uses human body part semantic features to overcome the above challenges. We use a two-stream deep neural network to extract the global appearance features and human body part semantic maps separately, and employ aggregation operations to generate final features. We argue that these features are more suitable for affinity measurement, which can be seen as the average of appearance similarity weighted by the corresponding human body part similarity. Next, our tracker adopts a hierarchical correlation clustering algorithm, which combines targets' appearance feature similarity with motion correlation for data association. We validate the effectiveness of our MTMCT\_HS method by demonstrating its superiority over the state-of-the-art method on DukeMTMC benchmark. Experiments show that the extracted features with human body part semantics are more effective for MTMCT compared with the methods solely employing global appearance features.",2019,CIKM,,10.1145/3357384.3358029,
4fef7fe1d8422c6d8a6d5d144470b089419e3b82,1,,1,0,1,0,MOTS: Multiple Object Tracking for General Categories Based On Few-Shot Method,"Most modern Multi-Object Tracking (MOT) systems typically apply REID-based paradigm to hold a balance between computational efficiency and performance. In the past few years, numerous attempts have been made to perfect the systems. Although they presented favorable performance, they were constrained to track specified category. Drawing on the ideas of few shot method, we pioneered a new multi-target tracking system, named MOTS, which is based on metrics but not limited to track specific category. It contains two stages in series: In the first stage, we design the self-Adaptive-matching module to perform simple targets matching, which can complete 88.76% assignments without sacrificing performance on MOT16 training set. In the second stage, a Fine-match Network was carefully designed for unmatched targets. With a newly built TRACK-REID data-set, the Fine-match Network can perform matching of 31 category targets, even generalizes to unseen categories.",2020,ArXiv,2005.09167,,https://arxiv.org/pdf/2005.09167.pdf
5061fe2817edf066307576d0de16143a57dc7e8b,1,[D2],,0,1,0,Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification,"Extracting effective and discriminative features is very important for addressing the challenging person re-identification (re-ID) task. Prevailing deep convolutional neural networks (CNNs) usually use high-level features for identifying pedestrian. However, some essential spatial information resided in low-level features such as shape, texture and color will be lost when learning the high-level features, due to extensive padding and pooling operations in the training stage. In addition, most existing person re-ID methods are mainly based on hand-craft bounding boxes where images are precisely aligned. It is unrealistic in practical applications, since the exploited object detection algorithms often produce inaccurate bounding boxes. This will inevitably degrade the performance of existing algorithms. To address these problems, we put forward a novel person re-ID model that fuses high- and low-level embeddings to reduce the information loss caused in learning high-level features. Then we divide the fused embedding into several parts and reconnect them to obtain the global feature and more significant local features, so as to alleviate the affect caused by the inaccurate bounding boxes. In addition, we also introduce the spatial and channel attention mechanisms in our model, which aims to mine more discriminative features related to the target. Finally, we reconstruct the feature extractor to ensure that our model can obtain more richer and robust features. Extensive experiments display the superiority of our approach compared with existing approaches. Our code is available at this https URL.",2020,ArXiv,2009.07536,,https://arxiv.org/pdf/2009.07536.pdf
507210135fa0c86e58713f15d295a596d4ad87e8,1,[D2],,1,1,0,Semantically Selective Augmentation for Deep Compact Person Re-Identification,"We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clustering-based network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces.",2018,,,,https://research-information.bris.ac.uk/files/203125984/Full_text_PDF_accepted_author_manuscript_.pdf
513d1979a56bfd03b825c7f3b2a3ba70869315d1,0,,1,1,0,0,Self-supervised Multi-view Person Association and Its Applications.,,2020,IEEE transactions on pattern analysis and machine intelligence,1805.08717,10.1109/TPAMI.2020.2974726,https://arxiv.org/pdf/1805.08717.pdf
51ea7f870a69d56bea93cf3fcfca06d9c09a53cf,0,,,0,0,1,Camera On-Boarding for Person Re-Identification Using Hypothesis Transfer Learning,"Most of the existing approaches for person re-identification consider a static setting where the number of cameras in the network is fixed. An interesting direction, which has received little attention, is to explore the dynamic nature of a camera network, where one tries to adapt the existing re-identification models after on-boarding new cameras, with little additional effort. There have been a few recent methods proposed in person re-identification that attempt to address this problem by assuming the labeled data in the existing network is still available while adding new cameras. This is a strong assumption since there may exist some privacy issues for which one may not have access to those data. Rather, based on the fact that it is easy to store the learned re-identifications models, which mitigates any data privacy concern, we develop an efficient model adaptation approach using hypothesis transfer learning that aims to transfer the knowledge using only source models and limited labeled data, but without using any source camera data from the existing network. Our approach minimizes the effect of negative transfer by finding an optimal weighted combination of multiple source models for transferring the knowledge. Extensive experiments on four challenging benchmark datasets with variable number of cameras well demonstrate the efficacy of our proposed approach over state-of-the-art methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2007.11149,10.1109/cvpr42600.2020.01216,https://arxiv.org/pdf/2007.11149.pdf
5351feb51da379af28164fbaa93fad7fd9b8d5b7,1,[D2],,1,1,0,Cross-Entropy Adversarial View Adaptation for Person Re-Identification,"Person re-identification (re-ID) is a task of matching pedestrians under disjoint camera views. To recognize paired snapshots, it has to cope with large cross-view variations caused by the camera view shift. The supervised deep neural networks are effective in producing a set of non-linear projections that can transform cross-view images into a common feature space. However, they typically impose a symmetric architecture, leaving the network ill-conditioned on its optimization. In this paper, we learn view-invariant subspace for person re-ID, and its corresponding similarity metric using an adversarial view adaptation approach. The main contribution is to learn coupled asymmetric mappings regarding view characteristics which are adversarially trained to address the view discrepancy by optimizing the cross-entropy view confusion objective. To determine the similarity value, the network is empowered with a similarity discriminator to promote features that are highly discriminant in distinguishing positive and negative pairs. The other contribution includes an adaptive weighing on the most difficult samples to address the imbalance of within-/between-identity pairs. Our approach achieves notably improved performance in comparison with the state-of-the-arts on benchmark datasets.",2020,IEEE Transactions on Circuits and Systems for Video Technology,1904.01755,10.1109/TCSVT.2019.2909549,https://arxiv.org/pdf/1904.01755.pdf
536210d839dd439b14637354e23aabd7689afae0,0,,,0,1,0,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-identification,"We address the problem of person re-identification from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our first contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances re-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.",2018,ECCV,,10.1007/978-3-030-01228-1_44,
557ba3d8f7bba0bbf00cf6363f3263484b541d6f,1,[D2],,1,1,0,A Strong Baseline and Batch Normalization Neck for Deep Person Re-Identification,"This study proposes a simple but strong baseline for deep person re-identification (ReID). Deep person ReID has achieved great progress and high performance in recent years. However, many state-of-the-art methods design complex network structures and concatenate multi-branch features. In the literature, some effective training tricks briefly appear in several papers or source codes. The present study collects and evaluates these effective training tricks in person ReID. By combining these tricks, the model achieves 94.5% rank-1 and 85.9% mean average precision on Market1501 with only using the global features of ResNet50. The performance surpasses all existing global- and part-based baselines in person ReID. We propose a novel neck structure named as batch normalization neck (BNNeck). BNNeck adds a batch normalization layer after global pooling layer to separate metric and classification losses into two different feature spaces because we observe they are inconsistent in one embedding space. Extended experiments show that BNNeck can boost the baseline, and our baseline can improve the performance of existing state-of-the-art methods. Our codes and models are available at: https://github.com/michuanhaohao/reid-strong-baseline",2020,IEEE Transactions on Multimedia,1906.08332,10.1109/TMM.2019.2958756,https://arxiv.org/pdf/1906.08332.pdf
55e615bbbb5ed7af3cc785f3dcf751946e0303c2,0,,,0,1,0,Improving Skin Lesion Segmentation with Generative Adversarial Networks,"This paper proposes a novel strategy that employs Generative Adversarial Networks (GANs) to augment data in the image segmentation field, and a Convolutional-Deconvolutional Neural Network (CDNN) to automatically generate lesion segmentation mask from dermoscopic images. Training the CDNN with our GAN generated data effectively improves the state-of-the-art.",2018,2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS),,10.1109/CBMS.2018.00086,https://iris.unimore.it/bitstream/11380/1161448/1/CBMS_2018_paper_177.pdf
577ed1325eea7f47f03bb6c6cc3f67d299927f0d,1,[D2],,0,1,0,Deep Learning Research With an Expectation-Maximization Model for Person Re-Identification,"In existing person re-identification methods based on deep learning, the extraction of good features is still a key step. Some efforts divide the image of a person into multiple parts to extract more detailed information from semantically coherent parts but ignore their correlation with each other. Others adopt self attention to reallocate weights of pixels for learning the association between different regions. This association can improve the accuracy of the person re-identification task, but the features obtained by this type of algorithm have high redundancy, which is not conducive to the expression of feature information. In order to address the above challenges, we propose a feature extraction method based on a novel attention mechanism which combines the expectation maximization (EM) algorithm and non-local operation. We embed the attention module into the ResNet50 backbone network. The attention module captures the correlation between different regional features through non-local operation and then reconstructs these features through the EM algorithm. In addition, we divide the network into a global branch and a local branch, where the global branch extracts the complete features, and the local branch uses the Batch DropBlock method to erase a portion of the features to achieve feature diversity. Finally, extensive experiments validate the superiority of the proposed model for person re-ID over a wide variety of state-of-the-art methods on three large-scale benchmarks, including DukeMTMC-ReID, Market-1501 and CUHK03.",2020,IEEE Access,,10.1109/ACCESS.2020.3019100,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09174970.pdf
583cfbc89a5ed2d3343ba4e5b08fbe1b2151b80a,0,,,0,1,0,End-To-End Chromosome Karyotyping with Data Augmentation Using GAN,"Classifying human chromosomes from input cell images, i.e., karyotyping, requires domain expertise and quantity of manual effort to perform. In this paper, we propose an end-to-end chromosome karyotyping method, which can automatically detect, segment and classify chromosomes from cell images. During detection, we explore Extremal Regions (ER) to obtain chromosome candidates in input images. During segmentation, we segment overlapping chromosome candidates by approximating chromosome shapes with eclipses. In classification, we first propose Multiple Distribution Generative Advertising Network (MD-GAN) to effectively cover diverse data modes and generate more labeled samples for data augmentation. Then, we finetune pre-trained convolutional neural network (CNN) to classify chromosomes with samples generated by MD-GAN. We demonstrate the accuracy of the proposed end-to-end method in detecting, segmenting and classifying by experiments on a self-collected dataset. Experiments also prove data augmentation with MD-GAN could improve classification performance of CNN.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451041,
587cf6001c8265dea75e42a204446fb51895e78e,0,,,1,0,0,Deep Generative Models and Applications,"Over the past few years, there have been fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. The amount of annotated data drastically increased and supervised deep discriminative models exceeded human-level performances in certain object detection tasks [Russakovsky et al., 2015, He et al., 2015]. The increasing availability in quantity and complexity of unlabelled data also opens up exciting possibilities for the development of unsupervised learning methods. Among the family of unsupervised methods, deep generative models find numerous applications. Moreover, as real-world applications include high dimensional data, the ability of generative models to automatically learn semantically meaningful subspaces makes their advancement an essential step toward developing more efficient algorithms. Generative Adversarial Networks (GANs) are a family of unsupervised generative algorithms that have demonstrated impressive performance for data synthesis and are now used in a wide range of computer vision tasks. Despite this success, they gained a reputation for being difficult to train, which results in a time-consuming and human-involved development process to use them. In the first part of this thesis, we focus on improving the stability and the performances of GANs. Foremost, we consider an alternative training process to the standard one, named SGAN, in which several adversarial “local” pairs of networks are trained independently so that a “global” supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well. Next, to further reduce the computational footprint while maintaining the stability and performance advantages of SGAN, we focus on training single pair of adversarial networks using variance reduced gradient. More precisely, we study the effect of the stochastic gradient noise on the training of generative adversarial networks (GANs) and show that it can prevent the convergence of standard game optimization methods, while the batch version converges. We address this issue with two stochastic variance-reduced gradient and extragradient optimization algorithms for GANs, named SVRG-GAN and SVRE, respectively. As batch extragradient is the only method that converges for simple examples of games, our analyses focus on SVRE, which method for a large class of games improves upon the previous convergence rates proposed in the literature. We observe empirically that SVRE performs similarly to a batch method",2020,,,10.5075/EPFL-THESIS-10257,
592906dc3d4909adca9981fea78a01ef4b925964,0,,,0,1,0,Mobile person re-identification with a lightweight trident CNN,"Engineering Research Center of Hubei Province for Clothing Information, School of Mathematics and Computer Science, Wuhan Textile University, Wuhan 430200, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan 430072, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an 710068, China",2020,Science China Information Sciences,,10.1007/s11432-019-2782-3,http://scis.scichina.com/en/2020/219102-supplementary.pdf
59455e1752f7587a0d7aea053ecd81cc359e3faf,1,[D2],,1,0,0,Robust Person Re-Identification by Modelling Feature Uncertainty,"We aim to learn deep person re-identification (ReID) models that are robust against noisy training data. Two types of noise are prevalent in practice: (1) label noise caused by human annotator errors and (2) data outliers caused by person detector errors or occlusion. Both types of noise pose serious problems for training ReID models, yet have been largely ignored so far. In this paper, we propose a novel deep network termed DistributionNet for robust ReID. Instead of representing each person image as a feature vector, DistributionNet models it as a Gaussian distribution with its variance representing the uncertainty of the extracted features. A carefully designed loss is formulated in DistributionNet to unevenly allocate uncertainty across training samples. Consequently, noisy samples are assigned large variance/uncertainty, which effectively alleviates their negative impacts on model fitting. Extensive experiments demonstrate that our model is more effective than alternative noise-robust deep models. The source code is available at: https://github.com/TianyuanYu/DistributionNet",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00064,https://www.pure.ed.ac.uk/ws/files/111974444/Robust_Person_Re_identification_YU_DoA220719_AFV.pdf
59f357015054bab43fb8cbfd3f3dbf17b1d1f881,1,[D1],,1,0,0,Unsupervised Multi-Object Detection for Video Surveillance Using Memory-Based Recurrent Attention Networks,"Nowadays, video surveillance has become ubiquitous with the quick development of artificial intelligence. Multi-object detection (MOD) is a key step in video surveillance and has been widely studied for a long time. The majority of existing MOD algorithms follow the “divide and conquer” pipeline and utilize popular machine learning techniques to optimize algorithm parameters. However, this pipeline is usually suboptimal since it decomposes the MOD task into several sub-tasks and does not optimize them jointly. In addition, the frequently used supervised learning methods rely on the labeled data which are scarce and expensive to obtain. Thus, we propose an end-to-end Unsupervised Multi-Object Detection framework for video surveillance, where a neural model learns to detect objects from each video frame by minimizing the image reconstruction error. Moreover, we propose a Memory-Based Recurrent Attention Network to ease detection and training. The proposed model was evaluated on both synthetic and real datasets, exhibiting its potential.",2018,Symmetry,,10.3390/sym10090375,https://pdfs.semanticscholar.org/59f3/57015054bab43fb8cbfd3f3dbf17b1d1f881.pdf
59f8d07bba3967f2909d36fe12d03f8a515aa01a,0,,,1,1,0,An Introduction to Person Re-identification with Generative Adversarial Networks,"Person re-identification is a basic subject in the field of computer vision. The traditional methods have several limitations in solving the problems of person illumination like occlusion, pose variation and feature variation under complex background. Fortunately, deep learning paradigm opens new ways of the person re-identification research and becomes a hot spot in this field. Generative Adversarial Nets (GANs) in the past few years attracted lots of attention in solving these problems. This paper reviews the GAN based methods for person re-identification focuses on the related papers about different GAN based frameworks and discusses their advantages and disadvantages. Finally, it proposes the direction of future research, especially the prospect of person re-identification methods based on GANs.",2019,ArXiv,1904.05992,,https://arxiv.org/pdf/1904.05992.pdf
5aa62121aeed6908bb90a4871a9dc7425477befb,0,,,1,0,0,Real-Time Online Multi-Object Tracking in Compressed Domain,"Recent online multi-object tracking (MOT) methods have achieved desirable tracking performance. However, the tracking speed of most existing methods is rather slow. Inspired from the fact that the adjacent frames are highly relevant and redundant, we divide the frames into key and non-key frames and track objects in the compressed domain. For the key frames, the RGB images are restored for detection and data association. To make data association more reliable, an appearance convolutional neural network (CNN) which can be jointly trained with the detector is proposed. For the non-key frames, the objects are directly propagated by a tracking CNN based on the motion information provided in the compressed domain. Compared with the state-of-the-art online MOT methods, our tracker is about $6\times $ faster while maintaining a comparable tracking performance.",2019,IEEE Access,,10.1109/ACCESS.2019.2921975,
5b062562a8067baae045df1c7f5a8455d0363b5a,1,[D2],,1,0,0,SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and Partial Person Re-Identification,"Holistic person re-identification (ReID) has received extensive study in the past few years and achieves impressive progress. However, persons are often occluded by obstacles or other persons in practical scenarios, which makes partial person re-identification non-trivial. In this paper, we propose a spatial-channel parallelism network (SCPNet) in which each channel in the ReID feature pays attention to a given spatial part of the body. The spatial-channel corresponding relationship supervises the network to learn discriminative feature for both holistic and partial person re-identification. The single model trained on four holistic ReID datasets achieves competitive accuracy on these four datasets, as well as outperforms the state-of-the-art methods on two partial ReID datasets without training.",2018,ACCV,1810.06996,10.1007/978-3-030-20890-5_2,https://arxiv.org/pdf/1810.06996.pdf
5bc8f2fd72b3ffa0432a971c4097dc370f6d6222,0,,,1,0,0,SoDA: Multi-Object Tracking with Soft Data Association,"Robust multi-object tracking (MOT) is a prerequisite fora safe deployment of self-driving cars. Tracking objects, however, remains a highly challenging problem, especially in cluttered autonomous driving scenes in which objects tend to interact with each other in complex ways and frequently get occluded. We propose a novel approach to MOT that uses attention to compute track embeddings that encode the spatiotemporal dependencies between observed objects. This attention measurement encoding allows our model to relax hard data associations, which may lead to unrecoverable errors. Instead, our model aggregates information from all object detections via soft data associations. The resulting latent space representation allows our model to learn to reason about occlusions in a holistic data-driven way and maintain track estimates for objects even when they are occluded. Our experimental results on the Waymo OpenDataset suggest that our approach leverages modern large-scale datasets and performs favorably compared to the state of the art in visual multi-object tracking.",2020,ArXiv,2008.07725,,https://arxiv.org/pdf/2008.07725.pdf
5c5f18feaa12fa62cd7c8abf1778230b9cc53601,1,[D2],,1,0,0,Contextual Multi-Scale Feature Learning for Person Re-Identification,"Representing features at multiple scales is significant for person re-identification (Re-ID). Most existing methods learn the multi-scale features by stacking streams and convolutions without considering the cooperation of multiple scales at a granular level. However, most scales are more discriminative only when they integrate other scales as contextual information. We termed that contextual multi-scale. In this paper, we proposed a novel architecture, namely contextual multi-scale network (CMSNet), for learning common and contextual multi-scale representations simultaneously. The building block of CMSNet obtains contextual multi-scale representations by bidirectionally hierarchical connection groups: the forward hierarchical connection group for stepwise inter-scale information fusion and the backward hierarchical connection group for leap-frogging inter-scale information fusion. Too rich scale features without a selection will confuse the discrimination. Additionally, we introduced a new channel-wise scale selection module to dynamically select scale features for corresponding input image. To the best of our knowledge, CMSNet is the most lightweight model for person Re-ID and it achieves state-of-the-art performance on four commonly used Re-ID datasets, surpassing most large-scale models.",2020,ACM Multimedia,,10.1145/3394171.3414038,
5c9f448bfb87e9a3db4f2c73a93be271cca0c932,0,,,1,0,0,Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking,"Deep learning-based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors for tracking-by-detection.This results in deep models that are detector biased and evaluations that are detector influenced. To resolve this issue, we introduce Deep Motion Modeling Network (DMM-Net) that can estimate multiple objects' motion parameters to perform joint detection and association in an end-to-end manner. DMM-Net models object features over multiple frames and simultaneously infers object classes, visibility, and their motion parameters. These outputs are readily used to update the tracklets for efficient MOT. DMM-Net achieves PR-MOTA score of 12.80 @ 120+ fps for the popular UA-DETRAC challenge, which is better performance and orders of magnitude faster. We also contribute a synthetic large-scale public dataset Omni-MOT for vehicle tracking that provides precise ground-truth annotations to eliminate the detector influence in MOT evaluation. This 14M+ frames dataset is extendable with our public script (Code at Dataset , Dataset Recorder , Omni-MOT Source ). We demonstrate the suitability of Omni-MOT for deep learning with DMMNet and also make the source code of our network public.",2020,ArXiv,2008.08826,,https://arxiv.org/pdf/2008.08826.pdf
5d52bdbb478f31749f33ead669cab87107af6b52,0,,,1,0,0,BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving,"In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/.",2019,2019 International Conference on Robotics and Automation (ICRA),1903.06405,10.1109/ICRA.2019.8793523,https://arxiv.org/pdf/1903.06405.pdf
5d8337ddffa5757bebf72c31de58c131fce599e6,1,[D2],,1,1,0,A Survey of Pruning Methods for Efficient Person Re-identification Across Domains,"Recent years have witnessed a substantial increase in the deep learning architectures proposed for visual recognition tasks like person re-identification, where individuals must be recognized over multiple distributed cameras. Although deep Siamese networks have greatly improved the state-of-the-art accuracy, the computational complexity of the CNNs used for feature extraction remains an issue, hindering their deployment on platforms with with limited resources, or in applications with real-time constraints. Thus, there is an obvious advantage to compressing these architectures without significantly decreasing their accuracy. This paper provides a survey of state-of-the-art pruning techniques that are suitable for compressing deep Siamese networks applied to person re-identification. These techniques are analysed according to their pruning criteria and strategy, and according to different design scenarios for exploiting pruning methods to fine-tuning networks for target applications. Experimental results obtained using Siamese networks with ResNet feature extractors, and multiple benchmarks re-identification datasets, indicate that pruning can considerably reduce network complexity while maintaining a high level of accuracy. In scenarios where pruning is performed with large pre-training or fine-tuning datasets, the number of FLOPS required by the ResNet feature extractor is reduced by half, while maintaining a comparable rank-1 accuracy (within 1\% of the original model). Pruning while training a larger CNNs can also provide a significantly better performance than fine-tuning smaller ones.",2019,ArXiv,1907.02547,,https://arxiv.org/pdf/1907.02547.pdf
5f59b1deae09152b2a1aff2312e558d8db3cb1f6,1,[D2],,1,1,0,Unsupervised person re-identification by hierarchical cluster and domain transfer,"Person re-identification (re-ID) has recently been tremendously boosted due to the advancement of deep convolutional neural networks. Unfortunately, the majority of deep re-ID methods focus on supervised, single-domain re-ID task, while less attention is paid on unsupervised domain adaptation. Therefore, these methods always fail to generalize well to real-world scenarios, which have attracted much attention from academia. To address this challenge, we propose a joint unsupervised domain adaptive re-ID method, named HCTL, which is aided by Hierarchical Clustering and Transfer Learning. Specifically, our method performs camera invariance learning using iStarGAN by transferring style of reliable images, which is mined by hierarchical clustering, to the style of other cameras in target domain. During training stage, HCTL integrates TriHard loss on top of ResNet-50 to reduce intra-class variance among dataset and enforce connectedness simultaneously between source domain and target domain. Comprehensive experiments based on Market-1501, DukeMTMC-reID and CUHK03 are conducted, results indicate that our method robustly achieves state-of-the-art performances with only a few reliable samples in target domain and outperform any existing approaches by a large margin.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-08723-x,
61470aaccabd195a361112d38d80d3779498f784,0,,,0,1,0,Stepwise Metric Promotion for Unsupervised Video Person Re-identification,"The intensive annotation cost and the rich but unlabeled data contained in videos motivate us to propose an unsupervised video-based person re-identification (re-ID) method. We start from two assumptions: 1) different video tracklets typically contain different persons, given that the tracklets are taken at distinct places or with long intervals; 2) within each tracklet, the frames are mostly of the same person. Based on these assumptions, this paper propose a stepwise metric promotion approach to estimate the identities of training tracklets, which iterates between cross-camera tracklet association and feature learning. Specifically, We use each training tracklet as a query, and perform retrieval in the cross-camera training set. Our method is built on reciprocal nearest neighbor search and can eliminate the hard negative label matches, i.e., the cross-camera nearest neighbors of the 0 matches in the initial rank list. The tracklet that passes the reciprocal nearest neighbor check is considered to have the same ID with the query. Experimental results on the PRID 2011, ILIDS-VID, and MARS datasets show that the proposed method achieves very competitive re-ID accuracy compared with its supervised counterparts.",2017,2017 IEEE International Conference on Computer Vision (ICCV),,10.1109/ICCV.2017.266,http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf
62dd6c64548e345d58cf2d98dd09375ceadb3611,1,"[D2], [D7]",,0,1,0,Progressive Cross-Camera Soft-Label Learning for Semi-Supervised Person Re-Identification,"In this paper, we focus on the semi-supervised person re-identification (Re-ID) case, which only has the intra-camera (within-camera) labels but not inter-camera (cross-camera) labels. In real-world applications, these intra-camera labels can be readily captured by tracking algorithms or few manual annotations, when compared with cross-camera labels. In this case, it is very difficult to explore the relationships between cross-camera persons in the training stage due to the lack of cross-camera label information. To deal with this issue, we propose a novel Progressive Cross-camera Soft-label Learning (PCSL) framework for the semi-supervised person Re-ID task, which can generate cross-camera soft-labels and utilize them to optimize the network. Concretely, we calculate an affinity matrix based on person-level features and adapt them to produce the similarities between cross-camera persons (i.e., cross-camera soft-labels). To exploit these soft-labels to train the network, we investigate the weighted cross-entropy loss and the weighted triplet loss from the classification and discrimination perspectives, respectively. Particularly, the proposed framework alternately generates progressive cross-camera soft-labels and gradually improves feature representations in the whole learning course. Extensive experiments on five large-scale benchmark datasets show that PCSL significantly outperforms the state-of-the-art unsupervised methods that employ labeled source domains or the images generated by the GANs-based models. Furthermore, the proposed method even has a competitive performance with respect to deep supervised Re-ID methods.",2020,IEEE Transactions on Circuits and Systems for Video Technology,1908.05669,10.1109/TCSVT.2020.2983600,https://arxiv.org/pdf/1908.05669.pdf
632329f488e49f5217880abd56da905d044209ca,0,,,1,0,0,Enhanced Multiple-Object Tracking Using Delay Processing and Binary-Channel Verification,"Tracking objects over time, i.e., identity (ID) consistency, is important when dealing with multiple object tracking (MOT). Especially in complex scenes with occlusion and interaction of objects this is challenging. Significant improvements in single object tracking (SOT) methods have inspired the introduction of SOT to MOT to improve the robustness, that is, maintaining object identities as long as possible, as well as helping alleviate the limitations from imperfect detections. SOT methods are constantly generalized to capture appearance changes of the object, and designed to efficiently distinguish the object from the background. Hence, simply extending SOT to a MOT scenario, which consists of a complex scene with spatially mixed, occluded, and similar objects, will encounter problems in computational efficiency and drifted results. To address this issue, we propose a binary-channel verification model that deeply excavates the potential of SOT in refining the representation while maintaining the identities of the object. In particular, we construct an integrated model that jointly processes the previous information of existing objects and new incoming detections, by using a unified correlation filter through the whole process to maintain consistency. A delay processing strategy consisting of the three parts—attaching, re-initialization, and re-claiming—is proposed to tackle drifted results caused by occlusion. Avoiding the fuzzy appearance features of complex scenes in MOT, this strategy can improve the ability to distinguish specific objects from each other without contaminating the fragile training space of a single object tracker, which is the main cause of the drift results. We demonstrate the effectiveness of our proposed approach on the MOT17 challenge benchmarks. Our approach shows better overall ID consistency performance in comparison with previous works.",2019,,,10.3390/app9224771,https://pdfs.semanticscholar.org/585b/a125d4e45b3679124a91b4c1f9629940f3bd.pdf
6429e901283ab2b5ce6ae51dc1179475fce0338c,1,[D6],,1,0,0,DoT-GNN: Domain-Transferred Graph Neural Network for Group Re-identification,"Most person re-identification (ReID) approaches focus on retrieving a person-of-interest from a database of collected individual images. In addition to the individual ReID task, matching a group of persons across different camera views also plays an important role in surveillance applications. This kind of Group Re-identification (GReID) task is very challenging since we face the obstacles not only from the appearance changes of individuals, but also from the group layout and membership changes. In order to obtain robust representation for the group image, we design a Domain-Transferred Graph Neural Network (DoT-GNN) method. The merits are three aspects: 1) Transferred Style. Due to the lack of training samples, we transfer the labeled ReID dataset to the G-ReID dataset style, and feed the transferred samples to the deep learning model. Taking the superiority of deep learning models, we achieve a discriminative individual feature model. 2) Graph Generation. We treat a group as a graph, where each node denotes the individual feature and each edge represents the relation of a couple of individuals. We propose a graph generation strategy to create sufficient graph samples. 3) Graph Neural Network. Employing the generated graph samples, we train the GNN so as to acquire graph features which are robust to large graph variations. The key to the success of DoT-GNN is that the transferred graph addresses the challenge of the appearance change, while the graph representation in GNN overcomes the challenge of the layout and membership change. Extensive experimental results demonstrate the effectiveness of our approach, outperforming the state-of-the-art method by 1.8% CMC-1 on Road Group dataset and 6.0% CMC-1 on DukeMCMT dataset respectively.",2019,ACM Multimedia,,10.1145/3343031.3351027,
64426bc27c9ec5d6fb50c73c4745cb2e1020b3c7,0,,,0,1,0,Exploiting Category Similarity-Based Distributed Labeling for Fine-Grained Visual Classification,"The fine-grained visual classification (FGVC) which aims to distinguish subtle differences among subcategories is an important computer vision task. However, one issue that limits model performance is the problem of diversity within subcategories. To this end, we propose a simple yet effective approach named category similarity-based distributed labeling (CSDL) to tackle this problem. Specifically, we first obtain the feature centers for various subcategories and utilize them to initialize the label distributions. Then we replace the ground-truth labels in a Deep Neural Network (DNN) with the distributed labels to calculate the loss and perform the optimization. Finally, the joint supervision of a softmax loss and a center loss is adopted to update the parameters of the DNN, the deep feature centers, and the distributed labels for learning discriminative deep features. Comprehensive experiments on three publicly available FGVC datasets demonstrate the superiority of our proposed approach.",2020,IEEE Access,,10.1109/ACCESS.2020.3030249,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09220917.pdf
649b7ffe51065c4d3579084812c0676335fbc777,0,,,0,1,0,Generation of synthetic traffic sign images using conditional generative adversarial networks,"В работе рассматривается метод генерации синтетических обучающих выборок для задачи классификации дорожных знаков. Метод основан на использовании порождающих конкурирующих нейросетей и метрики Васерштейна. Исследуется метод условной генерации изображений, когда на вход порождающей нейросети подается случайный шум и метка класса изображения, которое нужно сгенерировать. Для обучения такой нейросети предлагается использовать перекрестную энтропию в добавление к метрике Васерштейна. Для стабилизации процесса обучения используются веса для обучающей выборки. Экспериментальная оценка метода показывает, что условная порождающая сеть работает лучше, чем простая генерация дорожных знаков по иконке, однако не дотягивает до метода, в котором для каждого класса обучается отдельная порождающая нейросеть.",2018,,,,https://www.graphicon.ru/html/2018/papers/242-246.pdf
67289bd3b7c9406429c6012eb7292305e50dff0b,1,[D2],,1,1,0,Integration Convolutional Neural Network for Person Re-Identification in Camera Networks,"In this paper, we propose a novel deep model named integration convolutional neural network (ICNN) for person re-identification in camera networks, which jointly learns global and local features in a unified framework. To this end, the proposed ICNN simultaneously applies two kinds of loss functions. Specifically, we propose the soft triplet loss to learn global features which automatically adjusts the margin threshold within one batch. The soft triplet loss could alleviate the difficult in tuning parameters and therefore learns discriminative global features. In order to avoid the part misalignment problem, we learn latent local features by conducting local horizontal average pooling on the convolutional maps. Afterward, we implement the identification task on each local feature. We concatenate global and local features using a weighted strategy to present the pedestrian images. We evaluate the proposed ICNN on three large-scale databases. Our method achieves rank-1 accuracy of 92.13% on Market 1501, 61.4% on CUHK03 and 85.3% on DukeMTMC-reID, and the results outperform the state-of-the-art methods.",2018,IEEE Access,,10.1109/ACCESS.2018.2852712,
6918a94c71c4a4989ce245d8d90f6355e61591a5,0,,,1,0,0,The MTA Dataset for Multi Target Multi Camera Pedestrian Tracking by Weighted Distance Aggregation,"Existing multi target multi camera tracking (MTMCT) datasets are small in terms of the number of identities and video length. The creation of new real world datasets is hard as privacy has to be guaranteed and the labeling is tedious. Therefore in the scope of this work a mod for GTA V to record a MTMCT dataset has been developed and used to record a simulated MTMCT dataset called Multi Camera Track Auto (MTA). The MTA dataset contains over 2,800 person identities, 6 cameras and a video length of over 100 minutes per camera. Additionally a MTMCT system has been implemented to provide a baseline for the created dataset. The system’s pipeline consists of stages for person detection, person re-identification, single camera multi target tracking, track distance calculation, and track association. The track distance calculation comprises a weighted aggregation of the following distances: a single camera time constraint, a multi camera time constraint using overlapping camera areas, an appearance feature distance, a homography matching with pairwise camera homographies, and a linear prediction based on the velocity and the time difference of tracks. When using all partial distances, we were able to surpass the results of state-of-the-art single camera trackers by +13% IDF1 score. The MTA dataset, code, and baselines are available at github.com/schuar-iosb/mta-dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00529,
69a7c8bca699ee4100fbe6a83b72459c132a6f10,1,[D2],,1,1,0,Aware Person Re-identification across Multiple Resolutions,"Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about highand low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-ofthe-art results on all five datasets that we evaluate on. We then propose two new formulations of the person reID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints.",2018,,,,http://home.bharathh.info/pubs/pdfs/WangCVPR2018b.pdf
6ba425056604beb9201d58e542fad3c9110feadd,1,[D2],,1,0,0,Bilinear Attention Networks for Person Retrieval Supplementary Material,"Here, we show an additional ablation study to verify the effectiveness of Bi-attention with AiA on the DukeMTMCreID [3] and the MSMT17 [4] datasets in a single query setting. From this additional study, we can draw the same conclusions as in our main paper. Effect of Bilinear Attention. We evaluate the effect of Bi-attention on the feature extractors, and the results are shown in Table 1. The results on both datasets demonstrate that: Bi-attention improves the performance of both scenarios Fa and Fa + Fp, similar to the observations noticed on the Market-1501 and CUHK03 datasets.",2019,,,,https://pdfs.semanticscholar.org/6ba4/25056604beb9201d58e542fad3c9110feadd.pdf
6d05639926fdb48dee93a2dadd15d3ea70ad6f19,1,[D1],,1,0,0,CAN: Composite Appearance Network for Person Tracking and How to Model Errors in a Tracking System,"Tracking multiple people across multiple cameras is an open problem. It is typically divided into two tasks: (i) single-camera tracking (SCT) - identify trajectories in the same scene, and (ii) inter-camera tracking (ICT) - identify trajectories across cameras for real surveillance scenes. Many methods cater to SCT, while ICT still remains a challenge. In this paper, we propose a tracking method which uses motion cues and a feature aggregation network for template-based person re-identification by incorporating metadata such as person bounding box and camera information. We present a feature aggregation architecture called Composite Appearance Network (CAN) to address the above problem. The key structure of this architecture is called EvalNet that pays attention to each feature vector and learns to weight them based on gradients it receives for the overall template for optimal re-identification performance. We demonstrate the efficiency of our approach with experiments on the challenging multi-camera tracking dataset, DukeMTMC. We also survey existing tracking measures and present an online error metric called ""Inference Error"" (IE) that provides a better estimate of tracking/re-identification error, by treating SCT and ICT errors uniformly.",2018,,,,https://pdfs.semanticscholar.org/e27c/86e51a4c821f6e0f8992363417349a995820.pdf
6e3cc2cdfa44501a04bcea5afd246b63e9829d37,1,[D2],,0,1,0,HMM-Based Person Re-identification in Large-Scale Open Scenario,"This paper aims to tackle person re-identification (person re-ID) in large-scale open scenario, which differs from the conventional person re-ID tasks but is significant for some real suspect investigation cases. In the large-scale open scenario, the image background and person appearance may change immensely. There are a large number of irrelevant pedestrians appearing in the urban surveillance systems, some of which may have very similar appearance with the target person. Existing methods utilize only surveillance video information, which can not solve the problem well due to above challenges. In this paper, we explore that pedestrians’ paths from multiple spaces (such as surveillance space and geospatial space) are matched due to temporal-spatial consistency. Moreover, people have their unique behavior path due to the differences of individual behavioral. Inspired by these two observations, we propose to use the association relationship of paths from surveillance space and geospatial space to solve the person re-ID in large-scale open scenario. A Hidden Markov Model based Path Association(HMM-PA) framework is presented to jointly analyze image path and geospatial path. In addition, according to our research scenario, we manually annotate path description on two large-scale public re-ID datasets, termed as Duke-PDD and Market-PDD. Comprehensive experiments on these two datasets show proposed HMM-PA outperforms the state-of-art methods.",2020,MMM,,10.1007/978-3-030-37731-1_66,
6f8e829996a5d9627a42db5c9cc6ac898def0882,1,[D1],,1,0,0,Simulation Trust and the Internet of Things,"The urban environment is becoming increasingly more connected and complex. In the coming decades, we will be surrounded by billions of sensors, devices, and machines, the Internet of Things (IoT). As the world becomes more connected, we will become dependent on machines and simulation to make decisions on our behalf. When simulation systems use data from sensors, devices and machines (i.e., things) to make decisions, they need to learn how to trust that data, as well as the things they are interacting with. As embedded simulation becomes more commonplace in IoT and smart city applications, it is essential that decision makers are able to trust the simulation systems making decisions on their behalf. This paper looks at trust from an IoT perspective, describing a set of research projects conducted that span multiple dimensions of trust, and discusses whether these concepts of trust apply to simulation.",2019,2019 Winter Simulation Conference (WSC),,10.1109/WSC40007.2019.9004912,
7002d8c61be9f1ea210f88059df6955c88db62b7,1,[D2],,1,1,0,Person Transfer GAN to Bridge Domain Gap for Person Re-identification,"Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT171 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.08565,10.1109/CVPR.2018.00016,https://arxiv.org/pdf/1711.08565.pdf
71fffc2c348afea03d310c396038b0b264afe866,1,"[D2], [D4]",,1,1,0,AttKGCN: Attribute Knowledge Graph Convolutional Network for Person Re-identification,"Discriminative feature representation of person image is important for person re-identification (Re-ID) task. Recently, attributes have been demonstrated beneficially in guiding for learning more discriminative feature representations for Re-ID. As attributes normally co-occur in person images, it is desirable to model the attribute dependencies to improve the attribute prediction and thus Re-ID results. In this paper, we propose to model these attribute dependencies via a novel attribute knowledge graph (AttKG), and propose a novel Attribute Knowledge Graph Convolutional Network (AttKGCN) to solve Re-ID problem. AttKGCN integrates both attribute prediction and Re-ID learning together in a unified end-to-end framework which can boost their performances, respectively. AttKGCN first builds a directed attribute KG whose nodes denote attributes and edges encode the co-occurrence relationships of different attributes. Then, AttKGCN learns a set of inter-dependent attribute classifiers which are combined with person visual descriptors for attribute prediction. Finally, AttKGCN integrates attribute description and deeply visual representation together to construct a more discriminative feature representation for Re-ID task. Extensive experiments on several benchmark datasets demonstrate the effectiveness of AttKGCN on attribute prediction and Re-ID tasks.",2019,ArXiv,1911.10544,,https://arxiv.org/pdf/1911.10544.pdf
720890c2532a95a53280a33b81aff788d87e6abf,0,,,0,1,0,Appearance and Pose-Conditioned Human Image Generation using Deformable GANs,"In this paper, we address the problem of generating person images conditioned on both pose and appearance information. Specifically, given an image xa of a person and a target pose P(xb), extracted from a different image xb, we synthesize a new image of that person in pose P(xb), while preserving the visual details in xa. In order to deal with pixel-to-pixel misalignments caused by the pose differences between P(xa) and P(xb), we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. Quantitative and qualitative results, using common datasets and protocols recently proposed for this task, show that our approach is competitive with respect to the state of the art. Moreover, we conduct an extensive evaluation using off-the-shell person re-identification (Re-ID) systems trained with person-generation based augmented data, which is one of the main important applications for this task. Our experiments show that our Deformable GANs can significantly boost the Re-ID accuracy and are even better than data-augmentation methods specifically trained using Re-ID losses.",2019,IEEE transactions on pattern analysis and machine intelligence,1905.00007,10.1109/tpami.2019.2947427,https://arxiv.org/pdf/1905.00007.pdf
722514cf193ea8b301475de9da5a0061f2e47bdd,0,,,0,1,0,A survey of image synthesis and editing with generative adversarial networks,"This paper presents a survey of image synthesis and editing with Generative Adversarial Networks (GANs). GANs consist of two deep networks, a generator and a discriminator, which are trained in a competitive way. Due to the power of deep networks and the competitive training manner, GANs are capable of producing reasonable and realistic images, and have shown great capability in many image synthesis and editing applications. This paper surveys recent GAN papers regarding topics including, but not limited to, texture synthesis, image inpainting, image-to-image translation, and image editing.",2017,,,10.23919/TST.2017.8195348,https://pdfs.semanticscholar.org/7225/14cf193ea8b301475de9da5a0061f2e47bdd.pdf
726523c505a9ff96610d09d31f1568cba5d08449,1,[D2],,0,1,0,Improving Person Re-Identification With Iterative Impression Aggregation,"Our impression about one person often updates after we see more aspects of him/her and this process keeps iterating given more meetings. We formulate such an intuition into the problem of person re-identification (re-ID), where the representation of a query (probe) image is iteratively updated with new information from the candidates in the gallery. Specifically, we propose a simple attentional aggregation formulation to instantiate this idea and showcase that such a pipeline achieves competitive performance on standard benchmarks including CUHK03, Market-1501 and DukeMTMC. Not only does such a simple method improve the performance of the baseline models, it also achieves comparable performance with latest advanced re-ranking methods. Another advantage of this proposal is its flexibility to incorporate different representations and similarity metrics. By utilizing stronger representations and metrics, we further demonstrate state-of-the-art person re-ID performance, which also validates the general applicability of the proposed method.",2020,IEEE Transactions on Image Processing,2009.10066,10.1109/TIP.2020.3029415,https://arxiv.org/pdf/2009.10066.pdf
729f2450ab52be354254b1924f73b54bdda80dd2,1,[D2],,0,1,0,Person re-identification for 365-day video surveillance based on stride convolutional neural network,"Person re-identification (ReID) is an important task in video surveillance and can be applied in various practical applications. The traditional methods and deep learning model cannot satisfy the real-world challenges of environmental complexity and scene dynamics, especially under fixed scene. What’s more, most of the existing datasets are outdoor and has a single style, which is not good for indoor person re-identification. Focusing on these problems, the paper improves a Stride Convolutional Neural Network (S-CNN) to process indoor images based on multi-features fusion. The deep model is established in which the identity information, stride information and other information are learned to handle more challenging indoor images. Then a metric learning method (Joint Bayesian) is employed based on the deep model. Finally, the entire classifier is retrained with supervised learning. The experiment is tested on the OUC365 dataset created by us which is captured for 365 days including all seasons style. Compared with other state-of-the-art methods, the performance of the proposed method yields best results",2019,International Conference on Graphic and Image Processing,,10.1117/12.2524371,
73abb9a8575d24fd9a18f538f51503478c91ace4,1,[D5],,1,1,0,"A Systematic Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets","Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 11 feature extraction algorithms and 22 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR, DukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03, RAiD, iLIDSVID, HDA+, and Market1501. The evaluation codebase and results will be made publicly available for community use.",2019,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2018.2807450,
73c40c8aeb005ccb0dd22537b69e515b47fb416b,1,[D2],,1,0,0,Deep Local Binary Coding for Person Re-Identification by Delving into the Details,"Person re-identification (ReID) has recently received extensive research interests due to its diverse applications in multimedia analysis and computer vision. However, the majority of existing works focus on improving matching accuracy, while ignoring matching efficiency. In this work, we present a novel binary representation learning framework for efficient person ReID, namely Deep Local Binary Coding (DLBC). Different from existing deep binary ReID approaches, DLBC attempts to learn discriminative binary codes by explicitly interacting with local visual details. Specifically, DLBC first extracts a set of local features from spatially salient regions of pedestrian images. Subsequently, DLBC formulates a new binary-local semantic mutual information (BSMI) maximization term, based on which a self-lifting (SL) block is built to further exploit the semantic importance of local features. The BSMI term together with the SL block simultaneously enhances the dependency of binary codes on selected local features as well as their robustness to cross-view visual inconsistency. In addition, an efficient optimizing method is developed to train the proposed deep models with orthogonal and binary constraints. Extensive experiments reveal that DLBC significantly minimizes the accuracy gap between binary ReID methods and the state-of-the-art real-valued ones, whilst remarkably reducing query time and memory cost.",2020,ACM Multimedia,,10.1145/3394171.3413979,
74d3c6e6bd20d9ff76483f496e72ba3e05f5eac8,0,,,1,0,0,Pedestrian detection with unsupervised multispectral feature learning using deep neural networks,"Abstract Multispectral pedestrian detection is an important functionality in various computer vision applications such as robot sensing, security surveillance, and autonomous driving. In this paper, our motivation is to automatically adapt a generic pedestrian detector trained in a visible source domain to a new multispectral target domain without any manual annotation efforts. For this purpose, we present an auto-annotation framework to iteratively label pedestrian instances in visible and thermal channels by leveraging the complementary information of multispectral data. A distinct target is temporally tracked through image sequences to generate more confident labels. The predicted pedestrians in two individual channels are merged through a label fusion scheme to generate multispectral pedestrian annotations. The obtained annotations are then fed to a two-stream region proposal network (TS-RPN) to learn the multispectral features on both visible and thermal images for robust pedestrian detection. Experimental results on KAIST multispectral dataset show that our proposed unsupervised approach using auto-annotated training data can achieve performance comparable to state-of-the-art deep neural networks (DNNs) based pedestrian detectors trained using manual labels.",2019,Inf. Fusion,,10.1016/j.inffus.2018.06.005,http://e-motarjem.ir/storage/btn_uploaded/2018-08-27/1535365107_e-motarjem-EN11.pdf
76fb9e2963928bf8e940944d45c13d52db947702,0,,,0,1,0,Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification,"Person re-identification (ReID) is an important task in computer vision. Recently, deep learning with a metric learning loss has become a common framework for ReID. In this paper, we also propose a new metric learning loss with hard sample mining called margin smaple mining loss (MSML) which can achieve better accuracy compared with other metric learning losses, such as triplet loss. In experi- ments, our proposed methods outperforms most of the state-of-the-art algorithms on Market1501, MARS, CUHK03 and CUHK-SYSU.",2017,ArXiv,1710.00478,,https://arxiv.org/pdf/1710.00478.pdf
78fde57462fb68530a49f913c89343da5727580d,1,[D1],,1,1,0,DukeMTMC4ReID: A Large-Scale Multi-camera Person Re-identification Dataset,"In the past decade, research in person re-identification (re-id) has exploded due to its broad use in security and surveillance applications. Issues such as inter-camera viewpoint, illumination and pose variations make it an extremely difficult problem. Consequently, many algorithms have been proposed to tackle these issues. To validate the efficacy of re-id algorithms, numerous benchmarking datasets have been constructed. While early datasets contained relatively few identities and images, several large-scale datasets have recently been proposed, motivated by data-driven machine learning. In this paper, we introduce a new large-scale real-world re-id dataset, DukeMTMC4ReID, using 8 disjoint surveillance camera views covering parts of the Duke University campus. The dataset was created from the recently proposed fully annotated multi-target multi-camera tracking dataset DukeMTMC[36]. A benchmark summarizing extensive experiments with many combinations of existing re-id algorithms on this dataset is also provided for an up-to-date performance analysis.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.185,https://www.ecse.rpi.edu/Homepages/rjradke/papers/dukemtmc4reid.pdf
7b5ab0033996cf5eedce1eb95d1116864139ed96,1,[D2],,0,1,0,Multi-task Learning with Coarse Priors for Robust Part-aware Person Re-identification,"Part-level representations are important for robust person re-identification (ReID), but in practice feature quality suffers due to the body part misalignment problem. In this paper, we present a robust, compact, and easy-to-use method called the Multi-task Part-aware Network (MPN), which is designed to extract semantically aligned part-level features from pedestrian images. MPN solves the body part misalignment problem via multi-task learning (MTL) in the training stage. More specifically, it builds one main task (MT) and one auxiliary task (AT) for each body part on the top of the same backbone model. The ATs are equipped with a coarse prior of the body part locations for training images. ATs then transfer the concept of the body parts to the MTs via optimizing the MT parameters to identify part-relevant channels from the backbone model. Concept transfer is accomplished by means of two novel alignment strategies: namely, parameter space alignment via hard parameter sharing and feature space alignment in a class-wise manner. With the aid of the learned high-quality parameters, MTs can independently extract semantically aligned part-level features from relevant channels in the testing stage. Systematic experiments on four large-scale ReID databases demonstrate that MPN consistently outperforms state-of-the-art approaches by significant margins.",2020,IEEE transactions on pattern analysis and machine intelligence,2003.08069,10.1109/TPAMI.2020.3024900,https://arxiv.org/pdf/2003.08069.pdf
7b6b49adf60d56d1b33b428fdf66aff7426fca6e,0,,,1,0,0,Survey on Deep Learning Techniques for Person Re-Identification Task,"Intelligent video-surveillance is currently an active research field in computer vision and machine learning techniques. It provides useful tools for surveillance operators and forensic video investigators. Person re-identification (PReID) is one among these tools. It consists of recognizing whether an individual has already been observed over a camera in a network or not. This tool can also be employed in various possible applications such as off-line retrieval of all the video-sequences showing an individual of interest whose image is given a query, and online pedestrian tracking over multiple camera views. To this aim, many techniques have been proposed to increase the performance of PReID. Among the systems, many researchers utilized deep neural networks (DNNs) because of their better performance and fast execution at test time. Our objective is to provide for future researchers the work being done on PReID to date. Therefore, we summarized state-of-the-art DNN models being used for this task. A brief description of each model along with their evaluation on a set of benchmark datasets is given. Finally, a detailed comparison is provided among these models followed by some limitations that can work as guidelines for future research.",2018,ArXiv,1807.05284,,https://arxiv.org/pdf/1807.05284.pdf
7c58c7b455839272a5ac395bcd0529a527dd5667,1,[D2],,0,1,0,Unity Style Transfer for Person Re-Identification,"Style variation has been a major challenge for person re-identification, which aims to match the same pedestrians across different cameras. Existing works attempted to address this problem with camera-invariant descriptor subspace learning. However, there will be more image artifacts when the difference between the images taken by different cameras is larger. To solve this problem, we propose a UnityStyle adaption method, which can smooth the style disparities within the same camera and across different cameras. Specifically, we firstly create UnityGAN to learn the style changes between cameras, producing shape-stable style-unity images for each camera, which is called UnityStyle images. Meanwhile, we use UnityStyle images to eliminate style differences between different images, which makes a better match between query and gallery. Then, we apply the proposed method to Re-ID models, expecting to obtain more style-robust depth features for querying. We conduct extensive experiments on widely used benchmark datasets to evaluate the performance of the proposed framework, the results of which confirm the superiority of the proposed model.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.02068,10.1109/cvpr42600.2020.00692,https://arxiv.org/pdf/2003.02068.pdf
7dac9cc7e0b4ad6e63db59cdefd3a805bd1db279,1,[D2],,1,1,0,Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification,"Unsupervised domain adaptive person Re-IDentification (ReID) is challenging because of the large domain gap between source and target domains, as well as the lackage of labeled data on the target domain. This paper tackles this challenge through jointly enforcing visual and temporal consistency in the combination of a local one-hot classification and a global multi-class classification. The local one-hot classification assigns images in a training batch with different person IDs, then adopts a Self-Adaptive Classification (SAC) model to classify them. The global multi-class classification is achieved by predicting labels on the entire unlabeled training set with the Memory-based Temporal-guided Cluster (MTC). MTC predicts multi-class labels by considering both visual similarity and temporal consistency to ensure the quality of label prediction. The two classification models are combined in a unified framework, which effectively leverages the unlabeled data for discriminative feature learning. Experimental results on three large-scale ReID datasets demonstrate the superiority of proposed method in both unsupervised and unsupervised domain adaptive ReID tasks. For example, under unsupervised setting, our method outperforms recent unsupervised domain adaptive methods, which leverage more labels for training.",2020,ECCV,2007.10854,10.1007/978-3-030-58586-0_29,https://arxiv.org/pdf/2007.10854.pdf
7e22e678c070932f6ddd907ac8ea1ab6a7714808,1,[D2],,1,0,0,Discriminative Feature Learning With Consistent Attention Regularization for Person Re-Identification,"Person re-identification (Re-ID) has undergone a rapid development with the blooming of deep neural network. Most methods are very easily affected by target misalignment and background clutter in the training process. In this paper, we propose a simple yet effective feedforward attention network to address the two mentioned problems, in which a novel consistent attention regularizer and an improved triplet loss are designed to learn foreground attentive features for person Re-ID. Specifically, the consistent attention regularizer aims to keep the deduced foreground masks similar from the low-level, mid-level and high-level feature maps. As a result, the network will focus on the foreground regions at the lower layers, which is benefit to learn discriminative features from the foreground regions at the higher layers. Last but not least, the improved triplet loss is introduced to enhance the feature learning capability, which can jointly minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Experimental results on the Market1501, DukeMTMC-reID and CUHK03 datasets have shown that our method outperforms most of the state-of-the-art approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00813,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.pdf
7eac8af0344d6e078246bbd13d76842a70d1b148,1,[D2],,1,0,0,Relation Network for Person Re-identification,"Person re-identification (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model outperforms the state of the art on the Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations.",2020,AAAI,1911.09318,10.1609/AAAI.V34I07.6857,https://arxiv.org/pdf/1911.09318.pdf
7eeb12e7f709113d8626b1c7a7ec235367640aa9,1,,1,1,1,0,Front-End Smart Visual Sensing and Back-End Intelligent Analysis: A Unified Infrastructure for Economizing the Visual System of City Brain,"The visual data, which are acquired from the ubiquitous visual sensors deployed in metropolitans, are of great value and paramount significance to enhance the effectiveness and pursue the future development of smart cities. In this paper, the essential building blocks of the unified visual data management and analysis infrastructure that serve as the foundation for the economical visual system in the city brain, are introduced to facilitate the utilization of the visual signal in the artificial intelligence era. In particular, we start by the discussion of the front-end smart visual sensing in the context of economical communication and service with the heterogeneous network, and the functionalities and necessities of compact visual feature and deep learning model representations are detailed. Subsequently, the utilities of the infrastructure are demonstrated through two intelligent applications at the back-end, including vehicle re-identification and person re-identification. The standardizations regarding compact feature and deep neural network representations, which are regarded as the key ingredients in this infrastructure and greatly facilitate the construction of the visual system in the city brain, are also discussed. Finally, we envision how the potential issues regarding the economical visual communications for future smart cities might be pragmatically approached within this unified infrastructure.",2019,IEEE Journal on Selected Areas in Communications,,10.1109/JSAC.2019.2916488,
82297c21eb134f1d684628e89c1e426ad67faca6,0,,,1,0,0,Spatio-Temporal Correlation Graph for Association Enhancement in Multi-object Tracking,"Due to the frequent interaction between targets in real-world scenarios, various data association problems, such as association ambiguities and association failure, are caused by potential correlation between interactive tracklets, especially during crowded and cluttered scenes. To overcome the non-intuitionistic of tracklet interaction, spatio-temporal correlation graph (STCG) is proposed to model the potential correlation between pairwise tracklets. Three primitive interactions (aggregation, abruption, stability) are defined to model the completed period of the tracklet interaction. Furthermore, STCG model is applied into network flow tracking to exploit the potential correlation between tracklets and enhance the association of the interactive tracklets, especially when overlapping or occlusion is happened. Our method is effective on MOT challenge benchmarks and achieves considerable competitive results with current state-of-the-art trackers.",2019,KSEM,,10.1007/978-3-030-29551-6_35,
82e2ec85b5309f885fabc6a6a49fbaa4f054c082,0,,,1,0,0,Hierarchical-Matching-Based Online and Real-Time Multi-Object Tracking with Deep Appearance Features,"Based on tracking-by-detection, we propose a hierarchical-matching-based online and real-time multi-object tracking approach with deep appearance features, which can effectively reduce the 0 positives (FP) in tracking. For the purpose of increasing the accuracy rate of data association, we define the trajectory confidence using its position information, appearance information, and the information of historical relevant detections, after which we can classify the trajectories into different levels. In order to obtain discriminative appearance features, we developed a deep convolutional neural network to extract the appearance features of objects and trained it on a large-scale pedestrian re-identification dataset. Last but not least, we used the proposed diverse and hierarchical matching strategy to associate detection and trajectory sets. Experimental results on the MOT benchmark dataset show that our proposed approach performs well against other online methods, especially for the metrics of FP and frames per second (FPS).",2020,Algorithms,,10.3390/a13040080,
837c7e88634ba7b0ed93d8501c19f8f6a13f0f21,1,[D2],,1,1,0,Real-Time Person Re-identification at the Edge: A Mixed Precision Approach,"A critical part of multi-person multi-camera tracking is person re-identification (re-ID) algorithm, which recognizes and retains identities of all detected unknown people throughout the video stream. Many re-ID algorithms today exemplify state of the art results, but not much work has been done to explore the deployment of such algorithms for computation and power constrained real-time scenarios. In this paper, we study the effect of using a light-weight model, MobileNet-v2 for re-ID and investigate the impact of single (FP32) precision versus half (FP16) precision for training on the server and inference on the edge nodes. We further compare the results with the baseline model which uses ResNet-50 on state of the art benchmarks including CUHK03, Market-1501, and Duke-MTMC. The MobileNet-V2 mixed precision training method can improve both inference throughput on the edge node, and training time on server 3.25\(\times \) reaching to 27.77 fps and 1.75\(\times \), respectively and decreases power consumption on the edge node by 1.45\(\times \), while it deteriorates accuracy only 5.6% in respect to ResNet-50 single precision on the average for three different datasets. The code and pre-trained networks are publicly available. (https://github.com/TeCSAR-UNCC/person-reid)",2019,ICIAR,1908.07842,10.1007/978-3-030-27272-2_3,https://arxiv.org/pdf/1908.07842.pdf
83f803f455c3fb321ebf0046b1c94c4a69fe56d5,0,,,0,1,0,Cross-modality paired-images generation and augmentation for RGB-infrared person re-identification,"RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. Considering no correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing marginal distribution divergence between the entire RGB and IR sets. However, this set-level alignment strategy may lead to misalignment of some instances, which limit the performance for RGB-IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross-modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged features. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Third, our method learns a latent manifold space. In the space, we can random sample and generate lots of images of unseen classes. Training with those images, the learned identity feature space is more smooth can generalize better when test. Finally, extensive experimental results on two standard benchmarks demonstrate that the proposed model favorably against state-of-the-art methods.",2020,Neural Networks,,10.1016/j.neunet.2020.05.008,
845fbe3e7d750147717ee6e2411eba5b88c57ecc,0,,,0,1,0,Fast and Accurate Person Re-identification with Xception Conv-Net and C2F,"Person re-identification (re-id) is the task of identifying a person of interest across disjoint camera views in a multi-camera system. This is a challenging problem due to the different poses, viewpoints and lighting conditions. Deeply learned systems have become prevalent in the person re-identification field as they are capable to deal with the these obstacles. Conv-Net using a coarse-to-fine search framework (Conv-Net+C2F) is such a deeply learned system, which has been developed with both a high-retrieval accuracy as a fast query time in mind. We propose three contributions to improve Conv-Net+C2F: (1) training with an improved optimizer, (2) constructing Conv-Net using a different Convolutional Neural Network (CNN) not yet used for person re-id and (3) coarse descriptors having fewer dimensions for improved speed as well as increased accuracy. With these adaptations Xception Conv-Net+C2F achieves state-of-the-art results on Market-1501 (single-query, 72.4% mAP) and the new, challenging data split of CUHK03 (detected, 42.6% mAP).",2018,CIARP,,10.1007/978-3-030-13469-3_71,
850d2697fec9c2bb434907db1c0a11e200f32dbe,1,[D2],,0,1,0,Improving Person Re-identification by Body Parts Segmentation Generated by GAN,"Person re-identification(ReID) is a task of associating persons that cross the non-overlapping camera views at different locations and times. It is a challenging task due to the large variations in person pose, background, luminance, occlusion, low resolution, etc. How to extracting a powerful features representation is the prime problem in ReID and is still unsolved. In this paper, we propose a cascade network architecture combined with a generative adversarial networks(GANs) and a convolutional neural network(CNN) to improve the performance of person re-identification. The GANs first generates the person body parts segmentation from the person image, and then inputs the segmentation label into the connected CNN together with the original person image. Finally obtain a discriminative and robust feature representation for ReID task. The body parts segmentation partitioning the person image into multiple segments, such as background, head, face, arms, lags, etc. The body parts segmentation information contains accurate borders and category attributes for body parts, which makes the our model more accurate compared to other predefined rigid parts alignment models. Experiments are conduced on the CUHK03, Market1501, DukeMTMC-ReID datasets and the results demonstrate that this approach outperforms several existing state-of-the-art methods.",2018,2018 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2018.8489450,
85fca9771eb00e55c190885a580b0fef078f9deb,0,,,0,1,0,Tasks Integrated Networks: Joint Detection and Retrieval for Image Search,"The traditional object (person) retrieval (re-identification) task aims to learn a discriminative feature representation or metric on the cropped objects. However, in many real-world scenarios, the objects are seldom accurately annotated. Therefore, object-level retrieval becomes intractable without annotation, which leads to a new but challenging topic, i.e. image search with joint detection and retrieval. To address the image search issue, we introduce an end-to-end Integrated Net, which has four merits: 1) A Siamese architecture and an on-line pairing strategy for similar and dissimilar objects in the given images are designed. 2) A novel on-line pairing (OLP) loss is introduced with a dynamic feature dictionary, which alleviates the multi-task training stagnation problem, by automatically generating a number of negative pairs to restrict the positives. 3) Two modules are tailored to handle different tasks separately in the integrated framework, such that the task specification is guaranteed. 4) A class-center guided HEP loss (C2HEP) by exploiting the stored class centers is proposed, such that the intra-similarity and inter-dissimilarity can be captured. Extensive experiments on the CUHK-SYSU and PRW datasets for person search and the large-scale WebTattoo dataset for tattoo search, demonstrate that the proposed model outperforms the state-of-the-art image search models.",2020,IEEE transactions on pattern analysis and machine intelligence,,10.1109/tpami.2020.3009758,
863de3191fe66fed09cef82b3ce03f122e8ae1b4,1,[D2],,1,0,0,Collaborative Attention Network for Person Re-identification,"Jointly utilizing global and local features to improve model accuracy is becoming a popular approach for the person re-identification (ReID) problem, because previous works using global features alone have very limited capacity at extracting discriminative local patterns in the obtained feature representation. Existing works that attempt to collect local patterns either explicitly slice the global feature into several local pieces in a handcrafted way, or apply the attention mechanism to implicitly infer the importance of different local regions. In this paper, we show that by explicitly learning the importance of small local parts and part combinations, we can further improve the final feature representation for Re-ID. Specifically, we first separate the global feature into multiple local slices at different scale with a proposed multi-branch structure. Then we introduce the Collaborative Attention Network (CAN) to automatically learn the combination of features from adjacent slices. In this way, the combination keeps the intrinsic relation between adjacent features across local regions and scales, without losing information by partitioning the global features. Experiment results on several widely-used public datasets including Market-1501, DukeMTMC-ReID and CUHK03 prove that the proposed method outperforms many existing state-of-the-art methods.",2019,ArXiv,1911.13008,,https://arxiv.org/pdf/1911.13008.pdf
865113a6d44623aeda656d97f0d24f6cad3a186e,0,,,0,1,0,Mining Hard Negative Samples for SAR-Optical Image Matching Using Generative Adversarial Networks,"In this paper, we propose a generative framework to produce similar yet novel samples for a specified image. We then propose the use of these images as hard-negatives samples, within the framework of hard-negative mining, in order to improve the performance of classification networks in applications which suffer from sparse labelled training data. Our approach makes use of a variational autoencoder (VAE) which is trained in an adversarial manner in order to learn a latent distribution of the training data, as well as to be able to generate realistic, high quality image patches. We evaluate our proposed generative approach to hard-negative mining on a synthetic aperture radar (SAR) and optical image matching task. Using an existing SAR-optical matching network as the basis for our investigation, we compare the performance of the matching network trained using our approach to the baseline method, as well as to two other hard-negative mining methods. Our proposed generative architecture is able to generate realistic, very high resolution (VHR) SAR image patches which are almost indistinguishable from real imagery. Furthermore, using the patches as hard-negative samples, we are able to improve the overall accuracy, and significantly decrease the 0 positive rate of the SAR-optical matching task—thus validating our generative hard-negative mining approaches’ applicability to improve training in data sparse applications.",2018,Remote. Sens.,,10.3390/rs10101552,https://pdfs.semanticscholar.org/8651/13a6d44623aeda656d97f0d24f6cad3a186e.pdf
86ead1d2ff0153f148cafa2e36419efcd15552e6,1,,1,0,1,0,Global Based Deep Refineing Model For Person Retrieval,"The performance of traditional part model in person retrieval is greatly affected by the quality of parts. The recent work[1] consider refining the hard partitioned part when training the network itself and got state-of-the-art performance, but during our experimentation, we found the masks which it generated contains the problems that misguide the networks with additional constraints, Targeting to solve above problem, we proposed a new networks called Global Refine Net. The backbone network focus on learning the local information which improve the ability of extract feature of details, Global Refine block introduce global information to adjust the hard-shaped part generated by the backbone network in an end-to-end manner. Also we modified the self-adversarial training mechanism in [1]. We employ an special loss function to prevent the incorrect convergence and adjust the degree of self-adversarial training, the new regularization term we added in the loss benefit both in stabilizing and speeding the training process. The performance of our model beat most previous soft partitioned works, improved about 2.3% rank-1 accuracy and 5.1% mAP to the PCB baseline on market-1501 dataset.",2018,"2018 10th International Conference on Communications, Circuits and Systems (ICCCAS)",,10.1109/ICCCAS.2018.8769188,
87268a5407d4b6b42e7cc66f4b676187d5f502ac,1,[D2],,0,1,0,View-specific subspace learning and re-ranking for semi-supervised person re-identification,"Abstract Person re-identification (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace Learning (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each other’s ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasets (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107568,
8928371206f313d409eeb5242d646a8e71061d90,1,[D1],,1,0,0,"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects","We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for image sequences. It can reliably discover and track objects through the sequence; it can also conditionally generate future frames, thereby simulating expected motion of objects. This is achieved by explicitly encoding object numbers, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al. 2016), including unsupervised learning, made possible by inductive biases present in the model structure. We use a moving multi-\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how \textsc{sqair} overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.",2018,NeurIPS,1806.01794,,https://arxiv.org/pdf/1806.01794.pdf
8992a5459fe8c682b6023e3324e3c621c0699720,1,[D2],,1,1,0,Learning Deep Representations by Mutual Information for Person Re-identification,"Most existing person re-identification (ReID) methods have good feature representations to distinguish pedestrians with deep convolutional neural network (CNN) and metric learning methods. However, these works concentrate on the similarity between encoder output and ground-truth, ignoring the correlation between input and encoder output, which affects the performance of identifying different pedestrians. To address this limitation, We design a Deep InfoMax (DIM) network to maximize the mutual information (MI) between the input image and encoder output, which doesn't need any auxiliary labels. To evaluate the effectiveness of the DIM network, we propose end-to-end Global-DIM and Local-DIM models. Additionally, the DIM network provides a new solution for cross-dataset unsupervised ReID issue as it needs no extra labels. The experiments prove the superiority of MI theory on the ReID issue, which achieves the state-of-the-art results.",2019,ArXiv,1908.0586,,https://arxiv.org/pdf/1908.05860.pdf
8aad3cd389462e1d9b1771d074fa39e293861ea6,1,[D2],,0,1,0,Deep manifold clustering based optimal pseudo pose representation (DMC-OPPR) for unsupervised person re-identification,"Abstract Person re-identification (re-ID) is highly complex in a diverse surveillance environment. The existing person re-ID methods are evaluated as a closed set problem with limited environmental variation. It is highly challenging to estimate the diverse poses of a dynamically crowded environment using the traditional unsupervised person re-ID methods. To resolve this issue of handling complex diverse poses and camera angles, a contextual incremental multi-clustering based unsupervised person re-ID method have been proposed. Cam-pose based optimal similarity distance threshold is determined to label the unlabeled person re-ID images efficiently. Frequent intra and inter-camera pseudo pose sequences are represented with optimal distance threshold. This resolves the over-fitting issue created by the dominant samples of an identity and reduces the source-target domain gap. The experimental results show the supremacy of our proposed method over the existing unsupervised person re-ID methods in handling complex poses and camera angles in an incremental self-learning diverse surveillance environment.",2020,Image Vis. Comput.,,10.1016/j.imavis.2020.103956,
903d9fabec4ce5d1626ce0dedb26f401ecd8ca82,1,[D3],,1,1,1,GAN-Based Pose-Aware Regulation for Video-Based Person Re-Identification,"Video-based person re-identification deals with the inherent difficulty of matching sequences with different length, unregulated, and incomplete target pose/viewpoint structure. Common approaches operate either by reducing the problem to the still images case, facing a significant information loss, or by exploiting inter-sequence temporal dependencies as in Siamese Recurrent Neural Networks or in gait analysis. However, in all cases, the inter-sequences pose/viewpoint misalignment is considered, and the existing spatial approaches are mostly limited to the still images context. To this end, we propose a novel approach that can exploit more effectively the rich video information, by accounting for the role that the changing pose/viewpoint factor plays in the sequences matching process. In particular, our approach consists of two components. The first one attempts to complement the original pose-incomplete information carried by the sequences with synthetic GAN-generated images, and fuse their features vectors into a more discriminative viewpoint-insensitive embedding, namely Weighted Fusion (WF). Another one performs an explicit pose-based alignment of sequence pairs to promote coherent feature matching, namely Weighted-Pose Regulation (WPR). Extensive experiments on two large video-based benchmark datasets show that our approach outperforms considerably existing methods.",2019,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),1903.11552,10.1109/WACV.2019.00130,https://pureadmin.qub.ac.uk/ws/files/161094826/618.pdf
921a2dcb2298a434d301189d69237e7e94c3ceb4,0,,,0,1,0,Adversarial Generation of Training Examples for Vehicle License Plate Recognition,"Generative Adversarial Networks (GAN) have attracted much research attention recently, leading to impressive results for natural image generation. However, to date little success was observed in using GAN generated images for improving classification tasks. Here we attempt to explore, in the context of car license plate recognition, whether it is possible to generate synthetic training data using GAN to improve recognition accuracy. With a carefully-designed pipeline, we show that the answer is affirmative. First, a large-scale image set is generated using the generator of GAN, without manual annotation. Then, these images are fed to a deep convolutional neural network (DCNN) followed by a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM), which performs the feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experimental results on a few data sets demonstrate the effectiveness of using GAN images: an improvement of 7.5% over a strong baseline with moderate-sized real data being available. We show that the proposed framework achieves competitive recognition accuracy on challenging test datasets. We also leverage the depthwise separate convolution to construct a lightweight convolutional RNN, which is about half size and 2x faster on CPU. Combining this framework and the proposed pipeline, we make progress in performing accurate recognition on mobile and embedded devices.",2017,ArXiv,1707.03124,,https://arxiv.org/pdf/1707.03124.pdf
92a1105d68bf1edee44fe1ea78b47f9e2b692a3c,1,[D2],,1,1,0,Multi-level feature learning with attention for person re-identification,"Person re-identification (re-ID) aims to match a specific person in a large gallery with different cameras and locations. Previous part-based methods mainly focus on part-level features with uniform partition, which increases learning ability for discriminative feature but not efficient or robust to scenarios with large variances. To address this problem, in this paper, we propose a novel feature fusion strategy based on traditional convolutional neural network. Then, a multi-branch deeper feature fusion network architecture is designed to perform discriminative learning for three semantically aligned region. Based on it, a novel self-attention mechanism is employed to softly assign corresponding weights to the semantic aligned feature during back-propagation. Comprehensive experiments have been conducted on several large-scale benchmark datasets, which demonstrates that proposed approach yields consistent and competitive re-ID accuracy compared with current single-domain re-ID methods.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09569-z,
947b868aa1c38940df280ebeb8077d4e729fb988,0,,,1,0,0,"The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose","The availability of a large labeled dataset is a key requirement for applying deep learning methods to solve various computer vision tasks. In the context of understanding human activities, existing public datasets, while large in size, are often limited to a single RGB camera and provide only per-frame or per-clip action annotations. To enable richer analysis and understanding of human activities, we introduce IKEA ASM---a three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. Additionally, we benchmark prominent methods for video action recognition, object segmentation and human pose estimation tasks on this challenging dataset. The dataset enables the development of holistic methods, which integrate multi-modal and multi-view data to better perform on these tasks.",2020,ArXiv,2007.00394,,https://arxiv.org/pdf/2007.00394.pdf
952a8ef56f35376a52e4540d3df9e48f4077b09e,0,,,0,1,0,Deep feature embedding learning for person re-identification based on lifted structured loss,"Person re-identification (re-id) aims at matching the same individual in videos captured by multiple cameras, and much progress has been made in recent years due to large scale pedestrian data sets and deep learning-based techniques. In this paper, we propose deep feature embedding learning for person re-id based on lifted structured loss. Triplet loss is commonly used in deep neural networks for person re-id. However, the triplet loss-based framework is not able to make full use of the batch information, and thus needs to choose hard negative samples manually that is time-consuming. To address this problem, we adopt lifted structured loss for deep neural networks that makes the network learn better feature embedding by minimizing intra-class variation and maximizing inter-class variation. Extensive experiments on Market-1501, CUHK03, CUHK01 and VIPeR data sets demonstrate the superior performance of the proposed method over state-of-the-arts in terms of the cumulative match curve (CMC) metric.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-6408-4,
95549a8692f734b978f1177c76242e074d52e67a,0,,,0,1,0,Matching Adversarial Networks,"Generative Adversarial Nets (GANs) and Conditonal GANs (CGANs) show that using a trained network as loss function (discriminator) enables to synthesize highly structured outputs (e.g. natural images). However, applying a discriminator network as a universal loss function for common supervised tasks (e.g. semantic segmentation, line detection, depth estimation) is considerably less successful. We argue that the main difficulty of applying CGANs to supervised tasks is that the generator training consists of optimizing a loss function that does not depend directly on the ground truth labels. To overcome this, we propose to replace the discriminator with a matching network taking into account both the ground truth outputs as well as the generated examples. As a consequence, the generator loss function also depends on the targets of the training examples, thus facilitating learning. We demonstrate on three computer vision tasks that this approach can significantly outperform CGANs achieving comparable or superior results to task-specific solutions and results in stable training. Importantly, this is a general approach that does not require the use of task-specific loss functions.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00837,http://openaccess.thecvf.com/content_cvpr_2018/papers/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.pdf
957d185f5f7f810734adff507a623d5bca5a0c1c,0,,,1,0,0,Conditional GAN based individual and global motion fusion for multiple object tracking in UAV videos,"Abstract Multiple Object Tracking (MOT) meets great challenges in videos captured by Unmanned Aerial Vehicles (UAVs). Different from traditional videos, due to high altitude and abrupt motion changes of UAVs, the sizes of target objects in UAVs videos are usually very small and the appearance information of target objects is unreliable. The motion analysis is meaningful to associate multiple objects in UAV videos. However, the traditional motion analysis models inevitably suffer from the autonomous motion of UAVs. In this paper, we proposed a Conditional Generative Adversarial Networks (GAN) based model to predict complex motions in UAV videos. We regard the objects motions and the UAV movement as the individual motions and global motions respectively. They are complementary with each other and are employed jointly to facilitate accurate motion prediction. Specifically, a social Long Short Term Memory network is exploited to estimate the individual motion of objects, and a Siamese network is constructed to generate the global motion to reflect the view changes from UAVs, and a conditional GAN is developed to generate the final motion affinity. Extensive experimental results are conducted on public UAV datasets contained various types of objects and 4 different kinds of object detection inputs. Robust motion prediction and improved MOT performance are achieved compared with state-of-the-art methods.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2019.12.018,
982cb4421cedce057ae2fc864efac8e43d9c0a5a,0,,,1,0,0,TAO: A Large-Scale Benchmark for Tracking Any Object,"For many years, multi-object tracking benchmarks have focused on a handful of categories. Motivated primarily by surveillance and self-driving applications, these datasets provide tracks for people, vehicles, and animals, ignoring the vast majority of objects in the world. By contrast, in the related field of object detection, the introduction of large-scale, diverse datasets (e.g., COCO) have fostered significant progress in developing highly robust solutions. To bridge this gap, we introduce a similarly diverse dataset for Tracking Any Object (TAO). It consists of 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. Importantly, we adopt a bottom-up approach for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. To this end, we ask annotators to label objects that move at any point in the video, and give names to them post factum. Our vocabulary is both significantly larger and qualitatively different from existing tracking datasets. To ensure scalability of annotation, we employ a federated approach that focuses manual effort on labeling tracks for those relevant objects in a video (e.g., those that move). We perform an extensive evaluation of state-of-the-art trackers and make a number of important discoveries regarding large-vocabulary tracking in an open-world. In particular, we show that existing single- and multi-object trackers struggle when applied to this scenario in the wild, and that detection-based, multi-object trackers are in fact competitive with user-initialized ones. We hope that our dataset and analysis will boost further progress in the tracking community.",2020,ECCV,2005.10356,10.1007/978-3-030-58558-7_26,https://arxiv.org/pdf/2005.10356.pdf
98bda8768fd4a384695ecc736876a87f51c4ca0e,0,,,0,1,0,Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond,"State-of-the-art pedestrian detection models have achieved great success in many benchmarks. However, these models require lots of annotation information and the labeling process usually takes much time and efforts. In this paper, we propose a method to generate labeled pedestrian data and adapt them to support the training of pedestrian detectors. The proposed framework is built on the Generative Adversarial Network (GAN) with multiple discriminators, trying to synthesize realistic pedestrians and learn the background context simultaneously. To handle the pedestrians of different sizes, we adopt the Spatial Pyramid Pooling (SPP) layer in the discriminator. We conduct experiments on two benchmarks. The results show that our framework can smoothly synthesize pedestrians on background images of variations and different levels of details. To quantitatively evaluate our approach, we add the generated samples into training data of the baseline pedestrian detectors and show the synthetic images are able to improve the detectors' performance.",2018,ArXiv,1804.02047,,https://arxiv.org/pdf/1804.02047.pdf
9ad5f32b98c8b8d25813998d1ba7dcae2fde2b62,1,[D2],,0,1,0,An Efficient Person Re-Identification Model Based on New Regularization Technique,"The aim of person re-identification (ReID) is to recognize the same persons across different scenes. Due to the many demanding applications that utilize large-scale data, more and more attention has been devoted to matching efficiency and accuracy. Many methods that are based on binary coding have been presented to reach efficient ReID. Those methods learn projections to map the high-dimensional features into deep neural networks or compact binary codes through simple insertion of an extra fully connected layer with tanh-like activation. Nevertheless, the former approach needs hand-crafted feature extraction that also wastes a lot of time and complex (discrete) optimizations. In contrast, the latter approach lacks the essential discriminative information to a large extent because of the straightforward activation functions. A ReID framework is proposed in the current work, and it is inspired by the adversarial framework depending on the new regularization approach (ABC-NReg). We embedded the discriminative network into adversarial binary coding (ABC) with our new regularization, which improved the discriminative power combined with the triplet network. ABC-NReg and triplet networks were optimized, and three large-scale benchmark datasets, namely CUHK03, Market-1501, and DukeMTMC-reID datasets, were utilized to test the performance of our proposed model. We further compared the simulation results with the present hashing and non-hashing algorithms. Our model provided better results than other present models using the Market-1501 and DukeMTMC-reID datasets when considering Rank-1. For CUHK03 dataset, the proposed model exceeded the performance of other works when considering Rank 5 and Rank 20.",2020,IEEE Access,,10.1109/ACCESS.2020.3024120,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195809.pdf
9bc62e06f414e239f461ee0ee9318e5bbfe2e33a,0,,,0,1,0,From person to group re-identification via unsupervised transfer of sparse features,"Abstract The visual association of a person appearing in the field of view of different cameras is today well known as Person Re-Identification. Current approaches find a solution to such a problem by considering persons as individuals, hence avoiding the fact that frequently they form groups or move in crowds. In such cases, the information acquired by neighboring individuals can provide relevant visual context to boost the performance in re-identifying persons within the group. In light of enriched information, groups re-identification encompasses additional problems to the common person re-identification ones, such as severe occlusions and changes in the relative position of people within the group. In this paper, the single person re-identification knowledge is transferred by means of a sparse dictionary learning to group re-identification. First, patches extracted from single person images are used to learn a dictionary of sparse atoms. This is used to obtain a sparsity-driven residual group representation that is exploited to perform group re-identification. To evaluate the performance of the proposed approach, we considered the i-LIDS groups dataset that is the only group re-identification publicly available dataset. The benchmark datasets for single person re-identification evaluation do not include group information, hence we collected two additional datasets under challenging scenarios and used them to validate our solution.",2019,Image Vis. Comput.,,10.1016/J.IMAVIS.2019.02.009,
9c364f90dbd09e5366c2d492cd12f989bf8588e6,1,[D2],,0,1,0,Style Normalization and Restitution for Generalizable Person Re-Identification,"Existing fully-supervised person re-identification (ReID) methods usually suffer from poor generalization capability caused by domain gaps. The key to solving this problem lies in filtering out identity-irrelevant interference and learning domain-invariant person representations. In this paper, we aim to design a generalizable person ReID framework which trains a model on source domains yet is able to generalize/perform well on target domains. To achieve this goal, we propose a simple yet effective Style Normalization and Restitution (SNR) module. Specifically, we filter out style variations (e.g., illumination, color contrast) by Instance Normalization (IN). However, such a process inevitably removes discriminative information. We propose to distill identity-relevant feature from the removed information and restitute it to the network to ensure high discrimination. For better disentanglement, we enforce a dual causal loss constraint in SNR to encourage the separation of identity-relevant features and identity-irrelevant features. Extensive experiments demonstrate the strong generalization capability of our framework. Our models empowered by the SNR modules significantly outperform the state-of-the-art domain generalization approaches on multiple widely-used person ReID benchmarks, and also show superiority on unsupervised domain adaptation.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2005.11037,10.1109/cvpr42600.2020.00321,https://arxiv.org/pdf/2005.11037.pdf
9d117f1f1c3a787a61667573bcc94671a767d1c4,1,[D2],,0,1,0,Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification,"Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.06052,10.1109/ICCV.2019.00818,https://arxiv.org/pdf/1908.06052.pdf
9e44cfd8832acc2c1d7753733bb92b716bcdce2d,1,[D2],,1,1,0,Top-DB-Net: Top DropBlock for Activation Enhancement in Person Re-Identification,"Person Re-Identification is a challenging task that aims to retrieve all instances of a query image across a system of non-overlapping cameras. Due to the various extreme changes of view, it is common that local regions that could be used to match people are suppressed, which leads to a scenario where approaches have to evaluate the similarity of images based on less informative regions. In this work, we introduce the Top-DB-Net, a method based on Top DropBlock that pushes the network to learn to focus on the scene foreground, with special emphasis on the most task-relevant regions and, at the same time, encodes low informative regions to provide high discriminability. The Top-DB-Net is composed of three streams: (i) a global stream encodes rich image information from a backbone, (ii) the Top DropBlock stream encourages the backbone to encode low informative regions with high discriminative features, and (iii) a regularization stream helps to deal with the noise created by the dropping process of the second stream, when testing the first two streams are used. Vast experiments on three challenging datasets show the capabilities of our approach against state-of-the-art methods. Qualitative results demonstrate that our method exhibits better activation maps focusing on reliable parts of the input images.",2020,ArXiv,2010.05435,,https://arxiv.org/pdf/2010.05435.pdf
9e99f02d153728a8bcad2dbe8f60dad79a457154,1,[D2],,0,1,0,Surpassing Real-World Source Training Data: Random 3D Characters for Generalizable Person Re-Identification,"Person re-identification has seen significant advancement in recent years. However, the ability of learned models to generalize to unknown target domains still remains limited. One possible reason for this is the lack of large-scale and diverse source training data, since manually labeling such a dataset is very expensive and privacy sensitive. To address this, we propose to automatically synthesize a large-scale person re-identification dataset following a set-up similar to real surveillance but with virtual environments, and then use the synthesized person images to train a generalizable person re-identification model. Specifically, we design a method to generate a large number of random UV texture maps and use them to create different 3D clothing models. Then, an automatic code is developed to randomly generate various different 3D characters with diverse clothes, races and attributes. Next, we simulate a number of different virtual environments using Unity3D, with customized camera networks similar to real surveillance systems, and import multiple 3D characters at the same time, with various movements and interactions along different paths through the camera networks. As a result, we obtain a virtual dataset, called RandPerson, with 1,801,816 person images of 8,000 identities. By training person re-identification models on these synthesized person images, we demonstrate, for the first time, that models trained on virtual data can generalize well to unseen target images, surpassing the models trained on various real-world datasets, including CUHK03, Market-1501, DukeMTMC-reID, and almost MSMT17. The RandPerson dataset is available at https://github.com/VideoObjectSearch/RandPerson.",2020,ACM Multimedia,2006.12774,10.1145/3394171.3413815,https://arxiv.org/pdf/2006.12774.pdf
9f0251cc11c2be6216560646d9155bed6134e7e8,1,[D2],,0,1,0,Metric Attack and Defense for Person Re-identification,"Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance.  Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Meanwhile, we also present an early attempt of training a metric-preserving network, thereby defending the metric against adversarial attacks. At last, by benchmarking various adversarial settings, we expect that our work can facilitate the development of adversarial attack and defense in metric-based applications.",2019,,1901.1065,,https://arxiv.org/pdf/1901.10650.pdf
a213bbf9740854a276fdf71dad8f30cfbe3ea4d4,0,,,1,0,0,The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking,"With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision, delivering more efficiency and convenience than surveillance cameras with fixed camera angle, scale and view. However, very limited UAV datasets are proposed, and they focus only on a specific task such as visual tracking or object detection in relatively constrained scenarios. Consequently, it is of great importance to develop an unconstrained UAV benchmark to boost related researches. In this paper, we construct a new UAV benchmark focusing on complex scenarios with new level challenges. Selected from 10 hours raw videos, about 80,000 representative frames are fully annotated with bounding boxes as well as up to 14 kinds of attributes (e.g., weather condition, flying altitude, camera view, vehicle category, and occlusion) for three fundamental computer vision tasks: object detection, single object tracking, and multiple object tracking. Then, a detailed quantitative study is performed using most recent state-of-the-art algorithms for each task. Experimental results show that the current state-of-the-art methods perform relative worse on our dataset, due to the new challenges appeared in UAV based real scenes, e.g., high density, small object, and camera motion. To our knowledge, our work is the first time to explore such issues in unconstrained scenes comprehensively.",2018,ECCV,1804.00518,10.1007/978-3-030-01249-6_23,https://arxiv.org/pdf/1804.00518.pdf
a440f939cc7cb430a4bb0b2d84f8f59793bf3df0,1,"[D2], [D4]",,1,0,0,In-depth exploration of attribute information for person re-identification,"Pedestrian’s attribute information plays an important role in person re-identification (re-ID) for its complementary to pedestrian’s identity labels. However, there are few methods to utilize attribute information, which limits the development of re-ID community. In this paper, we analyze the effect of attribute information on re-ID to obtain both qualitative and quantitative results, indicating the potential for in-depth exploration of attribute information. On this basis, we propose an Identity Recognition Network (IRN) and an Attribute Recognition Network (ARN). IRN enhances the attention to pedestrian’s local information while identifying pedestrians’ identity. ARN calculates the attribute similarity among pedestrians accurately to promote the identification of IRN. The combination of them makes deep exploration of attribute information and is easy to implement. The experimental results on two large-scale re-ID benchmarks demonstrate the effectiveness of our method, which is on par with the state-of-the-art. In the DukeMTMC-reID dataset, mAP (rank-1) accuracy is improved from 58.4 (78.3) % to 66.4 (82.7) % for ResNet-50. In the Market1501 dataset, mAP (rank-1) accuracy is improved from 75.8 (90.5) % to 79.5 (92.8) % for ResNet-50.",2020,Applied Intelligence,,10.1007/s10489-020-01752-x,
a4958d26a3aaf77081fcb9b76393a30bf722e92e,1,[D2],,1,1,0,A Pose-Sensitive Embedding for Person Re-identification with Expanded Cross Neighborhood Re-ranking,"Person re-identification is a challenging retrieval task that requires matching a person's acquired image across non-overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discriminative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets. Code is available at https://github.com/pse-ecn.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.10378,10.1109/CVPR.2018.00051,https://arxiv.org/pdf/1711.10378.pdf
a502bee13c5eb7338efca474de049d0c98f989d1,0,,,0,1,0,Multi-task Learning with Bidirectional Language Models for Text Classification,"Multi-task learning is an effective approach to extract task-invariant features by leveraging potential information among related tasks, which improves the performance of a single task. Most existing work simply divides the whole model into shared and private spaces. Unfortunately, there is no explicit mechanism to prevent the two spaces from merging information from each other. As a result, the shared space may be mixed with task-specific features, while the private space may extract some task-invariant features. To alleviate the problem mentioned, in this paper, we propose a bidirectional language models based multi-task learning method for text classification. More specifically, we add language modelling as an auxiliary task to the private part, aiming to enhance its ability to extract task-specific features. In addition, to promote the shared part to learn common features, a loss constraint via uniform label distribution is introduced to the shared part. Finally, put task-specific features and taskinvariant features together in a weighted addition way to form the final representation, and it is then fed to the corresponding softmax layer. We do experiments on the FDU-MTL dataset which consists of 16 different text classification tasks. The experimental results show that our approach outperforms other typical methods.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852388,
a6a6211fa3c4f61f8d5941e89561be0c351e8648,1,[D2],,0,1,0,Contour-Guided Person Re-identification,"Feature representation is one of the crucial components in person re-identification(re-ID). Recently, local feature has attracted great attention from the re-ID community, and extra visual cues have been well exploited to guide local feature learning, such as pose cues, semantic parsing and etc. Besides, the latest research demonstrates that general CNN-based deep models have a bias to texture feature in pattern recognition, but ignore shape-based feature, which has been verified as significant for cross-domain invariance. As far as we know, there is little work focusing on shape-based feature on person re-ID. In this paper, we introduce a new data modality, pedestrian contour, into the re-ID community, which to our best knowledge is the first attempt to utilize contour explicitly in deep re-ID models. We hypothesize that, as an alternative of other exploited visual cues, pedestrian contour could guide deep models to learn robust shape-based feature, with build-in prior information. We propose several contour-guided architectures to explicitly use pedestrian contour, including plain ones and multi-scale one. Extensive experiments have validated the effectiveness of our models. Moreover, we transfer the methodology into a powerful part-based model, Part-based Convolutional Baseline(PCB), and boost the model performance, which verifies the promising prospect of contour-guided models to expand as an auxiliary mechanism in re-ID.",2019,PRCV,,10.1007/978-3-030-31726-3_25,
a72ad9ef3517a59df740ef098f110f76e06fd4e8,1,,1,1,0,0,A General Re-Ranking Method Based On Metric Learning For Person Re-Identification,"When Person Re-identification is considered as a retrieval task, re-ranking becomes a critical part of improving the re-identification accuracy. Most of the existing re-ranking methods focus on k -nearest neighbors, which requires a lot of queries and memory. In this paper, we propose a Feature Relation Map based Similarity Evaluation (FRM-SE) model to tackle this problem. The Feature Relation Map is utilized to automatically mine the latent relation between the k -neighbors through convolution operation. The re-ranking distance is learned through the FRM-SE model with metric learning. Further, we optimize the existing re-ranking method to utilize the advantage of the FRM-SE model for maintaining a balance between accuracy and complexity.The proposed approach is validated on two benchmark datasets, Market1501 and CUHK03. Results show that our re-ranking method is superior to the state-of-the-art re-ranking methods. Furthermore, in the transfer learning setting, the model trained on either Market1501 or CUHK03 can achieve a comparable accuracy improvement on the DuekMTMC dataset, which validates the generalization of our SE model.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME46284.2020.9102887,
a72c97ea3908e27fb1ecba770c49cec8e12e9008,0,,,1,0,0,Data Association for Multi-Object Tracking via Deep Neural Networks,"With recent advances in object detection, the tracking-by-detection method has become mainstream for multi-object tracking in computer vision. The tracking-by-detection scheme necessarily has to resolve a problem of data association between existing tracks and newly received detections at each frame. In this paper, we propose a new deep neural network (DNN) architecture that can solve the data association problem with a variable number of both tracks and detections including 0 positives. The proposed network consists of two parts: encoder and decoder. The encoder is the fully connected network with several layers that take bounding boxes of both detection and track-history as inputs. The outputs of the encoder are sequentially fed into the decoder which is composed of the bi-directional Long Short-Term Memory (LSTM) networks with a projection layer. The final output of the proposed network is an association matrix that reflects matching scores between tracks and detections. To train the network, we generate training samples using the annotation of Stanford Drone Dataset (SDD). The experiment results show that the proposed network achieves considerably high recall and precision rate as the binary classifier for the assignment tasks. We apply our network to track multiple objects on real-world datasets and evaluate the tracking performance. The performance of our tracker outperforms previous works based on DNN and comparable to other state-of-the-art methods.",2019,Sensors,,10.3390/s19030559,https://pdfs.semanticscholar.org/a72c/97ea3908e27fb1ecba770c49cec8e12e9008.pdf
a743127b44397b7a017a65a7ad52d0d7ccb4db93,1,[D5],,1,0,0,Domain Adaptation through Synthesis for Unsupervised Person Re-identification,"Drastic variations in illumination across surveillance cameras make the person re-identification problem extremely challenging. Current large scale re-identification datasets have a significant number of training subjects, but lack diversity in lighting conditions. As a result, a trained model requires fine-tuning to become effective under an unseen illumination condition. To alleviate this problem, we introduce a new synthetic dataset that contains hundreds of illumination conditions. Specifically, we use 100 virtual humans illuminated with multiple HDR environment maps which accurately model realistic indoor and outdoor lighting. To achieve better accuracy in unseen illumination conditions we propose a novel domain adaptation technique that takes advantage of our synthetic data and performs fine-tuning in a completely unsupervised way. Our approach yields significantly higher accuracy than semi-supervised and unsupervised state-of-the-art methods, and is very competitive with supervised techniques.",2018,ECCV,1804.10094,10.1007/978-3-030-01261-8_12,https://arxiv.org/pdf/1804.10094.pdf
a7c6fe91d1b95bd2ca6648a24cad481b6bbdf890,1,[D2],,0,1,0,Adaptive weight part-based convolutional network for person re-identification,"While part-based methods have been shown effective in the person re-identification task, it is unreasonable for most of them to treat each part equally, due to the retrieved image may be affected by deformation, occlusion and other factors, which makes the feature information of some parts unreliable. Instead of using the same weight of each part for the final person re-ID, we consider using an adaptive weight based on the part image information for each part for precise person retrieval. Specifically, we aim at learning discriminative part-informed features and propose an adaptive weight part-based convolutional network (AWPCN) for the person re-ID task. The core component of our AWPCN framework is an adaptive weight model, in which the part-based convolutional network and the adaptive weight model are used for feature refinement and feature-pair alignment, respectively. Given an image input at first, it outputs a convolutional descriptor consisting of several part-level features by the part-based convolutional network. And then, the corresponding weights of each part are determined by the adaptive weight model. Finally, we can use the adaptive weight part-based convolutional network joint to train each part loss and simultaneous optimization of its feature representations. We evaluate the proposed AWPCN model on Market-1501, DukeMTMC-reID and CUHK03 datasets. In extensive experiments, the AWPCN model outperforms most of the state-of-the-art methods on these representative datasets which clearly demonstrates the effectiveness of our proposed method. Our code will be released at https://github.com/deasonyuan/AWPCN.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09018-x,
a80d8506fa28334c947989ca153b70aafc63ac7f,1,[D2],,1,1,0,Pedestrian Retrieval via Part-Based Gradation Regularization in Sensor Networks,"In this paper, we propose a novel label distribution approach named part-based gradation regularization (PGR) for pedestrian retrieval in sensor networks. Considering different importance of various body parts, we present a gradual function to assign pedestrian label for each horizontal part. In this way, we can conduct part-based supervised learning using the identification network. The proposed PGR not only learns the discriminative local convolutional neural network-based features, but also considers the significance of assigning pedestrian label for different horizontal parts. Experimental results show that the proposed PGR obtains better performance than other approaches on three pedestrian retrieval databases, i.e., Market-1501, CUHK03, and DukeMTMC-reID databases.",2018,IEEE Access,,10.1109/ACCESS.2018.2854830,
aa70a72e135d03dd3ae40ff73c3e35fbc620e3d8,0,,,1,0,0,SAMOT: Switcher-Aware Multi-Object Tracking and Still Another MOT Measure,"Multi-Object Tracking (MOT) is a popular topic in computer vision. However, identity issue, i.e., an object is wrongly associated with another object of a different identity, still remains to be a challenging problem. To address it, switchers, i.e., confusing targets thatmay cause identity issues, should be focused. Based on this motivation,this paper proposes a novel switcher-aware framework for multi-object tracking, which consists of Spatial Conflict Graph model (SCG) and Switcher-Aware Association (SAA). The SCG eliminates spatial switch-ers within one frame by building a conflict graph and working out the optimal subgraph. The SAA utilizes additional information from potential temporal switcher across frames, enabling more accurate data association. Besides, we propose a new MOT evaluation measure, Still Another IDF score (SAIDF), aiming to focus more on identity issues.This new measure may overcome some problems of the previous measures and provide a better insight for identity issues in MOT. Finally,the proposed framework is tested under both the traditional measures and the new measure we proposed. Extensive experiments show that ourmethod achieves competitive results on all measure.",2020,ArXiv,2009.10338,,https://arxiv.org/pdf/2009.10338.pdf
af7bc853cfec0e31e6af6b673ee4dad1682a57fc,1,[D2],,0,1,0,Circulant Binary Convolutional Networks for Object Recognition,"The rapidly decreasing computation and memory cost has recently driven the success of many applications in the field of deep learning. Practical applications of deep learning in resource-limited hardware, such as embedded devices and smart phones, however, remain challenging. For binary convolutional networks, the reason lies in the degraded representation caused by binarizing full-precision filters. To address this problem, we propose new circulant filters (CiFs) and a circulant binary convolution (CBConv) to enhance the capacity of binarized convolutional features via our circulant back propagation (CBP). The CiFs can be easily incorporated into existing deep convolutional neural networks (DCNNs), which leads to new Circulant Binary Convolutional Networks (CBCNs). Extensive experiments confirm that the performance gap between the 1-bit and full-precision DCNNs is minimized by increasing the filter diversity, which further increases the representational ability in our networks. Our experiments on ImageNet show that CBCNs achieve 61.4% top-1 accuracy with ResNet18. Compared to the state-of-the-art such as XNOR, CBCNs can achieve up to 10% higher top-1 accuracy with more powerful representational ability. Also, CBCNs approximately achieve a storage reduction about 32 times. In particular, our method shows strong generalization on the object recognition task, i.e., face recognition, facial expression recognition and person re-identification.",2020,IEEE Journal of Selected Topics in Signal Processing,,10.1109/JSTSP.2020.2969516,
b17c685237bdf9c59d3ab31c43433c96923a168b,1,[D2],,1,0,0,Memory-Based Neighbourhood Embedding for Visual Recognition,"Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, i.e., how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.04992,10.1109/ICCV.2019.00620,https://arxiv.org/pdf/1908.04992.pdf
b22c08f71a8292d3848dde856839cd1f62d5fa86,1,[D2],,1,0,0,Deep Classification Consistency for Person Re-Identification,"Person re-identification (Re-ID) has greatly benefited from utilizing features of different levels. Most methods draw support from elegant network designs and elaborate fusion modules. In this paper, different from employing multi-level feature fusion like mainstream skip connection methods, we propose a Deep Classification Consistency(DCC) layer which imposes classification consistency on deep features from different stages in CNNs. DCC regularizes the training process of networks. It profoundly changes the distribution of learned features, making them more discriminative and transferable. Extensive experiments on Market-1501, DukeMTMC-reID, and CUHK03 datasets prove that the proposed method has obtained competitive performances when compared with state-of-the-art approaches, especially those advanced methods merely based on metric learning.",2020,IEEE Access,,10.1109/ACCESS.2020.3031935,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09229164.pdf
b23e760065eeac8ec4431b64bf18497ff78398e2,1,"[D2], [D8]",,1,1,0,Incomplete Descriptor Mining with Elastic Loss for Person Re-Identification,"In this paper, we propose a novel person Re-ID model, Consecutive Batch DropBlock Network (CBDB-Net), to help the person Re-ID model to capture the attentive and robust person descriptor. The CBDB-Net contains two novel modules: the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss. In the Consecutive Batch DropBlock Module (CBDBM), it firstly conducts uniform partition on the feature maps. And then, the CBDBM independently and continuously drops each patch from top to bottom on the feature maps, which outputs multiple incomplete features to push the model to capture the robust person descriptor. In the Elastic Loss, we design a novel weight control item to help the deep model adaptively balance hard sample pairs and easy sample pairs in the whole training process. Through an extensive set of ablation studies, we verify that the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss each contribute to the performance boosts of CBDB-Net. We demonstrate that our CBDB-Net can achieve the competitive performance on the three generic person Re-ID datasets (the Market-1501, the DukeMTMC-Re-ID, and the CUHK03 dataset), three occlusion Person Re-ID datasets (the Occluded DukeMTMC, the Partial-REID, and the Partial iLIDS dataset), and the other image retrieval dataset (In-Shop Clothes Retrieval dataset).",2020,,2008.0401,,https://arxiv.org/pdf/2008.04010.pdf
b2423a5a81f736de33945fa72ea9027b3178ccab,1,[D2],,0,1,0,Hdrnet: Person Re-Identification Using Hybrid Sampling in Deep Reconstruction Network,"Person re-identification (re-id) is the task of identifying a person across non-overlapping cameras. Most of the current techniques apply deep learning and achieve a significant accuracy. However, learning a deep model that can generalize well against the challenges of pose variation, occlusion, illumination changes, and low resolution is a difficult task. Toward this, we propose a deep reconstruction re-id network, comprising of an encoder and a multi-resolution decoder, which can learn embeddings invariant to pose, occlusion, illumination, and low resolution. In our model, the encoder acts as a conventional deep re-id network and outputs a discriminative feature embedding. The output feature is then used as an input to the multi-resolution decoder to reconstruct the input images of the same identity under different resolutions, such that they are similar in pose and illumination as well as free from occlusion. We further propose a hybrid sampling strategy to boost the effectiveness of the training loss function. In addition, we propose test set augmentation using the reconstructed images to explicitly transform single query to multi-query setting. In our multi-tasking approach, the feature robustness is enhanced by the multi-resolution decoder, and the overall accuracy is further improved by a sampling strategy and test data augmentation. Furthermore, we empirically show that the proposed network is robust to pose variations, occlusion, and low resolution. We perform rigorous qualitative and quantitative analysis in order to demonstrate that we achieve state-of-the-art person re-id accuracy.",2019,IEEE Access,,10.1109/ACCESS.2019.2908344,
b25aa5113b12bb7818da9e35c727457e84244f13,1,,1,1,0,0,"MVP Matching: A Maximum-Value Perfect Matching for Mining Hard Samples, With Application to Person Re-Identification","How to correctly stress hard samples in metric learning is critical for visual recognition tasks, especially in challenging person re-ID applications. Pedestrians across cameras with significant appearance variations are easily confused, which could bias the learned metric and slow down the convergence rate. In this paper, we propose a novel weighted complete bipartite graph based maximum-value perfect (MVP) matching for mining the hard samples from a batch of samples. It can emphasize the hard positive and negative sample pairs respectively, and thus relieve adverse optimization and sample imbalance problems. We then develop a new batch-wise MVP matching based loss objective and combine it in an end-to-end deep metric learning manner. It leads to significant improvements in both convergence rate and recognition performance. Extensive empirical results on five person re-ID benchmark datasets, i.e., Market-1501, CUHK03-Detected, CUHK03-Labeled, Duke-MTMC, and MSMT17, demonstrate the superiority of the proposed method. It can accelerate the convergence rate significantly while achieving state-of-the-art performance. The source code of our method is available at \url{https://github.com/IAAI-CVResearchGroup/MVP-metric}.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00684,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.pdf
b2ad0d45b8173961d410685dc40b20ace5e5a6be,1,[D2],,1,0,0,Progressive Learning Algorithm for Efficient Person Re-Identification,"This paper studies the problem of Person Re-Identification (ReID)for large-scale applications. Recent research efforts have been devoted to building complicated part models, which introduce considerably high computational cost and memory consumption, inhibiting its practicability in large-scale applications. This paper aims to develop a novel learning strategy to find efficient feature embeddings while maintaining the balance of accuracy and model complexity. More specifically, we find by enhancing the classical triplet loss together with cross-entropy loss, our method can explore the hard examples and build a discriminant feature embedding yet compact enough for large-scale applications. Our method is carried out progressively using Bayesian optimization, and we call it the Progressive Learning Algorithm (PLA). Extensive experiments on three large-scale datasets show that our PLA is comparable or better than the-state-of-the-arts. Especially, on the challenging Market-1501 dataset, we achieve Rank-1=94.7\%/mAP=89.4\% while saving at least 30\% parameters than strong part models.",2019,ArXiv,1912.07447,,https://arxiv.org/pdf/1912.07447.pdf
b2be518280db9f7cd1c73f88bcc7c53401c8ce03,1,[D2],,1,1,0,Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification,"Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins.",2020,ECCV,2007.10315,10.1007/978-3-030-58536-5_6,https://arxiv.org/pdf/2007.10315.pdf
b2d25f22a2add6350bf07044034a0694420be470,0,,,0,1,0,"Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11, 2019, Proceedings, Part I","We propose the channel feature enhanced detector (CFED) for ball detection, a challenging small object detection task. The proposed method achieves a good performance on small ball detection since we design a channel feature enhanced module to increase the discriminability of the target features. Moreover, we set up the BALL dataset for training and evaluation. Experimental results show that our method achieves a mAP of 90.2% on BALL dataset with an inference time of 11.4 milliseconds per image. The proposed lightweight network makes it possible to apply real-time detection to mobile devices.",2019,PRCV,,10.1007/978-3-030-31654-9,
b34b630a730a1ffee70fb260d1a08175707c099d,0,,,1,1,0,Taking A Closer Look at Synthesis: Fine-grained Attribute Analysis for Person Re-Identification,"Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has achieved remarkable performance. However, in pursuit of high accuracy, researchers in the academic always focus on training with large-scale datasets at a high cost of time and label expenses, while neglect to explore the potential of performing efficient training from millions of synthetic data. To facilitate development in this field, we reviewed the previously developed synthetic dataset GPR and built an improved one (GPR+) with larger number of identities and distinguished attributes. Based on it, we quantitatively analyze the influence of dataset attribute on re-ID system. To our best knowledge, we are among the first attempts to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. This research helps us have a deeper understanding of the fundamental problems in person re-ID, which also provides useful insights for dataset building and future practical usage.",2020,ArXiv,2010.08145,,https://arxiv.org/pdf/2010.08145.pdf
b36877dc7eac3b356e60d768bd624f8b9e867cec,1,[D2],,1,1,0,Illumination adaptive person reid based on teacher-student model and adversarial training,"Most existing works in Person Re-identification (ReID) focus on settings where illumination either is kept the same or has very little fluctuation. However, the changes in the illumination degree may affect the robustness of a ReID algorithm significantly. To address this problem, we proposed a Two-Stream Network that can separate ReID features from lighting features to enhance ReID performance. Its innovations are threefold: (1) A discriminative entropy loss to ensure the ReID features contain no lighting information. (2) A ReID Teacher model trained by images under ""neutral"" lighting conditions to guide ReID classification. (3) An illumination Teacher model trained by the differences between the illumination-adjusted and original images to guide illumination classification. We construct two augmented datasets by synthetically changing a set of predefined lighting conditions in two of the most popular ReID benchmarks: Market1501 and DukeMTMC-ReID. Experiments demonstrate that our algorithm outperforms other state-of-the-art works and particularly potent in handling images under extremely low light.",2020,ICIP,2002.01625,10.1109/icip40778.2020.9190796,https://arxiv.org/pdf/2002.01625.pdf
b3a45118534144f50a56653dac8109c73fc2c0e8,0,,,1,0,0,A Dataset for Persistent Multi-target Multi-camera Tracking in RGB-D,"Video surveillance systems are now widely deployed to improve our lives by enhancing safety, security, health monitoring and business intelligence. This has motivated extensive research into automated video analysis. Nevertheless, there is a gap between the focus of contemporary research, and the needs of end users of video surveillance systems. Many existing benchmarks and methodologies focus on narrowly defined problems in detection, tracking, re-identification or recognition. In contrast, end users face higher-level problems such as long-term monitoring of identities in order to build a picture of a person's activity across the course of a day, producing usage statistics of a particular area of space, and that these capabilities should be robust to challenges such as change of clothing. To achieve this effectively requires less widely studied capabilities such as spatio-temporal reasoning about people identities and locations within a space partially observed by multiple cameras over an extended time period. To bridge this gap between research and required capabilities, we propose a new dataset LIMA that encompasses the challenges of monitoring a typical home / office environment. LIMA contains 4.5 hours of RGB-D video from three cameras monitoring a four room house. To reflect the challenges of a realistic practical application, the dataset includes clothes changes and visitors to ensure the global reasoning is a realistic open-set problem. In addition to raw data, we provide identity annotation for benchmarking, and tracking results from a contemporary RGB-D tracker – thus allowing focus on the higher level monitoring problems.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.189,https://www.pure.ed.ac.uk/ws/files/36162552/layne2017lima.pdf
b4e902118d439adce28a92bc36ac6a37ad293b7c,0,,,0,1,0,Distributed person re-identification through network-wise rank fusion consensus,"Abstract The problem of re-identify persons across single disjoint camera-pairs has received great attention from the community. Despite this, when the re-identification process has to be carried out on a wide camera network additional problems arise and deny the direct application of existing solutions. Thus, a different approach has to be considered. In particular, existing approaches have neglected the importance of the network topology (i.e., the configuration of the monitored area) in such a process. To try filling such a gap, we propose a distributed person re-identification framework which brings in the following contributions: (i) a weighted camera matching cost that measures the re-identification performance between cameras in the network; (ii) a derivation of the distance vector algorithm that yields to network topology learning and allows us to prioritize and limit the cameras inquired for the re-identification; (iii) a network consensus weighted rank fusion solution that allows us to perform the re-identification in a robust fashion. Results on four benchmark datasets show that the proposed approach brings to significant network-wise re-identification improvements.",2019,Pattern Recognit. Lett.,,10.1016/J.PATREC.2018.12.015,
b5989d2088c40075d59a4de9a46c7e4fd04a89dd,0,,,0,1,0,Multi-level feature fusion model-based real-time person re-identification for forensics,"Person forensics aims to retrieve the specified person across non-overlapping cameras. It is difficult owing to the appearance variations caused by occlusion, human pose change, background clutter, illumination variation, etc. In this scenario, current models face great challenges in extracting effective features. Recent deep learning models mainly focus on extracting representative deep features to cope with appearance variations, while handcrafted features are not fully explored. In this paper, a multi-level feature fusion model (MFFM) is designed to combine both deep features and handcrafted features in real time. MFFM is first utilized to describe person appearance. Then, local binary pattern (LBP) and histogram of oriented gradient (HOG) are extracted to cope with geometric change and illumination variance. Using LBP and HOG, 11.89% on the CUHK03, 15.30% on the Market-1501 and 8.25% on the VIPeR top-1 recognition accuracy improvement for the proposed method are achieved with only 9.66%, 4.90%, and 7.59% extra processing time. Experimental results indicate MFFM can achieve the best performance compared to the state-of-the-art models on the Market1501, CUHK03, and VIPeR datasets.",2019,Journal of Real-Time Image Processing,,10.1007/s11554-019-00908-4,
b5d3ee3cf721d361c875f9c371325c2ab59645a9,0,,,0,1,0,Learning a Robust Representation via a Deep Network on Symmetric Positive Definite Manifolds,"Recent studies have shown that aggregating convolutional features of a pre-trained Convolutional Neural Network (CNN) can obtain impressive performance for a variety of visual tasks. The symmetric Positive Definite (SPD) matrix becomes a powerful tool due to its remarkable ability to learn an appropriate statistic representation to characterize the underlying structure of visual features. In this paper, we propose to aggregate deep convolutional features into an SPD matrix representation through the SPD generation and the SPD transformation under an end-to-end deep network. To this end, several new layers are introduced in our network, including a nonlinear kernel aggregation layer, an SPD matrix transformation layer, and a vectorization layer. The nonlinear kernel aggregation layer is employed to aggregate the convolutional features into a real SPD matrix directly. The SPD matrix transformation layer is designed to construct a more compact and discriminative SPD representation. The vectorization and normalization operations are performed in the vectorization layer for reducing the redundancy and accelerating the convergence. The SPD matrix in our network can be considered as a mid-level representation bridging convolutional features and high-level semantic features. To demonstrate the effectiveness of our method, we conduct extensive experiments on visual classification. Experiment results show that our method notably outperforms state-of-the-art methods.",2019,Pattern Recognit.,1711.0654,10.1016/j.patcog.2019.03.007,https://arxiv.org/pdf/1711.06540.pdf
b5da737c46acffad8d59a1a9ae1ecb28a15da5c2,1,[D2],,0,1,0,Memorizing Comprehensively to Learn Adaptively: Unsupervised Cross-Domain Person Re-ID with Multi-level Memory,"Unsupervised cross-domain person re-identification (Re-ID) aims to adapt the information from the labelled source domain to an unlabelled target domain. Due to the lack of supervision in the target domain, it is crucial to identify the underlying similarity-and-dissimilarity relationships among the unlabelled samples in the target domain. In order to use the whole data relationships efficiently in mini-batch training, we apply a series of memory modules to maintain an up-to-date representation of the entire dataset. Unlike the simple exemplar memory in previous works, we propose a novel multi-level memory network (MMN) to discover multi-level complementary information in the target domain, relying on three memory modules, i.e., part-level memory, instance-level memory, and domain-level memory. The proposed memory modules store multi-level representations of the target domain, which capture both the fine-grained differences between images and the global structure for the holistic target domain. The three memory modules complement each other and systematically integrate multi-level supervision from bottom to up. Experiments on three datasets demonstrate that the multi-level memory modules cooperatively boost the unsupervised cross-domain Re-ID task, and the proposed MMN achieves competitive results.",2020,ArXiv,2001.04123,,https://arxiv.org/pdf/2001.04123.pdf
b6b9c346c3fd80dffe8a5ab878edff68ff9720b4,1,[D2],,1,1,0,Deep Semi-Supervised Person Re-Identification with External Memory,"To overcome the scalability problem of supervised person re-identification (Re-ID), we consider the semi-supervised person Re-ID problem of learning from a limited number of labeled images of a few identities and a large number of unlabeled images. To this end, we propose an external-memory-based deep semi-supervised person Re-ID model (EDS). Based on the external memory, two loss functions are designed so as to effectively cope with the relation between labeled and unlabeled data for overcoming the limitation of batch size in each epoch in deep learning. Therefore, an effective deep semi-supervised learning method can be performed. Extensive experiments validate the superiority of the proposed method for semi-supervised person Re-ID.",2019,2019 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/ICME.2019.00192,
b6f959dea4ae8adc131e571096845a28120997f1,1,,1,1,0,0,Rethinking Temporal Fusion for Video-based Person Re-identification on Semantic and Time Aspect,"Recently, the research interest of person re-identification (ReID) has gradually turned to video-based methods, which acquire a person representation by aggregating frame features of an entire video. However, existing video-based ReID methods do not consider the semantic difference brought by the outputs of different network stages, which potentially compromises the information richness of the person features. Furthermore, traditional methods ignore important relationship among frames, which causes information redundancy in fusion along the time axis. To address these issues, we propose a novel general temporal fusion framework to aggregate frame features on both semantic aspect and time aspect. As for the semantic aspect, a multi-stage fusion network is explored to fuse richer frame features at multiple semantic levels, which can effectively reduce the information loss caused by the traditional single-stage fusion. While, for the time axis, the existing intra-frame attention method is improved by adding a novel inter-frame attention module, which effectively reduces the information redundancy in temporal fusion by taking the relationship among frames into consideration. The experimental results show that our approach can effectively improve the video-based re-identification accuracy, achieving the state-of-the-art performance.",2020,AAAI,1911.12512,10.1609/AAAI.V34I07.6770,https://arxiv.org/pdf/1911.12512.pdf
b852634098dd8c1fcdfc3c96c86d599d47f7c302,1,[D2],,1,1,0,"Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking","The success of DNNs has driven the extensive applications of person re-identification (ReID) into a new era. However, whether ReID inherits the vulnerability of DNNs remains unexplored. To examine the robustness of ReID systems is rather important because the insecurity of ReID systems may cause severe losses, e.g., the criminals may use the adversarial perturbations to cheat the CCTV systems. In this work, we examine the insecurity of current best-performing ReID models by proposing a learning-to-mis-rank formulation to perturb the ranking of the system output. As the cross-dataset transferability is crucial in the ReID domain, we also perform a back-box attack by developing a novel multi-stage network architecture that pyramids the features of different levels to extract general and transferable features for the adversarial perturbations. Our method can control the number of malicious pixels by using differentiable multi-shot sampling. To guarantee the inconspicuousness of the attack, we also propose a new perception loss to achieve better visual quality. Extensive experiments on four of the largest ReID benchmarks (i.e., Market1501, CUHK03, DukeMTMC, and MSMT17) not only show the effectiveness of our method, but also provides directions of the future improvement in the robustness of ReID systems. For example, the accuracy of one of the best-performing ReID systems drops sharply from 91.8% to 1.4% after being attacked by our method. Some attack results are shown in Fig. 1. The code is available at: https://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.04199,10.1109/CVPR42600.2020.00042,https://arxiv.org/pdf/2004.04199.pdf
b97209f50affe71d0b76f300d98ab96b6fe45b06,1,[D6],,1,0,0,Group Re-Identification with Multi-grained Matching and Integration,"The task of reidentifying groups of people under different camera views is an important yet less-studied problem. Group reidentification (Re-ID) is a very challenging task since it is not only adversely affected by common issues in traditional single-object Re-ID problems, such as viewpoint and human pose variations, but also suffers from changes in group layout and group membership. In this paper, we propose a novel concept of group granularity by characterizing a group image by multigrained objects: individual people and subgroups of two and three people within a group. To achieve robust group Re-ID, we first introduce multigrained representations which can be extracted via the development of two separate schemes, that is, one with handcrafted descriptors and another with deep neural networks. The proposed representation seeks to characterize both appearance and spatial relations of multigrained objects, and is further equipped with importance weights which capture variations in intragroup dynamics. Optimal group-wise matching is facilitated by a multiorder matching process which, in turn, dynamically updates the importance weights in iterative fashion. We evaluated three multicamera group datasets containing complex scenarios and large dynamics, with experimental results demonstrating the effectiveness of our approach.",2019,IEEE transactions on cybernetics,1905.07108,10.1109/TCYB.2019.2917713,https://arxiv.org/pdf/1905.07108.pdf
b986a535e45751cef684a30631a74476e911a749,1,[D2],,1,1,0,Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models,"Given a video or an image of a person acquired from a camera, person re-identification is the process of retrieving all instances of the same person from videos or images taken from a different camera with non-overlapping view. This task has applications in various fields, such as surveillance, forensics, robotics, multimedia. In this paper, we present a novel framework, named Saliency-Semantic Parsing Re-Identification (SSP-ReID), for taking advantage of the capabilities of both clues: saliency and semantic parsing maps, to guide a backbone convolutional neural network (CNN) to learn complementary representations that improves the results over the original backbones. The insight of fusing multiple clues is based on specific scenarios in which one response is better than another, thus favoring the combination of them to increase performance. Due to its definition, our framework can be easily applied to a wide variety of networks and, in contrast to other competitive methods, our training process follows simple and standard protocols. We present extensive evaluation of our approach through five backbones and three benchmarks. Experimental results demonstrate the effectiveness of our person re-identification framework. In addition, we combine our framework with re-ranking techniques to achieve state-of-the-art results on three benchmarks.",2019,Image Vis. Comput.,1807.05618,10.1016/j.imavis.2019.07.009,https://arxiv.org/pdf/1807.05618.pdf
ba5069f5a4f19005ab741b6cac4690e7c603472d,1,[D2],,1,1,0,Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification,"Most existing person re-identification(Re-ID) approaches achieve superior results based on the assumption that a large amount of pre-labelled data is usually available and can be put into training phrase all at once. However, this assumption is not applicable to most real-world deployment of the Re-ID task. In this work, we propose an alternative reinforcement learning based human-in-the-loop model which releases the restriction of pre-labelling and keeps model upgrading with progressively collected data. The goal is to minimize human annotation efforts while maximizing Re-ID performance. It works in an iteratively updating framework by refining the RL policy and CNN parameters alternately. In particular, we formulate a Deep Reinforcement Active Learning (DRAL) method to guide an agent (a model in a reinforcement learning process) in selecting training samples on-the-fly by a human user/annotator. The reinforcement learning reward is the uncertainty value of each human selected sample. A binary feedback (positive or negative) labelled by the human annotator is used to select the samples of which are used to fine-tune a pre-trained CNN Re-ID model. Extensive experiments demonstrate the superiority of our DRAL method for deep reinforcement learning based human-in-the-loop person Re-ID when compared to existing unsupervised and transfer learning models as well as active learning models.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00622,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/64521/2/Gong%20Deep%20reinforcement%20active%202020%20Accepted.pdf
ba82447e42ec9d21badd8172e830af9f8266657c,1,[D1],1,1,1,0,Dynamic Gallery for Real-Time Multi-Target Multi-Camera Tracking,"For multi-target multi-camera recognition tasks, tracking of objects of interest is one of the essential yet challenging issues due to the fact that the task requires re-identifying identical targets across distinct views. Multi-target multi-camera tracking (MTMCT) applications span a wide range of variety (e.g. crowd behavior analysis, anomaly individual tracking and sport player tracking), so how to make the system perform real-time tracking becomes a crucial research issue. In this paper, we propose an online hierarchical algorithm for extreme clustering based MTMCT framework. The system can automatically create a dynamic gallery with real-time fashion by collecting appearance information of multi-object tracking in single-camera view. We evaluate the effectiveness and efficiency of our framework, and compare the state-of-the-art methods on MOT16 as well as DukeMTMC for single and multiple camera tracking. The high-frame-rate performance and promising tracking results confirm our system can be used in realworld applications.",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909837,
baf8f45a749172a417e566698282659770287b4f,1,[D2],,0,1,0,Spatial Preserved Graph Convolution Networks for Person Re-identification,"Person Re-identification is a very challenging task due to inter-class ambiguity caused by similar appearances, and large intra-class diversity caused by viewpoints, illuminations, and poses. To address these challenges, in this article, a graph convolution network based model for person re-identification is proposed to learn more discriminative feature embeddings, where a graph-structured relationship between person images and person parts are together integrated. Graph convolution networks extract common characteristics of the same person, while pyramid feature embedding exploits parts relations and learns stable representation with each person image. We achieve a very competitive performance respectively on three widely used datasets, indicating that the proposed approach significantly outperforms the baseline methods and achieves the state-of-the-art performance.",2020,,,10.1145/3362988,
bafa74ff81e5de7d33bce36106de38d5ba585baf,0,,,0,1,1,Real-Time Person Re-Identification for Mobile Robots to Improve Human-Robot Interaction,"Mobile robots operating in seniors’ homes can serve as social companions and assist with daily tasks, thus enhancing the seniors’ quality of life [104]. In order for robots to assist seniors, it is crucial that they are equipped with sets of social and interactive skills to enable them to have natural and personalized interactions. Personalized interactions, such as using patients’ proper names or remembering personal preferences, is necessary to establish strong social relationships [4, 45], and is a key factor to improve trust in human-robot interaction [37]. A prerequisite for robots to achieve personalized interactions, however, is the ability to automatically recognize and re-identify people around them [4]. Existing person re-identification systems for mobile robots are highly restricted in terms of where robots can operate, and do not stimulate natural and personalized interactions because they need preliminary knowledge about the robot’s users [12, 18], rely on facial cues [113, 115], or use data collected from external sensors [45]. This thesis introduces two lightweight Siamese convolutional neural networks, LuNet Light and LuNet Lightest, designed for the problem of person re-identification in a robotic setting without relying on the aforementioned restrictions. Despite being significantly more lightweight than other person re-identification systems [3, 120], LuNet Lightest achieves near state-ofthe-art results on the MARS dataset evaluation protocols [135]. This thesis additionally presents a set of evaluation measures tailored to evaluate reidentification systems for robots operating in various environments. When simulating crowded environments, LuNet Lightest reaches 92.4% balanced accuracy on the proposed evaluation protocol. As a result of the lightweight architecture, LuNet Lightest achieves real-time frame-rates of 71.6 frames per second when using a GPU, 33.9 frames per second when using a CPU without GPU, and 15.7 frames per second when using only one core of the same CPU, rendering the proposed system highly suitable for low-cost, hardware-constrained robots. The proposed person re-identification system will enable assistive mobile robots to robustly and accurately identify their users, and is a preliminary step to improve trust and attain natural and personalized interaction between robots and patients.",2019,,,,
bafc809b6258d837932e50502ab4697c3bb6e8bd,1,,1,0,1,0,Efficient Feature Extraction for Person Re-Identification via Distillation,"Person re-identification has received increasing attention due to the high performance achieved by new methods based on deep learning. With larger networks of cameras being deployed, more surveillance videos need to be parsed, and extracting features for each frame remains a bottleneck. In addition, the feature extraction needs to be robust to images captured in a variety of scenarios. We propose using deep neural network distillation for training a feature extractor with a lower computational cost, while keeping track of its cross-domain ability. In the end, the proposed model is three times faster, without a decrease in accuracy. Results are validated on two popular person re-identification benchmark datasets and compared to a solution using ResNet.",2019,2019 27th European Signal Processing Conference (EUSIPCO),,10.23919/EUSIPCO.2019.8903080,
bb4f83458976755e9310b241a689c8d21b481238,0,,,0,1,0,Improving Face Verification and Person Re-Identification Accuracy Using Hyperplane Similarity,"The standard framework for using a convolutional neural network (CNN) for face verification is to compare the feature vectors taken from the penultimate network layer of a CNN trained to classify the identity of an input face using a softmax loss over identities. Feature vectors are typically compared using the simple L2 distance. We demonstrate that the L2 distance is not the best distance to use in this scenario, and propose the hyperplane similarity as a more appropriate similarity function that is derived from the soft-max loss function used to train the network. We demonstrate that hyperplane similarity improves verification results especially for low 0 acceptance rates which are usually the most important operating regimes for real applications. We also propose a fast algorithm for finding the separating hyperplanes needed to compute hyperplane similarity.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.183,http://www.merl.com/publications/docs/TR2017-155.pdf
bbdc130ec6a448c11f4a1e507175062013296d82,1,[D2],,1,1,0,Person re-identification based on multi-level feature complementarity of cross-attention with part metric learning,"Person re-identification is an image retrieval task, and its task is to perform a person matching in different cameras by a given person target. This research has been noticed and studied by more and more people. However, pose changes and occlusions often occur during a person walking. Especially in the most related methods, local features are not used to simply and effectively solve the problems of occlusion and pose changes. Moreover, the metric loss functions only consider the image-level case, and it cannot adjust the distance between local features well. To tackle the above problems, a novel person re-identification scheme is proposed. Through experiments, we found that we paid more attention to different parts of a person when we look at him from a horizontal or vertical perspective respectively. First, in order to solve the problem of occlusion and pose changes, we propose a Cross Attention Module (CAM). It enables the network to generate a cross attention map and improve the accuracy of person re-identification via the enhancement of the most significant local features of persons. The horizontal and vertical attention vectors of the feature maps are extracted and a cross attention map is generated, and the local key features are enhanced by this attention map. Second, in order to solve the problem of the lack of expression ability of the single-level feature maps, we propose a Multi-Level Feature Complementation Module (MLFCM). In this module, the missing information of high-level features is complemented by low-level features via short skip. Feature selection is also performed among deep features maps. The purpose of this module is to get the feature maps with complete information. Further, this module solves the problem of missing contour features in high-level semantic features. Third, in order to solve the problem that the current metric loss function cannot adjust the distance between local features, we propose Part Triple Loss Function (PTLF). It can reduce both within-class and increase between-class distance of the person parts. Experimental results show that our model achieves high values on Rank-k and mAP on Market-1501, Duke-MTMC and CUHK03-NP.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-08972-w,
bc3df092a7ae8f5b54045194f182bc4ff06300a4,1,[D2],,1,0,0,VMRFANet: View-Specific Multi-Receptive Field Attention Network for Person Re-identification,"Person re-identification (re-ID) aims to retrieve the same person across different cameras. In practice, it still remains a challenging task due to background clutter, variations on body poses and view conditions, inaccurate bounding box detection, etc. To tackle these issues, in this paper, we propose a novel multi-receptive field attention (MRFA) module that utilizes filters of various sizes to help network focusing on informative pixels. Besides, we present a view-specific mechanism that guides attention module to handle the variation of view conditions. Moreover, we introduce a Gaussian horizontal random cropping/padding method which further improves the robustness of our proposed network. Comprehensive experiments demonstrate the effectiveness of each component. Our method achieves 95.5% / 88.1% in rank-1 / mAP on Market-1501, 88.9% / 80.0% on DukeMTMC-reID, 81.1% / 78.8% on CUHK03 labeled dataset and 78.9% / 75.3% on CUHK03 detected dataset, outperforming current state-of-the-art methods.",2020,ICAART,2001.07354,10.5220/0008917004130420,https://arxiv.org/pdf/2001.07354.pdf
bd065cb99ad7236e2222604b7a4ef992447c565c,1,[D3],,1,0,1,Robust Re-Identification by Multiple Views Knowledge Distillation,"To achieve robustness in Re-Identification, standard methods leverage tracking information in a Video-To-Video fashion. However, these solutions face a large drop in performance for single image queries (e.g., Image-To-Video setting). Recent works address this severe degradation by transferring temporal information from a Video-based network to an Image-based one. In this work, we devise a training strategy that allows the transfer of a superior knowledge, arising from a set of views depicting the target object. Our proposal - Views Knowledge Distillation (VKD) - pins this visual variety as a supervision signal within a teacher-student framework, where the teacher educates a student who observes fewer views. As a result, the student outperforms not only its teacher but also the current state-of-the-art in Image-To-Video by a wide margin (6.3% mAP on MARS, 8.6% on Duke-Video-ReId and 5% on VeRi-776). A thorough analysis - on Person, Vehicle and Animal Re-ID - investigates the properties of VKD from a qualitatively and quantitatively perspective. Code is available at this https URL.",2020,ECCV,2007.04174,10.1007/978-3-030-58607-2_6,https://arxiv.org/pdf/2007.04174.pdf
bd27161a32d1f6c1d9b62b46ce23878046a57b98,0,,,1,0,0,"Exploiting Point Motion, Shape Deformation, and Semantic Priors for Dynamic 3D Reconstruction in the Wild","With the advent of affordable and high-quality smartphone cameras, any significant events will be massively captured both actively and passively from multiple perspectives.This opens up exciting opportunities for low-cost high-end VFX effects and large scale media analytics. However, automatically organizing large scale visual data and creating a comprehensive 3D scene model is still an unsolved problem. State of the art 3D reconstruction algorithms are mostly applicable to static scenes, mainly due to the lack of triangulation constraints for dynamic objects observedby unsynchronized cameras and the difficulties in finding reliable correspondences across cameras in diverse and dynamic settings. This thesis aims to provide a computational pipeline for high-quality 3D reconstructionof the dynamic scene captured by multiple unsynchronized video cameras in the wild. The key is to exploit the physics of motion dynamics, shape deformation, scene semantics, and the interplay between them. Toward this end, this thesismakes four enabling technical contributions. First, this thesis introduces a spatiotemporal bundle adjustment algorithm to accurately estimate a sparse set of 3D trajectories of dynamic objects from multiple unsynchronized mobile video cameras. The lack of triangulation constraint on dynamic points is solved by carefully integrating physics-based motion prior describing how points move over time. This algorithm takes advantage of the unsynchronized video streams to estimate 3D motion reconstruction in the wild at much higher temporalresolution than the input videos. Second, this thesis presents a simple but powerful self-supervised framework toadapt a generic person appearance descriptor to the unlabeled videos by exploiting motion tracking, mutual exclusion constraints, and multi-view geometry without anymanual annotations. The adapted descriptor is strongly discriminative and enables a tracking-by-clustering formulation. This advantage enables a first-of-a-kind accurateand consistent markerless motion tracking of multiple people participating in a complex group activity from mobile cameras in the wild with further application tomulti-angle video cutting for intuitive tracking visualization.Third, this thesis creates a framework for 3D tracking of the rigidly moving objects even in severe occlusions by fusing single-view unstructured tracklets and multi-view semantic structured keypoints reconstruction. No spatial correspondences are needed for the unstructured points. No temporal correspondences are needed for the structured points. The imprecise but accurate 3D structured keypoint is compensated by the sparse but precise 3D unstructured tracks, leading to improvements in both structured keypoints localization and motion tracking of the entire object.Fourth, this thesis presents a single-shot illumination decomposition method for dense dynamic shape capture of highly textured surfaces illuminated by multiple projectors. The decomposition scheme assumes smooth shape deformation and can accurately recover the illumination image of different projectors and the texture imagesof the scene from their mixed appearances.",2019,,,10.1184/R1/9816818.V1,http://www.cs.cmu.edu/~ILIM/publications/PDFs/MV-THESIS-19.pdf
bd963ee8ae9afee34b766913a9891b1db36275ca,0,,,0,1,0,A novel data augmentation scheme for pedestrian detection with attribute preserving GAN,"Abstract Recently pedestrian detection has progressed significantly. However, detecting pedestrians of small scale or in heavy occlusions is still notoriously difficult. Besides, the generalization ability of pre-trained detectors across different datasets remains to be improved. Both of these issues can be attributed to insufficient training data coverage. To cope with this, we present an efficient data augmentation scheme by transferring pedestrians from other datasets into the target scene with a novel Attribute Preserving Generative Adversarial Networks (APGAN). The proposed methodology consists of two steps: pedestrian embedding and style transfer. The former step can simulate pedestrian images of various scale and occlusion, in any pose or background, thus greatly promoting the data variation. The latter step aims to make the generated samples more realistic while guarantee the data coverage. To achieve this goal, we propose APGAN, which pursues both good visual quality and attribute preserving after style transfer. With the proposed method, we can make effective sample augmentations to improve the generalization ability of the trained detectors and enhance its robustness to scale change and occlusions. Extensive experiment results validate the effectiveness and advantages of our method.",2020,Neurocomputing,,10.1016/j.neucom.2020.02.094,
bdcc60fbc992cf34ad714cad9a327f572c100012,1,[D2],,1,1,0,Perceive Where to Focus: Learning Visibility-Aware Part-Level Features for Partial Person Re-Identification,"This paper considers a realistic problem in person re-identification (re-ID) task, i.e., partial re-ID. Under partial re-ID scenario, the images may contain a partial observation of a pedestrian. If we directly compare a partial pedestrian image with a holistic one, the extreme spatial misalignment significantly compromises the discriminative ability of the learned representation. We propose a Visibility-aware Part Model (VPM) for partial re-ID, which learns to perceive the visibility of regions through self-supervision. The visibility awareness allows VPM to extract region-level features and compare two images with focus on their shared regions (which are visible on both images). VPM gains two-fold benefit toward higher accuracy for partial re-ID. On the one hand, compared with learning a global feature, VPM learns region-level features and thus benefits from fine-grained information. On the other hand, with visibility awareness, VPM is capable to estimate the shared regions between two images and thus suppresses the spatial misalignment. Experimental results confirm that our method significantly improves the learned feature representation and the achieved accuracy is on par with the state of the art.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.00537,10.1109/CVPR.2019.00048,https://arxiv.org/pdf/1904.00537.pdf
be01ce9c11f1187b6e395322b8517348264df63b,1,[D6],,1,0,0,DotSCN: Group Re-identification via Domain-Transferred Single and Couple Representation Learning.,"Group re-identification (G-ReID) is an important yet less-studied task. Its challenges not only lie in appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also derive from group layout and membership changes. So the key task of G-ReID is to learn representations robust to such changes. To address this issue, we propose a Transferred Single and Couple Representation Learning Network (TSCN). Its merits are two aspects: 1) Due to the lack of labelled training samples, existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To gain the superiority of deep learning models, we treat a group as multiple persons and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style to learn single representations. 2) Taking into account the neighborhood relationship in a group, we further propose learning a novel couple representation between two group members, that achieves more discriminative power in G-ReID tasks. In addition, an unsupervised weight learning method is exploited to adaptively fuse the results of different views together according to result patterns. Extensive experimental results demonstrate the effectiveness of our approach that significantly outperforms state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by 39.0\% CMC-1 on the DukeMCMT dataset.",2020,,1905.04854,10.1109/tcsvt.2020.3031303,https://arxiv.org/pdf/1905.04854.pdf
be90dc791ebc764e242b24b83ff40ba797c09114,1,[D1],,1,0,0,Key-Track: A Lightweight Scalable LSTM-based Pedestrian Tracker for Surveillance Systems,"There has been a growing interest in leveraging state of the art deep learning techniques for tracking objects in recent years. Most of this work focuses on using redundant appearance models for predicting object tracklets for the next frame. Moreover, not much work has been done to explore the sequence learning properties of Long Short Term Memory (LSTM) Neural Networks for object tracking in video sequences. In this work we propose a novel LSTM tracker, Key-Track, which effectively learns the spatial and temporal behavior of pedestrians after analyzing movement patterns of human key-points provided to it by OpenPose [3]. We train Key-Track on single person sequences that we curated from the Duke Multi-target Multi-Camera (Duke-MTMC) [26] dataset and scale it to track multiple people at run-time, further testing its scalability. We report our results on the Duke-MTMC dataset for different time-series sequence lengths we feed to Key-Track and find three as the optimum time-step sequence length producing the highest Average Overlap Score (AOS). We further present our qualitative analysis on these different time-series sequence lengths producing different results depending on the type of video sequence. The total observed size of Key-Track is under 1 megabytes which paves its way into mobile devices for the purpose of tracking in real-time.",2019,ICIAR,,10.1007/978-3-030-27272-2_18,
bf5dc0958e04a4c77ed70593c90c274c56882eff,1,[D2],,0,1,0,Disjoint Label Space Transfer Learning with Common Factorised Space,"In this paper, a unified approach is presented to transfer learning that addresses several source and target domain label-space and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.",2019,AAAI,1812.02605,10.1609/aaai.v33i01.33013288,https://arxiv.org/pdf/1812.02605.pdf
bfab573b064217d0533f2d1e8651746cf295c182,1,[D2],,0,1,0,Single Camera Training for Person Re-identification,"Person re-identification (ReID) aims at finding the same person in different cameras. Training such systems usually requires a large amount of cross-camera pedestrians to be annotated from surveillance videos, which is labor-consuming especially when the number of cameras is large. Differently, this paper investigates ReID in an unexplored single-camera-training (SCT) setting, where each person in the training set appears in only one camera. To the best of our knowledge, this setting was never studied before. SCT enjoys the advantage of low-cost data collection and annotation, and thus eases ReID systems to be trained in a brand new environment. However, it raises major challenges due to the lack of cross-camera person occurrences, which conventional approaches heavily rely on to extract discriminative features. The key to dealing with the challenges in the SCT setting lies in designing an effective mechanism to complement cross-camera annotation. We start with a regular deep network for feature extraction, upon which we propose a novel loss function named multi-camera negative loss (MCNL). This is a metric learning loss motivated by probability, suggesting that in a multi-camera system, one image is more likely to be closer to the most similar negative sample in other cameras than to the most similar negative sample in the same camera. In experiments, MCNL significantly boosts ReID accuracy in the SCT setting, which paves the way of fast deployment of ReID systems with good performance on new target scenes.",2020,AAAI,1909.10848,10.1609/AAAI.V34I07.6985,https://arxiv.org/pdf/1909.10848.pdf
c0f01b8174a632448c20eb5472cd9d5b2c595e39,1,"[D1], [D2]",,1,1,0,Features for Multi-target Multi-camera Tracking and Re-identification,"Multi-Target Multi-Camera Tracking (MTMCT) tracks many people through video taken from several cameras. Person Re-Identification (Re-ID) retrieves from a gallery images of people similar to a person query image. We learn good features for both MTMCT and Re-ID with a convolutional neural network. Our contributions include an adaptive weighted triplet loss for training and a new technique for hard-identity mining. Our method outperforms the state of the art both on the DukeMTMC benchmarks for tracking, and on the Market-1501 and DukeMTMC-ReID benchmarks for Re-ID. We examine the correlation between good Re-ID and good MTMCT scores, and perform ablation studies to elucidate the contributions of the main components of our system. Code is available1.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.10859,10.1109/CVPR.2018.00632,https://arxiv.org/pdf/1803.10859.pdf
c0fda1dd28e355db93e88eeeb85b420099b475e6,1,[D2],,1,1,0,Attribute analysis with synthetic dataset for person re-identification,"Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, have achieved remarkable performance. However, existing synthetic datasets are in small size and lack of diversity, which hinders the development of person re-ID in real-world scenarios. To address this problem, firstly, we develop a large-scale synthetic data engine, the salient characteristic of this engine is controllable. Based on it, we build a large-scale synthetic dataset, which are diversified and customized from different attributes, such as illumination and viewpoint. Secondly, we quantitatively analyze the influence of dataset attributes on re-ID system. To our best knowledge, this is the first attempt to explicitly dissect person re-ID from the aspect of attribute on synthetic dataset. Comprehensive experiments help us have a deeper understanding of the fundamental problems in person re-ID. Our research also provides useful insights for dataset building and future practical usage.",2020,ArXiv,2006.07139,,https://arxiv.org/pdf/2006.07139.pdf
c19e05afbd7cf6f31a32093e91b91c1d2d72ad67,1,[D2],,1,0,0,Enhance Part-Based Model for Person Re-Identification with Fused Multi-Scale Features,"In recent years, part-based models have been verified their effectiveness for person Re-identification (Re-ID). Since they learn an embedding only by partitioning single-scale features of the highest layer in the backbone network, their performances highly depend on the well-aligned parts of the extracted feature maps. However, misalignments occur very commonly in person Re-ID tasks due to the variations of viewpoints and poses. To address the part-misalignment problem and learn a more discriminative embedding for person Re-ID, we propose a novel Part-based model with fused Multi-Scale features (PMS), which innovatively upscales the low-layer features by using UpShuffle Modules and smoothly integrates the high-layer features. The fused multi-scale features are very robust to the variations of pedestrian scale and beneficial to resolve the part-misalignment problem. Experimental results on three commonly used datasets, including Market-1501, DukeMTMC-reID and CUHK03, have validated our model by outperforming the state-of-the-art methods with no need of re-ranking.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9054659,
c2981caccc49e67c4f4a9cc66911faf954c01e9a,1,[D3],,0,0,1,Spatially and Temporally Efficient Non-local Attention Network for Video-based Person Re-Identification,"Video-based person re-identification (Re-ID) aims at matching video sequences of pedestrians across non-overlapping cameras. It is a practical yet challenging task of how to embed spatial and temporal information of a video into its feature representation. While most existing methods learn the video characteristics by aggregating image-wise features and designing attention mechanisms in Neural Networks, they only explore the correlation between frames at high-level features. In this work, we target at refining the intermediate features as well as high-level features with non-local attention operations and make two contributions. (i) We propose a Non-local Video Attention Network (NVAN) to incorporate video characteristics into the representation at multiple feature levels. (ii) We further introduce a Spatially and Temporally Efficient Non-local Video Attention Network (STE-NVAN) to reduce the computation complexity by exploring spatial and temporal redundancy presented in pedestrian videos. Extensive experiments show that our NVAN outperforms state-of-the-arts by 3.8% in rank-1 accuracy on MARS dataset and confirms our STE-NVAN displays a much superior computation footprint compared to existing methods.",2019,BMVC,1908.01683,,https://arxiv.org/pdf/1908.01683.pdf
c2bb9d275dfab4b7798fe1cadd3bf9209de3ef5e,1,[D3],,1,0,1,Exploiting Global Camera Network Constraints for Unsupervised Video Person Re-identification,"Many unsupervised approaches have been proposed recently for the video-based re-identification problem since annotations of samples across cameras are time-consuming. However, higher-order relationships across the entire camera network are ignored by these methods, leading to contradictory outputs when matching results from different camera pairs are combined. In this paper, we address the problem of unsupervised video-based re-identification by proposing a consistent cross-view matching (CCM) framework, in which global camera network constraints are exploited to guarantee the matched pairs are with consistency. Specifically, we first propose to utilize the first neighbor of each sample to discover relations among samples and find the groups in each camera. Additionally, a cross-view matching strategy followed by global camera network constraints is proposed to explore the matching relationships across the entire camera network. Finally, we learn metric models for camera pairs progressively by alternatively mining consistent cross-view matching pairs and updating metric models using these obtained matches. Rigorous experiments on two widely-used benchmarks for video re-identification demonstrate the superiority of the proposed method over current state-of-the-art unsupervised methods; for example, on the MARS dataset, our method achieves an improvement of 4.2\% over unsupervised methods, and even 2.5\% over one-shot supervision-based methods for rank-1 accuracy.",2019,,1908.10486,,https://arxiv.org/pdf/1908.10486.pdf
c3725184a31e281b543467673753c719dcc2393e,1,[D2],,1,0,0,"MSBA: Multiple Scales, Branches and Attention Network With Bag of Tricks for Person Re-Identification","Person re-identification (Re-ID) has become a hot topic in both research and industry. We joined in a person Re-ID challenge of the First National Artificial Intelligence Challenge (China, 2019) and found some model designs and training tricks work great or not on a super big private dataset. In this paper, we propose a model that combines the most effective designs, including multi-scale, multi-branch and attention mechanism, and report training tricks that are no less or even more important in improving person Re-ID performance. We analyze four commonly used public datasets: Market1501, DukeMTMC-ReID, CUHK03, and MSMT17, and achieve the state-of-the-art performance. Besides, we analyze and confirm the effectiveness of the designs by ablation studies. We also share strategies that play a key role in the challenge and experience of model designs that do not generalize well on large datasets.",2020,IEEE Access,,10.1109/ACCESS.2020.2984915,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09052718.pdf
c37c3853ab428725f13906bb0ff4936ffe15d6af,1,[D2],,1,1,0,Unsupervised Person Re-identification by Deep Learning Tracklet Association,"Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation re-id methods using six person re-id benchmarking datasets.",2018,ECCV,1809.02874,10.1007/978-3-030-01225-0_45,https://arxiv.org/pdf/1809.02874.pdf
c380808a70fc2325ebd2be2e92b76b7e5c48bca8,1,"[D1], [D2]",,1,1,0,Deep Association: End-to-end Graph-Based Learning for Multiple Object Tracking with Conv-Graph Neural Network,"Multiple Object Tracking (MOT) has a wide range of applications in surveillance retrieval and autonomous driving. The majority of existing methods focus on extracting features by deep learning and hand-crafted optimizing bipartite graph or network flow. In this paper, we proposed an efficient end-to-end model, Deep Association Network (DAN), to learn the graph-based training data, which are constructed by spatial-temporal interaction of objects. DAN combines Convolutional Neural Network (CNN), Motion Encoder (ME) and Graph Neural Network (GNN). The CNNs and Motion Encoders extract appearance features from bounding box images and motion features from positions respectively, and then the GNN optimizes graph structure to associate the same object among frames together. In addition, we presented a novel end-to-end training strategy for Deep Association Network. Our experimental results demonstrate the effectiveness of DAN up to the state-of-the-art methods without extra-dataset on MOT16 and DukeMTMCT.",2019,ICMR,,10.1145/3323873.3325010,
c3980c4ce42ebcd493514180897829096c189337,0,,,0,1,0,"Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part II","In selection processes, decisions follow a sequence of stages. Early stages have more applicants and general information, while later stages have fewer applicants but specific data. This is represented by a dual funnel structure, in which the sample size decreases from one stage to the other while the information increases. Training classifiers for this case is challenging. In the early stages, the information may not contain distinct patterns to learn, causing underfitting. In later stages, applicants have been filtered out and the small sample can cause overfitting. We redesign the multi-stage problem to address both cases by combining adversarial autoencoders (AAE) and multi-task semi-supervised learning (MTSSL) to train an end-to-end neural network for all stages together. The AAE learns the representation of the data and performs data imputation in missing values. The generated dataset is fed to an MTSSL mechanism that trains all stages together, encouraging related tasks to contribute to each other using a temporal regularization structure. Using real-world data, we show that our approach outperforms other state-ofthe-art methods with a gain of 4x over the standard case and a 12% improvement over the second-best method.",2020,PAKDD,,10.1007/978-3-030-47436-2,
c43ed9b34cad1a3976bac7979808eb038d88af84,0,,,0,1,0,Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model,"We propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with a wide range of expressions, poses, and illuminations conditioned by synthetic images sampled from a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with a large volume of paired data or train highly under-constrained two-way generative networks in an unsupervised fashion. We propose a semi-supervised adversarial learning framework to constrain the two-way networks by a small number of paired real and synthetic images, along with a large volume of unpaired data. A set-based loss is also proposed to preserve identity coherence of generated images. Qualitative results show that generated face images of new identities contain pose, lighting and expression diversity. They are also highly constrained by the synthetic input images while adding photorealism and retaining identity information. We combine face images generated by the proposed method with a real data set to train face recognition algorithms and evaluate the model quantitatively on two challenging data sets: LFW and IJB-A. The generated images by our framework consistently improve the performance of deep face recognition networks trained with the Oxford VGG Face dataset, and achieve comparable results to the state-of-the-art.",2018,ECCV,1804.03675,10.1007/978-3-030-01252-6_14,https://arxiv.org/pdf/1804.03675.pdf
c5bfbda61a29cd1206ea6670094c7f38c589c51b,1,[D2],,0,1,0,ReadNet: Towards Accurate ReID with Limited and Noisy Samples,"Person re-identification (ReID) is an essential cross-camera retrieval task to identify pedestrians. However, the photo number of each pedestrian usually differs drastically, and thus the data limitation and imbalance problem hinders the prediction accuracy greatly. Additionally, in real-world applications, pedestrian images are captured by different surveillance cameras, so the noisy camera related information, such as the lights, perspectives and resolutions, result in inevitable domain gaps for ReID algorithms. These challenges bring difficulties to current deep learning methods with triplet loss for coping with such problems. To address these challenges, this paper proposes ReadNet, an adversarial camera network (ACN) with an angular triplet loss (ATL). In detail, ATL focuses on learning the angular distance among different identities to mitigate the effect of data imbalance, and guarantees a linear decision boundary as well, while ACN takes the camera discriminator as a game opponent of feature extractor to filter camera related information to bridge the multi-camera gaps. ReadNet is designed to be flexible so that either ATL or ACN can be deployed independently or simultaneously. The experiment results on various benchmark datasets have shown that ReadNet can deliver better prediction performance than current state-of-the-art methods.",2020,ArXiv,2005.0574,,https://arxiv.org/pdf/2005.05740.pdf
c5e645dc2d34c7c3777456a25e04ac8b05f78177,1,[D2],,0,1,0,Cross-view Identical Part Area Alignment for Person Re-identification,"Person re-identification aims to associate images captured by non-overlapping cameras. It is a challenging task because images are often in different conditions such as background clutter, illumination variation, viewpoint changes and different camera settings. Viewpoint changes and pose variations often cause body part self-occlusion and misalignment. To deal with the problem, local features from human body parts are extracted. However, with viewpoint changes, the body parts also rotate horizontally. It is inappropriate to extract feature from entire area of body parts directly because the visible surface of body parts would turn away if viewpoint changes. Comparing identical areas provides a new way to pay attention to the details of person images. In this paper, we propose a Rotation Invariant Network to find the identical areas in cross-view images to extract robust local features. Extensive experiment show the effectiveness of our method on public datasets including CUHK03, Market1501 and DukeMTMC.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683137,http://mmap.whu.edu.cn/wp-content/uploads/2019/03/20190218051144_367651_4317.pdf
c62d524b3f74137efcf824c32252bd5993c07c4e,1,[D3],,0,0,1,Supplementary Material for “Spatial-Temporal Graph Convolutional Network for Video-based Person Re-identification”,"for Video-based Person Re-identification” Jinrui Yang , Wei-Shi Zheng1,2,3∗ , Qize Yang , Yingcong Chen , and Qi Tian 1 School of Data and Computer Science, Sun Yat-sen University, China 2 Peng Cheng Laboratory, Shenzhen 518005, China 3 Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China 4 The Chinese University of Hong Kong, China The Huawei Noah’s Ark Lab, China {yangjr27,yangqz}@mail2.sysu.edu.cn,wszheng@ieee.org,yingcong.ian.chen@gmail.com ,tian.qi1@huawei.com",2020,,,,http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Yang_Spatial-Temporal_Graph_Convolutional_CVPR_2020_supplemental.pdf
c640807b029250bc6d302f60f0966d9cb49eb89a,0,,,1,0,0,Re-identification in Home Environments using Facial and Appearance Features,"Re-identification is a vital task in video surveillance, and it is often viewed as an image retrieval task (i.e. given one image to search relevant images from all other images captured by different cameras.). The majority of the state-of-art methodologies focus on data which is obtained in an outdoor environment and are based on spatial and temporal features [1]. However, there is a need for methodologies that can handle data from the home environment due to the increasing demand for health monitoring. The LIMA dataset [2] , where the images are captured by a low-cost camera in the home environment, provided by the SPHERE project, includes realistic challenges. As an example, people in the scene may change clothes, and an unknown identity may appear in the scene. Since these are challenging situations, facial features can help to discriminate better amongst different identities that appear in the scene. Therefore, We have provided some re-identification results for LIMA dataset and methodologies focus on both facial and appearance features. Besides, we also label each image in the LIMA dataset to provide a LIMA face dataset (binary label, face or not containing face) to support the face detection performance measurement. Both traditional and modern approaches are employed to compare the performance in face detection and recognition. Furthermore, CNN with only appearance features is trained to see the drawbacks of the appearance features. Then, we propose a two-stream convolutional neural network to combine facial and appearance features together to predict the ID of the person, and it can be shown that the facial features indeed improve the performance through a simple experiment.",2018,,,,https://pdfs.semanticscholar.org/c640/807b029250bc6d302f60f0966d9cb49eb89a.pdf
c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3,0,,,0,0,1,Generalizing from a Few Examples: A Survey on Few-Shot Learning,"Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",2019,,,,
ca24213986799272302f190248f8cdedaa97c3d1,1,[D2],,0,1,0,Good practices on building effective CNN baseline model for person re-identification,"Person re-identification is indeed a challenging visual recognition task due to the critical issues of human pose variation, human body occlusion, camera view variation, etc. To address this, most of the state-of-the-art approaches are proposed based on deep convolutional neural network (CNN), being leveraged by its strong feature learning power and classification boundary fitting capacity. Although the vital role towards person re-identification, how to build effective CNN baseline model has not been well studied yet. To answer this open question, we propose 3 good practices in this paper from the perspectives of adjusting CNN architecture and training procedure. In particular, they are adding batch normalization after the global pooling layer, executing identity categorization directly using only one fully-connected layer, and using Adam as optimizer. The extensive experiments on 3 widely-used benchmark datasets demonstrate that, our propositions essentially facilitate the CNN baseline model to achieve the state-of-the-art performance without any other high-level domain knowledge or low-level technical trick.",2019,International Conference on Graphic and Image Processing,1807.11042,10.1117/12.2524386,https://arxiv.org/pdf/1807.11042.pdf
cab6c4898fc6c8f9bec493b7fd60fc9d19a165a2,1,"[D2], [D4]",,0,1,0,Multi-task Network Learning Representation Features of Attributes and Identity for Person Re-identification,"Person re-identification (re-ID) has become increasingly popular due to its significance in practical application. In most of the available methods for person re-ID, the solutions focus on verification and recognition of the person identity and pay main attention to the appearance details of person. In this paper, we propose multi-task network architecture to learn powerful representation features of attributes and identity for person re-ID. Firstly, we utilize the semantic descriptor on attributes such as gender, clothing details to effectively learn representation features. Secondly, we employ joint supervision of softmax loss and center loss for person identification to obtain deep features with inter-class dispersion and intra-class compactness. Finally, we use the convolutional neural network (CNN) and multi-task learning strategy to integrate the person attributes and identity to complete classifications tasks for person re-ID. Experiments are conducted on Market1501 and DukeMTMC-reID to verify the efficiency of our method.",2018,CCBR,,10.1007/978-3-319-97909-0_73,
cbe8fa1b7d7d602049a186c9340fb46f8b791a23,0,,,0,1,0,GENERATIVE ADVERSARIAL NETWORK BASED ACOUSTIC SCENE TRAINING SET AUGMENTATION AND SELECTION USING SVM HYPERPLANE,"Although it is typically expected that using a large amount of labeled training data would lead to improve performance in deep learning, it is generally difficult to obtain such DataBase (DB). In competitions such as the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge Task 1, participants are constrained to use a relatively small DB as a rule, which is similar to the aforementioned issue. To improve Acoustic Scene Classification (ASC) performance without employing additional DB, this paper proposes to use Generative Adversarial Networks (GAN) based method for generating additional training DB. Since it is not clear whether every sample generated by GAN would have equal impact in classification performance, this paper proposes to use Support Vector Machine (SVM) hyper plane for each class as reference for selecting samples, which have class discriminative information. Based on the crossvalidated experiments on development DB, the usage of the generated features could improve ASC performance.",2017,,,,http://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Mun_215.pdf
cbf5b3469c7216c37733efca6c2cdb94357b14a7,1,"[D2], [D4]",,1,1,0,Person Re-identification Based on Feature Fusion and Triplet Loss Function,"The task of Person re-identification (re-ID) is to recognize an individual observed by non-overlapping cameras. Robust feature representation is a crucial problem in re-ID. With the rise of deep learning, most current approaches adopt convolutional neural networks (CNN) to extract features. However, the feature representation learned by CNN is often global and lacks detailed local information. To address this issue, this paper proposes a simple CNN architecture consisting of a re-ID subnetwork and an attribute sub-network. In re-ID sub-network, global feature and semantic feature are extracted and fused in a weighted manner, and triplet loss is adopted to further improve the discriminative ability of the learned fusion feature. On the other hand, attribute sub-network focuses on local aspects of a person and offers local structural information that is helpful for re-ID. The two sub-networks are combined on the loss level and their complementary aspects are leveraged to improve the re-ID accuracy. Comparative evaluations demonstrate that our method outperforms several state-of-the-art ones. On the challenging Market1501 and DukeMTMC datasets, 86.3% rank-1 accuracy and 69.4% mAP, and 72.1% rank-1 accuracy and 53.4% mAP are achieved respectively.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8546082,
cc2433902036de4004dec9be5e4099544741e500,1,[D2],,0,1,0,Pose Guided Gated Fusion for Person Re-identification,"Person re-identification is an important yet challenging problem in visual recognition. Despite the recent advances with deep learning (DL) models for spatio-temporal and multi-modal fusion, re-identification approaches often fail to leverage the contextual information (e.g., pose and illumination) to dynamically select the most discriminant con-volutional filters (i.e., appearance features) for feature representation and inference. State-of-the-art techniques for gated fusion employ complex dedicated part- or attention-based architectures for late fusion, and do not incorporate pose and appearance information to train the backbone network. In this paper, a new DL model is proposed for pose-guided re-identification, comprised of a deep backbone, pose estimation, and gated fusion network. Given a query image of an individual, the backbone convolutional NN produces a feature embedding required for pair-wise matching with embeddings for reference images, where feature maps from the pose network and from mid-level CNN layers are combined by the gated fusion network to generate pose-guided gating. The proposed framework allows to dynamically activate the most discriminant CNN filters based on pose information in order to perform a finer grained recognition. Extensive experiments on three challenging benchmark datasets indicate that integrating the pose-guided gated fusion into the state-of-the-art re-identification backbone architecture allows to improve their recognition accuracy. Experimental results also support our intuition on the advantages of gating backbone appearance information using the pose feature maps at mid-level CNN layers.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093370,http://openaccess.thecvf.com/content_WACV_2020/papers/Bhuiyan_Pose_Guided_Gated_Fusion_for_Person_Re-identification_WACV_2020_paper.pdf
cc6378854c605e604d40d48410edbfa59bf37e5a,1,[D5],,0,1,0,Uncertainty-optimized deep learning model for small-scale person re-identification,"In recent years, deep learning has developed rapidly and is widely used in various fields, such as computer vision, speech recognition, and natural language processing. For end-to-end person re-identification, most deep learning methods rely on large-scale datasets. Relatively few methods work with small-scale datasets. Insufficient training samples will affect neural network accuracy significantly. This problem limits the practical application of person re-identification. For small-scale person re-identification, the uncertainty of person representation and the overfitting problem associated with deep learning remain to be solved. Quantifying the uncertainty is difficult owing to complex network structures and the large number of hyperparameters. In this study, we consider the uncertainty of pedestrian representation for small-scale person re-identification. To reduce the impact of uncertain person representations, we transform parameters into distributions and conduct multiple sampling by using multilevel dropout in a testing process. We design an improved Monte Carlo strategy that considers both the average distance and shortest distance for matching and ranking. When compared with state-of-the-art methods, the proposed method significantly improve accuracy on two small-scale person re-identification datasets and is robust on four large-scale datasets.",2019,Science China Information Sciences,,10.1007/s11432-019-2675-3,
cca002c219413b1152a85e8d88a6245725d9a1c2,0,,,0,1,0,Multi-level Similarity Perception Network for Person Re-identification,"In this article, we propose a novel deep Siamese architecture based on a convolutional neural network (CNN) and multi-level similarity perception for the person re-identification (re-ID) problem. According to the distinct characteristics of diverse feature maps, we effectively apply different similarity constraints to both low-level and high-level feature maps during training stage. Due to the introduction of appropriate similarity comparison mechanisms at different levels, the proposed approach can adaptively learn discriminative local and global feature representations, respectively, while the former is more sensitive in localizing part-level prominent patterns relevant to re-identifying people across cameras. Meanwhile, a novel strong activation pooling strategy is utilized on the last convolutional layer for abstract local-feature aggregation to pursue more representative feature representations. Based on this, we propose final feature embedding by simultaneously encoding original global features and discriminative local features. In addition, our framework has two other benefits: First, classification constraints can be easily incorporated into the framework, forming a unified multi-task network with similarity constraints. Second, as similarity-comparable information has been encoded in the network’s learning parameters via back-propagation, pairwise input is not necessary at test time. That means we can extract features of each gallery image and build an index in an off-line manner, which is essential for large-scale real-world applications. Experimental results on multiple challenging benchmarks demonstrate that our method achieves splendid performance compared with the current state-of-the-art approaches.",2019,ACM Trans. Multim. Comput. Commun. Appl.,,10.1145/3309881,
cca9d893b9ce6f21e2565442680690b99c74fa8f,1,[D2],,0,1,0,Interaction-And-Aggregation Network for Person Re-Identification,"Person re-identification (reID) benefits greatly from deep convolutional neural networks (CNNs) which learn robust feature embeddings. However, CNNs are inherently limited in modeling the large variations in person pose and scale due to their fixed geometric structures. In this paper, we propose a novel network structure, Interaction-and-Aggregation (IA), to enhance the feature representation capability of CNNs. Firstly, Spatial IA (SIA) module is introduced. It models the interdependencies between spatial features and then aggregates the correlated features corresponding to the same body parts. Unlike CNNs which extract features from fixed rectangle regions, SIA can adaptively determine the receptive fields according to the input person pose and scale. Secondly, we introduce Channel IA (CIA) module which selectively aggregates channel features to enhance the feature representation, especially for small-scale visual cues. Further, IA network can be constructed by inserting IA blocks into CNNs at any depth. We validate the effectiveness of our model for person reID by demonstrating its superiority over state-of-the-art methods on three benchmark datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1907.08435,10.1109/CVPR.2019.00954,https://arxiv.org/pdf/1907.08435.pdf
cdcd5cfc25793b861b1f8db79f3bbeab45533ace,1,[D2],,0,1,0,SaADB: A Self-attention Guided ADB Network for Person Re-identification,"Recently, Batch DropBlock network (BDB) has demonstrated its effectiveness on person image representation and re-ID task via feature erasing. However, BDB drops the features randomly which may lead to sub-optimal results. In this paper, we propose a novel Self-attention guided Adaptive DropBlock network (SaADB) for person re-ID which can adaptively erase the most discriminative regions. Specifically, SaADB first obtains a self-attention map by channel-wise pooling and returns a drop mask by thresholding the self-attention map. Then, the input features and self-attention guided drop mask are multiplied to generate the dropped feature maps. Meanwhile, we utilize the spatial and channel attention to learn a better feature map and iteratively train with the feature dropping module for person re-ID. Experiments on several benchmark datasets demonstrate that the proposed SaADB significantly beats the prevalent competitors in person re-ID.",2020,ArXiv,2007.03584,,https://arxiv.org/pdf/2007.03584.pdf
cf8adfd3d6ab5f2a2f24fae821bd056e996663e5,0,,,1,0,0,Self-balance Motion and Appearance Model for Multi-object Tracking in UAV,"Under the tracking-by-detection framework, multi-object tracking methods try to connect object detections with target trajectories by reasonable policy. Most methods represent objects by the appearance and motion. The inference of the association is mostly judged by a fusion of appearance similarity and motion consistency. However, the fusion ratio between appearance and motion are often determined by subjective setting. In this paper, we propose a novel self-balance method fusing appearance similarity and motion consistency. Extensive experimental results on public benchmarks demonstrate the effectiveness of the proposed method with comparisons to several state-of-the-art trackers.",2019,MMAsia,,10.1145/3338533.3366561,
d4a5c9b2197b6bc476aa296b8d59515c9684e97d,1,"[D2], [D4]",,1,1,0,CA3Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification,"Person re-identification aims to identify the same pedestrian across non-overlapping camera views. Deep learning techniques have been applied for person re-identification recently, towards learning representation of pedestrian appearance. This paper presents a novel Contextual-Attentional Attribute-Appearance Network ($\rm CA^3Net$) for person re-identification. The $\rm CA^3Net$ simultaneously exploits the complementarity between semantic attributes and visual appearance, the semantic context among attributes, visual attention on attributes as well as spatial dependencies among body parts, leading to discriminative and robust pedestrian representation. Specifically, an attribute network within $\rm CA^3Net$ is designed with an Attention-LSTM module. It concentrates the network on latent image regions related to each attribute as well as exploits the semantic context among attributes by a LSTM module. An appearance network is developed to learn appearance features from the full body, horizontal and vertical body parts of pedestrians with spatial dependencies among body parts. The $\rm CA^3Net$ jointly learns the attribute and appearance features in a multi-task learning manner, generating comprehensive representation of pedestrians. Extensive experiments on two challenging benchmarks, i.e., Market-1501 and DukeMTMC-reID datasets, have demonstrated the effectiveness of the proposed approach.",2018,ACM Multimedia,1811.07544,10.1145/3240508.3240585,https://arxiv.org/pdf/1811.07544.pdf
d6e1fd34daf96536534a56f663b23bf58c99cf62,0,,,0,1,0,Geometry Guided Pose-Invariant Facial Expression Recognition,"Driven by recent advances in human-centered computing, Facial Expression Recognition (FER) has attracted significant attention in many applications. However, most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifier for each pose. Different from existing methods, this paper proposes an end-to-end deep learning model that allows to simultaneous facial image synthesis and pose-invariant facial expression recognition by exploiting shape geometry of the face image. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, given an input face and a target pose and expression designated by a set of facial landmarks, an identity-preserving face can be generated through guiding by the target pose and expression. Second, the identity representation is explicitly disentangled from both expression and pose variations through the shape geometry delivered by facial landmarks. Third, our model can automatically generate face images with different expressions and poses in a continuous way to enlarge and enrich the training set for the FER task. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on both controlled and in-the-wild benchmark datasets including Multi-PIE, BU-3DFE, and SFEW. The code is included in the supplementary material.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2972114,
d8be272d7981e5b8d0a6d0eb2c02b9f73ae84ce2,1,[D2],,0,1,0,Person Re-identification with Joint-Loss,"Person re-identification is a technique that search the given target in the video surveillance network. This technique has been widely pplied to security and surveillance system, and also become a esearch hotspot in computer vision. Person re-identification has been challenging due to the large number of cameras in the network and ariation in camera angles, illumination, occlusion and poses. In this paper, we proposed a person re-id approach that can resist occlusions and variations based on a human pose guided convolution neural network framework with joint loss functions. We extract local features from body parts localized by landmarks, merge it with global features to learn the similarity metric. Identification loss and pose-constrained triplet loss function are jointly employed to train the model. Our approach outperforms most state-of-the-art methods on three large-scale datasets, with an accuracy of 83.31%, 86.1% and 72.6% on Cuhk03, Market1501 and Duke MTMC-reID respectively.",2017,2017 International Conference on Virtual Reality and Visualization (ICVRV),,10.1109/ICVRV.2017.00010,
da1e7d442d2e5d6b39a0015e241a2b3fe4036a13,0,,,1,0,0,"MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection","We present the Multiview Extended Video with Activities (MEVA) dataset, a new and very-large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. Our dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity. We have annotated 144 hours for 37 activity types, marking bounding boxes of actors and props. Our collection observed approximately 100 actors performing scripted scenarios and spontaneous background activity over a three-week period at an access-controlled venue, collecting in multiple modalities with overlapping and non-overlapping indoor and outdoor viewpoints. The resulting data includes video from 38 RGB and thermal IR cameras, 42 hours of UAV footage, as well as GPS locations for the actors. 122 hours of annotation are sequestered in support of the NIST Activity in Extended Video (ActEV) challenge; the other 22 hours of annotation and the corresponding video are available on our website, along with an additional 306 hours of ground camera data, 4.6 hours of UAV data, and 9.6 hours of GPS logs. Additional derived data includes camera models geo-registering the outdoor cameras and a dense 3D point cloud model of the outdoor scene. The data was collected with IRB oversight and approval and released under a CC-BY-4.0 license.",2020,ArXiv,2012.00914,,https://arxiv.org/pdf/2012.00914.pdf
daf5a4a48f9f007e41ff54b4aa2016449ad9c22d,1,[D1],,1,0,0,Tracking-by-Counting: Using Network Flows on Crowd Density Maps for Tracking Multiple Targets,"State-of-the-art multi-object tracking~(MOT) methods follow the tracking-by-detection paradigm, where object trajectories are obtained by associating per-frame outputs of object detectors. In crowded scenes, however, detectors often fail to obtain accurate detections due to heavy occlusions and high crowd density. In this paper, we propose a new MOT paradigm, tracking-by-counting, tailored for crowded scenes. Using crowd density maps, we jointly model detection, counting, and tracking of multiple targets as a network flow program, which simultaneously finds the global optimal detections and trajectories of multiple targets over the whole video. This is in contrast to prior MOT methods that either ignore the crowd density and thus are prone to errors in crowded scenes, or rely on a suboptimal two-step process using heuristic density-aware point-tracks for matching targets.Our approach yields promising results on public benchmarks of various domains including people tracking, cell tracking, and fish tracking.",2020,ArXiv,2007.09509,,https://arxiv.org/pdf/2007.09509.pdf
dc8135c8925eed81c2ced5157df3a7f1b47c4769,0,,,1,0,0,Amur Tiger Re-identification in the Wild,"Monitoring the population and movements of endangered species is an important task to wildlife conversation. Traditional tagging methods do not scale to large populations, while applying computer vision methods to camera sensor data requires re-identification (re-ID) algorithms to obtain accurate counts and moving trajectory of wildlife. However, existing re-ID methods are largely targeted at persons and cars, which have limited pose variations and constrained capture environments. This paper tries to fill the gap by introducing a novel large-scale dataset, the Amur Tiger Re-identification in the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. In contrast to typical re-ID datasets, the tigers are captured in a diverse set of unconstrained poses and lighting conditions. We demonstrate with a set of baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we propose a novel method for tiger re-identification, which introduces precise pose parts modeling in deep neural networks to handle large pose variation of tigers, and reaches notable performance improvement over existing re-ID methods. The dataset will be public available at this https URL .",2019,ArXiv,1906.05586,,https://arxiv.org/pdf/1906.05586.pdf
dd929617cd9554a974620a8e052e3c53de725481,0,,,1,0,0,FLAT MANIFOLD VAES,"Latent-variable models represent observed data by mapping a prior distribution over some latent space to an observed space. Often, the prior distribution is specified by the user to be very simple, effectively shifting the burden of a learning algorithm to the estimation of a highly non-linear likelihood function. This poses a problem for the calculation of a popular distance function, the geodesic between data points in the latent space, as this is often solved iteratively via numerical methods. These are less effective if the problem at hand is not well captured by first or secondorder approximations. In this work, we propose less complex likelihood functions by allowing complex distributions and explicitly penalising the curvature of the decoder. This results in geodesics which are approximated well by the Euclidean distance in latent space, decreasing the runtime by a factor of 1,000 with little loss in accuracy.",2019,,,,https://pdfs.semanticscholar.org/dd92/9617cd9554a974620a8e052e3c53de725481.pdf
dda70f6e730ecdf56da7ae1ceb448f2426ebfb13,1,[D2],,1,1,0,Occluded Person Re-Identification,"Person re-identification (re-id) suffers from a serious occlusion problem when applied to crowded public places. In this paper, we propose to retrieve a full-body person image by using a person image with occlusions. This differs significantly from the conventional person re-id problem where it is assumed that person images are detected without any occlusion. We thus call this new problem the occluded person re-identitification. To address this new problem, we propose a novel Attention Framework of Person Body (AFPB) based on deep learning, consisting of 1) an Occlusion Simulator (OS) which automatically generates artificial occlusions for full-body person images, and 2) multi-task losses that force the neural network not only to discriminate a person's identity but also to determine whether a sample is from the occluded data distribution or the full-body data distribution. Experiments on a new occluded person re-id dataset and three existing benchmarks modified to include full-body person images and occluded person images show the superiority of the proposed method.",2018,2018 IEEE International Conference on Multimedia and Expo (ICME),1804.02792,10.1109/ICME.2018.8486568,https://arxiv.org/pdf/1804.02792.pdf
df527756b33d1c2722bd005b246f7df75ea0520a,0,,,0,1,0,Selective deep ensemble for instance retrieval,"In public security systems, visual instance retrieval has an explosive growing requirement, especially for large-scale image or video databases. Due to its wide range of applications in surveillance scenario, this paper aims at the retrieval tasks centered around ‘vehicle’ and ‘pedestrian’ targets. Many previous CNN-based methods have not exploited the ensemble abilities of different models, which achieve limited accuracy since a certain kind of deep architecture is not comprehensive. On the other hand, some features in the original deep representation are useless for retrieval tasks, while the attention-aware compact representation will be much more efficient and effective. To address the above problems, we propose a Selective Deep Ensemble (SDE) framework to combine various models and features in a complementary way, inspired by the attention mechanism. It is demonstrated that a large improvement can be acquired with slight increase on computation cost. Finally, we evaluate the performance on three public instance-retrieval datasets, VehicleID, VeRi and Market-1501, outperforming state-of-the-art methods by a large margin.",2018,Multimedia Tools and Applications,,10.1007/s11042-018-5967-8,
e0747adcb8853e5deea4c093da5d1c59b8f7b04e,0,,,1,0,0,Improved Mutual Mean-Teaching for Unsupervised Domain Adaptive Re-ID,"In this technical report, we present our submission to the VisDA Challenge in ECCV 2020 and we achieved one of the top-performing results on the leaderboard. Our solution is based on Structured Domain Adaptation (SDA) and Mutual Mean-Teaching (MMT) frameworks. SDA, a domain-translation-based framework, focuses on carefully translating the source-domain images to the target domain. MMT, a pseudo-label-based framework, focuses on conducting pseudo label refinery with robust soft labels. Specifically, there are three main steps in our training pipeline. (i) We adopt SDA to generate source-to-target translated images, and (ii) such images serve as informative training samples to pre-train the network. (iii) The pre-trained network is further fine-tuned by MMT on the target domain. Note that we design an improved MMT (dubbed MMT+) to further mitigate the label noise by modeling inter-sample relations across two domains and maintaining the instance discrimination. Our proposed method achieved 74.78% accuracies in terms of mAP, ranked the 2nd place out of 153 teams.",2020,ArXiv,2008.10313,,https://arxiv.org/pdf/2008.10313.pdf
e095b62847324a4b48c4457f990a07ce6b85f641,1,[D2],,1,1,1,Self-Similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-Identification,"Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC→Market1501) and 4.4% (Market1501→DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00621,
e0c5f3e40c8b6ab4c0ca70c1cdd4518a78a9fc63,1,[D3],,0,0,1,Consistent Cross-view Matching for Unsupervised Person Re-identification,,2019,ArXiv,,,
e0db189e68e3b008836940d7e6971b3f718d2a10,0,,,0,1,0,Generating Person Images with Appearance-aware Pose Stylizer,"Generation of high-quality person images is challenging, due to the sophisticated entanglements among image factors, e.g., appearance, pose, foreground, background, local details, global structures, etc. In this paper, we present a novel end-to-end framework to generate realistic person images based on given person poses and appearances. The core of our framework is a novel generator called Appearance-aware Pose Stylizer (APS) which generates human images by coupling the target pose with the conditioned person appearance progressively. The framework is highly flexible and controllable by effectively decoupling various complex person image factors in the encoding phase, followed by re-coupling them in the decoding phase. In addition, we present a new normalization method named adaptive patch normalization, which enables region-specific normalization and shows a good performance when adopted in person image generation model. Experiments on two benchmark datasets show that our method is capable of generating visually appealing and realistic-looking results using arbitrary image and pose inputs.",2020,IJCAI,2007.09077,10.24963/ijcai.2020/87,https://arxiv.org/pdf/2007.09077.pdf
e14432ee3fe3ae41c8724349f88f28f59dc9a69c,1,[D2],,0,1,0,Beyond Scalar Neuron: Adopting Vector-Neuron Capsules for Long-Term Person Re-Identification,"Current person re-identification (re-ID) works mainly focus on the short-term scenario where a person is less likely to change clothes. However, in the long-term re-ID scenario, a person has a great chance to change clothes. A sophisticated re-ID system should take such changes into account. To facilitate the study of long-term re-ID, this paper introduces a large-scale re-ID dataset called “Celeb-reID” to the community. Unlike previous datasets, the same person can change clothes in the proposed Celeb-reID dataset. Images of Celeb-reID are acquired from the Internet using street snap-shots of celebrities. There is a total of 1,052 IDs with 34,186 images making Celeb-reID being the largest long-term re-ID dataset so far. To tackle the challenge of cloth changes, we propose to use vector-neuron (VN) capsules instead of the traditional scalar neurons (SN) to design our network. Compared with SN, one extra-dimensional information in VN can perceive cloth changes of the same person. We introduce a well-designed ReIDCaps network and integrate capsules to deal with the person re-ID task. Soft Embedding Attention (SEA) and Feature Sparse Representation (FSR) mechanisms are adopted in our network for performance boosting. Experiments are conducted on the proposed long-term re-ID dataset and two common short-term re-ID datasets. Comprehensive analyses are given to demonstrate the challenge exposed in our datasets. Experimental results show that our ReIDCaps can outperform existing state-of-the-art methods by a large margin in the long-term scenario. The new dataset and code will be released to facilitate future researches.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2948093,
e1af55ad7bb26e5e1acde3ec6c5c43cffe884b04,1,"[D2], [D4]",,1,1,0,Person Re-identification by Mid-level Attribute and Part-based Identity Learning,"Existing deep models using attributes usually take global features for identity classification and attribute recognition. However, some attributes exist in local position, such as a hat and shoes, therefore global feature alone is insufficient for person representation. In this work, we propose to use the attribute recognition as an auxiliary task for person re-identification. The attributes are recognised from the local regions of mid-level layers. Besides, we extract local features and global features from a high-level layer for identity classification. The mid-level attribute learning improves the discrimination of high-level features, and the local feature is complementary to the global feature. We report competitive results on two large-scale person re-identification benchmarks, Market-1501 and DukeMTMC-reID datasets, which demonstrate the effectiveness of the proposed method.",2018,ACML,,,https://pdfs.semanticscholar.org/e1af/55ad7bb26e5e1acde3ec6c5c43cffe884b04.pdf
e278218ba1ff1b85d06680e99b08e817d0962dab,1,[D1],,1,0,0,Tracking Persons-of-Interest via Unsupervised Representation Adaptation,"Multi-face tracking in unconstrained videos is a challenging problem as faces of one person often can appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Existing multi-target tracking methods often use low-level features which are not sufficiently discriminative for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face representations using convolutional neural networks (CNNs). Unlike existing CNN-based approaches which are only trained on large-scale face image datasets offline, we automatically generate a large number of training samples using the contextual constraints for a given video, and further adapt the pre-trained face CNN to the characters in the specific videos using discovered training samples. The embedding feature space is fine-tuned so that the Euclidean distance in the space corresponds to the semantic face similarity. To this end, we devise a symmetric triplet loss function which optimizes the network more effectively than the conventional triplet loss. With the learned discriminative features, we apply an EM clustering algorithm to link tracklets across multiple shots to generate the final trajectories. We extensively evaluate the proposed algorithm on two sets of TV sitcoms and YouTube music videos, analyze the contribution of each component, and demonstrate significant performance improvement over existing techniques.",2019,International Journal of Computer Vision,1710.02139,10.1007/s11263-019-01212-1,https://arxiv.org/pdf/1710.02139.pdf
e2e5236d6feb07556c4910cfabee4c371c3e2788,0,,,1,0,0,FGAGT: Flow-Guided Adaptive Graph Tracking,"Multi-object tracking (MOT) has always been a very important research direction in computer vision and has great applications in autonomous driving, video object behavior prediction, traffic management, and accident prevention. Recently, some methods have made great progress on MOT, such as CenterTrack, which predicts the trajectory position based on optical flow then tracks it, and FairMOT, which uses higher resolution feature maps to extract Re-id features. In this article, we propose the FGAGT tracker. Different from FairMOT, we use Pyramid Lucas Kanade optical flow method to predict the position of the historical objects in the current frame, and use ROI Pooling\cite{He2015} and fully connected layers to extract the historical objects' appearance feature vectors on the feature maps of the current frame. Next, input them and new objects' feature vectors into the adaptive graph neural network to update the feature vectors. The adaptive graph network can update the feature vectors of the objects by combining historical global position and appearance information. Because the historical information is preserved, it can also re-identify the occluded objects. In the training phase, we propose the Balanced MSE LOSS to balance the sample distribution. In the Inference phase, we use the Hungarian algorithm for data association. Our method reaches the level of state-of-the-art, where the MOTA index exceeds FairMOT by 2.5 points, and CenterTrack by 8.4 points on the MOT17 dataset, exceeds FairMOT by 7.2 points on the MOT16 dataset.",2020,ArXiv,,,
e2e833dc94a34c75c629cbb292c790e55bbab295,1,[D2],,0,1,0,Deep adversarial data augmentation with attribute guided for person re-identification,"Person re-identification (Re-ID) is aimed at matching the identity class of pedestrian image across multiple different camera views. Most existing Re-ID methods rely on learning model from labeled pairwise training data. This leads to poor scalability and usability due to the lack of mass identity labeling of images for every camera pairs. In this paper, we address this problem by proposing a deep adversarial learning approach capable of generating images for person Re-ID. Specifically, we propose a deep adversarial data augmentation method with attribute (DADAA) which generates various person images by generative adversarial augmentation. The mid-level attribute information is integrated into the proposed DADAA, which is formulated as learning a one-to-many mapping from labeled source dataset to a large-scale target dataset for increasing data diversity against overfitting. Extensive comparative evaluations show that the DADAA method significantly improves the performance of person Re-ID and validate the superiority of this DADAA method over some state-of-the-art methods on Market-1501 and DukeMTMC-ReID.",2019,,,10.1007/S11760-019-01523-3,
e6872f20a16fe4f07793cc458884d9378b3a82f5,0,,,0,1,0,Pose-Based View Synthesis for Vehicles: A Perspective Aware Method,"In this paper, we focus on the problem of novel view synthesis for vehicles. Some previous works solve the problem of novel view synthesis in a controlled 3D environment by exploiting additional 3D details (i.e., camera viewpoints and underlying 3D models). However, in real scenarios, the 3D details are difficult to obtain. In this case, we find that introducing vehicle pose to represent the views of vehicles is an alternative paradigm to solve the lack of 3D details. In novel view synthesis, preserving local details is one of the most challenging problems. To address this problem, we propose a perspective-aware generative model (PAGM). We are motivated by the prior that vehicles are made of quadrilateral planes. Preserving these rigid planes during image generation ensures that image details are kept. To this end, a classic image transformation method is leveraged, i.e., perspective transformation. In our GAN-based system, the perspective transformation is applied to the encoder feature maps, and the resulting maps are regarded as new conditions for the decoder. This strategy preserves the quadrilateral planes all the way through the network, thus shuttling the texture details from the input image to the generated image. In the experiments, we show that PAGM can generate high-quality vehicle images with fine details. Quantitatively, our method is superior to several competing approaches employing either GAN or the perspective transformation. Code is available at: https://github.com/ilvkai/view-synthesis-for-vehicles",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2980130,https://ieeexplore.ieee.org/ielx7/83/8835130/09042874.pdf
e6ace3e178d6277675f7c48849c21f3f503c6b62,1,[D2],,1,1,0,View-Invariant and Similarity Learning for Robust Person Re-Identification,"Person re-identification aims to identify pedestrians across non-overlapping camera views. Deep learning methods have been successfully applied to solving the problem and have achieved impressive results. However, these methods rely either on feature extraction or metric learning alone ignoring the joint benefit and mutual complementary effects of the person view-specific representation. In this paper, we propose a multi-view deep network architecture coupled with n-pair loss (JNPL) to eliminate the complex view discrepancy and learn nonlinear mapping functions that are view-invariant. We show that the problem of the large variation in viewpoints of a pedestrian can be well solved using a multi-view network. We simultaneously exploit the complementary representation shared between views and propose an adaptive similarity loss function to better learn a similarity metric. In detail, we first extract view-invariant feature representation from n-pair of images using multi-stream CNN and then aggregate these features for predictions. Given n-positive pairs and a negative example, the network aggregate the feature map of the n-positive pairs and predicts the identity of the person and at the same time learns features that discriminate positive pairs against the negative sample. Extensive evaluations on three large scale datasets demonstrate the substantial advantages of our method over existing state-of-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2960030,
e746447afc4898713a0bcf2bb560286eb4d20019,1,[D2],,1,1,0,Leveraging Virtual and Real Person for Unsupervised Person Re-Identification,"Person re-identification (re-ID) is a challenging instance retrieval problem, especially when identity annotations are not available for training. Although modern deep re-ID approaches have achieved great improvement, it is still difficult to optimize the deep re-ID model and learn discriminative person representation without annotations in training data. To address this challenge, this study considers the problem of unsupervised person re-ID and introduces a novel approach to solve this problem by leveraging virtual and real data. Our approach includes two components: virtual person generation and training of the deep re-ID model. For virtual person generation, we learn a person generation model and a camera style transfer model using unlabeled real data to generate virtual persons with different poses and camera styles. The virtual data is formed as labeled training data, enabling subsequent training deep re-ID model in supervision. For training of the deep re-ID model, we divide it into three steps: 1) pre-training a coarse re-ID model by using virtual data; 2) collaborative filtering based positive pair mining from the real data; and 3) fine-tuning of the coarse re-ID model by leveraging the mined positive pairs and virtual data. The final re-ID model is achieved by iterating between step 2 and step 3 until convergence. Extensive experiments demonstrate the effectiveness of our method. Experimental results on two large-scale datasets, Market-1501 and DukeMTMC-reID, show the advantages of our method over state-of-the-art approaches in unsupervised person re-ID. Our code is now available online.1",2020,IEEE Transactions on Multimedia,1811.02074,10.1109/TMM.2019.2957928,https://arxiv.org/pdf/1811.02074.pdf
e762ec6b873df5279176949f44fc497fb85ba5ed,1,"[D2], [D4]",,1,1,0,Person Re-Identification With Joint Verification and Identification of Identity-Attribute Labels,"One of the major challenges in person Re-Identification (ReID) is the inconsistent visual appearance of a person. Current works on visual feature and distance metric learning have achieved significant achievements, but still suffer from the limited robustness to pose variations, viewpoint changes, etc. This makes person ReID among multiple cameras still challenging. This work is motivated to learn mid-level human attributes which are robust to visual appearance variations and could be used as efficient features for person matching. We propose a supervised multi-task learning framework which considers attribute label information with joint identification-verification network to simultaneously learn an attribute-semantic and identity-discriminative feature representation. Specifically, this framework adopts the part-based deep neural network and learn three different tasks simultaneously: person identification, person verifications and attribute identification, so as to discover and capture concurrently complementary discriminative information about a person image from global and local image features and mid-level attribute features in one deep neural network. With the multi-task learning architecture, we obtain a discriminative model that reaches a synergy in distinguishing different person images, as manifested with the competitive accuracy on three person ReID datasets: Market1501, DukeMTMC-reID and VIPeR.",2019,IEEE Access,,10.1109/ACCESS.2019.2939071,
e78265f1ce6f08d003f56586e051b3fa7febcd87,1,[D2],,1,1,0,Joint multi-scale discrimination and region segmentation for person re-ID,"Abstract Most existing person re-identification methods are mainly based on human part partition with horizontal stripes or human body semantic segmentation. In this paper, we propose a method called MDRS (Multi-scale Discriminative network with Region Segmentation) to integrate multi-scale discriminative feature learning, horizontal stripe partition and semantic segmentation in a single framework, in which multi-scale horizontal stripe partition and usage of both global and local features make the framework be robust to human pose variation, occlusion and background clutter, and semantic segmentation boosts the performance of person identification via shared multi-scale feature extraction. MDRS is trained end-to-end with a multi-task learning strategy that considers three tasks simultaneously: person identification, triplet prediction and pixel-wise semantic segmentation. Comprehensive experiments confirm that our approach exceeds many methods and robustly achieves excellent performances on mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.08.022,
e7dfe797f2de0b1864f04422d83ef49149500231,1,[D2],,1,0,0,DGD: Densifying the Knowledge of Neural Networks with Filter Grafting and Knowledge Distillation,"With a fixed model structure, knowledge distillation and filter grafting are two effective ways to boost single model accuracy. However, the working mechanism and the differences between distillation and grafting have not been fully unveiled. In this paper, we evaluate the effect of distillation and grafting in the filter level, and find that the impacts of the two techniques are surprisingly complementary: distillation mostly enhances the knowledge of valid filters while grafting mostly reactivates invalid filters. This observation guides us to design a unified training framework called DGD, where distillation and grafting are naturally combined to increase the knowledge density inside the filters given a fixed model structure. Through extensive experiments, we show that the knowledge densified network in DGD shares both advantages of distillation and grafting, lifting the model accuracy to a higher level.",2020,ArXiv,2004.12311,,https://arxiv.org/pdf/2004.12311.pdf
e9fb48c89bf525e2be0e11492b817f88e97359ee,1,[D2],,0,1,0,Learning Task-oriented Disentangled Representations for Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to address the mismatch between the distributions of training and testing data, but unfortunately, they ignore the task-oriented information across domains and are inflexible to perform well in complicated open-set scenarios. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance.",2020,ArXiv,2007.13264,,https://arxiv.org/pdf/2007.13264.pdf
eb7a3d294158a26ae4af9bfc2c2e0419ea691a8e,0,,,1,0,0,Deep Motion Model for Pedestrian Tracking in 360 Degrees Videos,This paper proposes a deep convolutional neural network (CNN) for pedestrian tracking in 360\(^{\circ }\) videos based on the target’s motion.,2019,ICIAP,,10.1007/978-3-030-30642-7_4,
eb7e871e42d1bbdbad11762e9b26e9e1a9b866e9,1,[D1],,1,0,0,Filtering Point Targets via Online Learning of Motion Models,"Filtering point targets in highly cluttered and noisy data frames can be very challenging, especially for complex target motions. Fixed motion models can fail to provide accurate predictions, while learning based algorithm can be difficult to design (due to the variable number of targets), slow to train and dependent on separate train/test steps. To address these issues, this paper proposes a multi-target filtering algorithm which learns the motion models, on the fly, using a recurrent neural network with a long short-term memory architecture, as a regression block. The target state predictions are then corrected using a novel data association algorithm, with a low computational complexity. The proposed algorithm is evaluated over synthetic and real point target filtering scenarios, demonstrating a remarkable performance over highly cluttered data sequences.",2019,ArXiv,1902.0763,,https://arxiv.org/pdf/1902.07630.pdf
eca9511fa49679a9a8dab45552f8c83e4d96700d,1,[D2],,1,1,0,Learning to Generate Novel Domains for Domain Generalization,"This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model's ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.",2020,ECCV,2007.03304,10.1007/978-3-030-58517-4_33,https://arxiv.org/pdf/2007.03304.pdf
ed16d32d0c40c08620d882df9045cc720442aa7e,1,[D2],,1,1,0,Deep Fusion Feature Representation Learning With Hard Mining Center-Triplet Loss for Person Re-Identification,"Person re-identification (Re-ID) is a challenging task in the field of computer vision and focuses on matching people across images from different cameras. The extraction of robust feature representations from pedestrian images through CNNs with a single deterministic pooling operation is problematic as the features in real pedestrian images are complex and diverse. To address this problem, we propose a novel center-triplet (CT) model that combines the learning of robust feature representation and the optimization of metric loss function. Firstly, we design a fusion feature learning network (FFLN) with a novel fusion strategy consisting of max pooling and average pooling. Instead of adopting a single deterministic pooling operation, the FFLN combines two pooling operations that can learn high response values, bright features, and low response values, discriminative features simultaneously. Our model obtains more discriminative fusion features by adaptively learning the weights of the features learned by the corresponding pooling operations. In addition, we design a hard mining center-triplet loss (HCTL), a novel improved triplet loss, which effectively optimizes the intra/inter-class distance and reduces the cost of computing and mining hard training samples simultaneously, thereby enhancing the learning of robust feature representation. Finally, we proved our method can learn robust and discriminative feature representations for complex pedestrian images in real scenes. The experimental results also illustrate that our method achieves an 81.8% mAP and a 93.8% rank-1 accuracy on Market1501, a 68.2% mAP and an 83.3% rank-1 accuracy on DukeMTMC-ReID, and a 43.6% mAP and a 74.3% rank-1 accuracy on MSMT17, outperforming most state-of-the-art methods and achieving better performance for person re-identification.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2020.2972125,
edf9df3f6fac8892dad184a04303e873c1f123ab,1,"[D2], [D4]",,1,0,0,Person Search by Text Attribute Query As Zero-Shot Learning,"Existing person search methods predominantly assume the availability of at least one-shot imagery sample of the queried person. This assumption is limited in circumstances where only a brief textual (or verbal) description of the target person is available. In this work, we present a deep learning method for attribute text description based person search without any query imagery. Whilst conventional cross-modality matching methods, such as global visual-textual embedding based zero-shot learning and local individual attribute recognition, are functionally applicable, they are limited by several assumptions invalid to person search in deployment scale, data quality, and/or category name semantics. We overcome these issues by formulating an Attribute-Image Hierarchical Matching (AIHM) model. It is able to more reliably match text attribute descriptions with noisy surveillance person images by jointly learning global category-level and local attribute-level textual-visual embedding as well as matching. Extensive evaluations demonstrate the superiority of our AIHM model over a wide variety of state-of-the-art methods on three publicly available attribute labelled surveillance person search benchmarks: Market-1501, DukeMTMC, and PA100K.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00375,https://qmro.qmul.ac.uk/xmlui/bitstream/123456789/64519/2/Gong%20Person%20search%20by%202020%20Accepted.pdf
ee53e93b60bd3573deeb80eb91f139d3a67afcc5,1,[D2],,1,1,0,Unsupervised Domain Adaptation with Noise Resistible Mutual-Training for Person Re-identification,"Unsupervised domain adaptation (UDA) in the task of person re-identification (re-ID) is highly challenging due to large domain divergence and no class overlap between domains. Pseudo-label based selftraining is one of the representative techniques to address UDA. However, label noise caused by unsupervised clustering is always a trouble to selftraining methods. To depress noises in pseudo-labels, this paper proposes a Noise Resistible Mutual-Training (NRMT) method, which maintains two networks during training to perform collaborative clustering and mutual instance selection. On one hand, collaborative clustering eases the fitting to noisy instances by allowing the two networks to use pseudolabels provided by each other as an additional supervision. On the other hand, mutual instance selection further selects reliable and informative instances for training according to the peer-confidence and relationship disagreement of the networks. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art UDA methods for person re-ID.",2020,ECCV,,10.1007/978-3-030-58621-8_31,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560511.pdf
efdc83256186c2a641c3df7b83202f7011eac81a,0,,,0,1,0,Adversarial Open-World Person Re-Identification,"In a typical real-world application of re-id, a watch-list (gallery set) of a handful of target people (e.g. suspects) to track around a large volume of non-target people are demanded across camera views, and this is called the open-world person re-id. Different from conventional (closed-world) person re-id, a large portion of probe samples are not from target people in the open-world setting. And, it always happens that a non-target person would look similar to a target one and therefore would seriously challenge a re-id system. In this work, we introduce a deep open-world group-based person re-id model based on adversarial learning to alleviate the attack problem caused by similar non-target people. The main idea is learning to attack feature extractor on the target people by using GAN to generate very target-like images (imposters), and in the meantime the model will make the feature extractor learn to tolerate the attack by discriminative learning so as to realize group-based verification. The framework we proposed is called the adversarial open-world person re-identification, and this is realized by our Adversarial PersonNet (APN) that jointly learns a generator, a person discriminator, a target discriminator and a feature extractor, where the feature extractor and target discriminator share the same weights so as to makes the feature extractor learn to tolerate the attack by imposters for better group-based verification. While open-world person re-id is challenging, we show for the first time that the adversarial-based approach helps stabilize person re-id system under imposter attack more effectively.",2018,ECCV,1807.10482,10.1007/978-3-030-01216-8_18,https://arxiv.org/pdf/1807.10482.pdf
f0169abf130c8eaa79d89b9a227abe870d4602de,1,[D2],,0,1,0,A segmentation-based human alignment network for person re-identification with frequency weighting re-ranking,"Pedestrian re-identification (re-ID) is a computer vision technology to recognize an individual in non-overlapping multi-camera scenes. However, this process suffers from misalignment due to the influence of dramatic changes in person poses and views, and background interference. To address this issue, we focus on the discriminative feature representations by proposing a Segmentation-based Human Alignment Network, named SegHAN. It is exploited based on the segmentation of human body to alleviate the misalignment issue, which not only guarantees the robustness to pedestrian spatial locations, but also reduces background interference. In addition, a Frequency Weighting Re-ranking model (FWR) is designed to further enhance the performance of the proposed SegHAN. In this process, the frequency information obtained from the construction of similarity set is considered into the re-ranking model, which ranks the related images with high frequency in front. Experiments show that the proposed SegHAN and re-ranking model are beneficial to enhance re-ID performance and they both achieve a competitive performance when compared with state-of-the-art methods on three challenging re-ID datasets.",2019,,,,https://www.academiapublishing.org/print/Geng%20et%20al.pdf
f03234e32b87ac4b08655253cf206b1c0bb3a369,1,[D2],,1,0,0,MFBN: An Efficient Base Model For Person Re-Identification,"Person Re-IDentification (Re-ID) has developed rapidly with deep learning methods, as for these methods, the base mod- els used in most of them are not customized for Re-ID task. Although some studies have carefully designed the models special for Re-ID task, these models are always not easy to be the base model and expand with new methods due to their great complexity. In this paper, we propose a novel efficient base model named as Multi-granularity Feature Boosting Network (MFBN). MFBN consists of branches with information in different granularities. MFBN combines these branches into one whole, so MFBN is easy to be ex- tended as a base model. Moreover, MFBN applies feature boosting technique to boost fine granularitiy branch features with coarse granularity branch features, and applies channel- wise attention to increase diversities between features in multiple granularities. With these improves, MFBN has surpassed popular base models and got state-of-the-art results on mainstream Re-ID datasets including Market-1501, DukeMTMC-reID and CUHK-03. MFBN achieves results of rank-1/mAP=95.2%/93.2% on Market-1501 dataset and rank-1/mAP=90.3%/88.3% on DukeMTMC-reID dataset. Code and pretrained models are available at https://github.com/hsyi/Multi-granularity-Feature-Boosting-Network",2019,ICMAI 2019,,10.1145/3325730.3325764,
f07c4112063a624597cdc40c93a9e8d3f175f177,1,[D2],,1,1,0,UNSUPERVISED DEEP TRANSFER LEARNING APPROACH TO PERSON RE-IDENTIFICATION 1,"Person re-identification (Re-ID) aims at recognizing the same person from images taken across different cameras. To address this task, one typically requires a large amount labeled data for training an effective Re-ID model, which might not be practical for real-world applications. To alleviate this limitation, we choose to exploit a sufficient amount of pre-existing labeled data from a different (auxiliary) dataset. By jointly considering such an auxiliary dataset and the dataset of interest (but without label information), our proposed adaptation and re-identification network (ARN) performs unsupervised domain adaptation, which leverages information across datasets and derives domain-invariant features for Re-ID purposes. In our experiments, we verify that our network performs favorably against state-of-the-art unsupervised Re-ID approaches, and even outperforms a number of baseline ReID methods which require fully supervised data for training.",2018,,,,https://pdfs.semanticscholar.org/f07c/4112063a624597cdc40c93a9e8d3f175f177.pdf
f4a44b94d4b184e4638cb1019526170590f99ac4,1,[D2],,1,1,0,AM-LFS: AutoML for Loss Function Search,"Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1905.07375,10.1109/ICCV.2019.00850,https://arxiv.org/pdf/1905.07375.pdf
f4e65ab81a0f4ffa50d0c9bc308d7365e012cc75,1,[D3],,1,1,1,Deep Active Learning for Video-based Person Re-identification,"It is prohibitively expensive to annotate a large-scale video-based person re-identification (re-ID) dataset, which makes fully supervised methods inapplicable to real-world deployment. How to maximally reduce the annotation cost while retaining the re-ID performance becomes an interesting problem. In this paper, we address this problem by integrating an active learning scheme into a deep learning framework. Noticing that the truly matched tracklet-pairs, also denoted as 1 positives (TP), are the most informative samples for our re-ID model, we propose a sampling criterion to choose the most TP-likely tracklet-pairs for annotation. A view-aware sampling strategy considering view-specific biases is designed to facilitate candidate selection, followed by an adaptive resampling step to leave out the selected candidates that are unnecessary to annotate. Our method learns the re-ID model and updates the annotation set iteratively. The re-ID model is supervised by the tracklets' pesudo labels that are initialized by treating each tracklet as a distinct class. With the gained annotations of the actively selected candidates, the tracklets' pesudo labels are updated by label merging and further used to re-train our re-ID model. While being simple, the proposed method demonstrates its effectiveness on three video-based person re-ID datasets. Experimental results show that less than 3\% pairwise annotations are needed for our method to reach comparable performance with the fully-supervised setting.",2018,ArXiv,1812.05785,,https://arxiv.org/pdf/1812.05785.pdf
f8c4959ca67846d0c08f371ee884bb8a0845af1e,1,[D2],,1,0,0,Enhancing Model Performance of Person Re-Indentification on Unknown Target Domain,"Person re-identification(ReID) is the task that aims at retrieving the same person from the images taken across different cameras. Benefiting from the improvement of deep learning algorithms and the appearance of large datasets, the performance of ReID models has been greatly improved. However, most ReID models focus on a single dataset and their performance will drop dramatically when the train-set and test-set are from different datasets. To improve the generalization ability of the ReID model, this paper proposes a method that takes the advantage of triplet loss and multi-dataset training. And the experiment results show that this method can enhance the model performace in cross dataset usage.",2018,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),,10.1109/ICSESS.2018.8663745,
f9b87773c250616202c32f07ea92b2fd13cc680a,0,,,0,1,1,PoseTrackReID: Dataset Description,"Current datasets for video-based person re-identification (re-ID) do not include structural knowledge in form of human pose annotations for the persons of interest. Nonetheless, pose information is very helpful to disentangle useful feature information from background or occlusion noise. Especially real-world scenarios, such as surveillance, contain a lot of occlusions in human crowds or by obstacles. On the other hand, video-based person re-ID can benefit other tasks such as multi-person pose tracking in terms of robust feature matching. For that reason, we present PoseTrackReID, a largescale dataset for multi-person pose tracking and video-based person re-ID. With PoseTrackReID, we want to bridge the gap between person re-ID and multi-person pose tracking. Additionally, this dataset provides a good benchmark for current state-of-the-art methods on multi-frame person re-ID.",2020,ArXiv,2011.06243,,https://arxiv.org/pdf/2011.06243.pdf
fae3ff37995414fb9c5f1cac19301b7dff2f2bc8,0,,,1,0,0,The 2019 AI City Challenge,"The AI City Challenge has been created to accelerate intelligent video analysis that helps make cities smarter and safer. With millions of traffic video cameras acting as sensors around the world, there is a significant opportunity for real-time and batch analysis of these videos to provide actionable insights. These insights will benefit a wide variety of agencies, from traffic control to public safety. The 2019 AI City Challenge is the third annual edition in the AI City Challenge series with significant growing attention and participation. AI City Challenge 2019 enabled 334 academic and industrial research teams from 44 countries to solve real-world problems using real city-scale traffic camera video data. The Challenge was launched with three tracks. Track 1 addressed city-scale multi-camera vehicle tracking, Track 2 addressed city-scale vehicle re-identification, and Track 3 addressed traffic anomaly detection. Each track was chosen in consultation with departments of transportation focusing on problems of greatest public value. With the largest available dataset for such tasks, and ground truth for each track, the 2019 AI City Challenge received 129 submissions from 96 individuals teams (there were 22, 84, 23 team submissions from Tracks 1, 2, and 3 respectively). Participation in this challenge has grown five-fold this year as tasks have become more relevant to traffic optimization and challenging to the computer vision community. Results observed strongly underline the value AI brings to city-scale video analysis for traffic optimization.",2019,CVPR Workshops,,,https://pdfs.semanticscholar.org/87c0/1e88e58fa9059c6c3b4d8f3ebdecc8be0541.pdf
fdaf6546ce8920639b24325bebdf51e92fa0e39a,0,,,0,1,0,Group-Group Loss-Based Global-Regional Feature Learning for Vehicle Re-Identification,"Vehicle Re-Identification (Re-ID) is challenging because vehicles of the same model commonly show similar appearance. We tackle this challenge by proposing a Global-Regional Feature (GRF) that depicts extra local details to enhance discrimination power in addition to the global context. It is motivated by the observation that, vehicles of same color, maker, and model can be distinguished by their regional difference, e.g., the decorations on the windshields. To accelerate the GRF learning and promote its discrimination power, we propose a Group-Group Loss (GGL) to optimize the distance within and across vehicle image groups. Different from the siamese or triplet loss, GGL is directly computed on image groups rather than individual sample pairs or triplets. By avoiding traversing numerous sample combinations, GGL makes the model training easier and more efficient. Those two contributions highlight this work from previous methods on vehicle Re-ID task, which commonly learn global features with triplet loss or its variants. We evaluate our methods on two large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental results show our methods achieve promising performance in comparison with recent works.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2950796,
fea0895326b663bf72be89151a751362db8ae881,1,[D2],,1,1,0,Homocentric Hypersphere Feature Embedding for Person Re-Identification,"Triplet loss and softmax loss are two widely used loss functions in Person Re-Identification (Person ReID). However, previous works that try to apply these two loss functions have measure inconsistency during training and testing stage and among different parts of the total loss function, which would cause inferior performance of models. To address this issue, we propose a novel homocentric hypersphere embedding scheme to decouple magnitude and orientation information for both feature and weight vectors, and reformulate the triplet loss and the softmax loss to their angular versions and combine them into an angular discriminative loss. We evaluate our proposed method extensively on the widely used Person ReID benchmarks. Our method demonstrates leading performance on all datasets.",2019,2019 IEEE International Conference on Image Processing (ICIP),1804.08866,10.1109/ICIP.2019.8803735,https://arxiv.org/pdf/1804.08866.pdf
ffbe733a352c1d995f6f5c99ac0c7f01567165dc,1,,1,1,0,0,Track-Clustering Error Evaluation for Track-Based Multi-camera Tracking System Employing Human Re-identification,"In this study, we present a set of new evaluation measures for the track-based multi-camera tracking (T-MCT) task leveraging the clustering measurements. We demonstrate that the proposed evaluation measures provide notable advantages over previous ones. Moreover, a distributed and online T-MCT framework is proposed, where re-identification (Re-id) is embedded in T-MCT, to confirm the validity of the proposed evaluation measures. Experimental results reveal that with the proposed evaluation measures, the performance of T-MCT can be accurately measured, which is highly correlated to the performance of Re-id. Furthermore, it is also noted that our T-MCT framework achieves competitive score on the DukeMTMC dataset when compared to the previous work that used global optimization algorithms. Both the evaluation measures and the inter-camera tracking framework are proven to be the stepping stone for multi-camera tracking.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.184,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w17/papers/Wu_Track-Clustering_Error_Evaluation_CVPR_2017_paper.pdf
ffc82c0a385f9eec23a3249cac877b959d34e367,1,[D2],,1,1,0,Self Attention based multi branch Network for Person Re-Identification,"Recent progress in the field of person re-identification have shown promising improvement by designing neural networks to learn most discriminative features representations. Some efforts utilize similar parts from different locations to learn better representation with the help of soft attention, while others search for part based learning methods to enhance consecutive regions relationships in the learned features. However, only few attempts have been made to learn non-local similar parts directly for the person re-identification problem. In this paper, we propose a novel self attention based multi branch(classifier) network to directly model long-range dependencies in the learned features. Multi classifiers assist the model to learn discriminative features while self attention module encourages the learning to be independent of the feature map locations. Spectral normalization is applied in the whole network to improve the training dynamics and for the better convergence of the model. Experimental results on two benchmark datasets have shown the robustness of the proposed work.",2020,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),,10.23919/SpliTech49282.2020.9243741,
ffe4cf3720e378a67e5d43d0af044080ae286e12,0,,,0,0,1,One-Shot Learning on Attributed Sequences,"One-shot learning has become an important research topic in the last decade with many real-world applications. The goal of one-shot learning is to classify unlabeled instances when there is only one labeled example per class. Conventional problem setting of one-shot learning mainly focuses on the data that is already in a feature space (such as images). However, the data instances in real-world applications are often more complex and feature vectors may not be available. In this paper, we study the problem of one-shot learning on attributed sequences, where each instance is composed of a set of attributes (e.g., user profile) and a sequence of categorical items (e.g., clickstream). This problem is important for a variety of real-world applications ranging from fraud prevention to network intrusion detection. This problem is more challenging than the conventional one-shot learning since there are dependencies between attributes and sequences. We design a deep learning framework OLAS to tackle this problem. The proposed OLAS utilizes a twin network to generalize the features from pairwise attributed sequence examples. Empirical results on real-world datasets demonstrate the proposed OLAS can outperform the state-of-the-art methods under a rich variety of parameter settings.",2018,2018 IEEE International Conference on Big Data (Big Data),,10.1109/BigData.2018.8622257,https://web.cs.wpi.edu/~xkong/publications/papers/bigdata18.pdf