paperId,cites D9,cites D10,title,abstract,year,venue,arxivId,doi,pdfUrl
0008cb017b9659277632d26566e512a0bdfde553,1,0,Adaptive Variance Based Label Distribution Learning for Facial Age Estimation,"Estimating age from a single facial image is a classic and challenging topic in computer vision. One of its most intractable issues is label ambiguity, i.e., face images from adjacent age of the same person are often indistinguishable. Some existing methods adopt distribution learning to tackle this issue by exploiting the semantic correlation between age labels. Actually, most of them set a fixed value to the variance of Gaussian label distribution for all the images. However, the variance is closely related to the correlation between adjacent ages and should vary across ages and identities. To model a sample-specific variance, in this paper, we propose an adaptive variance based distribution learning (AVDL) method for facial age estimation. AVDL introduces the data-driven optimization framework, meta-learning, to achieve this. Specifically, AVDL performs a meta gradient descent step on the variable (i.e. variance) to minimize the loss on a clean unbiased validation set. By adaptively learning proper variance for each sample, our method can approximate the true age probability distribution more effectively. Extensive experiments on FG-NET and MORPH II datasets show the superiority of our proposed approach to the existing state-of-the-art methods.",2020,ECCV,,10.1007/978-3-030-58592-1_23,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680375.pdf
0026adb3591dd8a614f089cbb076a103b738b7ca,0,1,Photo Privacy Detection based on Text Classification and Face Clustering,"Nowadays, the photo privacy detection is becoming an acute task due to a wide spread of mobile devices with photos published on social networks. As a photo might contain private or sensitive data, there is an urgent need to accurately determine them and impose restrictions on their processing. In this paper we focus on the task of personal data detection in a photo gallery. A novel two-stage approach is proposed. At first, text of scanned documents is processed based on an EAST text detector, and extracted text is recognized using Tesseract and neural network classifier. At the second stage, face clustering is implemented for the remaining photos to identify large groups of people (friends, relatives) whose photos also refer to personal data and must be processed directly on a mobile device. The remaining images can be sent to a remote server for processing with higher accuracy. The experimental results of text recognition and face clustering methods using various convolutional networks for facial features extraction are presented. Keywords—photo privacy detection, face clustering, text detection and classification",2020,,,,http://ceur-ws.org/Vol-2665/paper39.pdf
00281325c4b0a662a2b7e15eabb647923f65dda2,0,1,MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets,"When there is a mismatch between the target identity and the driver identity, face reenactment suffers severe degradation in the quality of the result, especially in a few-shot setting. The identity preservation problem, where the model loses the detailed information of the target leading to a defective output, is the most common failure mode. The problem has several potential sources such as the identity of the driver leaking due to the identity mismatch, or dealing with unseen large poses. To overcome such problems, we introduce components that address the mentioned problem: image attention block, target feature alignment, and landmark transformer. Through attending and warping the relevant features, the proposed architecture, called MarioNETte, produces high-quality reenactments of unseen identities in a few-shot setting. In addition, the landmark transformer dramatically alleviates the identity preservation problem by isolating the expression geometry through landmark disentanglement. Comprehensive experiments are performed to verify that the proposed framework can generate highly realistic faces, outperforming all other baselines, even under a significant mismatch of facial characteristics between the target and the driver.",2020,AAAI,1911.08139,10.1609/AAAI.V34I07.6721,https://arxiv.org/pdf/1911.08139.pdf
004c7dbd5578865ac72cfa7b6ebc51c7fa7cda31,1,0,Two-Branch Recurrent Network for Isolating Deepfakes in Videos,"The current spike of hyper-realistic faces artificially generated using deepfakes calls for media forensics solutions that are tailored to video streams and work reliably with a low false alarm rate at the video level. We present a method for deepfake detection based on a two-branch network structure that isolates digitally manipulated faces by learning to amplify artifacts while suppressing the high-level face content. Unlike current methods that extract spatial frequencies as a preprocessing step, we propose a two-branch structure: one branch propagates the original information, while the other branch suppresses the face content yet amplifies multi-band frequencies using a Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate manipulated faces, we derive a novel cost function that, unlike regular classification, compresses the variability of natural faces and pushes away the unrealistic facial samples in the feature space. Our two novel components show promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview benchmarks, when compared to prior work. We then offer a full, detailed ablation study of our network architecture and cost function. Finally, although the bar is still high to get very remarkable figures at a very low false alarm rate, our study shows that we can achieve good video-level performance when cross-testing in terms of video-level AUC.",2020,ECCV,2008.03412,10.1007/978-3-030-58571-6_39,https://arxiv.org/pdf/2008.03412.pdf
00b73a32b94006a394789b01d01b98fe54c32ff0,0,1,COVID-MobileXpert: On-Device COVID-19 Patient Triage and Follow-up using Chest X-rays,"During the COVID-19 pandemic, there has been an emerging need for rapid, dedicated, and point-of-care COVID-19 patient disposition techniques to optimize resource utilization and clinical workflow. In view of this need, we present COVID-MobileXpert: a lightweight deep neural network (DNN) based mobile app that can use chest X-ray (CXR) for COVID-19 case screening and radiological trajectory prediction. We design and implement a novel three-player knowledge transfer and distillation (KTD) framework including a pre-trained attending physician (AP) network that extracts CXR imaging features from a large scale of lung disease CXR images, a fine-tuned resident fellow (RF) network that learns the essential CXR imaging features to discriminate COVID-19 from pneumonia and/or normal cases with a small amount of COVID-19 cases, and a trained lightweight medical student (MS) network to perform on-device COVID-19 patient triage and follow-up. To tackle the challenge of vastly similar and dominant fore- and background in medical images, we employ novel loss functions and training schemes for the MS network to learn the robust features. We demonstrate the significant potential of COVID-MobileXpert for rapid deployment via extensive experiments with diverse MS architecture and tuning parameter settings. The source codes for cloud and mobile based models are available from the following url: this https URL.",2020,,2004.03042,,https://arxiv.org/pdf/2004.03042.pdf
00f17b623cea39b09a543b0380e9e1291035d956,1,0,Crystal Loss and Quality Pooling for Unconstrained Face Verification and Recognition,"In recent years, the performance of face verification and recognition systems based on deep convolutional neural networks (DCNNs) has significantly improved. A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images or videos. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we propose a new loss function, called Crystal Loss, that restricts the features to lie on a hypersphere of a fixed radius. The loss can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly improves the performance of face verification and recognition systems. We achieve state-of-the-art performance for face verification and recognition on challenging LFW, IJB-A, IJB-B and IJB-C datasets over a large range of false alarm rates (10-1 to 10-7).",2018,ArXiv,1804.01159,,https://arxiv.org/pdf/1804.01159.pdf
01048ea47f35026433c488dbf7b06bfe98ebda55,1,0,ILRA: Novelty Detection in Face-Based Intervener Re-Identification,"Transparency laws facilitate citizens to monitor the activities of political representatives. In this sense, automatic or manual diarization of parliamentary sessions is required, the latter being time consuming. In the present work, this problem is addressed as a person re-identification problem. Re-identification is defined as the process of matching individuals under different camera views. This paper, in particular, deals with open world person re-identification scenarios, where the captured probe in one camera is not always present in the gallery collected in another one, i.e., determining whether the probe belongs to a novel identity or not. This procedure is mandatory before matching the identity. In most cases, novelty detection is tackled applying a threshold founded in a linear separation of the identities. We propose a threshold-less approach to solve the novelty detection problem, which is based on a one-class classifier and therefore it does not need any user defined threshold. Unlike other approaches that combine audio-visual features, an Isometric LogRatio transformation of a posteriori (ILRA) probabilities is applied to local and deep computed descriptors extracted from the face, which exhibits symmetry and can be exploited in the re-identification process unlike audio streams. These features are used to train the one-class classifier to detect the novelty of the individual. The proposal is evaluated in real parliamentary session recordings that exhibit challenging variations in terms of pose and location of the interveners. The experimental evaluation explores different configuration sets where our system achieves significant improvement on the given scenario, obtaining an average F measure of 71.29% for online analyzed videos. In addition, ILRA performs better than face descriptors used in recent face-based closed world recognition approaches, achieving an average improvement of 1.6% with respect to a deep descriptor.",2019,Symmetry,,10.3390/sym11091154,https://pdfs.semanticscholar.org/625b/1ae5f9483fc6a65c83970ad06f170fc0f366.pdf
01187c198b1cf620d15d1e3c35f14f6539cc227f,0,1,3D Instance Embedding Learning With a Structure-Aware Loss Function for Point Cloud Segmentation,"This letter presents a framework for 3D instance segmentation on point clouds. A 3D convolutional neural network is used as the backbone to generate semantic predictions and instance embeddings simultaneously. In addition to the embedding information, point clouds also provide 3D geometric information which reflects the relation between points. Considering both types of information, the structure-aware loss function is proposed to achieve discriminative embeddings for each 3D instance. To eliminate the quantization error caused by 3D voxel, the attention-based $k$-nearest neighbor (kNN) is proposed. Different from the average strategy, it learns different weights for different neighbors to aggregate and update the instance embeddings. Our network can be trained in an end-to-end style. Experiments show that our approach achieves state-of-the-art performance on two challenging datasets for instance segmentation.",2020,IEEE Robotics and Automation Letters,,10.1109/LRA.2020.3004802,
0123b06af5694d2389e530d8b7d0d66a6f5684a0,0,1,Deep Position-Aware Hashing for Semantic Continuous Image Retrieval,"Preserving the semantic similarity is one of the most important goals of hashing. Most existing deep hashing methods employ pairs or triplets of samples in training stage, which only consider the semantic similarity within a minibatch and depict the local positional relationship in Hamming space, leading to intermittent semantic similarity preservation. In this paper, we propose Deep Position-Aware Hashing (DPAH) to ensure continuous semantic similarity in Hamming space by modeling global positional relationship. Specifically, we introduce a set of learnable class centers as the global proxies to represent the global information and generate discriminative binary codes by constraining the distance between data points and class centers. In addition, in order to reduce the information loss caused by relaxing the binary codes to real-values in optimization, we propose kurtosis loss (KT loss) to handle the distribution of real-valued features before thresholding to be double-peak, and then enable the real-valued features to be more binarylike. Comprehensive experiments on three datasets show that our DPAH outperforms state-of-the-art methods.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093468,http://openaccess.thecvf.com/content_WACV_2020/papers/Wang_Deep_Position-Aware_Hashing_for_Semantic_Continuous_Image_Retrieval_WACV_2020_paper.pdf
019bd8a33a5ac7d548b26eaa3c83605f2e9cb07f,1,0,"Mitigating Bias in Gender, Age and Ethnicity Classification: A Multi-task Convolution Neural Network Approach","This work explores joint classification of gender, age and race. Specifically, we here propose a Multi-Task Convolution Neural Network (MTCNN) employing joint dynamic loss weight adjustment towards classification of named soft biometrics, as well as towards mitigation of soft biometrics related bias. The proposed algorithm achieves promising results on the UTKFace and the Bias Estimation in Face Analytics (BEFA) datasets and was ranked first in the BEFA Challenge of the European Conference of Computer Vision (ECCV) 2018.",2018,ECCV Workshops,,10.1007/978-3-030-11009-3_35,https://hal.inria.fr/hal-01892103/file/DasDantchevaBremond_ECCVW_18.pdf
01a5a65451f5343bca8f3c75e583b7c4c89187dd,0,1,MMA Regularization: Decorrelating Weights of Neural Networks by Maximizing the Minimal Angles,"The strong correlation between neurons or filters can significantly weaken the generalization ability of neural networks. Inspired by the well-known Tammes problem, we propose a novel diversity regularization method to address this issue, which makes the normalized weight vectors of neurons or filters distributed on a hypersphere as uniformly as possible, through maximizing the minimal pairwise angles (MMA). This method can easily exert its effect by plugging the MMA regularization term into the loss function with negligible computational overhead. The MMA regularization is simple, efficient, and effective. Therefore, it can be used as a basic regularization method in neural network training. Extensive experiments demonstrate that MMA regularization is able to enhance the generalization ability of various modern models and achieves considerable performance improvements on CIFAR100 and TinyImageNet datasets. In addition, experiments on face verification show that MMA regularization is also effective for feature learning.",2020,ArXiv,2006.06527,,https://arxiv.org/pdf/2006.06527.pdf
01c995681d6e825b82a554072f935552c9a725ff,0,1,PF-cpGAN: Profile to Frontal Coupled GAN for Face Recognition in the Wild,"In recent years, due to the emergence of deep learning, face recognition has achieved exceptional success. However, many of these deep face recognition models perform relatively poorly in handling profile faces compared to frontal faces. The major reason for this poor performance is that it is inherently difficult to learn large pose invariant deep representations that are useful for profile face recognition. In this paper, we hypothesize that the profile face domain possesses a gradual connection with the frontal face domain in the deep feature space. We look to exploit this connection by projecting the profile faces and frontal faces into a common latent space and perform verification or retrieval in the latent domain. We leverage a coupled generative adversarial network (cpGAN) structure to find the hidden relationship between the profile and frontal images in a latent common embedding subspace. Specifically, the cpGAN framework consists of two GAN-based sub-networks, one dedicated to the frontal domain and the other dedicated to the profile domain. Each sub-network tends to find a projection that maximizes the pair-wise correlation between two feature domains in a common embedding feature subspace. The efficacy of our approach compared with the state-of-the-art is demonstrated using the CFP, CMU MultiPIE, IJB-A, and IJB-C datasets.",2020,ArXiv,2005.02166,,https://arxiv.org/pdf/2005.02166.pdf
02023b554ee2ef34764c5c5f63aa1c5d3ba64830,0,1,Multi-Modal Face Presentation Attack Detection,"Abstract For the last ten years, face biometric research has been intensively studied by the computer vision community. Face recognition systems have been used in mobile, banking, and surveillance ...",2020,,,10.2200/s01032ed1v01y202007cov017,https://www.morganclaypoolpublishers.com/catalog_Orig/samples/9781681739236_sample.pdf
0251807835a2d863c809c25b3d5899d6431dbe89,1,0,Robust Conditional Generative Adversarial Networks,"Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise. The regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGAN unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Our model augments the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and experimentally verify that our model outperforms existing state-of-the-art cGAN architectures by a large margin in a variety of domains including images from natural scenes and faces.",2019,ICLR,1805.08657,,https://arxiv.org/pdf/1805.08657.pdf
0260782796630906e3e24563498f9c18d8ac90df,0,1,Speaker Embeddings Incorporating Acoustic Conditions for Diarization,"We present our work on training speaker embeddings, especially effective for speaker diarization. For various speaker recognition tasks, extracting speaker embeddings using Deep Neural Networks (DNNs) has become major methods. These embeddings are generally trained to be discriminate speakers and be robust with respect to different acoustic conditions. In speaker diarization, however, the acoustic conditions can be used as consistent information for discriminating speakers. Such information can include the distances to a microphone in a meeting, or the channels for each speaker in telephone conversation recorded in monaural. Hence, the proposed speaker-embedding network leverages differences in acoustic conditions to train effective speaker embeddings for speaker diarization. The information on acoustic conditions can be anything that contributes to distinguishing between recording environments; for example, we explore using i-vectors. Experiments conducted on a practical diarization system demonstrated that the proposed embeddings significantly improve performance over embeddings without information on acoustic conditions.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9054273,
0263c0b6ceae59b8cea0acff6ccaf1d6dbffbf29,0,1,Statistical insights into deep neural network learning in subspace classification,"Correspondence Hao Wu, Department of Mathematics, Dornsife College of Letters, Arts and Sciences, University of Southern California, Los Angeles, CA 90089-0001, USA. Email: hwu409@usc.edu Deep learning has benefited almost every aspect of modern big data applications. Yet its statistical properties still remain largely unexplored. It is commonly believed nowadays that deep neural networks (DNNs) benefit from representational learning. To gain some statistical insights into this, we design a simple simulation setting where we generate data from some latent subspace structure with each subspace regarded as a cluster. We empirically demonstrate that the performance of DNN is very similar to that of the two-step procedure of clustering followed by classification (unsupervised plus supervised). This motivates us to ask: Does DNN indeed mimic the two-step procedure statistically? That is, do bottom layers in DNN try to cluster first and then top layers classify within each cluster? To answer this question, we conduct a series of simulation studies, and to our surprise, none of the hidden layers in DNN conduct successful clustering. In some sense, our results provide an important complement to the common belief of representational learning, suggesting that at least in some model settings, although the performance of DNN is comparable with that of the ideal two-step procedure knowing the true latent cluster information a priori, it does not really do clustering in any of its layers. We also provide some statistical insights and heuristic arguments to support our empirical discoveries and further demonstrate the revealed phenomenon on the real data application of traffic sign recognition.",2020,,,10.1002/sta4.273,http://faculty.marshall.usc.edu/jinchi-lv/publications/Stat-WFL20.pdf
02ae0dbbb26616e8d70ff1bc52f386cf651d5e2e,1,1,Support Vector Guided Softmax Loss for Face Recognition,"Face recognition has witnessed significant progresses due to the advances of deep convolutional neural networks (CNNs), the central challenge of which, is feature discrimination. To address it, one group tries to exploit mining-based strategies (\textit{e.g.}, hard example mining and focal loss) to focus on the informative examples. The other group devotes to designing margin-based loss functions (\textit{e.g.}, angular, additive and additive angular margins) to increase the feature margin from the perspective of ground truth class. Both of them have been well-verified to learn discriminative features. However, they suffer from either the ambiguity of hard examples or the lack of discriminative power of other classes. In this paper, we design a novel loss function, namely support vector guided softmax loss (SV-Softmax), which adaptively emphasizes the mis-classified points (support vectors) to guide the discriminative features learning. So the developed SV-Softmax loss is able to eliminate the ambiguity of hard examples as well as absorb the discriminative power of other classes, and thus results in more discrimiantive features. To the best of our knowledge, this is the first attempt to inherit the advantages of mining-based and margin-based losses into one framework. Experimental results on several benchmarks have demonstrated the effectiveness of our approach over state-of-the-arts.",2018,ArXiv,1812.11317,,https://arxiv.org/pdf/1812.11317.pdf
02d2aacdf892661aa7fe48e184952a75bb7ae65e,1,1,Delving into the Adversarial Robustness on Face Recognition,"Face recognition has recently made substantial progress and achieved high accuracy on standard benchmarks based on the development of deep convolutional neural networks (CNNs). However, the lack of robustness in deep CNNs to adversarial examples has raised security concerns to enormous face recognition applications. To facilitate a better understanding of the adversarial vulnerability of the existing face recognition models, in this paper we perform comprehensive robustness evaluations, which can be applied as reference for evaluating the robustness of subsequent works on face recognition. We investigate 15 popular face recognition models and evaluate their robustness by using various adversarial attacks as an important surrogate. These evaluations are conducted under diverse adversarial settings, including dodging and impersonation attacks, $\ell_2$ and $\ell_\infty$ attacks, white-box and black-box attacks. We further propose a landmark-guided cutout (LGC) attack method to improve the transferability of adversarial examples for black-box attacks, by considering the special characteristics of face recognition. Based on our evaluations, we draw several important findings, which are crucial for understanding the adversarial robustness and providing insights for future research on face recognition. Code is available at \url{this https URL}.",2020,ArXiv,2007.04118,,https://arxiv.org/pdf/2007.04118.pdf
0307102f70490339cd8c07ae8d5df26b3cd35d75,1,0,Scalable algorithms for large-scale machine learning problems : Application to multiclass classification and asynchronous distributed optimization. (Algorithmes d'apprentissage pour les grandes masses de données : Application à la classification multi-classes et à l'optimisation distribuée asynchron,"This thesis focuses on developing scalable algorithms for large scale machine learning. In this work, we present two perspectives to handle large data. First, we consider the problem of large-scale multiclass classification. We introduce the task of multiclass classification and the challenge of classifying with a large number of classes. To alleviate these challenges, we propose an algorithm which reduces the original multiclass problem to an equivalent binary one. Based on this reduction technique, we introduce a scalable method to tackle the multiclass classification problem for very large number of classes and perform detailed theoretical and empirical analyses.In the second part, we discuss the problem of distributed machine learning. In this domain, we introduce an asynchronous framework for performing distributed optimization. We present application of the proposed asynchronous framework on two popular domains: matrix factorization for large-scale recommender systems and large-scale binary classification. In the case of matrix factorization, we perform Stochastic Gradient Descent (SGD) in an asynchronous distributed manner. Whereas, in the case of large-scale binary classification we use a variant of SGD which uses variance reduction technique, SVRG as our optimization algorithm.",2017,,,,https://pdfs.semanticscholar.org/0307/102f70490339cd8c07ae8d5df26b3cd35d75.pdf
0363e3be1971703d74ebeb2d80dd0941f4f8ed2d,0,1,Score Normalization of X-Vector Speaker Verification System for Short-Duration Speaker Verification Challenge,"In this paper we present our contribution to the task 2 of the short-duration speaker verification (SdSV) challenge. The main task for this challenge is to find new technologies for text-dependent and text-independent speaker verification in short duration scenario. Some of the approaches used by the authors during participation in the challenge are presented. Described speaker verification systems include baseline x-vector system with PLDA backend and score normalization, x-vector system with neural PLDA backend and fusion of both systems.",2020,SPECOM,,10.1007/978-3-030-60276-5_44,
03878f5348ed24fcb20538efa30089ee9b699f8d,1,0,Learning Neural Bag-of-Features for Large-Scale Image Retrieval,"In this paper, the well-known bag-of-features (BoFs) model is generalized and formulated as a neural network that is composed of three layers: 1) a radial basis function (RBF) layer; 2) an accumulation layer; and 3) a fully connected layer. This formulation allows for decoupling the representation size from the number of used codewords, as well as for better modeling the feature distribution using a separate trainable scaling parameter for each RBF neuron. The resulting network, called retrieval-oriented neural BoF (RN-BoF), is trained using regular back propagation and allows for fast extraction of compact image representations. It is demonstrated that the RN-BoF model is capable of: 1) increasing the object encoding and retrieval speed; 2) reducing the extracted representation size; and 3) increasing the retrieval precision. A symmetry-aware spatial segmentation technique is also proposed to further reduce the encoding time and the storage requirements and allows the method to efficiently scale to large datasets. The proposed method is evaluated and compared to other state-of-the-art techniques using five different image datasets, including the large-scale YouTube Faces database.",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",,10.1109/TSMC.2017.2680404,http://poseidon.csd.auth.gr/papers/PUBLISHED/JOURNAL/pdf/2017/fast-bof-retrieval.pdf
039be37ad95d009c8114b7bfa963b09624cd427b,1,0,Semantic-Aware Makeup Cleanser,"Face verification aims at determining whether a pair of face images belongs to the same identity. Recent studies have revealed the negative impact of facial makeup on the verification performance. With the rapid development of deep generative models, this paper proposes a semantic-aware makeup cleanser (SAMC) to remove facial makeup under different poses and expressions and achieve verification via generation. The intuition lies in the fact that makeup is a combined effect of multiple cosmetics and tailored treatments should be imposed on different cosmetic regions. To this end, we present both unsupervised and supervised semantic-aware learning strategies in SAMC. At image level, an unsupervised attention module is jointly learned with the generator to locate cosmetic regions and estimate the degree. At feature level, we resort to the effort of face parsing merely in training phase and design a localized texture loss to serve complements and pursue superior synthetic quality. The experimental results on four makeup-related datasets verify that SAMC not only produces appealing de-makeup outputs at a resolution of 256 × 256, but also facilitates makeup-invariant face verification through image generation.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",,10.1109/BTAS46853.2019.9186001,
03c1d6c9dc967e4f54041f30f69e5bc1b8997d45,0,1,S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning,"Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero-shot retrieval applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives. However, generalization capacity is known to scale with the embedding space dimensionality. Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications. To remedy this, we propose S2SD - Simultaneous Similarity-based Self-distillation. S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offering notable improvements of up to 7% in Recall@1, while also setting a new state-of-the-art. Code available at this https URL.",2020,ArXiv,2009.08348,,https://arxiv.org/pdf/2009.08348.pdf
03c6691759a7a4ee7f6021be12cf9e1c448eb4d8,0,1,Additive Angular Margin for Few Shot Learning to Classify Clinical Endoscopy Images,"Endoscopy is a widely used imaging modality to diagnose and treat diseases in hollow organs as for example the gastrointestinal tract, the kidney and the liver. However, due to varied modalities and use of different imaging protocols at various clinical centers impose significant challenges when generalising deep learning models. Moreover, the assembly of large datasets from different clinical centers can introduce a huge label bias that renders any learnt model unusable. Also, when using new modality or presence of images with rare patterns, a bulk amount of similar image data and their corresponding labels are required for training these models. In this work, we propose to use a few-shot learning approach that requires less training data and can be used to predict label classes of test samples from an unseen dataset. We propose a novel additive angular margin metric in the framework of prototypical network in few-shot learning setting. We compare our approach to the several established methods on a large cohort of multi-center, multi-organ, and multi-modal endoscopy data. The proposed algorithm outperforms existing state-of-the-art methods.",2020,MLMI@MICCAI,2003.10033,10.1007/978-3-030-59861-7_50,https://arxiv.org/pdf/2003.10033.pdf
0417722d4005efbf0e7b81ea8b70176863c5348e,0,1,Norm-Aware Embedding for Efficient Person Search,"Person Search is a practically relevant task that aims to jointly solve Person Detection and Person Re-identification (re-ID). Specifically, it requires to find and locate all instances with the same identity as the query person in a set of panoramic gallery images. One major challenge comes from the contradictory goals of the two sub-tasks, i.e., person detection focuses on finding the commonness of all persons while person re-ID handles the differences among multiple identities. Therefore, it is crucial to reconcile the relationship between the two sub-tasks in a joint person search model. To this end, We present a novel approach called Norm-Aware Embedding to disentangle the person embedding into norm and angle for detection and re-ID respectively, allowing for both effective and efficient multi-task training. We further extend the proposal-level person embedding to pixel-level, whose discrimination ability is less affected by mis-alignment. We outperform other one-step methods by a large margin and achieve comparable performance to two-step methods on both CUHK-SYSU and PRW. Also, Our method is easy to train and resource-friendly, running at 12 fps on a single GPU.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.01263,http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Norm-Aware_Embedding_for_Efficient_Person_Search_CVPR_2020_paper.pdf
042b6332052555a047beb80617a4c9b885985f97,1,0,SuperFront: From Low-resolution to High-resolution Frontal Face Synthesis,"Advances in face rotation, along with other face-based generative tasks, are more frequent as we advance further in topics of deep learning. Even as impressive milestones are achieved in synthesizing faces, the importance of preserving identity is needed in practice and should not be overlooked. Also, the difficulty should not be more for data with obscured faces, heavier poses, and lower quality. Existing methods tend to focus on samples with variation in pose, but with the assumption data is high in quality. We propose a generative adversarial network (GAN) -based model to generate high-quality, identity preserving frontal faces from one or multiple low-resolution (LR) faces with extreme poses. Specifically, we propose SuperFront GAN (SF-GAN) to synthesize a high-resolution (HR), frontal face from oneto-many LR faces with various poses and with the identitypreserved. We integrate a super-resolution (SR) side-view module into SF-GAN to preserve identity information and fine details of the side-views in HR space, which helps model reconstruct high-frequency information of faces (i.e. periocular, nose, and mouth regions). Moreover, SF-GAN accepts multiple LR faces as input, and improves each added sample. We squeeze additional gain in performance with an orthogonal constraint in the generator to penalize redundant latent representations and, hence, diversify the learned features space. Quantitative and qualitative results demonstrate the superiority of SF-GAN over others.",2020,ArXiv,2012.04111,,https://arxiv.org/pdf/2012.04111.pdf
04855baeea6bf804375ab88e7c1059eb90b7599b,1,0,Image-Image Translation to Enhance Near Infrared Face Recognition,"With the rapid development of facial recognition, the research field of near infrared (NIR) face recognition, which is less sensitive to illumination levels, has attracted increased attention. Unfortunately, directly applying the face recognition model trained using visible light (VIS) data to NIR face data does not produce a satisfactory performance. This is due to the domain bias between the NIR images and the VIS images. To this end, we created the Outdoor NIR-VIS Face (ONVF) database and Indoor NIR Face (INF) database to increase the number of near infrared facial images for system training and evaluation. In this paper, we propose an efficient NIR face recognition method, which consists of face detection and alignment, NIR-VIS image translation and face embedding. The NIR-VIS image conversion model is capable of transforming near-infrared facial images into their corresponding VIS images whilst maintaining sufficient identity information to enable existing VIS facial recognition models to perform recognition. Extensive experiments using the INF dataset and the CSIST database have demonstrated that the proposed method yields a consistent and competitive performance for near infrared face recognition.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8804414,
04d050642c11c1744607926e36c35cb99d6f5205,0,1,Angular Margin Centroid Loss for Text-Independent Speaker Recognition,"Speaker recognition for unseen speakers out of the training dataset relies on the discrimination of speaker embedding. Recent studies use the angular softmax losses with angular margin penalties to enhance the intra-class compactness of speaker embedding, which achieve obvious performance improvement. However, the classification layer encounters the problem of dimension explosion in these losses with the growth of training speakers. In this paper, like the prototype network loss in the few-short learning and the generalized end-to-end loss, we optimize the cosine distances between speaker embeddings and their corresponding centroids rather than the weight vectors in the classification layer. For the intra-class compactness, we impose the additive angular margin to shorten the cosine distance between speaker embeddings belonging to the same speaker. Meanwhile, we also explicitly improve the inter-class separability by enlarging the cosine distance between different speaker centroids. Experiments show that our loss achieves comparable performance with the stat-of-the-art angular margin softmax loss in both verification and identification tasks and markedly reduces the training iterations.",2020,INTERSPEECH,,10.21437/interspeech.2020-2538,https://isca-speech.org/archive/Interspeech_2020/pdfs/2538.pdf
04d5936f677a54a62349fed61e2962ede0db905d,0,1,Rectified Wing Loss for Efficient and Robust Facial Landmark Localisation with Convolutional Neural Networks,"Efficient and robust facial landmark localisation is crucial for the deployment of real-time face analysis systems. This paper presents a new loss function, namely Rectified Wing (RWing) loss, for regression-based facial landmark localisation with Convolutional Neural Networks (CNNs). We first systemically analyse different loss functions, including L2, L1 and smooth L1. The analysis suggests that the training of a network should pay more attention to small-medium errors. Motivated by this finding, we design a piece-wise loss that amplifies the impact of the samples with small-medium errors. Besides, we rectify the loss function for very small errors to mitigate the impact of inaccuracy of manual annotation. The use of our RWing loss boosts the performance significantly for regression-based CNNs in facial landmarking, especially for lightweight network architectures. To address the problem of under-representation of samples with large pose variations, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation strategies. Last, the proposed approach is extended to create a coarse-to-fine framework for robust and efficient landmark localisation. Moreover, the proposed coarse-to-fine framework is able to deal with the small sample size problem effectively. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits of our RWing loss and prove the superiority of the proposed method over the state-of-the-art approaches.",2019,International Journal of Computer Vision,,10.1007/s11263-019-01275-0,https://link.springer.com/content/pdf/10.1007/s11263-019-01275-0.pdf
04fbf11a121beef28338e18815a878deefa338a9,1,0,Wasserstein CNN: Learning Invariant Features for NIR-VIS Face Recognition,"Heterogeneous face recognition (HFR) aims at matching facial images acquired from different sensing modalities with mission-critical applications in forensics, security and commercial sectors. However, HFR presents more challenging issues than traditional face recognition because of the large intra-class variation among heterogeneous face images and the limited availability of training samples of cross-modality face image pairs. This paper proposes the novel Wasserstein convolutional neural network (WCNN) approach for learning invariant features between near-infrared (NIR) and visual (VIS) face images (i.e., NIR-VIS face recognition). The low-level layers of the WCNN are trained with widely available face images in the VIS spectrum, and the high-level layer is divided into three parts: the NIR layer, the VIS layer and the NIR-VIS shared layer. The first two layers aim at learning modality-specific features, and the NIR-VIS shared layer is designed to learn a modality-invariant feature subspace. The Wasserstein distance is introduced into the NIR-VIS shared layer to measure the dissimilarity between heterogeneous feature distributions. W-CNN learning is performed to minimize the Wasserstein distance between the NIR distribution and the VIS distribution for invariant deep feature representations of heterogeneous face images. To avoid the over-fitting problem on small-scale heterogeneous face data, a correlation prior is introduced on the fully-connected WCNN layers to reduce the size of the parameter space. This prior is implemented by a low-rank constraint in an end-to-end network. The joint formulation leads to an alternating minimization for deep feature representation at the training stage and an efficient computation for heterogeneous data at the testing stage. Extensive experiments using three challenging NIR-VIS face recognition databases demonstrate the superiority of the WCNN method over state-of-the-art methods.",2019,IEEE Transactions on Pattern Analysis and Machine Intelligence,1708.02412,10.1109/TPAMI.2018.2842770,https://arxiv.org/pdf/1708.02412.pdf
0501b8a99270a20c7536ed2f6df6569413810f6d,1,1,Apprentissage neuronal profond pour l'analyse de contenus multimodaux et temporels. (Deep learning for multimodal and temporal contents analysis),"Notre perception est par nature multimodale, i.e. fait appel a plusieurs de nos sens. Pour resoudre certaines tâches, il est donc pertinent d’utiliser differentes modalites, telles que le son ou l’image.Cette these s’interesse a cette notion dans le cadre de l’apprentissage neuronal profond. Pour cela, elle cherche a repondre a une problematique en particulier : comment fusionner les differentes modalites au sein d’un reseau de neurones ?Nous proposons tout d’abord d’etudier un probleme d’application concret : la reconnaissance automatique des emotions dans des contenus audio-visuels.Cela nous conduit a differentes considerations concernant la modelisation des emotions et plus particulierement des expressions faciales. Nous proposons ainsi une analyse des representations de l’expression faciale apprises par un reseau de neurones profonds.De plus, cela permet d’observer que chaque probleme multimodal semble necessiter l’utilisation d’une strategie de fusion differente.C’est pourquoi nous proposons et validons ensuite deux methodes pour obtenir automatiquement une architecture neuronale de fusion efficace pour un probleme multimodal donne, la premiere se basant sur un modele central de fusion et ayant pour visee de conserver une certaine interpretation de la strategie de fusion adoptee, tandis que la seconde adapte une methode de recherche d'architecture neuronale au cas de la fusion, explorant un plus grand nombre de strategies et atteignant ainsi de meilleures performances.Enfin, nous nous interessons a une vision multimodale du transfert de connaissances. En effet, nous detaillons une methode non traditionnelle pour effectuer un transfert de connaissances a partir de plusieurs sources, i.e. plusieurs modeles pre-entraines. Pour cela, une representation neuronale plus generale est obtenue a partir d’un modele unique, qui rassemble la connaissance contenue dans les modeles pre-entraines et conduit a des performances a l'etat de l'art sur une variete de tâches d'analyse de visages.",2019,,,,https://pdfs.semanticscholar.org/0501/b8a99270a20c7536ed2f6df6569413810f6d.pdf
0523c90754956535f5daa3069d7a26ac7ca0df78,0,1,Server-Driven Video Streaming for Deep Learning Inference,"Video streaming is crucial for AI applications that gather videos from sources to servers for inference by deep neural nets (DNNs). Unlike traditional video streaming that optimizes visual quality, this new type of video streaming permits aggressive compression/pruning of pixels not relevant to achieving high DNN inference accuracy. However, much of this potential is left unrealized, because current video streaming protocols are driven by the video source (camera) where the compute is rather limited. We advocate that the video streaming protocol should be driven by real-time feedback from the server-side DNN. Our insight is two-fold: (1) server-side DNN has more context about the pixels that maximize its inference accuracy; and (2) the DNN's output contains rich information useful to guide video streaming. We present DDS (DNN-Driven Streaming), a concrete design of this approach. DDS continuously sends a low-quality video stream to the server; the server runs the DNN to determine where to re-send with higher quality to increase the inference accuracy. We find that compared to several recent baselines on multiple video genres and vision tasks, DDS maintains higher accuracy while reducing bandwidth usage by upto 59% or improves accuracy by upto 9% with no additional bandwidth usage.",2020,SIGCOMM,,10.1145/3387514.3405887,
056d7a8e5f05249ac106f24e53f148c30e6c724c,1,0,RGB-D Face Recognition: A Comparative Study of Representative Fusion Schemes,"RGB-D face recognition (FR) has drawn increasing attention in recent years with the advances of new RGB-D sensing technologies, and the decrease in sensor price. While a number of multi-modality fusion methods are available in face recognition, there is not known conclusion how the RGB and depth should be fused. We provide a comparative study of four representative fusion schemes in RGB-D face recognition, covering signal-level, feature-level, score-level fusions, and a hybrid fusion we designed for RGB-D face recognition. The proposed method achieves state-of-the-art performance on two large RGB-D datasets. A number of insights are provided based on the experimental evaluations.",2018,CCBR,,10.1007/978-3-319-97909-0_39,
0598704b50ca85672f721e20a559df6d1a363787,1,0,Deep Learning for Face Recognition: Pride or Prejudiced?,"Do very high accuracies of deep networks suggest pride of effective AI or are deep networks prejudiced? Do they suffer from in-group biases (own-race-bias and own-age-bias), and mimic the human behavior? Is in-group specific information being encoded sub-consciously by the deep networks?  This research attempts to answer these questions and presents an in-depth analysis of `bias' in deep learning based face recognition systems. This is the first work which decodes if and where bias is encoded for face recognition. Taking cues from cognitive studies, we inspect if deep networks are also affected by social in- and out-group effect. Networks are analyzed for own-race and own-age bias, both of which have been well established in human beings. The sub-conscious behavior of face recognition models is examined to understand if they encode race or age specific features for face recognition. Analysis is performed based on 36 experiments conducted on multiple datasets. Four deep learning networks either trained from scratch or pre-trained on over 10M images are used. Variations across class activation maps and feature visualizations provide novel insights into the functioning of deep learning systems, suggesting behavior similar to humans. It is our belief that a better understanding of state-of-the-art deep learning networks would enable researchers to address the given challenge of bias in AI, and develop fairer systems.",2019,ArXiv,1904.01219,,https://arxiv.org/pdf/1904.01219.pdf
05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628,1,0,The iNaturalist Species Classification and Detection Dataset,"Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00914,https://authors.library.caltech.edu/87114/1/1916.pdf
05fbc7ebc416a75f0a3cbccae902e83fbae35e24,0,1,Fast Face Recognition Model without Pruning,"This paper proposes a real-time high accuracy face recognition model which can be deployed on performanceconstrained embedded platforms or mobile terminals without pruning. In the proposed model, we optimize the network structure by balancing the memory access cost (MAC) and Floatingpoint Operations (FLOPs) and propose a new factor to measure the efficiency of learning parameters. Based on these two metrics, we designed two structures called Simple SqueezeFaceNet (SSN) and Channel-Split Network (CSN) to achieve a good balance between the high precision and high FPS (Frames per Second). After being trained with the refined MS1M-refine-v2 dataset, our architectures achieve 0.992 (or 0.990) accuracy on the Labeled Faces in the Wild (LFW) with 155 (or 180) FPS. This indicates that our models can also be applied to real-time multi-face recognition in video.",2019,2019 IEEE Symposium Series on Computational Intelligence (SSCI),,10.1109/SSCI44817.2019.9002922,
061cb02de79c5bb2a19537597789e7b12b58a0f7,1,0,Rethinking Feature Learning Approach for Face Verification,"Convolutional neural networks (CNNs) have been broadly used in various computer vision tasks. Almost all of them achieved fascinating results, some even surpassed human performance. In the field of face verification, there are also some attempts of utilizing CNNs for better verification rate. Most of these methods use cross-entropy loss. In order to reinforce the discriminability of the learned features, which is especially important for face verification, we introduce a set of supervision signals in this paper. Specifically, the supervision signals can be separated into two groups: one for redundancy reduction, named decovariance loss, which forces the learned features to diverge; the other called intra-inter center loss(IICL), which achieves both intra-class compactness and interclass discrimination. Experiments validate that the proposed method is both effective and of necessity to the purpose of learning discriminative features. It achieved 98.8% and 93.8% verification accuracy on LFW and YouTube Faces respectively, under the circumstance of using only 0.46M training images.",2017,2017 4th IAPR Asian Conference on Pattern Recognition (ACPR),,10.1109/ACPR.2017.63,
0632516ebc022a9464ed7ef7832d8451b5a9da26,0,1,Affinity guided Geometric Semi-Supervised Metric Learning,"In this paper, we address the semi-supervised metric learning problem, where we learn a distance metric using very few labeled examples, and additionally available unlabeled data. To address the limitations of existing semi-supervised approaches, we integrate some of the best practices across metric learning, to achieve the state-of-the-art in the semi-supervised setting. In particular, we make use of a graph-based approach to propagate the affinities or similarities among the limited labeled pairs to the unlabeled data. Considering the neighborhood of an example, we take into account the propagated affinities to mine triplet constraints. An angular loss is imposed on these triplets to learn a metric. Additionally, we impose orthogonality on the parameters of the learned embedding to avoid a model collapse. In contrast to existing approaches, we propose a stochastic approach that scales well to large-scale datasets. We outperform various semi-supervised metric learning approaches on a number of benchmark datasets.",2020,ArXiv,2002.12394,,https://arxiv.org/pdf/2002.12394.pdf
06523323911d546d106fb5c5ceb9c098f90a63bd,1,0,"Audio-visual TED corpus: enhancing the TED-LIUM corpus with facial information, contextual text and object recognition","We present a variety of new visual features in extension to the TED-LIUM corpus. We re-aligned the original TED talk audio transcriptions with official TED.com videos. By utilizing state-of-the-art models for face and facial landmarks detection, optical character recognition, object detection and classification, we extract four new visual features that can be used for Large-Vocabulary Continuous Speech Recognition (LVCSR) systems, including facial images, landmarks, text, and objects in the scenes. The facial images and landmarks can be used in combination with audio for audio-visual acoustic modeling where the visual modality provides robust features in adverse acoustic environments. The contextual information, i.e. extracted text and detected objects in the scene can be used as prior knowledge to create contextual language models. Experimental results showed the efficacy of using visual features on top of acoustic features for speech recognition in overlapping speech scenarios.",2019,UbiComp/ISWC '19,,10.1145/3341162.3344861,
0652590a0084f701503a1e44fab94f10994db1aa,1,0,Soft-Ranking Label Encoding for Robust Facial Age Estimation,"Automatic facial age estimation can be used in a wide range of real-world applications. However, this process is challenging due to the randomness and slowness of the aging process. Accordingly, in this paper, we propose a novel method aimed at overcoming the challenges associated with facial age estimation. First, we propose a novel age encoding method, referred to as ‘Soft-ranking’, which encodes two important properties of facial age, <inline-formula> <tex-math notation=""LaTeX"">${i.e.}$ </tex-math></inline-formula>, the ordinal property and the correlation between adjacent ages. Therefore, Soft-ranking provides a richer supervision signal for training deep models. Moreover, we carefully analyze existing evaluation protocols for age estimation, finding that the overlap in identity between the training and testing sets affects the relative performance of different age encoding methods. Moreover, we achieve state-of-the-art performance on four most popular age databases, <inline-formula> <tex-math notation=""LaTeX"">${i.e.}$ </tex-math></inline-formula>, Morph II, AgeDB, CLAP2015, and CLAP2016.",2020,IEEE Access,1906.03625,10.1109/ACCESS.2020.3010815,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09145576.pdf
0695cca0d33202eee3d894c113abf4c0bdaa9e74,1,0,The Shallow End: Empowering Shallower Deep-Convolutional Networks through Auxiliary Outputs,"Depth is one of the key factors behind the success of convolutional neural networks (CNNs). Since ResNet, we are able to train very deep CNNs as the gradient vanishing issue has been largely addressed by the introduction of skip connections. However, we observe that, when the depth is very large, the intermediate layers (especially shallow layers) may fail to receive sufficient supervision from the loss due to the severe transformation through a long backpropagation path. As a result, the representation power of intermediate layers can be very weak and the model becomes very redundant with limited performance. In this paper, we first investigate the supervision vanishing issue in existing backpropagation (BP) methods. And then, we propose to address it via an effective method, called Multi-way BP (MW-BP), which relies on multiple auxiliary losses added to the intermediate layers of the network. The proposed MW-BP method can be applied to most deep architectures with slight modifications, such as ResNet and MobileNet. Our method often gives rise to much more compact models (denoted by ""Mw+Architecture"") than existing methods. For example, MwResNet-44 with 44 layers performs better than ResNet-110 with 110 layers on CIFAR-10 and CIFAR-100. More critically, the resultant models even outperform the light models obtained by state-of-the-art model compression methods. Last, our method inherently produces multiple compact models with different depths at the same time, which is helpful for model selection.",2016,ArXiv,1611.01773,,https://arxiv.org/pdf/1611.01773.pdf
06c4c5570b01bdcd864d2ed44657d9d163b02ba9,1,0,Deep neural networks in computer vision and biomedical image analysis,"This thesis proposes different models for a variety of applications, such as semantic segmentation, in-the-wild face recognition, microscopy cell counting and detection, standardized re-orientation of 3D ultrasound fetal brain and Magnetic Resonance (MR) cardiac video segmentation. Our approach is to employ the large-scale machine learning models, in particular deep neural networks. Expert knowledge is either mathematically modelled as a differentiable hidden layer in the Artificial Neural Networks, or we tried to break the complex tasks into several small and easy-to-solve tasks. Multi-scale contextual information plays an important role in pixel-wise predic- tion, e.g. semantic segmentation. To capture the spatial contextual information, we present a new block for learning receptive field adaptively by within-layer recurrence. While interleaving with the convolutional layers, receptive fields are effectively enlarged, reaching across the entire feature map or image. The new block can be initialized as identity and inserted into any pre-trained networks, therefore taking benefit from the ""pre-train and fine-tuning"" paradigm. Current face recognition systems are mostly driven by the success of image classification, where the models are trained to by identity classification. We propose a multi-column deep comparator networks for face recognition. The architecture takes two sets (each contains an arbitrary number of faces) of images or frames as inputs, facial part-based (e.g. eyes, noses) representations of each set are pooled out, dynamically calibrated based on the quality of input images, and further compared with local ""experts"" in a pairwise way. Unlike the computer vision applications, collecting data and annotation is usually more expensive in biomedical image analysis. Therefore, the models that can be trained with fewer data and weaker annotations are of great importance. We approach the microscopy cell counting and detection based on density estimation, where only central dot annotations are needed. The proposed fully convolutional regression networks are first trained on a synthetic dataset of cell nuclei, later fine-tuned and shown to generalize to real data. In 3D fetal ultrasound neurosonography, establishing a coordinate system over the fetal brain serves as a precursor for subsequent tasks, e.g. localization of anatomical landmarks, extraction of standard clinical planes for biometric assessment of fetal growth, etc. To align brain volumes into a common reference coordinate system, we decompose the complex transformation into several simple ones, which can be easily tackled with Convolutional Neural Networks. The model is therefore designed to leverage the closely related tasks by sharing low-level features, and the task-specific predictions are then combined to reproduce the transformation matrix as the desired output. Finally, we address the problem of MR cardiac video analysis, in which we are interested in assisting clinical diagnosis based on the fine-grained segmentation. To facilitate segmentation, we present one end-to-end trainable model that achieves multi-view structure detection, alignment (standardized re-orientation), and fine- grained segmentation simultaneously. This is motivated by the fact that the CNNs in essence is not rotation equivariance or invariance, therefore, adding the pre-alignment into the end-to-end trainable pipeline can effectively decrease the complexity of segmentation for later stages of the model.",2017,,,,https://pdfs.semanticscholar.org/06c4/c5570b01bdcd864d2ed44657d9d163b02ba9.pdf
0728e595e19d35616157c33e6f166d57a4fc5dc8,0,1,Striking the Right Balance With Uncertainty,"Learning unbiased models on imbalanced datasets is a significant challenge. Rare classes tend to get a concentrated representation in the classification space which hampers the generalization of learned boundaries to new test examples. In this paper, we demonstrate that the Bayesian uncertainty estimates directly correlate with the rarity of classes and the difficulty level of individual samples. Subsequently, we present a novel framework for uncertainty based class imbalance learning that follows two key insights: First, classification boundaries should be extended further away from a more uncertain (rare) class to avoid over-fitting and enhance its generalization. Second, each sample should be modeled as a multi-variate Gaussian distribution with a mean vector and a covariance matrix defined by the sample's uncertainty. The learned boundaries should respect not only the individual samples but also their distribution in the feature space. Our proposed approach efficiently utilizes sample and class uncertainty information to learn robust features and more generalizable classifiers. We systematically study the class imbalance problem and derive a novel loss formulation for max-margin learning based on Bayesian uncertainty measure. The proposed method shows significant performance improvements on six benchmark datasets for face verification, attribute prediction, digit/object classification and skin lesion detection.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1901.0759,10.1109/CVPR.2019.00019,https://arxiv.org/pdf/1901.07590.pdf
072c716ededeee9c574f598eb6e1af37fa81f5aa,0,1,On Adversarial Patches: Real-World Attack on ArcFace-100 Face Recognition System,"Recent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.",2019,"2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)",1910.07067,10.1109/SIBIRCON48586.2019.8958134,https://arxiv.org/pdf/1910.07067.pdf
073181eff88c30a6936a150515c6dfb8ea418081,0,1,Point cloud based deep convolutional neural network for 3D face recognition,Face recognition is a challenging task as it has to deal with several issues such as illumination orientation and variability among the different faces. Previous works have shown that 3D face is a robust biometric trait and is less sensitive to light and pose variations. Also due to availability of inexpensive sensors and new 3D data acquisition techniques it has become easy to capture 3D data. A 3D depth image of a face is found to be rich in information and biometric recognition performance can be enhanced by using 3D face data along with convolutional neural network. However the shortcoming of this approach is the conversion of 3D data to lower dimensions (depth image) which suffer from loss of geometric information and the network becomes computationally expensive. In this work we endeavor to apply deep learning method for 3D face recognition and propose a deep convolutional neural network based on PointNet architecture which consumes point cloud directly as input and siamese network for similarity learning. Further we propose a solution to the issue of a limited database by applying data augmentation at the point cloud level. Our proposed technique shows encouraging performance on Bosphorus and IIT Indore 3D face databases.,2020,Multimedia Tools and Applications,,10.1007/s11042-020-09008-z,
0732df185bdfcb9c908ec30bb441252593f58875,1,0,MovieNet: A Holistic Dataset for Movie Understanding,"Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet -- a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92K tags of cinematic style. To the best of our knowledge, MovieNet is the largest dataset with richest annotations for comprehensive movie understanding. Based on MovieNet, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. MovieNet will be published in compliance with regulations at this https URL.",2020,ECCV,2007.10937,10.1007/978-3-030-58548-8_41,https://arxiv.org/pdf/2007.10937.pdf
0782024d8aee50980cf16257e5940ff2645b977b,1,0,MobiFace: A Lightweight Deep Learning Face Recognition on Mobile Devices,"Deep neural networks have been widely used in numerous computer vision applications, particularly in face recognition. However, deploying deep neural network face recognition on mobile devices has recently become a trend but still limited since most high-accuracy deep models are both time and GPU consumption in the inference stage. Therefore, developing a lightweight deep neural network is one of the most practical solutions to deploy face recognition on mobile devices. Such the lightweight deep neural network requires efficient memory with small number of weights representation and low cost operators. In this paper, a novel deep neural network named MobiFace, a simple but effective approach, is proposed to productively deploy face recognition on mobile devices. The experimental results have shown that our lightweight MobiFace is able to achieve high performance with 99.73% on LFW database and 91.3% on large-scale challenging Megaface database. It is also eventually competitive against large-scale deep-networks face recognition while significant reducing computational time and memory consumption.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",1811.1108,10.1109/BTAS46853.2019.9185981,https://arxiv.org/pdf/1811.11080.pdf
078ac3d05afb953585ae17e914c6b4d0571f6421,0,1,Manifold Projection for Adversarial Defense on Face Recognition,"Although deep convolutional neural network based face recognition system has achieved remarkable success, it is susceptible to adversarial images: carefully constructed imperceptible perturbations can easily mislead deep neural networks. A recent study has shown that in addition to regular off-manifold adversarial images, there are also adversarial images on the manifold. In this paper, we propose Adversarial Variational AutoEncoder (A-VAE), a novel framework to tackle both types of attacks. We hypothesize that both off-manifold and on-manifold attacks move the image away from the high probability region of image manifold. We utilize variational autoencoder (VAE) to estimate the lower bound of the log-likelihood of image and explore to project the input images back into the high probability regions of image manifold again. At inference time, our model synthesizes multiple similar realizations of a given image by random sampling, then the nearest neighbor of the given image is selected as the final input of the face recognition model. As a preprocessing operation, our method is attack-agnostic and can adapt to a wide range of resolutions. The experimental results on LFW demonstrate that our method achieves state-of-the-art defense success rate against conventional off-manifold attacks such as FGSM, PGD, and C&W under both grey-box and white-box settings, and even on-manifold attack.",2020,ECCV,,10.1007/978-3-030-58577-8_18,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750290-supp.pdf
07929d975c492d68261b27ff6c4b6b41ca1097e4,0,1,Heatmap Regression via Randomized Rounding,"Heatmap regression has become the mainstream methodology for deep learning-based semantic landmark localization, including in facial landmark localization and human pose estimation. Though heatmap regression is robust to large variations in pose, illumination, and occlusion in unconstrained settings, it usually suffers from a sub-pixel localization problem. Specifically, considering that the activation point indices in heatmaps are always integers, quantization error thus appears when using heatmaps as the representation of numerical coordinates. Previous methods to overcome the sub-pixel localization problem usually rely on high-resolution heatmaps. As a result, there is always a trade-off between achieving localization accuracy and computational cost, where the computational complexity of heatmap regression depends on the heatmap resolution in a quadratic manner. In this paper, we formally analyze the quantization error of vanilla heatmap regression and propose a simple yet effective quantization system to address the sub-pixel localization problem. The proposed quantization system induced by the randomized rounding operation 1) encodes the fractional part of numerical coordinates into the ground truth heatmap using a probabilistic approach during training; and 2) decodes the predicted numerical coordinates from a set of activation points during testing. We prove that the proposed quantization system for heatmap regression is unbiased and lossless. Experimental results on four popular facial landmark localization datasets (WFLW, 300W, COFW, and AFLW) demonstrate the effectiveness of the proposed method for efficient and accurate semantic landmark localization. Code is available at this http URL.",2020,ArXiv,2009.00225,,https://arxiv.org/pdf/2009.00225.pdf
07c341ec8b82a3c4df9e0aeff53ef43a73f59829,1,1,AdaptiveFace: Adaptive Margin and Sampling for Face Recognition,"Training large-scale unbalanced data is the central topic in face recognition. In the past two years, face recognition has achieved remarkable improvements due to the introduction of margin based Softmax loss. However, these methods have an implicit assumption that all the classes possess sufficient samples to describe its distribution, so that a manually set margin is enough to equally squeeze each intra-class variations. However, real face datasets are highly unbalanced, which means the classes have tremendously different numbers of samples. In this paper, we argue that the margin should be adapted to different classes. We propose the Adaptive Margin Softmax to adjust the margins for different classes adaptively. In addition to the unbalance challenge, face data always consists of large-scale classes and samples. Smartly selecting valuable classes and samples to participate in the training makes the training more effective and efficient. To this end, we also make the sampling process adaptive in two folds: Firstly, we propose the Hard Prototype Mining to adaptively select a small number of hard classes to participate in classification. Secondly, for data sampling, we introduce the Adaptive Data Sampling to find valuable samples for training adaptively. We combine these three parts together as AdaptiveFace. Extensive analysis and experiments on LFW, LFW BLUFR and MegaFace show that our method performs better than state-of-the-art methods using the same network architecture and training dataset. Code is available at https://github.com/haoliu1994/AdaptiveFace.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.01222,http://www.cbsr.ia.ac.cn/users/zlei/papers/HLIU-CVPR-2019.pdf
07f34571005bde02dd31886cf58279591a1ae914,1,0,Multi-way backpropagation for training compact deep neural networks,"Depth is one of the key factors behind the success of convolutional neural networks (CNNs). Since ResNet (He et al., 2016), we are able to train very deep CNNs as the gradient vanishing issue has been largely addressed by the introduction of skip connections. However, we observe that, when the depth is very large, the intermediate layers (especially shallow layers) may fail to receive sufficient supervision from the loss due to severe transformation through long backpropagation path. As a result, the representation power of intermediate layers can be very weak and the model becomes very redundant with limited performance. In this paper, we first investigate the supervision vanishing issue in existing backpropagation (BP) methods. And then, we propose to address it via an effective method, called Multi-way BP (MW-BP), which relies on multiple auxiliary losses added to the intermediate layers of the network. The proposed MW-BP method can be applied to most deep architectures with slight modifications, such as ResNet and MobileNet. Our method often gives rise to much more compact models (denoted by ""Mw+Architecture"") than existing methods. For example, MwResNet-44 with 44 layers performs better than ResNet-110 with 110 layers on CIFAR-10 and CIFAR-100. More critically, the resultant models even outperform the light models obtained by state-of-the-art model compression methods. Last, our method inherently produces multiple compact models with different depths at the same time, which is helpful for model selection. Extensive experiments on both image classification and face recognition demonstrate the superiority of the proposed method.",2020,Neural Networks,,10.1016/j.neunet.2020.03.001,https://tanmingkui.github.io/files/publications/Multi-way.pdf
08402ffdecdb76bcba75d23910635fb371c6db7b,1,0,Improving face representation learning with center invariant loss,"Abstract In this paper, we address on the deep face representation learning with imbalanced data. With a large number of available face images of different people for training, Convolutional Neural Networks could learn deep face representation through classifying these people. However, uniformed distributed data for all people are hard to get. Some people come with more images but some come with less. In learning the deep face representation, the imbalanced images between people introduce the bias towards these people that have more images. Existing methods focus on the intra-class and inter-class variations but not well address the imbalanced data problem. To generate a robust and discriminative face representation for all people, we propose a center invariant loss which adds penalty to the differences between each center of classes. The center invariant loss could align the center of each person to the mean of all centers, which could force the deeply learned face features to have a good representation for all people with better generalization ability. Extensive experiments well demonstrate the effectiveness of the proposed approach. Many existing methods in learning deep face representation are further improved after adding the proposed center invariant loss.",2018,Image Vis. Comput.,,10.1016/j.imavis.2018.09.010,
089217c89100286a0c93a6d5b936589d38191dfc,0,1,Iterative Dynamic Generic Learning For Single Sample Face Recognition With A Contaminated Gallery,"This paper studies a new challenging problem in face recognition (FR) with single sample per person (SSPP), i.e., SSPP FR with a contaminated gallery (SSPP-CG FR), where the gallery is contaminated by variations. In SSPP-CG FR, the popular generic learning methods will suffer serious performance degradation because the applied prototype plus variation (P+V) model is not suitable in such scenarios. The reasons are twofold: 1) The contaminated gallery samples yield bad prototypes to represent the persons; 2) The generated variation dictionary is simply based on the subtraction of average face from generic samples of the same person and cannot well depict the intra-personal variations. To tackle SSPPCG FR, we propose a novel Iterative Dynamic Generic Learning (IDGL) method, where the labeled gallery and unlabeled query sets are fed into a dynamic label feedback network for learning. Specifically, IDGL first recovers the prototypes via a semi-supervised low-rank representation (SSLRR) framework and learns a representative variation dictionary by extracting the “sample-specific” corruptions from an auxiliary generic set. Then, it puts them into the P+V model to estimate labels for query samples. Subsequently, the estimated labels are used as the feedbacks to modify the SSLRR, thus updating new prototypes for the next round of P+V based label estimation. With the dynamic learning network, the accuracy of the estimated labels is improved iteratively owing to the steadily enhanced prototypes. Experiments on various benchmark databases have verified the superiority of IDGL.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/icme46284.2020.9102792,
089c4e170b114a5b3f8564e02eaae767f30ebed8,0,1,An a-contrario Biometric Fusion Approach,"Fusion is a key component in many biometric systems: it is one of the most widely used techniques to improve their accuracy. Each time we need to combine the output of systems that use different biometric traits, or different samples of the same biometric trait, or even different algorithms, we need to define a fusion strategy. Independently of the fusion method used, there is always a decision step, in which it is decided if the traits being compared correspond to the same individual or not. In this work, we present a statistical decision criterion based on the a-contrario framework, which has already proven to be useful in biometric applications. The proposed method and its theoretical background is described in detail, and its application to biometric fusion is illustrated with simulated and real data.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00419,
08a578d7f7f3d0edf46470e33f92e2335d19b70b,0,1,"Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX","Land cover classification of satellite imagery is an important step toward analyzing the Earth’s surface. Existing models assume a closed-set setting where both the training and testing classes belong to the same label set. However, due to the unique characteristics of satellite imagery with extremely vast area of versatile cover materials, the training data are bound to be non-representative. In this paper, we study the problem of open-set land cover classification that identifies the samples belonging to unknown classes during testing, while maintaining performance on known classes. Although inherently a classification problem, both representative and discriminative aspects of data need to be exploited in order to better distinguish unknown classes from known. We propose a representative-discriminative open-set recognition (RDOSR) framework, which 1) projects data from the raw image space to the embedding feature space that facilitates differentiating similar classes, and further 2) enhances both the representative and discriminative capacity through transformation to a so-called abundance space. Experiments on multiple satellite benchmarks demonstrate effectiveness of the proposed method. We also show the generality of the proposed approach by achieving promising results on open-set classification tasks using RGB images.",2020,ECCV,,10.1007/978-3-030-58577-8,
094956ae0af2d187ee73ad0511f93f252a8c6f2e,0,1,Frame Aggregation and Multi-Modal Fusion Framework for Video-Based Person Recognition,"Video-based person recognition is challenging due to persons being blocked and blurred, and the variation of shooting angle. Previous research always focused on person recognition on still images, ignoring similarity and continuity between video frames. To tackle the challenges above, we propose a novel Frame Aggregation and Multi-Modal Fusion (FAMF) framework for video-based person recognition, which aggregates face features and incorporates them with multi-modal information to identify persons in videos. For frame aggregation, we propose a novel trainable layer based on NetVLAD (named AttentionVLAD), which takes arbitrary number of features as input and computes a fixed-length aggregation feature based on feature quality. We show that introducing an attention mechanism to NetVLAD can effectively decrease the impact of low-quality frames. For the multi-model information of videos, we propose a Multi-Layer Multi-Modal Attention (MLMA) module to learn the correlation of multi-modality by adaptively updating Gram matrix. Experimental results on iQIYI-VID-2019 dataset show that our framework outperforms other state-of-the-art methods.",2020,ArXiv,2010.0929,,https://arxiv.org/pdf/2010.09290.pdf
09626a688cf1800fd2eb832661d8fce0877ad33a,1,1,Neural Architecture Refinement: A Practical Way for Avoiding Overfitting in NAS,"Neural architecture search (NAS) is proposed to automate the architecture design process and attracts overwhelming interest from both academia and industry. However, it is confronted with overfitting issue due to the high-dimensional search space composed by operator selection and skip connection of each layer. This paper explores the architecture overfitting issue in depth based on the reinforcement learning-based NAS framework. We show that the policy gradient method has deep correlations with the cross entropy minimization. Based on this correlation, we further demonstrate that, though the reward of NAS is sparse, the policy gradient method implicitly assign the reward to all operations and skip connections based on the sampling frequency. However, due to the inaccurate reward estimation, curse of dimensionality problem and the hierachical structure of neural networks, reward charateristics for operators and skip connections have intrinsic differences, the assigned rewards for the skip connections are extremely noisy and inaccurate. To alleviate this problem, we propose a neural architecture refinement approach that working with an initial state-of-the-art network structure and only refining its operators. Extensive experiments have demonstrated that the proposed method can achieve fascinated results, including classification, face recognition etc.",2019,ArXiv,1905.02341,,https://arxiv.org/pdf/1905.02341.pdf
0981a71137c64ec2628714017db5b016571b26f4,1,1,Common CNN-based Face Embedding Spaces are (Almost) Equivalent,"CNNs are the dominant method for creating face embeddings for recognition. It might be assumed that, since these networks are distinct, complex, nonlinear functions, that their embeddings are network specific, and thus have some degree of anonymity. However, recent research has shown that distinct networks' features can be directly mapped with little performance penalty (median 1.9% reduction across 90 distinct mappings) in the context of the 1,000 object ImageNet recognition task. This finding has revealed that embeddings coming from different systems can be meaningfully compared, provided the mapping. However, prior work only considered networks trained and tested on a closed set classification task. Here, we present evidence that a linear mapping between feature spaces can be easily discovered in the context of open set face recognition. Specifically, we demonstrate that the feature spaces of four face recognition models, of varying architecture and training datasets, can be mapped between with no more than a 1.0% penalty in recognition accuracy on LFW . This finding, which we also replicate on YouTube Faces, demonstrates that embeddings from different systems can be readily compared once the linear mapping is determined. In further analysis, fewer than 500 pairs of corresponding embeddings from two systems are required to calculate the full mapping between embedding spaces, and reducing the dimensionality of the mapping from 512 to 64 produces negligible performance penalty.",2020,ArXiv,,,
0986f2ac6755df5d196ceb09b5bdf19593cbbaef,1,0,Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking,"Deep Neural Networks (DNNs) are increasingly deployed in highly energy-constrained environments such as autonomous drones and wearable devices while at the same time must operate in real-time. Therefore, reducing the energy consumption has become a major design consideration in DNN training. This paper proposes the first end-to-end DNN training framework that provides quantitative energy consumption guarantees via weighted sparse projection and input masking. The key idea is to formulate the DNN training as an optimization problem in which the energy budget imposes a previously unconsidered optimization constraint. We integrate the quantitative DNN energy estimation into the DNN training process to assist the constrained optimization. We prove that an approximate algorithm can be used to efficiently solve the optimization problem. Compared to the best prior energy-saving methods, our framework trains DNNs that provide higher accuracies under same or lower energy budgets.",2019,ICLR,1806.04321,,https://arxiv.org/pdf/1806.04321.pdf
09ad80c4e80e1e02afb8fa4cb6dab260fb66df53,1,0,Feature Learning for One-Shot Face Recognition,"One-shot face recognition is a challenging open problem which requires recognizing novel identities from only one gallery face. One-shot classes are squeezed and neglected in the feature space for classification due to data imbalance. Moreover, training samples deficience is a major obstacle to intra-class clustering. In this paper, we propose a novel framework based on CNN of balancing regularizer and shifting center regeneration which regulates norms of weight vector into same scale and adjusts clustering center to deal with deficient training data. Comprehensive evaluations on MS-celeb-1M low-shot face dataset demonstrate that our methods improve one-shot face recognition notablely which achieve 88.78% coverage at precision=0.99 using restricted data without hybrid classifiers or multi-model. Moreover, experiments on LFW prove that CNN model trained with proposed methods can obtain more discriminative and compact feature representations. Since there are many identities that have only few training samples available online, our methods have great significance for improving data utilization and strengthening feature representation for face recognition.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451464,
0a0321785c8beac1cbaaec4d8ad0cfd4a0d6d457,1,0,Learning Invariant Deep Representation for NIR-VIS Face Recognition,"Visual versus near infrared (VIS-NIR) face recognition is still a challenging heterogeneous task due to large appearance difference between VIS and NIR modalities. This paper presents a deep convolutional network approach that uses only one network to map both NIR and VIS images to a compact Euclidean space. The low-level layers of this network are trained only on large-scale VIS data. Each convolutional layer is implemented by the simplest case of maxout operator. The highlevel layer is divided into two orthogonal subspaces that contain modality-invariant identity information and modalityvariant spectrum information respectively. Our joint formulation leads to an alternating minimization approach for deep representation at the training time and an efficient computation for heterogeneous data at the testing time. Experimental evaluations show that our method achieves 94% verification rate at FAR=0.1% on the challenging CASIA NIR-VIS 2.0 face recognition dataset. Compared with state-of-the-art methods, it reduces the error rate by 58% only with a compact 64-D representation.",2017,AAAI,,,
0a23bdc55fb0d04acdac4d3ea0a9994623133562,1,0,Large-scale Bisample Learning on ID vs. Spot Face Recognition,"In many face recognition applications, there is large amount of face data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (relatively small class number) and sufficient depth (many samples for each class). They would meet great challenges when applied on this ID vs. Spot (IvS) data, including the under-represented intra-class variations and the excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem that there are only two samples for each class, a classification-verification-classification (CVC) training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax (DP-softmax) is incorporated to make the deep learning applicable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance than previous ones, validating the effectiveness of LBL on IvS face recognition.",2018,ArXiv,,,
0aa44b4b252fa84dc3e181393b659bd366d9c269,0,1,Socially Assistive Robots for Older Adults and People with Autism: An Overview,"Over one billion people in the world suffer from some form of disability. Nevertheless, according to the World Health Organization, people with disabilities are particularly vulnerable to deficiencies in services, such as health care, rehabilitation, support, and assistance. In this sense, recent technological developments can mitigate these deficiencies, offering less-expensive assistive systems to meet users’ needs. This paper reviews and summarizes the research efforts toward the development of these kinds of systems, focusing on two social groups: older adults and children with autism.",2020,,,10.3390/electronics9020367,https://pdfs.semanticscholar.org/a7be/a099b42bdc908598fbd8e914c947de4e8f2d.pdf
0ab7cff2ccda7269b73ff6efd9d37e1318f7db25,1,1,Facial Coding Scheme Reference 1 Craniofacial Distances,"Face recognition is a long-standing challenge in the field of Artificial Intelligence (AI). The goal is to create systems that detect, recognize, verify and understand characteristics of human faces. There are significant technical hurdles in making these systems accurate, particularly in unconstrained settings, due to confounding factors related to pose, resolution, illumination, occlusion and viewpoint. However, with recent advances in neural networks, face recognition has achieved unprecedented accuracy, built largely on data-driven deep learning methods. While this is encouraging, a critical aspect limiting face recognition performance in practice is intrinsic facial diversity. Every face is different. Every face reflects something unique about us. Aspects of our heritage – including race, ethnicity, culture, geography – and our individual identity – age, gender and visible forms of self-expression – are reflected in our faces. Faces are personal. We expect face recognition to work accurately for each of us. Performance should not vary for different individuals or different populations. As we rely on data-driven methods to create face recognition technology, we need to answer a fundamental question: does the training data for these systems fairly represent the distribution of faces we see in the world? At the heart of this core question are deeper scientific questions about how to measure facial diversity, what features capture intrinsic facial variation and how to evaluate coverage and balance for face image data sets. Towards the goal of answering these questions, Diversity in Faces (DiF ) provides a new data set of annotations of one million publicly available face images for advancing the study of facial diversity. The annotations are generated using ten facial coding schemes that provide human-interpretable quantitative measures of intrinsic facial features. We believe that making these descriptors available will encourage deeper research on this important topic and accelerate efforts towards creating more fair and accurate face recognition systems.",2019,,,,https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/documents/Diversity-in-Faces-Publication.pdf
0b4894a512021ce79e534dc1749ad83ca417d8f6,1,0,Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks,"Deep neural networks (DNN) are known to be vulnerable to adversarial attacks. Numerous efforts either try to patch weaknesses in trained models, or try to make it difficult or costly to compute adversarial examples that exploit them. In our work, we explore a new ""honeypot"" approach to protect DNN models. We intentionally inject trapdoors, honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples. Attackers' optimization algorithms gravitate towards trapdoors, leading them to produce attacks similar to trapdoors in the feature space. Our defense then identifies attacks by comparing neuron activation signatures of inputs to those of trapdoors. In this paper, we introduce trapdoors and describe an implementation of a trapdoor-enabled defense. First, we analytically prove that trapdoors shape the computation of adversarial attacks so that attack inputs will have feature representations very similar to those of trapdoors. Second, we experimentally show that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks (PGD, optimization-based CW, Elastic Net, BPDA), with negligible impact on normal classification. These results generalize across classification domains, including image, facial, and traffic-sign recognition. We also present significant results measuring trapdoors' robustness against customized adaptive attacks (countermeasures).",2020,CCS,1904.08554,10.1145/3372297.3417231,http://people.cs.uchicago.edu/~ravenben/publications/pdf/trapdoor-ccs20.pdf
0bac4f77943c7645cc5dafab4f705f65d854b44a,0,1,GP-NAS: Gaussian Process Based Neural Architecture Search,"Neural architecture search (NAS) advances beyond the state-of-the-art in various computer vision tasks by automating the designs of deep neural networks. In this paper, we aim to address three important questions in NAS: (1) How to measure the correlation between architectures and their performances? (2) How to evaluate the correlation between different architectures? (3) How to learn these correlations with a small number of samples? To this end, we first model these correlations from a Bayesian perspective. Specifically, by introducing a novel Gaussian Process based NAS (GP-NAS) method, the correlations are modeled by the kernel function and mean function. The kernel function is also learnable to enable adaptive modeling for complex correlations in different search spaces. Furthermore, by incorporating a mutual information based sampling method, we can theoretically ensure the high-performance architecture with only a small set of samples. After addressing these problems, training GP-NAS once enables direct performance prediction of any architecture in different scenarios and may obtain efficient networks for different deployment platforms. Extensive experiments on both image classification and face recognition tasks verify the effectiveness of our algorithm.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.01195,http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_GP-NAS_Gaussian_Process_Based_Neural_Architecture_Search_CVPR_2020_paper.pdf
0bca9c124df3498ae9ad71d71173ffe134ae8182,0,1,Multi-Proxy Constraint Loss for Vehicle Re-Identification,"Vehicle re-identification plays an important role in cross-camera tracking and vehicle search in surveillance videos. Large variance in the appearance of the same vehicle captured by different cameras and high similarity of different vehicles with the same model poses challenges for vehicle re-identification. Most existing methods use a center proxy to represent a vehicle identity; however, the intra-class variance leads to great difficulty in fitting images of the same identity to one center feature and the images with high similarity belonging to different identities cannot be separated effectively. In this paper, we propose a sampling strategy considering different viewpoints and a multi-proxy constraint loss function which represents a class with multiple proxies to perform different constraints on images of the same vehicle from different viewpoints. Our proposed sampling strategy contributes to better mine samples corresponding to different proxies in a mini-batch using the camera information. The multi-proxy constraint loss function pulls the image towards the furthest proxy of the same class and pushes the image from the nearest proxy of different class further away, resulting in a larger margin between decision boundaries. Extensive experiments on two large-scale vehicle datasets (VeRi and VehicleID) demonstrate that our learned global features using a single-branch network outperforms previous works with more complicated network and those that further re-rank with spatio-temporal information. In addition, our method is easy to plug into other classification methods to improve the performance.",2020,Sensors,,10.3390/s20185142,https://pdfs.semanticscholar.org/68b9/57df6210d927fbb82b4bd2b9d134aef26ca8.pdf
0bf6148ef14c220caeca0f1990f409a6f66bf2b9,0,1,Temporal-Contextual Attention Network for Video-Based Person Re-identification,"Video-based person re-identification aims to identify a specific person in surveillance videos from different cameras. This paper presents a new Temporal-Contextual Attention Network (TCA-Net) for person re-identification in videos. The TCA-Net exploits temporally local context among consecutive frames to concentrate selectively on crucial frames within a video sequence. Specifically, the network consists of a Convolutional Neural Network (CNN) module and a temporal-contextual attention block. The CNN module embeds each video frame into a convolutional representation, and the temporal-contextual attention block learns the importance of a video frame for re-identification by exploiting the local context among the frame and its neighboring frames. The feature of a video sequence is then obtained by aggregating frame-level features weighted by frame importance. We evaluate the proposed TCA-Net on a challenging dataset MARS. The experimental results have demonstrated the effectiveness of the proposed approach.",2018,PCM,,10.1007/978-3-030-00776-8_14,
0c5c7798bc87a1364f249b1dc87b38e60ecd150a,1,0,Kinship Classification through Latent Adaptive Subspace,"We tackle the challenging kinship classification problem. Different from kinship verification, which tells two persons have certain kinship relation or not, kinship classification aims to identify the family that a person belongs to. Beyond age and appearance gap across parents and children, the difficulties of kinship classification lie in that any data of the children to be classified are unavailable in advance to help training. To handle this challenge, an auxiliary database with complete parents and children modalities is employed to uncover the parent-children latent knowledge. Specifically, we propose a Latent Adaptive Subspace learning (LAS) to uncover the shared knowledge between two modalities so that the unseen test children are implicitly modeled as latent factors for kinship classification. Moreover, person-wise and family-wise constraints are designed to enhance the individual similarity and couple the parents and children within families for discriminative features. Comprehensive experiments on two large kinship datasets show that the proposed algorithm can effectively inherit knowledge from different databases and modalities and achieve the state-of-the-art performance.",2018,2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018),,10.1109/FG.2018.00030,https://web.northeastern.edu/smilelab/papers/fiw_fg2018.pdf
0c641b46a0863788186234632b33ac9831dc21bb,0,1,A Novel Deep Multi-Modal Feature Fusion Method for Celebrity Video Identification,"In this paper, we develop a novel multi-modal feature fusion method for the 2019 iQIYI Celebrity Video Identification Challenge, which is held in conjunction with ACM MM 2019. The purpose of this challenge is to retrieve all the video clips of a given identity in the testing set. In this challenge, the multi-modal features of a celebrity are encouraged to be combined for a promising performance, such as face features, head features, body features, and audio features. As we know, the features from different modalities usually have their own influences on the results. To achieve better results, a novel weighted multi-modal feature fusion method is designed to obtain the final feature representation. After many experimental verification, we found that different feature fusion weights for training and testing make the method robust to multi-modal person identification. Experiments on the iQIYI-VID-2019 dataset show that our multi-modal feature fusion strategy effectively improves the accuracy of person identification. Specifically, for competition, we use a single model to get the result of 0.8952 in mAP, which ranks TOP-5 among all the competitive results.",2019,ACM Multimedia,,10.1145/3343031.3356067,
0c65226edb466204189b5aec8f1033542e2c17aa,1,0,A study of CNN outside of training conditions,"Convolution neural networks (CNN) are the main development in face recognition in recent years. However, their description capacities have been somewhat understudied. In this paper, we show that training CNN only with color images is enough to properly describe depth and near infrared face images by assessing the performance of three publicly available CNN models on these other modalities. Furthermore, we find that, despite displaying results comparable to the human performance on LFW, not all CNN behave like humans recognizing faces in other scenarios.",2017,2017 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2017.8296997,
0d31ec5153fb805e1d632b6e8ee0f08985f71fdb,0,1,A Study on Angular Based Embedding Learning for Text-independent Speaker Verification,"Learning a good speaker embedding is important for many automatic speaker recognition tasks, including verification, identification and diarization. The embeddings learned by softmax are not discriminative enough for open-set verification tasks. Angular based embedding learning target can achieve such discriminativeness by optimizing angular distance and adding margin penalty. We apply several different popular angular margin embedding learning strategies in this work and explicitly compare their performance on Voxceleb speaker recognition dataset. Observing the fact that encouraging inter-class separability is important when applying angular based embedding learning, we propose an exclusive inter-class regularization as a complement for angular based loss. We verify the effectiveness of these methods for learning a discriminative embedding space on ASV task with several experiments. These methods together, we manage to achieve an impressive result with 16.5% improvement on equal error rate (EER) and 18.2% improvement on minimum detection cost function comparing with baseline softmax systems.",2019,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),1908.0399,10.1109/APSIPAASC47483.2019.9023165,https://arxiv.org/pdf/1908.03990.pdf
0d5a05bd182e27df36a115a10ebba766962653fc,0,1,PuppeteerGAN: Arbitrary Portrait Animation With Semantic-Aware Appearance Transformation,"Portrait animation, which aims to animate a still portrait to life using poses extracted from target frames, is an important technique for many real-world entertainment applications. Although recent works have achieved highly realistic results on synthesizing or controlling human head images, the puppeteering of arbitrary portraits is still confronted by the following challenges: 1) identity/personality mismatch; 2) training data/domain limitations; and 3) low-efficiency in training/fine-tuning. In this paper, we devised a novel two-stage framework called PuppeteerGAN for solving these challenges. Specifically, we first learn identity-preserved semantic segmentation animation which executes pose retargeting between any portraits. As a general representation, the semantic segmentation results could be adapted to different datasets, environmental conditions or appearance domains. Furthermore, the synthesized semantic segmentation is filled with the appearance of the source portrait. To this end, an appearance transformation network is presented to produce fidelity output by jointly considering the wrapping of semantic features and conditional generation. After training, the two networks can directly perform end-to-end inference on unseen subjects without any retraining or fine-tuning. Extensive experiments on cross-identity/domain/resolution situations demonstrate the superiority of the proposed PuppetterGAN over existing portrait animation methods in both generation quality and inference speed.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.01353,http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_PuppeteerGAN_Arbitrary_Portrait_Animation_With_Semantic-Aware_Appearance_Transformation_CVPR_2020_paper.pdf
0dbc3c0a9d5d794cb18a6e35b761b7d5b016e16e,1,0,The Blessing and the Curse of the Noise behind Facial Landmark Annotations,"The evolving algorithms for 2D facial landmark detection empower people to recognize faces, analyze facial expressions, etc. However, existing methods still encounter problems of unstable facial landmarks when applied to videos. Because previous research shows that the instability of facial landmarks is caused by the inconsistency of labeling quality among the public datasets, we want to have a better understanding of the influence of annotation noise in them. In this paper, we make the following contributions: 1) we propose two metrics that quantitatively measure the stability of detected facial landmarks, 2) we model the annotation noise in an existing public dataset, 3) we investigate the influence of different types of noise in training face alignment neural networks, and propose corresponding solutions. Our results demonstrate improvements in both accuracy and stability of detected facial landmarks.",2020,ArXiv,2007.15269,10.2352/issn.2470-1173.2020.8.imawm-186,https://arxiv.org/pdf/2007.15269.pdf
0deb30ba829ed9326b6f1b72b0369e211756a9d4,1,0,DeepVisage: Making Face Recognition Simple Yet With Powerful Generalization Skills,"Face recognition (FR) methods report significant performance by adopting the convolutional neural network (CNN) based learning methods. Although CNNs are mostly trained by optimizing the softmax loss, the recent trend shows an improvement of accuracy with different strategies, such as task-specific CNN learning with different loss functions, fine-tuning on target dataset, metric learning and concatenating features from multiple CNNs. Incorporating these tasks obviously requires additional efforts. Moreover, it demotivates the discovery of efficient CNN models for FR which are trained only with identity labels. We focus on this fact and propose an easily trainable and single CNN based FR method. Our CNN model exploits the residual learning framework. Additionally, it uses normalized features to compute the loss. Our extensive experiments show excellent generalization on different datasets. We obtain very competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and CACD datasets.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),1703.08388,10.1109/ICCVW.2017.197,https://arxiv.org/pdf/1703.08388.pdf
0dfcaaa4b9e4a70ed6cd02f0faefc1917e68e9d1,0,1,Additive Angular Margin Loss in Deep Graph Neural Network Classifier for Learning Graph Edit Distance,"The recent success of graph neural networks (GNNs) in the area of pattern recognition (PR) has increased the interest of researchers to use these frameworks in non-euclidean structures. This non-euclidean structure includes graphs or manifolds that are called geometric deep learning (GDL). It has opened a new direction for researchers to deal with graphs using deep learning in document processing, outperforming conventional methods. We propose a Deep Graph Neural Network (DGNN) classifier-based on additive angular margin loss for the classification task in document analysis. Another contribution of this work is to investigate the performance of a DGNN as a classifier using different loss functions, which helps to minimize the loss for the document analysis problem. We compare additive angular margin loss, Cosine angular margin loss, and multiplicative angular margin loss. Furthermore, we give a comparison between the mentioned loss functions and the Softmax loss function. We also present the comparisons of results using different graph edit distance (GED) methods. Our quantitative results suggest, that by applying the additive angular marginal loss function makes more compact intra-class ability and increases the inter-class discrepancy which enhances the discriminating power of the DGNN. Enhancing the decision boundaries between the classes increase the intra-class compactness and inter-class discrimination power of the model.",2020,IEEE Access,,10.1109/ACCESS.2020.3035886,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09250458.pdf
0dfd73ad08c9a10fe5628257e2170c728e78a357,0,1,FH-GAN: Face Hallucination and Recognition using Generative Adversarial Network,"There are many factors affecting visual face recognition, such as low resolution images, aging, illumination and pose variance, etc. One of the most important problem is low resolution face images which can result in bad performance on face recognition. Most of the general face recognition algorithms usually assume a sufficient resolution for the face images. However, in practice many applications often do not have sufficient image resolutions. The modern face hallucination models demonstrate reasonable performance to reconstruct high-resolution images from its corresponding low resolution images. However, they do not consider identity level information during hallucination which directly affects results of the recognition of low resolution faces. To address this issue, we propose a Face Hallucination Generative Adversarial Network (FH-GAN) which improves the quality of low resolution face images and accurately recognize those low quality images. Concretely, we make the following contributions: 1) we propose FH-GAN network, an end-to-end system, that improves both face hallucination and face recognition simultaneously. The novelty of this proposed network depends on incorporating identity information in a GAN-based face hallucination algorithm via combining a face recognition network for identity preserving. 2) We also propose a new face hallucination network, namely Dense Sparse Network (DSNet), which improves upon the state-of-art in face hallucination. 3) We demonstrate benefits of training the face recognition and GAN-based DSNet jointly by reporting good result on face hallucination and recognition.",2019,ICONIP,1905.06537,10.1007/978-3-030-36708-4_1,https://arxiv.org/pdf/1905.06537.pdf
0e3471582d6c32378677faeccaba6f080e44c6a4,0,1,Functional immune mapping with deep-learning enabled phenomics applied to immunomodulatory and COVID-19 drug discovery,"Development of accurate disease models and discovery of immune-modulating drugs is challenged by the immune system’s highly interconnected and context-dependent nature. Here we apply deep-learning-driven analysis of cellular morphology to develop a scalable “phenomics” platform and demonstrate its ability to identify dose-dependent, high-dimensional relationships among and between immunomodulators, toxins, pathogens, genetic perturbations, and small and large molecules at scale. High-throughput screening on this platform demonstrates rapid identification and triage of hits for TGF-β- and TNF-α-driven phenotypes. We deploy the platform to develop phenotypic models of active SARS-CoV-2 infection and of COVID-19-associated cytokine storm, surfacing compounds with demonstrated clinical benefit and identifying several new candidates for drug repurposing. The presented library of images, deep learning features, and compound screening data from immune profiling and COVID-19 screens serves as a deep resource for immune biology and cellular-model drug discovery with immediate impact on the COVID-19 pandemic.",2020,,,10.1101/2020.08.02.233064,https://www.biorxiv.org/content/biorxiv/early/2020/08/14/2020.08.02.233064.full.pdf
0e3be473c72d9449d13dace5c9c21d9310163630,1,1,Dynamic Region-Aware Convolution,"We propose a new convolution called Dynamic Region-Aware Convolution (DRConv), which can automatically assign multiple filters to corresponding spatial regions where features have similar representation. In this way, DRConv outperforms standard convolution in modeling semantic variations. Standard convolution can increase the number of channels to extract more visual elements but results in high computational cost. More gracefully, our DRConv transfers the increasing channel-wise filters to spatial dimension with learnable instructor, which significantly improves representation ability of convolution and maintains translation-invariance like standard convolution. DRConv is an effective and elegant method for handling complex and variable spatial information distribution. It can substitute standard convolution in any existing networks for its plug-and-play property. We evaluate DRConv on a wide range of models (MobileNet series, ShuffleNetV2, etc.) and tasks (Classification, Face Recognition, Detection and Segmentation.). On ImageNet classification, DRConv-based ShuffleNetV2-0.5x achieves state-of-the-art performance of 67.1% at 46M multiply-adds level with 6.3% relative improvement.",2020,ArXiv,2003.12243,,https://arxiv.org/pdf/2003.12243.pdf
0e50d7b5535791f08a68232805a89cad158a992c,1,1,A Method for Curation of Web-Scraped Face Image Datasets,"Web-scraped, in-the-wild datasets have become the norm in face recognition research. The numbers of subjects and images acquired in web-scraped datasets are usually very large, with number of images on the millions scale. A variety of issues occur when collecting a dataset in-the-wild, including images with the wrong identity label, duplicate images, duplicate subjects and variation in quality. With the number of images being in the millions, a manual cleaning procedure is not feasible. But fully automated methods used to date result in a less-than-ideal level of clean dataset. We propose a semi-automated method, where the goal is to have a clean dataset for testing face recognition methods, with similar quality across men and women, to support comparison of accuracy across gender. Our approach removes near-duplicate images, merges duplicate subjects, corrects mislabeled images, and removes images outside a defined range of pose and quality. We conduct the curation on the Asian Face Dataset (AFD) and VGGFace2 test dataset. The experiments show that a state-of-the-art method achieves a much higher accuracy on the datasets after they are curated. Finally, we release our cleaned versions of both datasets to the research community.",2020,2020 8th International Workshop on Biometrics and Forensics (IWBF),2004.03074,10.1109/IWBF49977.2020.9107950,https://arxiv.org/pdf/2004.03074.pdf
0ea81dc0cedefcc547927b790a3062ea239df9be,1,1,Caption-Supervised Face Recognition: Training a State-of-the-Art Face Model without Manual Annotation,"The advances over the past several years have pushed the performance of face recognition to an amazing level. This great success, to a large extent, is built on top of millions of annotated samples. However, as we endeavor to take the performance to the next level, the reliance on annotated data becomes a major obstacle. We desire to explore an alternative approach, namely using captioned images for training, as an attempt to mitigate this difficulty. Captioned images are widely available on the web, while the captions often contain the names of the subjects in the images. Hence, an effective method to leverage such data would significantly reduce the need of human annotations. However, an important challenge along this way needs to be tackled: the names in the captions are often noisy and ambiguous, especially when there are multiple names in the captions or multiple people in the photos. In this work, we propose a simple yet effective method, which trains a face recognition model by progressively expanding the labeled set via both selective propagation and caption-driven expansion. We build a large-scale dataset of captioned images, which contain 6.3M faces from 305K subjects. Our experiments show that using the proposed method, we can train a state-of-the-art face recognition model without manual annotation (99.65% in LFW). This shows the great potential of caption-supervised face recognition.",2020,,,,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620137.pdf
0f43412e114ecb29fbeec0f1f613c8763adde5e2,1,1,MaaFace: Multiplicative and Additive Angular Margin Loss for Deep Face Recognition,,2019,ICIG,,10.1007/978-3-030-34113-8_53,
0f86a138ecb83b3468f6f8213328d100cdd1a1de,1,0,CNN Customizations With Transfer Learning for Face Recognition Task,"Several kinds of pretrained convolutional neural networks (CNN) exist nowadays. Utilizing these networks with the new classification task requires the retraining with new data sets. With the small embedded device, large network cannot be implemented. The authors study the use of pretrained models and customizing them towards accuracy and size against face recognition tasks. The results show 1) the performance of existing pretrained networks (e.g., AlexNet, GoogLeNet, CaffeNet, SqueezeNet), as well as size, and 2) demonstrate the layers customization towards the model size and accuracy. The studied results show that among the various networks with different data sets, SqueezeNet can achieve the same accuracy (0.99) as others with small size (up to 25 times smaller). Secondly, the two customizations with layer skipping are presented. The experiments show the example of SqueezeNet layer customizing, reducing the network size while keeping the accuracy (i.e., reducing the size by 7% with the slower convergence time). The experiments are measured based on Caffe 0.15.14.",2019,,,10.4018/978-1-5225-7862-8.CH003,
0f9d773ad4af1c010f8ff84693a67e8d2ab09dac,0,1,Learning to Learn in a Semi-Supervised Fashion,"To address semi-supervised learning from both labeled and unlabeled data, we present a novel meta-learning scheme. We particularly consider that labeled and unlabeled data share disjoint ground truth label sets, which can be seen tasks like in person re-identification or image retrieval. Our learning scheme exploits the idea of leveraging information from labeled to unlabeled data. Instead of fitting the associated class-wise similarity scores as most meta-learning algorithms do, we propose to derive semantics-oriented similarity representations from labeled data, and transfer such representation to unlabeled ones. Thus, our strategy can be viewed as a self-supervised learning scheme, which can be applied to fully supervised learning tasks for improved performance. Our experiments on various tasks and settings confirm the effectiveness of our proposed approach and its superiority over the state-of-the-art methods.",2020,ArXiv,2008.11203,,https://arxiv.org/pdf/2008.11203.pdf
0fa15f54f537f41fdfb80a3eccfd710bc4346dc4,1,1,Attribute Adaptive Margin Softmax Loss using Privileged Information,"We present a novel framework to exploit privileged information for recognition which is provided only during the training phase. Here, we focus on recognition task where images are provided as the main view and soft biometric traits (attributes) are provided as the privileged data (only available during training phase). We demonstrate that more discriminative feature space can be learned by enforcing a deep network to adjust adaptive margins between classes utilizing attributes. This tight constraint also effectively reduces the class imbalance inherent in the local data neighborhood, thus carving more balanced class boundaries locally and using feature space more efficiently. Extensive experiments are performed on five different datasets and the results show the superiority of our method compared to the state-of-the-art models in both tasks of face recognition and person re-identification.",2020,ArXiv,2009.01972,,https://arxiv.org/pdf/2009.01972.pdf
0fd73819814928b90b2d68020efeaaa8a32d9d78,1,0,Synthetic Data Augmentation for Facial Re-identification,"Facial Re-identification datasets which facilitate the training of Deep Neural Networks (DNNs), tend to be high quality images of celebrities harvested from the internet. There is however a domain gap between these datasets, and the low quality samples used in real-world systems and scenarios such as surveillance footage. In this work we describe a novel process of data augmentation using synthetically generated images, which aids cross-domain generalisability, without the need to acquire large amounts of real data in the target domain. We also contribute a new dataset derived from this process: syn-Face. Our approach is validated by training with standard high quality datasets with synthetic augmentation and testing in 2 different realistic sets.",2019,,,,https://pureadmin.qub.ac.uk/ws/files/175644034/IMVIP2019_paper_11.pdf
104e401557a38ee93aabfb1a6ab4d6398b93385e,0,1,Open Source Iris Recognition Hardware and Software with Presentation Attack Detection,"This paper proposes the first known to us open source hardware and software iris recognition system with presentation attack detection (PAD), which can be easily assembled for about 75 USD using Raspberry Pi board and a few peripherals. The primary goal of this work is to offer a low-cost baseline for spoof-resistant iris recognition, which may (a) stimulate research in iris PAD and allow for easy prototyping of secure iris recognition systems, (b) offer a low-cost secure iris recognition alternative to more sophisticated systems, and (c) serve as an educational platform. We propose a lightweight image complexity-guided convolutional network for fast and accurate iris segmentation, domain-specific human-inspired Binarized Statistical Image Features (BSIF) to build an iris template, and to combine 2D (iris texture) and 3D (photometric stereo-based) features for PAD. The proposed iris recognition runs in about 3.2 seconds and the proposed PAD runs in about 4.5 seconds on Raspberry Pi 3B+. The hardware specifications and all source codes of the entire pipeline are made available along with this paper.",2020,ArXiv,2008.0822,,https://arxiv.org/pdf/2008.08220.pdf
104fce24d7d9677c69d73d6eeceb4a6af2afd81b,1,0,Age Range Estimation Using MTCNN and VGG-Face Model,"The Convolutional Neural Network has amazed us with its usage on several applications. Age range estimation using CNN is emerging due to its application in myriad of areas which makes it a state-of-the-art area for research and improve the estimation accuracy. A deep CNN model is used for identification of people's age range in our proposed work. At first, we extracted only face images from image dataset using MTCNN to remove unnecessary features other than face from the image. Secondly, we used random crop technique for data augmentation to improve the model performance. We have used the concept of transfer learning in our research. A pretrained face recognition model i.e VGG-Face is used to build our model for identification of age range whose performance is evaluated on Adience Benchmark for confirming the efficacy of our work. The performance in test set outperformed existing state-of-the-art by substantial margins.",2020,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",,10.1109/ICCCNT49239.2020.9225443,
1066910b7cf6acf063c3a114c34bf14a3667237e,1,0,A review on face recognition systems: recent approaches and challenges,"Face recognition is an efficient technique and one of the most preferred biometric modalities for the identification and verification of individuals as compared to voice, fingerprint, iris, retina eye scan, gait, ear and hand geometry. This has over the years necessitated researchers in both the academia and industry to come up with several face recognition techniques making it one of the most studied research area in computer vision. A major reason why it remains a fast-growing research lies in its application in unconstrained environments, where most existing techniques do not perform optimally. Such conditions include pose, illumination, ageing, occlusion, expression, plastic surgery and low resolution. In this paper, a critical review on the different issues of face recognition systems are presented, and different approaches to solving these issues are analyzed by presenting existing techniques that have been proposed in the literature. Furthermore, the major and challenging face datasets that consist of the different facial constraints which depict real-life scenarios are also discussed stating the shortcomings associated with them. Also, recognition performance on the different datasets by researchers are also reported. The paper is concluded, and directions for future works are highlighted.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09261-2,
107677ff59b068d67b1a1f8b66435a5db310f264,1,0,FACE RECOGNITION WITH LOW FALSE POSITIVE ERROR RATE,"Abstract. Nowadays face recognition systems are widely used in the world. In China these systems are used in safe cities projects in production, in Russia they are used mostly in closed-loop systems like factories, business centers with biometric access control or stadiums. Closed loop means that we need to identify people from a fixed dataset: in factory it’s a list of employees, in stadium it’s a list of ticket owners. The most challenging task is to identify people from some large city with an open dataset: we don’t have a fixed set of people in the city, it’s rapidly changing due to migration. Another limit is the accuracy of the system: we can’t make a lot of false positive errors (when a person is incorrectly recognized as another person) because number of human operators is limited and they are expensive. We propose an approach to maximize face recognition accuracy for a fixed false positive error rate using limited amount of hardware.",2019,,,10.5194/ISPRS-ARCHIVES-XLII-2-W12-11-2019,https://pdfs.semanticscholar.org/1076/77ff59b068d67b1a1f8b66435a5db310f264.pdf
10915b826bfd5eb720b5bc35eee162ef32846759,0,1,ModelHub.AI: Dissemination Platform for Deep Learning Models,"Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present this http URL (this http URL), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on this http URL and allows both researchers and developers to utilize models out-of-the-box. this http URL is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.",2019,ArXiv,1911.13218,,https://arxiv.org/pdf/1911.13218.pdf
10c109e86c94a241e14842b8b810b0a527295edf,0,1,Multi-scale multi-patch person re-identification with exclusivity regularized softmax,"Abstract Discriminative feature learning is critical for person re-identification. To obtain abundant visual information from the input person image, we first propose a novel network that extracts multi-scale patch-level deep features. Then, we propose an improved softmax loss function for learning more compact and more discriminative feature vectors. Specifically, we integrate feature pyramid blocks and region-level global average pooling functions into the feature extraction network, introduce the well-established normalization techniques in face recognition algorithms into person re-ID, and penalize the redundancy in feature vectors by minimizing the l1,2 norm of the weight matrix in the softmax layer. Experiments on three large-scale datasets under the standard settings show the effectiveness of the proposed method. Moreover, we report our cross-domain re-ID results by training re-ID models on source datasets and testing them on other target datasets.",2020,Neurocomputing,,10.1016/j.neucom.2019.11.062,https://xinggangw.info/pubs/msmpreid.pdf
10e2dc8da67b88e612f0c5b3b5120ca0f151f24b,1,0,Towards a General Model of Knowledge for Facial Analysis by Multi-Source Transfer Learning,"This paper proposes a step toward obtaining general models of knowledge for facial analysis, by addressing the question of multi-source transfer learning. More precisely, the proposed approach consists in two successive training steps: the first one consists in applying a combination operator to define a common embedding for the multiple sources materialized by different existing trained models. The proposed operator relies on an auto-encoder, trained on a large dataset, efficient both in terms of compression ratio and transfer learning performance. In a second step we exploit a distillation approach to obtain a lightweight student model mimicking the collection of the fused existing models. This model outperforms its teacher on novel tasks, achieving results on par with state-of-the-art methods on 15 facial analysis tasks (and domains), at an affordable training cost. Moreover, this student has 75 times less parameters than the original teacher and can be applied to a variety of novel face-related tasks.",2019,ArXiv,1911.03222,,https://arxiv.org/pdf/1911.03222.pdf
11246c0e71ce594c0107cf4fd3f82d644c5c76d8,1,0,Deep Poisoning Functions: Towards Robust Privacy-safe Image Data Sharing,"As deep networks are applied to an ever-expanding set of computer vision tasks, protecting general privacy in image data has become a critically important goal. This paper presents a new framework for privacy-preserving data sharing that is robust to adversarial attacks and overcomes the known issues existing in previous approaches. We introduce the concept of a Deep Poisoning Function (DPF), which is a module inserted into a pre-trained deep network designed to perform a specific vision task. The DPF is optimized to deliberately poison image data to prevent known adversarial attacks, while ensuring that the altered image data is functionally equivalent to the non-poisoned data for the original task. Given this equivalence, both poisoned and non-poisoned data can be used for further retraining or fine-tuning. Experimental results on image classification and face recognition tasks prove the efficacy of the proposed method.",2019,ArXiv,1912.06895,,https://arxiv.org/pdf/1912.06895.pdf
118c87b84976ce204040e16128c71a512e80c934,0,1,Face recognition on 3D point clouds,"Point cloud has achieved great attention in 3D object classification, segmentation and indoor scene semantic parsing. In terms of face recognition, although image-based algorithm become more accurate and faster, open world face recognition still suffers from the influences i.e. illumination, occlusion, pose, etc. 3D face recognition based on point cloud containing both shape and texture information can compensate these shortcomings. However training a network to extract discriminative 3D feature is model complex and time inefficient due to the lack of large training dataset. To address these problems, we propose a novel 3D face recognition network(FPCNet) using modified PointNet++ and a 3D augmentation technique. Face-based loss and multi-label loss are used to train the FPCNet to enhance the learned features more discriminative. Moreover, a 3D face data augmentation method is proposed to synthesize more identity-variance and expression-variance 3D faces from limited data. Our proposed method shows excellent recognition results on CASIA-3D, Bosphorus and FRGC2.0 datasets and generalizes well for other datasets.",2019,International Conference on Optical and Photonic Engineering,,10.1117/12.2541704,
119ec1d45ee3e598dae66a34d0105b863073bef5,1,0,Face Recognition via Active Annotation and Learning,"In this paper, we introduce an active annotation and learning framework for the face recognition task. Starting with an initial label deficient face image training set, we iteratively train a deep neural network and use this model to choose the examples for further manual annotation. We follow the active learning strategy and derive the Value of Information criterion to actively select candidate annotation images. During these iterations, the deep neural network is incrementally updated. Experimental results conducted on LFW benchmark and MS-Celeb-1M challenge demonstrate the effectiveness of our proposed framework.",2016,ACM Multimedia,,10.1145/2964284.2984059,http://zhengyingbin.cc/publication/mm16-face.pdf
11ad162b3165b4353df8d7b4153fb26d6a310d11,1,0,Recognizing Families In the Wild (RFIW): Data Challenge Workshop in conjunction with ACM MM 2017,"Recognizing Families In the Wild (RFIW) is a large-scale, multi-track automatic kinship recognition evaluation, supporting both kinship verification and family classification on scales much larger than ever before. It was organized as a Data Challenge Workshop hosted in conjunction with ACM Multimedia 2017. This was achieved with the largest image collection that supports kin-based vision tasks. In the end, we use this manuscript to summarize evaluation protocols, progress made and some technical background and performance ratings of the algorithms used, and a discussion on promising directions for both research and engineers to be taken next in this line of work.",2017,RFIW '17,,10.1145/3134421.3134424,https://web.northeastern.edu/smilelab/fiw/papers/RFIW17.pdf
11f7ca99836c0151dae9d22df5cff874f42992e2,1,0,Deep learning with noisy labels: exploring techniques and remedies in medical image analysis,"Supervised training of deep learning models requires large labeled datasets. There is a growing interest in obtaining such datasets for medical image analysis applications. However, the impact of label noise has not received sufficient attention. Recent studies have shown that label noise can significantly impact the performance of deep learning models in many machine learning and computer vision applications. This is especially concerning for medical applications, where datasets are typically small, labeling requires domain expertise and suffers from high inter- and intra-observer variability, and erroneous predictions may influence decisions that directly impact human health. In this paper, we first review the state-of-the-art in handling label noise in deep learning. Then, we review studies that have dealt with label noise in deep learning for medical image analysis. Our review shows that recent progress on handling label noise in deep learning has gone largely unnoticed by the medical image analysis community. To help achieve a better understanding of the extent of the problem and its potential remedies, we conducted experiments with three medical imaging datasets with different types of label noise, where we investigated several existing strategies and developed new methods to combat the negative effect of label noise. Based on the results of these experiments and our review of the literature, we have made recommendations on methods that can be used to alleviate the effects of different types of label noise on deep models trained for medical image analysis. We hope that this article helps the medical image analysis researchers and developers in choosing and devising new techniques that effectively handle label noise in deep learning.",2020,Medical Image Anal.,1912.02911,10.1016/j.media.2020.101759,https://arxiv.org/pdf/1912.02911.pdf
1200ee15baa9ec6c929f7b134ea298ced3575ac7,1,0,Algorithmic Discrimination: Formulation and Exploration in Deep Learning-based Face Biometrics,"The most popular face recognition benchmarks assume a distribution of subjects without much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. The main aim of this study is focused on a better understanding of the feature space generated by deep models, and the performance achieved over different demographic groups. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments are conducted over the new DiveFace database composed of 24K identities from six different demographic groups. Two popular face recognition models are considered in the experimental framework: ResNet-50 and VGG-Face. We experimentally show that demographic groups highly represented in popular face databases have led to popular pre-trained deep face models presenting strong algorithmic discrimination. That discrimination can be observed both qualitatively at the feature space of the deep models and quantitatively in large performance differences when applying those models in different demographic groups, e.g. for face biometrics.",2020,SafeAI@AAAI,1912.01842,,https://arxiv.org/pdf/1912.01842.pdf
122c4a2714eed77a6b005d1d3ab8c210b2257910,0,1,Detection of Makeup Presentation Attacks based on Deep Face Representations,"Facial cosmetics have the ability to substantially alter the facial appearance, which can negatively affect the decisions of a face recognition. In addition, it was recently shown that the application of makeup can be abused to launch so-called makeup presentation attacks. In such attacks, the attacker might apply heavy makeup in order to achieve the facial appearance of a target subject for the purpose of impersonation. In this work, we assess the vulnerability of a COTS face recognition system to makeup presentation attacks employing the publicly available Makeup Induced Face Spoofing (MIFS) database. It is shown that makeup presentation attacks might seriously impact the security of the face recognition system. Further, we propose an attack detection scheme which distinguishes makeup presentation attacks from genuine authentication attempts by analysing differences in deep face representations obtained from potential makeup presentation attacks and corresponding target face images. The proposed detection system employs a machine learning-based classifier, which is trained with synthetically generated makeup presentation attacks utilizing a generative adversarial network for facial makeup transfer in conjunction with image warping. Experimental evaluations conducted using the MIFS database reveal a detection equal error rate of 0.7% for the task of separating genuine authentication attempts from makeup presentation attacks.",2020,ArXiv,2006.05074,,https://arxiv.org/pdf/2006.05074.pdf
123bbc9e6987c1374442bfebaf195409ec6c2e4e,0,1,ByeGlassesGAN: Identity Preserving Eyeglasses Removal for Face Images,"In this paper, we propose a novel image-to-image GAN framework for eyeglasses removal, called ByeGlassesGAN, which is used to automatically detect the position of eyeglasses and then remove them from face images. Our ByeGlassesGAN consists of an encoder, a face decoder, and a segmentation decoder. The encoder is responsible for extracting information from the source face image, and the face decoder utilizes this information to generate glasses-removed images. The segmentation decoder is included to predict the segmentation mask of eyeglasses and completed face region. The feature vectors generated by the segmentation decoder are shared with the face decoder, which facilitates better reconstruction results. Our experiments show that ByeGlassesGAN can provide visually appealing results in the eyeglasses-removed face images even for semi-transparent color eyeglasses or glasses with glare. Furthermore, we demonstrate significant improvement in face recognition accuracy for face images with glasses by applying our method as a pre-processing step in our face recognition experiment.",2020,ECCV,2008.11042,10.1007/978-3-030-58526-6_15,https://arxiv.org/pdf/2008.11042.pdf
124f6992202777c09169343d191c254592e4428c,1,0,Visual Psychophysics for Making Face Recognition Algorithms More Explainable,"Scientific fields that are interested in faces have developed their own sets of concepts and procedures for understanding how a target model system (be it a person or algorithm) perceives a face under varying conditions. In computer vision, this has largely been in the form of dataset evaluation for recognition tasks where summary statistics are used to measure progress. While aggregate performance has continued to improve, understanding individual causes of failure has been difficult, as it is not always clear why a particular face fails to be recognized, or why an impostor is recognized by an algorithm. Importantly, other fields studying vision have addressed this via the use of visual psychophysics: the controlled manipulation of stimuli and careful study of the responses they evoke in a model system. In this paper, we suggest that visual psychophysics is a viable methodology for making face recognition algorithms more explainable. A comprehensive set of procedures is developed for assessing face recognition algorithm behavior, which is then deployed over state-of-the-art convolutional neural networks and more basic, yet still widely used, shallow and handcrafted feature-based approaches.",2018,ECCV,1803.0714,10.1007/978-3-030-01267-0_16,https://arxiv.org/pdf/1803.07140.pdf
126dab238b018e9538cf3a89451a38982c3ecda7,0,1,Deep person re-identification in UAV images,"The person re-identification is one of the most significant problems in computer vision and surveillance systems. The recent success of deep convolutional neural networks in image classification has inspired researchers to investigate the application of deep learning to the person re-identification. However, the huge amount of research on this problem considers classical settings, where pedestrians are captured by static surveillance cameras, although there is a growing demand for analyzing images and videos taken by drones. In this paper, we aim at filling this gap and provide insights on the person re-identification from drones. To our knowledge, it is the first attempt to tackle this problem under such constraints. We present the person re-identification dataset, named DRone HIT (DRHIT01), which is collected by using a drone. It contains 101 unique pedestrians, which are annotated with their identities. Each pedestrian has about 500 images. We propose to use a combination of triplet and large-margin Gaussian mixture (L-GM) loss to tackle the drone-based person re-identification problem. The proposed network equipped with multi-branch design, channel group learning, and combination of loss functions is evaluated on the DRHIT01 dataset. Besides, transfer learning from the most popular person re-identification datasets is evaluated. Experiment results demonstrate the importance of transfer learning and show that the proposed model outperforms the classic deep learning approach.",2019,,,10.1186/s13634-019-0647-z,
12769b9cee7e23ea0fbb1645a346dec1b71329d2,0,1,Metric Learning Loss Functions to Reduce Domain Mismatch in the x-Vector Space for Language Recognition,"State-of-the-art language recognition systems are based on dis-criminative embeddings called x-vectors. Channel and gender distortions produce mismatch in such x-vector space where em-beddings corresponding to the same language are not grouped in an unique cluster. To control this mismatch, we propose to train the x-vector DNN with metric learning objective functions. Combining a classification loss with the metric learning n-pair loss allows to improve the language recognition performance. Such a system achieves a robustness comparable to a system trained with a domain adaptation loss function but without using the domain information. We also analyze the mismatch due to channel and gender, in comparison to language proximity, in the x-vector space. This is achieved using the Maximum Mean Discrepancy divergence measure between groups of x-vectors. Our analysis shows that using the metric learning loss function reduces gender and channel mismatch in the x-vector space, even for languages only observed on one channel in the train set.",2020,INTERSPEECH,,10.21437/INTERSPEECH.2020-1708,https://hal.archives-ouvertes.fr/hal-02920460/file/raphael_interspeech_v9.pdf
1299ecacc8facfb20c68b3fc6b36021f47013608,1,0,Cast Search via Two-Stream Label Propagation,"We address the problem of Cast Search by Portraits (CSP) where a facial portrait of a cast member is provided to retrieve from a given video clip those frames containing the query target. The underlying CSP formulation is related to the task of person re-identification. However, CSP is more challenging in that the provided query image is only a portrait of a certain cast member, and the instances of the target in the candidate video could have a very different visual appearance. Such drastic visual variations are not common in addressing the problem of person re-id. We propose a two-stream network architecture for tackling the CSP challenge and also participate in the public CSP competition. The overall outcome in the competition is promising and worth further effort to improve our proposed model.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00231,http://openaccess.thecvf.com/content_ICCVW_2019/papers/WIDER/Wu_Cast_Search_via_Two-Stream_Label_Propagation_ICCVW_2019_paper.pdf
12fcb2637f241d515827fb193a650b7e6bf6b1f7,1,0,Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations,"We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.",2018,AAAI,1705.08841,,https://arxiv.org/pdf/1705.08841.pdf
1344317f255a9d338fb80f276126951b9644f7e3,1,0,M-VAD names: a dataset for video captioning with naming,"Current movie captioning architectures are not capable of mentioning characters with their proper name, replacing them with a generic “someone” tag. The lack of movie description datasets with characters’ visual annotations surely plays a relevant role in this shortage. Recently, we proposed to extend the M-VAD dataset by introducing such information. In this paper, we present an improved version of the dataset, namely M-VAD Names, and its semi-automatic annotation procedure. The resulting dataset contains 63 k visual tracks and 34 k textual mentions, all associated with character identities. To showcase the features of the dataset and quantify the complexity of the naming task, we investigate multimodal architectures to replace the “someone” tags with proper character names in existing video captions. The evaluation is further extended by testing this application on videos outside of the M-VAD Names dataset.",2018,Multimedia Tools and Applications,1903.01489,10.1007/s11042-018-7040-z,https://arxiv.org/pdf/1903.01489.pdf
1345fb7700389f9d02f203b3cb25ac3594855054,1,0,Hierarchical Training for Large Scale Face Recognition with Few Samples Per Subject,"Recent progress of face recognition benefits a lot from large-scale face datasets with deep Convoluitonal Neural Networks(CNN). However, when dataset contains a large number of subjects but with few samples for each subject, conventional CNN with softmax loss is heavily prone to overfitting. To address this issue, we propose a hierarchical training schema to optimize CNN with coarse-to-fine class labels, referred to as Hit-CNN. Firstly trained with coarse class labels and then refined with fine class labels, Hit-CNN is enabled the to capture the distribution of data from major variations to fine variations progressively, which can effectively relieve the overfitting and lead to better generalization. In this work, the hierarchical coarse-to-fine class labels are obtained via hierarchical k-means clustering according to the face identities. Evaluated on two face datasets, the proposed Hit-CNN provides better results compared with the conventional CNN under the circumstances of large-scale data with few samples per subject.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451561,
137e71083d7fbb1e065d541fffc8e5b3896048d7,0,1,A New Localization Objective for Accurate Fine-Grained Affordance Segmentation Under High-Scale Variations,"Fine-grained affordance segmentation for object parts can greatly benefit robotics and scene understanding applications. In this work, we propose an instance-segmentation framework that can accurately localize functionality and affordance of individual object parts. We build on the standard Mask-RCNN framework and propose two novelties to the localization objective that can lead to improved part detection and affordance segmentation results. Specifically, we notice two problems with the conventional IOU based regression loss, (a) the small boxes, that are specially relevant for fine-grained detection, have a higher risk of being ignored during the optimization process and (b) a constant value of IOU for non-overlapping candidates means no supervision is available to encourage the reduction in loss function. To address these limitations, we propose a novel Angular Intersection Over Larger (AIOL) measure. Our experiments show consistent improvement over other baselines and state of the art localization loss functions for the fine-grained affordance segmentation task.",2020,IEEE Access,,10.1109/ACCESS.2019.2958608,https://ieeexplore.ieee.org/ielx7/6287639/8948470/08930497.pdf
13ac62a5c6a43c4f3f9ff578f7756f764ebbea5b,1,0,Backdoor Attack with Sample-Specific Triggers,"Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). Attackers intend to inject hidden backdoor into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if the hidden backdoor is activated by an attacker-defined trigger. Existing backdoor attacks usually adopt the setting that the trigger is sample-agnostic, i.e., different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm that the backdoor trigger is sample-specific. Specifically, inspired by the recent advance in DNN-based image steganography, we generate sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when DNNs are trained on the poisoned dataset. Extensive experiments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses.",2020,ArXiv,2012.03816,,https://arxiv.org/pdf/2012.03816.pdf
13b2bc8101a2a7a0c95412c48f40ef95e798e9fb,1,1,Learning towards Minimum Hyperspherical Energy,"Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.",2018,NeurIPS,1805.09298,,https://arxiv.org/pdf/1805.09298.pdf
13b76c87be8f0a5d8b2e1c3cf40dc1656e2f0fb7,1,1,Learning Discriminative Representation For Facial Expression Recognition From Uncertainties,"Recent progresses on Facial Expression Recognition (FER) heavily rely on deep learning models trained with large scale datasets. However, large-scale facial expression datasets always suffer from annotation uncertainties caused by ambiguous expressions, low-quality facial images, and the subjectiveness of annotators, which limits FER performance. To address this challenge, this paper introduces novel Rayleigh and weighted-softmax loss from two aspects. First, we propose Rayleigh loss to extract discriminative representation, which aims at minimizing within-class distances and maximizing inter-class distances simultaneously. Moreover, Rayleigh loss has a Euclidean form which make it easily be optimized with SGD and be combined with other forms. Second, we introduce a weight to measure the uncertainty of a given sample, by considering its distance to class center. Extensive experiments on RAF-DB, FERPlus and AffectNet show the effectiveness of our method with SOTA performance.",2020,2020 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP40778.2020.9190643,
13d7deb38eaad59a0121392731c2e6832b2d83f3,0,1,Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative Adversarial Networks,"Generating realistic 3D faces is of high importance for computer graphics and computer vision applications. Generally, research on 3D face generation revolves around linear statistical models of the facial surface. Nevertheless, these models cannot represent faithfully either the facial texture or the normals of the face, which are very crucial for photo-realistic face synthesis. Recently, it was demonstrated that Generative Adversarial Networks (GANs) can be used for generating high-quality textures of faces. Nevertheless, the generation process either omits the geometry and normals, or independent processes are used to produce 3D shape information. In this paper, we present the first methodology that generates high-quality texture, shape, and normals jointly, which can be used for photo-realistic synthesis. To do so, we propose a novel GAN that can generate data from different modalities while exploiting their correlations. Furthermore, we demonstrate how we can condition the generation on the expression and create faces with various facial expressions. The qualitative results shown in this paper are compressed due to size limitations, full-resolution results and the accompanying video can be found in the supplementary documents. The code and models are available at the project page: this https URL.",2020,ECCV,1909.02215,10.1007/978-3-030-58526-6_25,https://arxiv.org/pdf/1909.02215.pdf
13f91a455c89890e75470d507b0eafc57cd83041,1,1,SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing,"Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee ""subtle perturbation"" by limiting the $L_p$ norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate ""unrestricted adversarial examples"".  In particular, we propose an algorithm \emph{SemanticAdv} which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various ""adversarial"" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against \emph{real-world black-box} services such as Azure face verification service based on transferability.  To further demonstrate the applicability of \emph{SemanticAdv} beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.",2019,ArXiv,1906.07927,,https://arxiv.org/pdf/1906.07927.pdf
13f96c71f53025dfb9e1bfe4cc1f85688cda19a0,1,0,Visual Analytics of Political Networks From Face-Tracking of News Video,"The rich nature of news makes it a classic subject of visual analytics research. Such analysis is often based on rich textual data. However, we want to test how much we can understand the news from video information through face detection and tracking. Towards this goal, we propose a visual analytics system and discuss its design and implementation to support media experts in understanding political interactions in an archive of 12 years of the Japanese public broadcaster NHK's News 7 program. After identifying the tasks and abstraction required for our analysis, we construct links from face detection and tracking to derive multiple political networks. Our proposed design embeds this rich data into a visual analytics framework that presents four levels of abstraction: time period, network, timeline, and face-tracks within video. We present how the exploration of the archive with our system results in good understanding of the Japanese politico-media scene during these 12 years while finding evidence of “presidentialization” of the media.",2016,IEEE Transactions on Multimedia,,10.1109/TMM.2016.2614224,https://hal.archives-ouvertes.fr/hal-01454822/file/faceTrackingVA-double_column.pdf
144b900318091c5beb70102be3cfd5905a5e10b6,1,0,Fully End-to-End Composite Recurrent Convolution Network for Deformable Facial Tracking In The Wild,"Human facial tracking is an important task in computer vision, which has recently lost pace compared to other facial analysis tasks. The majority of current available tracker possess two major limitations: their little use of temporal information and the widespread use of handcrafted features, without taking full advantage of the large annotated datasets that have recently become available. In this paper we present a fully end-to-end facial tracking model based on current state of the art deep model architectures that can be effectively trained from the available annotated facial landmark datasets. We build our model from the recently introduced general object tracker Re3, which allows modeling the short and long temporal dependency between frames by means of its internal Long Short Term Memory (LSTM) layers. Facial tracking experiments on the challenging 300-VW dataset show that our model can produce state of the art accuracy and far lower failure rates than competing approaches. We specifically compare the performance of our approach modified to work in tracking-by-detection mode and showed that, as such, it can produce results that are comparable to state of the art trackers. However, upon activation of our tracking mechanism, the results improve significantly, confirming the advantage of taking into account temporal dependencies.",2019,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),,10.1109/FG.2019.8756630,
145016b645e7a563a910a9bf59d82f8153214732,0,1,Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos,"In many real-world problems, there is typically a large discrepancy between the characteristics of data used in training versus deployment. A prime example is the analysis of aggression videos: in a criminal incidence, typically suspects need to be identified based on their clean portrait-like photos, instead of their prior video recordings. This results in three major challenges; large domain discrepancy between violence videos and ID-photos, the lack of video examples for most individuals and limited training data availability. To mimic such scenarios, we formulate a realistic domain-transfer problem, where the goal is to transfer the recognition model trained on clean posed images to the target domain of violent videos, where training videos are available only for a subset of subjects. To this end, we introduce the WildestFaces dataset, tailored to study cross-domain recognition under a variety of adverse conditions. We divide the task of transferring a recognition model from the domain of clean images to the violent videos into two sub-problems and tackle them using (i) stacked affine-transforms for classifier-transfer, (ii) attention-driven pooling for temporal-adaptation. We additionally formulate a self-attention based model for domain-transfer. We establish a rigorous evaluation protocol for this clean-to-violent recognition task, and present a detailed analysis of the proposed dataset and the methods. Our experiments highlight the unique challenges introduced by the WildestFaces dataset and the advantages of the proposed approach.",2020,ArXiv,2009.07576,,https://arxiv.org/pdf/2009.07576.pdf
14ac5016a3f1df12418933f167c8b1c625669e06,1,0,Visualizing Important Areas for Facial Verification,iii,2017,,,,https://pdfs.semanticscholar.org/14ac/5016a3f1df12418933f167c8b1c625669e06.pdf
14d70a77afaab8d1e5a62590dbf5ec0de9d0af47,0,1,Improving Detection And Recognition Of Degraded Faces By Discriminative Feature Restoration Using GAN,"Face detection and recognition in the wild is currently one of the most interesting and challenging problems. Many algorithms with high performance have already been proposed and applied in real-world applications. However, the problem of detecting and recognising degraded faces from low-quality images and videos mostly remains unsolved. In this paper, we present an algorithm capable of recovering facial features from low-quality videos and images. The resulting output image boosts the performance of existing face detection and recognition algorithms. It contains an effective method involving metric learning and different loss function components operating on different parts of the generator. This enhances the degraded faces by restoring their lost features rather than its perceptual quality. Our approach has been experimentally proven to enhance face detection and recognition, e.g., the face detection rate is improved by 3.08% for S3FD [1] and the area under the ROC curve for recognition is improved by 2.55% for ArcFace [2] on the SCFace dataset.",2020,2020 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP40778.2020.9191246,
14ea09da023d015263e239c1876f547adcd2974c,0,1,Centralized embedding hypersphere feature learning for person re-identification,"ABSTRACT Deep metric learning has become a general method for person re-identification (ReID) recently. Existing methods train ReID model with various loss functions to learn feature representation and identify pedestrian. However, the interaction between person features and classification vectors in the training process is rarely concerned. Distribution of pedestrian features will greatly affect convergence of the model and the pedestrian similarity computing in the test phase. In this paper, we formulate improved softmax function to learn pedestrian features and classification vectors. Our method applies pedestrian feature representation to be scattered across the coordinate space and embedding hypersphere to solve the classification problem. Then, we propose an end-to-end convolutional neural network (CNN) framework with improved softmax function to improve the performance of pedestrian features. Finally, experiments are performed on four challenging datasets. The results demonstrate that our work is competitive compared to the state-of-the-art.",2019,,,10.1080/13682199.2019.1647947,
152643de74ce040e9990c324daa7798cb6909519,1,0,Collaborations on YouTube: From Unsupervised Detection to the Impact on Video and Channel Popularity,"YouTube is one of the most popular platforms for streaming of user-generated video. Nowadays, professional YouTubers are organized in so called multi-channel networks (MCNs). These networks offer services such as brand deals, equipment, and strategic advice in exchange for a share of the YouTubers' revenue. A major strategy to gain more subscribers and, hence, revenue is collaborating with other YouTubers. Yet, collaborations on YouTube have not been studied in a detailed quantitative manner. This paper aims to close this gap with the following contributions. First, we collect a YouTube dataset covering video statistics over three months for 7,942 channels. Second, we design a framework for collaboration detection given a previously unknown number of persons featuring in YouTube videos. We denote this framework for the analysis of collaborations in YouTube videos using a Deep Neural Network (DNN) based approach as CATANA. Third, we analyze about 2.4 years of video content and use CATANA to answer research questions providing guidance for YouTubers and MCNs for efficient collaboration strategies. Thereby, we focus on (i) collaboration frequency and partner selectivity, (ii) the influence of MCNs on channel collaborations, (iii) collaborating channel types, and (iv) the impact of collaborations on video and channel popularity. Our results show that collaborations are in many cases significantly beneficial in terms of viewers and newly attracted subscribers for both collaborating channels, showing often more than 100% popularity growth compared with non-collaboration videos.",2018,,,,
152ae5bb447fae0975a1863fa9fd70ade5e70989,1,0,IMPROVED TECHNIQUES FOR MODEL INVERSION ATTACK,"Model inversion (MI) attacks in the whitebox setting are aimed at reconstructing training data from model parameters. Such attacks have triggered increasing concerns about privacy, especially given a growing number of online model repositories. However, existing MI attacks against deep neural networks (DNNs) have large room for performance improvement. A natural question is whether the underperformance is because the target model does not memorize much about its training data or it is simply an artifact of imperfect attack algorithm design? This paper shows that it is the latter. We present a variety of new techniques that can significantly boost the performance of MI attacks against DNNs. Recent advances to attack DNNs are largely attributed to the idea of training a general generative adversarial network (GAN) with potential public data and using it to regularize the search space for reconstructed images. We propose to customize the training of a GAN to the inversion task so as to better distill knowledge useful for performing attacks from public data. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model private data distribution in order to better reconstruct representative data points. Our experiments show that the combination of these techniques can lead to state-of-the-art attack performance on a variety of datasets and models, even when the public data has a large distributional shift from the private data.",2020,,2010.04092,,https://arxiv.org/pdf/2010.04092.pdf
153fbae25efd061f9046970071d0cfe739a35a0e,1,0,FaceLiveNet: End-to-End Networks Combining Face Verification with Interactive Facial Expression-Based Liveness Detection,"The effectiveness of the state-of-the-art face verifi-cation/recognition algorithms and the convenience of face recognition greatly boost the face-related biometric authentication applications. However, existing face verification architectures seldom integrate any liveness detection or keep such stage isolated from face verification as if it was irrelevant. This may potentially result in the system being exposed to spoof attacks between the two stages. This work introduces FaceLiveNet, a holistic end-to-end deep networks which can perform face verification and liveness detection simultaneously. An interactive scheme for facial expression recognition is proposed to perform liveness detection, providing better generalization capacity and higher security level. The proposed framework is low-cost as it relies on commodity hardware instead of costly sensors, and lightweight with much fewer parameters comparing to the other popular deep networks such as VGG16 and FaceNet. Experimental results on the benchmarks LFW, YTF, CK+, OuluCASIA, SFEW, FER2013 demonstrate that the proposed FaceLiveNet can achieve state-of-art performance or better for both face verification and facial expression recognition. We also introduce a new protocol to evaluate the global performance for face authentication with the fusion of face verification and interactive facial expression-based liveness detection.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545274,
1584003eda0efca3aef4ce37089dc6ada1062fe4,1,1,Driver Face Verification with Depth Maps,"Face verification is the task of checking if two provided images contain the face of the same person or not. In this work, we propose a fully-convolutional Siamese architecture to tackle this task, achieving state-of-the-art results on three publicly-released datasets, namely Pandora, High-Resolution Range-based Face Database (HRRFaceD), and CurtinFaces. The proposed method takes depth maps as the input, since depth cameras have been proven to be more reliable in different illumination conditions. Thus, the system is able to work even in the case of the total or partial absence of external light sources, which is a key feature for automotive applications. From the algorithmic point of view, we propose a fully-convolutional architecture with a limited number of parameters, capable of dealing with the small amount of depth data available for training and able to run in real time even on a CPU and embedded boards. The experimental results show acceptable accuracy to allow exploitation in real-world applications with in-board cameras. Finally, exploiting the presence of faces occluded by various head garments and extreme head poses available in the Pandora dataset, we successfully test the proposed system also during strong visual occlusions. The excellent results obtained confirm the efficacy of the proposed method.",2019,Sensors,,10.3390/s19153361,https://pdfs.semanticscholar.org/c9b7/8803a9f44bf87a63f62d3d4977601d0078b1.pdf
1586725207bab1345a50ea3d8ee8464248ba80ea,0,1,A Metric Learning Approach to Misogyny Categorization,"The task of automatic misogyny identification and categorization has not received as much attention as other natural language tasks have, even though it is crucial for identifying hate speech in social Internet interactions. In this work, we address this sentence classification task from a representation learning perspective, using both a bidirectional LSTM and BERT optimized with the following metric learning loss functions: contrastive loss, triplet loss, center loss, congenerous cosine loss and additive angular margin loss. We set new state-of-the-art for the task with our fine-tuned BERT, whose sentence embeddings can be compared with a simple cosine distance, and we release all our code as open source for easy reproducibility. Moreover, we find that almost every loss function performs equally well in this setting, matching the regular cross entropy loss.",2020,RepL4NLP@ACL,,10.18653/v1/2020.repl4nlp-1.12,https://pdfs.semanticscholar.org/37cc/5b7729679cfabc66ce0d8fa5b40515a16720.pdf
15be981b66c2e15ca50f917c0613122e575000fe,0,1,Vulnerability Analysis of Face Morphing Attacks from Landmarks and Generative Adversarial Networks,"Morphing attacks is a threat to biometric systems where the biometric reference in an identity document can be altered. This form of attack presents an important issue in applications relying on identity documents such as border security or access control. Research in face morphing attack detection is developing rapidly, however very few datasets with several forms of attacks are publicly available. This paper bridges this gap by providing a new dataset with four different types of morphing attacks, based on OpenCV, FaceMorpher, WebMorph and a generative adversarial network (StyleGAN), generated with original face images from three public face datasets. We also conduct extensive experiments to assess the vulnerability of the state-of-the-art face recognition systems, notably FaceNet, VGG-Face, and ArcFace. The experiments demonstrate that VGG-Face, while being less accurate face recognition system compared to FaceNet, is also less vulnerable to morphing attacks. Also, we observed that naı̈ve morphs generated with a StyleGAN do not pose a significant threat.",2020,,2012.05344,,https://arxiv.org/pdf/2012.05344.pdf
15ccf2ffd6b2149f50919ac6b570e31a9319ff4e,1,1,A Performance Evaluation of Loss Functions for Deep Face Recognition,"Face recognition is one of the most widely publicized feature in the devices today and hence represents an important problem that should be studied with the utmost priority. As per the recent trends, the Convolutional Neural Network (CNN) based approaches are highly successful in many tasks of Computer Vision including face recognition. The loss function is used on the top of CNN to judge the goodness of any network. In this paper, we present a performance comparison of different loss functions such as Cross-Entropy, Angular Softmax, Additive-Margin Softmax, ArcFace and Marginal Loss for face recognition. The experiments are conducted with two CNN architectures namely, ResNet and MobileNet. Two widely used face datasets namely, CASIA-Webface and MS-Celeb1M are used for the training and benchmark Labeled Faces in the Wild (LFW) face dataset is used for the testing.",2019,,,,https://pdfs.semanticscholar.org/15cc/f2ffd6b2149f50919ac6b570e31a9319ff4e.pdf
1612f6cf5d944cf02cfc451bf83c1220181a7100,0,1,Attentional Feature-Pair Relation Networks for Accurate Face Recognition,"Human face recognition is one of the most important research areas in biometrics. However, the robust face recognition under a drastic change of the facial pose, expression, and illumination is a big challenging problem for its practical application. Such variations make face recognition more difficult. In this paper, we propose a novel face recognition method, called Attentional Feature-pair Relation Network (AFRN), which represents the face by the relevant pairs of local appearance block features with their attention scores. The AFRN represents the face by all possible pairs of the 9x9 local appearance block features, the importance of each pair is considered by the attention map that is obtained from the low-rank bilinear pooling, and each pair is weighted by its corresponding attention score. To increase the accuracy, we select top-K pairs of local appearance block features as relevant facial information and drop the remaining irrelevant. The weighted top-K pairs are propagated to extract the joint feature-pair relation by using bilinear attention network. In experiments, we show the effectiveness of the proposed AFRN and achieve the outstanding performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C datasets.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.06255,10.1109/ICCV.2019.00557,https://arxiv.org/pdf/1908.06255.pdf
16c10d0242739d0edfe076bcac58b507d46da79a,1,1,DBLFace: Domain-Based Labels for NIR-VIS Heterogeneous Face Recognition,"Deep learning-based domain-invariant feature learning methods are advancing in near-infrared and visible (NIR-VIS) heterogeneous face recognition. However, these methods are prone to overfitting due to the large intra-class variation and the lack of NIR images for training. In this paper, we introduce Domain-Based Label Face (DBLFace), a learning approach based on the assumption that a subject is not represented by a single label but by a set of labels. Each label represents images of a specific domain. In particular, a set of two labels per subject, one for the NIR images and one for the VIS images, are used for training a NIR-VIS face recognition model. The classification of images into different domains reduces the intra-class variation and lessens the negative impact of data imbalance in training. To train a network with sets of labels, we introduce a domain-based angular margin loss and a maximum angular loss to maintain the inter-class discrepancy and to enforce the close relationship of labels in a set. Quantitative experiments confirm that DBLFace significantly improves the rank-1 identification rate by 6.7% on the EDGE20 dataset and achieves state-of-the-art performance on the CASIA NIR-VIS 2.0 dataset.",2020,ArXiv,2010.03771,,https://arxiv.org/pdf/2010.03771.pdf
173657da03e3249f4e47457d360ab83b3cefbe63,1,1,HKU-Face : A Large Scale Dataset for Deep Face Recognition Final Report,"Current face recognition usually faces problems with the training dataset due to the insufficient size and potential manual labelling errors. The project introduces a dataset construction and filtering process to deal the problem with less cost. FaceNet[35] and Sphereface[29] are harnessed for the purpose of filtering the dataset scratched from Google. Results show the impressive effectiveness of automatic filtering and purity enhancement after filtering with considerable attention on labeling errors in the view of web search. Except exclusively self-constructed dataset, filtered and merged dataset from CASIA-WebFace[54] and VGG Face [32] were also tested and analyzed. Subsequent research and experiment can target at the further improvement of filtering process with lower false negative rate as well as getting rid of labeling errors due to web search. And those further improvements are expected to contribute more to the unsupervised learning in the general fine-grained object recognition.",2018,,,,https://pdfs.semanticscholar.org/1736/57da03e3249f4e47457d360ab83b3cefbe63.pdf
173a413d286fd204c245497b135ecf2311a9c9f1,1,1,A New Deep Neural Architecture Search Pipeline for Face Recognition,"With the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature with simple detection methods and good recognition accuracy. Face recognition is one of the main functions of electronic equipment propaganda. The previous work in this field mainly focused on converting loss function in traditional deep convolution neural networks without changing the network structure. With the development of AutoML, neural architecture search (NAS) has shown remarkable performance in image classification tasks. In this paper, we first propose a new deep neural architecture search pipeline combined with NAS technology and reinforcement learning strategy into face recognition. We quote the framework of NAS, which trains the child and controller networks alternately. At the same time, we optimize NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize the policy gradient algorithm to search the architecture automatically with the cross-entropy loss. The network architectures we searched out have achieved state-of-the-art accuracy in the large-scale face dataset, which achieved 98.77% top-1 in the MS-Celeb-1M dataset and 99.89% in LFW dataset with relatively small network size.",2020,IEEE Access,,10.1109/ACCESS.2020.2994207,
17a51f44b43098f629f80f9481fb1875ccc1604e,1,0,Addressing Model Vulnerability to Distributional Shifts Over Image Transformation Sets,"We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1903.119,10.1109/ICCV.2019.00807,https://arxiv.org/pdf/1903.11900.pdf
17c696b977c045859398efb62af52ac492e573ed,1,0,An Improved Face Synthesis Model for Two-Pathway Generative Adversarial Network,"Synthesizing photorealistic frontal face images from multiple-view profile face images has a wide range of applications in the field of face recognition. However, existing models still have some disadvantages such as high cost and high computational complexity. At present, the Two-Pathway Generative Adversarial Network (TP-GAN) is the state-of-the-art face synthesis model, which can perceive the global structure and local details at the same time. It solves the prier problems but has disadvantages such as training difficulty and lack of diversity of generated samples. Based on Wasserstein GAN with Gradient Penalty (WGAN-GP), this paper proposes a novel Two-Pathway Wasserstein GAN with Gradient Penalty (TPWGAN-GP) model to tackle these defects. TPWGAN-GP uses a gradient penalty method to satisfy the Lipschitz continuity condition, which solves the problems of difficulty in hyper-parameter adjustment and gradient explosion in the TP-GAN, making the convergence speed faster and the model more stable in training process. The generated samples are of higher quality, resulting in more photorealistic faces for recognition tasks.",2019,ICMLC '19,,10.1145/3318299.3318346,
1816f98e2a4dd54690c2689cf529699d8843e847,1,0,Talking Face Generation by Adversarially Disentangled Audio-Visual Representation,"Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.",2019,AAAI,1807.0786,10.1609/aaai.v33i01.33019299,https://arxiv.org/pdf/1807.07860.pdf
1929863fff917ee7f6dc428fc1ce732777668eca,1,1,UV-GAN: Adversarial Facial UV Map Completion for Pose-Invariant Face Recognition,"Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, 94.05%, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1712.04695,10.1109/CVPR.2018.00741,https://arxiv.org/pdf/1712.04695.pdf
195a124a5e21bf72fb096005aff72a55daee0f7a,0,1,Suppressing Spoof-irrelevant Factors for Domain-agnostic Face Anti-spoofing,"Face anti-spoofing aims to prevent false authentications of face recognition systems by distinguishing whether an image is originated from a human face or a spoof medium. We propose a novel method called Doubly Adversarial Suppression Network (DASN) for domain-agnostic face antispoofing; DASN improves the generalization ability to unseen domains by learning to effectively suppress spoofirrelevant factors (SiFs) (e.g., camera sensors, illuminations). To achieve our goal, we introduce two types of adversarial learning schemes. In the first adversarial learning scheme, multiple SiFs are suppressed by deploying multiple discrimination heads that are trained against an encoder. In the second adversarial learning scheme, each of the discrimination heads is also adversarially trained to suppress a spoof factor, and the group of the secondary spoof classifier and the encoder aims to intensify the spoof factor by overcoming the suppression. We evaluate the proposed method on four public benchmark datasets, and achieve remarkable evaluation results. The results demonstrate the effectiveness of the proposed method.",2020,ArXiv,2012.01271,,https://arxiv.org/pdf/2012.01271.pdf
19a3236ed23de4830b55d605b25ddffc347fb6a7,0,1,Spatio-temporal convolutional features with nested LSTM for facial expression recognition,"Abstract In this paper, we propose a novel end-to-end architecture termed Spatio-Temporal Convolutional features with Nested LSTM (STC-NLSTM), which learns the muti-level appearance features and temporal dynamics of facial expressions in a joint fashion. More precisely, 3DCNN is used to extract spatio-temporal convolutional features from the image sequences that represent facial expressions, and the dynamics of expressions are modeled by Nested LSTM, which is actually coupled by two sub-LSTMs, saying T-LSTM and C-LSTM. Namely, T-LSTM is used to model the temporal dynamics of the spatio-temporal features in each convolutional layer, and C-LSTM is adopted to integrate the outputs of all T-LSTMs together so as to encode the multi-level features encoded in the intermediate layers of the network. We conduct experiments on four benchmark databases, CK+, Oulu-CASIA, MMI and BP4D, and the results show that the proposed method achieves a performance superior to the state-of-the-art methods.",2018,Neurocomputing,,10.1016/j.neucom.2018.07.028,
19d53bb35baf6ab02368756412800c218a2df71c,1,0,DeepDeblur: Fast one-step blurry face images restoration,"We propose a very fast and effective one-step restoring method for blurry face images. In the last decades, many blind deblurring algorithms have been proposed to restore latent sharp images. However, these algorithms run slowly because of involving two steps: kernel estimation and following non-blind deconvolution or latent image estimation. Also they cannot handle face images in small size. Our proposed method restores sharp face images directly in one step using Convolutional Neural Network. Unlike previous deep learning involved methods that can only handle a single blur kernel at one time, our network is trained on totally random and numerous training sample pairs to deal with the variances due to different blur kernels in practice. A smoothness regularization as well as a facial regularization are added to keep facial identity information which is the key to face image applications. Comprehensive experiments demonstrate that our proposed method can handle various blur kenels and achieve state-of-the-art results for small size blurry face images restoration. Moreover, the proposed method shows significant improvement in face recognition accuracy along with increasing running speed by more than 100 times.",2017,ArXiv,1711.09515,,https://arxiv.org/pdf/1711.09515.pdf
19fdaeff329c54bcffaf2e858d5a2584c229cb64,1,1,Voice-Face Cross-modal Matching and Retrieval: A Benchmark,"Cross-modal associations between voice and face from a person can be learnt algorithmically, which can benefit a lot of applications. The problem can be defined as voice-face matching and retrieval tasks. Much research attention has been paid on these tasks recently. However, this research is still in the early stage. Test schemes based on random tuple mining tend to have low test confidence. Generalization ability of models can not be evaluated by small scale datasets. Performance metrics on various tasks are scarce. A benchmark for this problem needs to be established. In this paper, first, a framework based on comprehensive studies is proposed for voice-face matching and retrieval. It achieves state-of-the-art performance with various performance metrics on different tasks and with high test confidence on large scale datasets, which can be taken as a baseline for the follow-up research. In this framework, a voice anchored L2-Norm constrained metric space is proposed, and cross-modal embeddings are learned with CNN-based networks and triplet loss in the metric space. The embedding learning process can be more effective and efficient with this strategy. Different network structures of the framework and the cross language transfer abilities of the model are also analyzed. Second, a voice-face dataset (with 1.15M face data and 0.29M audio data) from Chinese speakers is constructed, and a convenient and quality controllable dataset collection tool is developed. The dataset and source code of the paper will be published together with this paper.",2019,ArXiv,1911.09338,,https://arxiv.org/pdf/1911.09338.pdf
1a106eb244f8ba13e9d698d69f504c3c1080ad73,0,1,Gaussian Vector: An Efficient Solution for Facial Landmark Detection,"Significant progress has been made in facial landmark detection with the development of Convolutional Neural Networks. The widely-used algorithms can be classified into coordinate regression methods and heatmap based methods. However, the former loses spatial information, resulting in poor performance while the latter suffers from large output size or high post-processing complexity. This paper proposes a new solution, Gaussian Vector, to preserve the spatial information as well as reduce the output size and simplify the post-processing. Our method provides novel vector supervision and introduces Band Pooling Module to convert heatmap into a pair of vectors for each landmark. This is a plug-and-play component which is simple and effective. Moreover, Beyond Box Strategy is proposed to handle the landmarks out of the face bounding box. We evaluate our method on 300W, COFW, WFLW and JD-landmark. That the results significantly surpass previous works demonstrates the effectiveness of our approach.",2020,ArXiv,2010.01318,,https://arxiv.org/pdf/2010.01318.pdf
1a7e4b9d7d830d2beaea03563d385eb37cb52cdf,1,0,Automated Cleaning of Identity Label Noise in A Large-scale Face Dataset Using A Face Image Quality Control,Automated Cleaning of Identity Label Noise in A Large-scale Face Dataset Using A Face Image Quality Control,2018,,,10.33915/etd.3700,https://pdfs.semanticscholar.org/8d44/4a91258a6e37a6d6f5417000b61ebd4f6ba8.pdf
1a86eb42952412ee02e3f6da06f874f1946eff6b,0,1,Deep Cross-Modal Projection Learning for Image-Text Matching,"The key point of image-text matching is how to accurately measure the similarity between visual and textual inputs. Despite the great progress of associating the deep cross-modal embeddings with the bi-directional ranking loss, developing the strategies for mining useful triplets and selecting appropriate margins remains a challenge in real applications. In this paper, we propose a cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss for learning discriminative image-text embeddings. The CMPM loss minimizes the KL divergence between the projection compatibility distributions and the normalized matching distributions defined with all the positive and negative samples in a mini-batch. The CMPC loss attempts to categorize the vector projection of representations from one modality onto another with the improved norm-softmax loss, for further enhancing the feature compactness of each class. Extensive analysis and experiments on multiple datasets demonstrate the superiority of the proposed approach.",2018,ECCV,,10.1007/978-3-030-01246-5_42,http://openaccess.thecvf.com/content_ECCV_2018/papers/Ying_Zhang_Deep_Cross-Modal_Projection_ECCV_2018_paper.pdf
1a95e9ef22298944fc3add6bbd83d7c572eb15dc,0,1,σ2R Loss: a Weighted Loss by Multiplicative Factors using Sigmoidal Functions,"In neural networks, the loss function represents the core of the learning process that leads the optimizer to an approximation of the optimal convergence error. Convolutional neural networks (CNN) use the loss function as a supervisory signal to train a deep model and contribute significantly to achieving the state of the art in some fields of artificial vision. Cross-entropy and Center loss functions are commonly used to increase the discriminating power of learned functions and increase the generalization performance of the model. Center loss minimizes the class intra-class variance and at the same time penalizes the long distance between the deep features inside each class. However, the total error of the center loss will be heavily influenced by the majority of the instances and can lead to a freezing state in terms of intra-class variance. To address this, we introduce a new loss function called sigma squared reduction loss ($\sigma^2$R loss), which is regulated by a sigmoid function to inflate/deflate the error per instance and then continue to reduce the intra-class variance. Our loss has clear intuition and geometric interpretation, furthermore, we demonstrate by experiments the effectiveness of our proposal on several benchmark datasets showing the intra-class variance reduction and overcoming the results obtained with center loss and soft nearest neighbour functions.",2020,ArXiv,,,
1af88a4a702f57d699c3ae6e825fd702b7787c91,1,0,A Deeper Look at Facial Expression Dataset Bias,"Datasets play an important role in the progress of facial expression recognition algorithms, but they may suffer from obvious biases caused by different cultures and collection conditions. To look deeper into this bias, we first conduct comprehensive experiments on dataset recognition and crossdataset generalization tasks, and for the first time explore the intrinsic causes of the dataset discrepancy. The results quantitatively verify that current datasets have a strong buildin bias and corresponding analyses indicate that the conditional probability distributions between source and target datasets are different. However, previous researches are mainly based on shallow features with limited discriminative ability under the assumption that the conditional distribution remains unchanged across domains. To address these issues, we further propose a novel deep Emotion-Conditional Adaption Network (ECAN) to learn domain-invariant and discriminative feature representations, which can match both the marginal and the conditional distributions across domains simultaneously. In addition, the largely ignored expression class distribution bias is also addressed by a learnable re-weighting parameter, so that the training and testing domains can share similar class distribution. Extensive cross-database experiments on both lab-controlled datasets (CK+, JAFFE, MMI and Oulu-CASIA) and real-world databases (AffectNet, FER2013, RAF-DB 2.0 and SFEW 2.0) demonstrate that our ECAN can yield competitive performances across various facial expression transfer tasks and outperform the state-of-theart methods.",2019,ArXiv,1904.1115,10.1109/TAFFC.2020.2973158,https://arxiv.org/pdf/1904.11150.pdf
1b0a3b78bdefb9b317402dfc7586e0ce0011ec74,1,1,Non-Visual to Visual Translation for Cross-Domain Face Recognition,"Reducing the cross-modality gap between two different domains is a challenging problem for heterogeneous face recognition (HFR). The current visual domain face recognition system is not easy to solve the discrepancy of cross-modality when two comparing domains are heterogeneous. Moreover, the amount of HFR dataset is significantly insufficient, making it considerable difficulty in training. This paper proposes a novel two-step framework that consists of the image translation module and the feature learning module to obtain an enhanced cross-modality matching system for heterogeneous datasets. First, the image translation module consists of a Preprocessing Chain (PC) method, CycleGAN, and the Siamese network. It enables to meet the conditions for preserving contents along with changing the styles from the source domain to the target domain. Second, in the feature learning module, the training dataset and its translated images are used together for fine-tuning the pre-trained backbone model in the visual domain. This allows for discriminative and robust feature matching of the probe and gallery test datasets in the visual domain. The experimental results are evaluated with two scenarios, using the CUHK Face Sketch FERET (CUFSF) dataset and the CASIA NIR-VIS 2.0 dataset. The proposed method achieves a better recognition performance in comparison to the state-of-the-art methods.",2020,IEEE Access,,10.1109/ACCESS.2020.2980047,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09032120.pdf
1b305dbfb789a19013d7ab8fa4f26ab33d99f6ed,0,1,x-Vectors Meet Adversarial Attacks: Benchmarking Adversarial Robustness in Speaker Verification,"Automatic Speaker Verification (ASV) enables high-security applications like user authentication or criminal investigation. However, ASV can be subjected to malicious attacks, which could compromise that security. The ASV literature mainly studies spoofing (a.k.a impersonation) attacks such as voice replay, synthesis or conversion. Meanwhile, other kinds of attacks, known as adversarial attacks, have become a threat to all kind of machine learning systems. Adversarial attacks introduce an imperceptible perturbation in the input signal that radically changes the behavior of the system. These attacks have been intensively studied in the image domain but less in the speech domain. In this work, we investigate the vulnerability of state-ofthe-art ASV systems to adversarial attacks. We consider a threat model consisting in adding a perturbation noise to the test waveform to alter the ASV decision. We also discuss the methodology and metrics to benchmark adversarial attacks and defenses in ASV. We evaluated three x-vector architectures, which performed among the best in recent ASV evaluations, against fast gradient sign and Carlini-Wagner attacks. All networks were highly vulnerable in the white-box attack scenario, even for high SNR (30-60 dB). Furthermore, we successfully transferred attacks generated with smaller white-box networks to attack a larger black-box network.",2020,INTERSPEECH,,10.21437/interspeech.2020-2458,https://isca-speech.org/archive/Interspeech_2020/pdfs/2458.pdf
1b3e7caf4b456e3762a827aa623c3fb88ca0b1a0,0,1,Contrapositive Margin Softmax Loss for Face Verification,"The performance of face recognition has been boosted by the features extracted from deep convolutional neural networks. Ideal features should have minimum intra-class variations and maximum inter-class variations. The most commonly used loss function for classification, softmax loss, however, does not necessarily learn features discriminative enough. Large margin classifiers have nice generalization properties in statistical machine learning. These properties have lead to the application of margin to deep learning in recent years. We hereby propose a new loss function called Contrapositive Margin Softmax loss for face verification task, which helps to learn invariant and discriminative features by introducing margins to both target logits and maximum negative logits of softmax loss. Competitive results on LFW (99.28%) and YTF (95.34%) demonstrate the effectiveness of our approach.",2018,ICRCA '18,,10.1145/3265639.3265679,
1bab0f5df6fc9506e922dcf6e2dee3cc8ad8a427,1,1,Automatic Quality Assessment for Audio-Visual Verification Systems. The LOVe submission to NIST SRE Challenge 2019,"Fusion of scores is a cornerstone of multimodal biometric systems composed of independent unimodal parts. In this work, we focus on quality-dependent fusion for speaker-face verification. To this end, we propose a universal model which can be trained for automatic quality assessment of both face and speaker modalities. This model estimates the quality of representations produced by unimodal systems which are then used to enhance the score-level fusion of speaker and face verification modules. We demonstrate the improvements brought by this quality-dependent fusion on the recent NIST SRE19 Audio-Visual Challenge dataset.",2020,INTERSPEECH,2008.05889,10.21437/interspeech.2020-1434,https://arxiv.org/pdf/2008.05889.pdf
1bd2fba7083829042d0ba0765dbed8ec692cb335,1,1,Towards Gender-Neutral Face Descriptors for Mitigating Bias in Face Recognition,"State-of-the-art deep networks implicitly encode gender information while being trained for face recognition. Gender is often viewed as an important attribute with respect to identifying faces. However, the implicit encoding of gender information in face descriptors has two major issues: (a.) It makes the descriptors susceptible to privacy leakage, i.e. a malicious agent can be trained to predict the face gender from such descriptors. (b.) It appears to contribute to gender bias in face recognition, i.e. we find a significant difference in the recognition accuracy of DCNNs on male and female faces. Therefore, we present a novel `Adversarial Gender De-biasing algorithm (AGENDA)' to reduce the gender information present in face descriptors obtained from previously trained face recognition networks. We show that AGENDA significantly reduces gender predictability of face descriptors. Consequently, we are also able to reduce gender bias in face verification while maintaining reasonable recognition performance.",2020,,2006.07845,,https://arxiv.org/pdf/2006.07845.pdf
1bf011a0a5c471d98c26134a2e67d573b8601c8f,0,1,Transfer of Pretrained Model Weights Substantially Improves Semi-supervised Image Classification,"Deep neural networks produce state-of-the-art results when trained on a large number of labeled examples but tend to overfit when small amounts of labeled examples are used for training. Creating a large number of labeled examples requires considerable resources, time, and effort. If labeling new data is not feasible, so-called semi-supervised learning can achieve better generalisation than purely supervised learning by employing unlabeled instances as well as labeled ones. The work presented in this paper is motivated by the observation that transfer learning provides the opportunity to potentially further improve performance by exploiting models pretrained on a similar domain. More specifically, we explore the use of transfer learning when performing semi-supervised learning using self-learning. The main contribution is an empirical evaluation of transfer learning using different combinations of similarity metric learning methods and label propagation algorithms in semi-supervised learning. We find that transfer learning always substantially improves the model’s accuracy when few labeled examples are available, regardless of the type of loss used for training the neural network. This finding is obtained by performing extensive experiments on the SVHN, CIFAR10, and Plant Village image classification datasets and applying pretrained weights from Imagenet for transfer learning.",2020,Australasian Conference on Artificial Intelligence,,10.1007/978-3-030-64984-5_34,https://www.cs.waikato.ac.nz/~eibe/pubs/Transfer_Learning_camera_ready.pdf
1ca63ee80e1d8a94a0440c2d777f67bfde4c95cd,1,0,Compact Web Video Summarization Via Supervised Learning,"Ever growing consumption of online videos from search, recommendation, and sharing has generated a strong demand on compact summarization, to allow users to quickly understand the video content and make the whether-to-watch decision. This paper explores achieving this using a compact set of four thumbnails, via supervised learning methods. Due to the ubiquitous disagreements among the thumbnail sets preferred by different labelers, there exists no unique ground truth set. To address this problem, we propose the pair wise ranking method, which trains the model to best predict the user preference over each pair of thumbnail set candidates. Experimental results on a large video dataset showed that the proposed method outperforms the existing schemes by a large margin.",2018,2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),,10.1109/ICMEW.2018.8551530,
1cc8e9675dc9c39fc34def087699c3359eff495c,0,1,Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,"Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1905.08233,10.1109/ICCV.2019.00955,https://arxiv.org/pdf/1905.08233.pdf
1d6a6d0132a4971135fa1a4bff2221ecc803da25,0,1,Deep Index-of-Maximum Hashing for Face Template Protection,"Face authentication is one of the most common biometrics available nowadays. Ensuring security of facial templates is vital to circumvent impersonation and privacy invasion. One of the notable remedies for facial template protection is cancelable biometrics whereby the compromised template can be revoked and replaced. In this work, we propose a cancelable facial template technique based on the Index-of- Maximum (IoM) hashing by means of deep neural networks, termed as Deep IoM (DIoM) hashing. Unlike data-agnostic IoM hashing, the DIoM hashing is data-driven and trained by supervision to render a discriminative cancelable facial template. The DIoM hashing relies upon a permutable pretrained deep feature learning network and a hashing network responsible for optimizing the DIoM hash codes. The hashing network consolidates maxout and softmax function, namely softmaxout to approximate the discrete DIoM hash code. A dedicated loss function is designed in order to achieve similaritypreserving learning, code balancing and quantization. The proposed network is assessed on unconstraint Labeled Faces in the Wild dataset and shown outperformed vanilla IoM hashing significantly.",2020,2020 5th International Conference on Computer and Communication Systems (ICCCS),,10.1109/ICCCS49078.2020.9118594,
1d6cfd6faff67b034cac825863a8071c3ef8a9ae,0,1,Face Recognition in Unconstrained Conditions: A Systematic Review,"Face recognition is a biometric which is attracting significant research, commercial and government interest, as it provides a discreet, non-intrusive way of detecting, and recognizing individuals, without need for the subject's knowledge or consent. This is due to reduced cost, and evolution in hardware and algorithms which have improved their ability to handle unconstrained conditions. Evidently affordable and efficient applications are required. However, there is much debate over which methods are most appropriate, particularly in the context of the growing importance of deep neural network-based face recognition systems. This systematic review attempts to provide clarity on both issues by organizing the plethora of research and data in this field to clarify current research trends, state-of-the-art methods, and provides an outline of their benefits and shortcomings. Overall, this research covered 1,330 relevant studies, showing an increase of over 200% in research interest in the field of face recognition over the past 6 years. Our results also demonstrated that deep learning methods are the prime focus of modern research due to improvements in hardware databases and increasing understanding of neural networks. In contrast, traditional methods have lost favor amongst researchers due to their inherent limitations in accuracy, and lack of efficiency when handling large amounts of data.",2019,ArXiv,1908.04404,,https://arxiv.org/pdf/1908.04404.pdf
1da8178bfca7c76cae53ec34364d86c7d5713fdd,1,1,Pairwise Relational Networks using Local Appearance Features for Face Recognition,"We propose a new face recognition method, called a pairwise relational network (PRN), which takes local appearance features around landmark points on the feature map, and captures unique pairwise relations with the same identity and discriminative pairwise relations between different identities. The PRN aims to determine facial part-relational structure from local appearance feature pairs. Because meaningful pairwise relations should be identity dependent, we add a face identity state feature, which obtains from the long short-term memory (LSTM) units network with the sequential local appearance features. To further improve accuracy, we combined the global appearance features with the pairwise relational feature. Experimental results on the LFW show that the PRN achieved 99.76% accuracy. On the YTF, PRN achieved the state-of-the-art accuracy (96.3%). The PRN also achieved comparable results to the state-of-the-art for both face verification and face identification tasks on the IJB-A and IJB-B. This work is already published on ECCV 2018.",2018,ArXiv,1811.06405,,https://arxiv.org/pdf/1811.06405.pdf
1db487adf05ac9ace04b2e551e09b934d61b05c8,0,1,CN-Celeb: A Challenging Chinese Speaker Recognition Dataset,"Recently, researchers set an ambitious goal of conducting speaker recognition in unconstrained conditions where the variations on ambient, channel and emotion could be arbitrary. However, most publicly available datasets are collected under constrained environments, i.e., with little noise and limited channel variation. These datasets tend to deliver over-optimistic performance and do not meet the request of research on speaker recognition in unconstrained conditions.In this paper, we present CN-Celeb, a large-scale speaker recognition dataset collected ‘in the wild’. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world. Experiments conducted with two state-of-the-art speaker recognition approaches (i-vector and x-vector) show that the performance on CN-Celeb is far inferior to the one obtained on Vox-Celeb, a widely used speaker recognition dataset. This result demonstrates that in real-life conditions, the performance of existing techniques might be much worse than it was thought. Our database is free for researchers and can be downloaded from http://project.cslt.org.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1911.01799,10.1109/ICASSP40776.2020.9054017,https://arxiv.org/pdf/1911.01799.pdf
1dcaa4803f58f230a1feac6ae20c00bd9d7c65c4,0,1,Quadruplet-Center Loss for Face Verification,"Deep learning for face verification applications has proven to be productive. Most existing face verification methods focus on enhance the discriminative power of the deeply learned features with softmax loss or learning discriminative features with deep metric learning, which have their own advantages, but the combination of these two directions is more or less ignored. In this paper, a novel loss named quadruplet-center loss is proposed to learn more discriminative features for face verification task. The proposed quadruplet-center loss learns a center for deep features of each class, which forces the distances between the samples and centers from different classes are larger than those from same class and regardless of whether they contain different probe images or not. It is worth mentioning that a dynamic margin is presented based on the average distance between samples and corresponding centers for loss functions in a batch. Our method is evaluated on two widely-used benchmarks for face verification, which outperforms most of the state-of-the-art algorithms. The experimental results clearly demonstrate the effectiveness of our proposed classification loss.",2019,2019 Chinese Automation Congress (CAC),,10.1109/CAC48633.2019.8997490,
1de68f12db136526116c6ac7064fd13965f2c966,0,1,"A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects","Convolutional Neural Network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention both of industry and academia in the past few years. The existing reviews mainly focus on the applications of CNN in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide novel ideas and prospects in this fast-growing field as much as possible. Besides, not only two-dimensional convolution but also one-dimensional and multi-dimensional ones are involved. First, this review starts with a brief introduction to the history of CNN. Second, we provide an overview of CNN. Third, classic and advanced CNN models are introduced, especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for function selection. Fifth, the applications of one-dimensional, two-dimensional, and multi-dimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed to serve as guidelines for future work.",2020,ArXiv,2004.02806,,https://arxiv.org/pdf/2004.02806.pdf
1e301588035547c6e24e74142ec6b2efcc6bd368,1,1,More Information Supervised Probabilistic Deep Face Embedding Learning,"Researches using margin based comparison loss demonstrate the effectiveness of penalizing the distance between face feature and their corresponding class centers. Despite their popularity and excellent performance, they do not explicitly encourage the generic embedding learning for an open set recognition problem. In this paper, we analyse margin based softmax loss in probability view. With this perspective, we propose two general principles: 1) monotonic decreasing and 2) margin probability penalty, for designing new margin loss functions. Unlike methods optimized with single comparison metric, we provide a new perspective to treat open set face recognition as a problem of information transmission. And the generalization capability for face embedding is gained with more clean information. An auto-encoder architecture called Linear-Auto-TS-Encoder(LATSE) is proposed to corroborate this finding. Extensive experiments on several benchmarks demonstrate that LATSE help face embedding to gain more generalization capability and it boosted the single model performance with open training dataset to more than $99\%$ on MegaFace test.",2020,ICML 2020,2006.04518,,https://arxiv.org/pdf/2006.04518.pdf
1e31eb6ffe0d163e0306a7a6f015364f103e461b,0,1,Selective Pseudo-Labeling with Reinforcement Learning for Semi-Supervised Domain Adaptation,"Recent domain adaptation methods have demonstrated impressive improvement on unsupervised domain adaptation problems. However, in the semi-supervised domain adaptation (SSDA) setting where the target domain has a few labeled instances available, these methods can fail to improve performance. Inspired by the effectiveness of pseudo-labels in domain adaptation, we propose a reinforcement learning based selective pseudo-labeling method for semi-supervised domain adaptation. It is difficult for conventional pseudolabeling methods to balance the correctness and representativeness of pseudo-labeled data. To address this limitation, we develop a deep Q-learning model to select both accurate and representative pseudo-labeled instances. Moreover, motivated by large margin loss’s capacity on learning discriminative features with little data, we further propose a novel target margin loss for our base model training to improve its discriminability. Our proposed method is evaluated on several benchmark datasets for SSDA, and demonstrates superior performance to all the comparison methods.",2020,ArXiv,2012.03438,,https://arxiv.org/pdf/2012.03438.pdf
1e34b7505fef52b2881f4de6e78cfb90d51085a0,1,1,"Trunk-Branch Ensemble Convolutional Neural Networks for Large Scale, Few-Shot Video-to-Still Face Recognition","For the real-world face recognition, factors such as occlusion and pose-variant (cross face) would affect the identification/verification performance. In addition, large number of classes also increase the complexity, which makes verification/identification even harder. In order to deal with these issues, how to extract discriminative embeddings is a challenging task for the researchers. This research aims at video-to-still (V2S) face identification, which means given few images per person (p.p.) as our database (also called gallery), we tend to identify if a person in a video is someone in our database or not. We use end-to-end Trunk-Branch Ensemble Convolutional Neural Networks (TBE-CNN) combined with state-of-the-art InceptionNet to create informative image patches and boost the features for occlusion and pose-variant issues. The images for learning and identifying comprise occlusions, different face directions and also resolution issues. Moreover, we create a large scale, few shot video-to-still (still-to-still) face recognition dataset with different settings to evaluate the the models and find the preferred settings for real-world face recognition application. How to ensure the accuracy under these practical noises and settings is the goal of this research.",2019,,,,https://pdfs.semanticscholar.org/1e34/b7505fef52b2881f4de6e78cfb90d51085a0.pdf
1e987075566eb86f8fabab2664fd12ca0fef78aa,1,0,Comparison of Face Recognition Loss Functions,"Significant progresses have been made to face recognition algorithms in recent years. The progresses include the improvements of the solutions and the availability of more challenging databases. As the performance on previous benchmark databases, such as MPIE and LFW, saturates, more challenging databases are emerging and keep driving the advancements of face recognition solutions. The loss function considered in the solution usually plays the most critical role. We compare a few latest loss functions with the same feature embedding network by evaluating their performance on two recently-released databases, the IARPA Janus BenchmarkB (IJB-B) [1], and IARPA Janus BenchmarkC (IJB-C) [2], and highlight the directions for the next phase research.",2019,2019 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS),,10.1109/ISPACS48206.2019.8986257,
1ede4513462dcd8aad056d7fc420302f9f1dfc40,1,1,UID-GAN: Unsupervised Image Deblurring via Disentangled Representations,"Recent advances in deep convolutional neural networks (DCNNs) and generative adversarial networks (GANs) have significantly improved the performance of single image blind deblurring algorithms. However, most of the existing algorithms require paired training data. In this paper, we present an unsupervised method for single-image deblurring without paired training images. We introduce a disentangled framework to split the content and blur features of a blurred image, which yields improved deblurring performance. To handle the unpaired training data, a blurring branch and the cycle-consistency loss are added to guarantee that the content structures of the restored results match the original images. We also add a perceptual loss to further mitigate the artifacts. For natural image deblurring, we introduce a color loss to reduce color distortions in outputs. Extensive experiments on both domain-specific and natural image deblurring show the proposed method achieves competitive results compared to recent state-of-the-art deblurring approaches.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2019.2959133,
1ee0da73fb2c576686630b01fffa475e5ec2fbae,1,0,Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning,"To address the problem of data inconsistencies among different facial expression recognition (FER) datasets, many cross-domain FER methods (CD-FERs) have been extensively devised in recent years. Although each declares to achieve superior performance, fair comparisons are lacking due to the inconsistent choices of the source/target datasets and feature extractors. In this work, we first analyze the performance effect caused by these inconsistent choices, and then re-implement some well-performing CD-FER and recently published domain adaptation algorithms. We ensure that all these algorithms adopt the same source datasets and feature extractors for fair CD-FER evaluations. We find that most of the current leading algorithms use adversarial learning to learn holistic domain-invariant features to mitigate domain shifts. However, these algorithms ignore local features, which are more transferable across different datasets and carry more detailed content for fine-grained adaptation. To address these issues, we integrate graph representation propagation with adversarial learning for cross-domain holistic-local feature co-adaptation by developing a novel adversarial graph representation adaptation (AGRA) framework. Specifically, it first builds two graphs to correlate holistic and local regions within each domain and across different domains, respectively. Then, it extracts holistic-local features from the input image and uses learnable per-class statistical distributions to initialize the corresponding graph nodes. Finally, two stacked graph convolution networks (GCNs) are adopted to propagate holistic-local features within each domain to explore their interaction and across different domains for holistic-local feature co-adaptation. We conduct extensive and fair evaluations on several popular benchmarks and show that the proposed AGRA framework outperforms previous state-of-the-art methods.",2020,,2008.00923,,https://arxiv.org/pdf/2008.00923.pdf
1f58885f410b20e1e9dc850c8e967a960ccafee4,0,1,EasyQuant: Post-training Quantization via Scale Optimization,"The 8 bits quantization has been widely applied to accelerate network inference in various deep learning applications. There are two kinds of quantization methods, training-based quantization and post-training quantization. Training-based approach suffers from a cumbersome training process, while post-training quantization may lead to unacceptable accuracy drop. In this paper, we present an efficient and simple post-training method via scale optimization, named EasyQuant (EQ),that could obtain comparable accuracy with the training-based method.Specifically, we first alternately optimize scales of weights and activations for all layers target at convolutional outputs to further obtain the high quantization precision. Then, we lower down bit width to INT7 both for weights and activations, and adopt INT16 intermediate storage and integer Winograd convolution implementation to accelerate inference.Experimental results on various computer vision tasks show that EQ outperforms the TensorRT method and can achieve near INT8 accuracy in 7 bits width post-training.",2020,ArXiv,2006.16669,,https://arxiv.org/pdf/2006.16669.pdf
1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd,1,0,Large Scale Incremental Learning,"Modern machine learning suffers from \textit{catastrophic forgetting} when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to \textbf{scale up to a large number of classes}. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\% and 13.2\% respectively.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1905.1326,10.1109/CVPR.2019.00046,https://arxiv.org/pdf/1905.13260.pdf
1f6f7d6a099b0d2fdc80b3e90cc541b525722241,1,0,FaceFeat-GAN: a Two-Stage Approach for Identity-Preserving Face Synthesis,"The advance of Generative Adversarial Networks (GANs) enables realistic face image synthesis. However, synthesizing face images that preserve facial identity as well as have high diversity within each identity remains challenging. To address this problem, we present FaceFeat-GAN, a novel generative model that improves both image quality and diversity by using two stages. Unlike existing single-stage models that map random noise to image directly, our two-stage synthesis includes the first stage of diverse feature generation and the second stage of feature-to-image rendering. The competitions between generators and discriminators are carefully designed in both stages with different objective functions. Specially, in the first stage, they compete in the feature domain to synthesize various facial features rather than images. In the second stage, they compete in the image domain to render photo-realistic images that contain high diversity but preserve identity. Extensive experiments show that FaceFeat-GAN generates images that not only retain identity information but also have high diversity and quality, significantly outperforming previous methods.",2018,ArXiv,1812.01288,,https://arxiv.org/pdf/1812.01288.pdf
1f761ce039e0564d8d1451610388ebdcca7480e4,0,1,A Novel 2-D Current Signal-Based Residual Learning With Optimized Softmax to Identify Faults in Ball Screw Actuators,"Ball screw electro-mechanical actuators are commonly found in high precision motion control applications including aerospace systems as well as automated setups for industries. These actuators perform flight / application critical job and ball screw drives are responsible to provide precise linear motion while carrying thrust loading. A failure in ball screw drive may disturb positioning accuracy of overall system. At present, few techniques are available to monitor electro-mechanical actuators for aerospace and industrial systems. This paper provides a deep learning based intelligent technique to monitor condition of ball screw actuators. The proposed scheme utilizes modified residual learning scheme to extract features from two-dimensional transformed motor current signals. The current signal data was collected under different load domains in terms of magnitude and direction reversal. A 2D-Remanant-CNN (2D-Rem-CNN) model was developed for features extraction with proposed optimized softmax for classification of mechanical faults. The proposed technique was validated against different ball screw fault cases. The testing results prove the superiority of 2D-Rem-CNN model against different state of the art techniques. The proposed framework was also tested for system’s stability under different load domains.",2020,IEEE Access,,10.1109/ACCESS.2020.3004489,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09123348.pdf
1fc249ec69b3e23856b42a4e591c59ac60d77118,1,0,Evaluation of a 3D-aided pose invariant 2D face recognition system,"A few well-developed face recognition pipelines have been reported in recent years. Most of the face-related work focuses on a specific module or demonstrates a research idea. In this paper, we present a pose-invariant 3D-aided 2D face recognition system (3D2D-PIFR) that is robust to pose variations as large as 90° by leveraging deep learning technology. We describe the architecture and the interface of 3D2D-PIFR, and introduce each module in detail. Experiments are conducted on the UHDB31 and IJB-A, demonstrating that 3D2D-PIFR outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on UHDB31 and 3% on IJB-A dataset on average. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques.",2017,2017 IEEE International Joint Conference on Biometrics (IJCB),,10.1109/BTAS.2017.8272729,
1fd5d08394a3278ef0a89639e9bfec7cb482e0bf,1,0,Exploring Disentangled Feature Representation Beyond Face Identification,"This paper proposes learning disentangled but complementary face features with a minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity verification performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1804.03487,10.1109/CVPR.2018.00222,https://arxiv.org/pdf/1804.03487.pdf
1fe3019eecb975092b93062570a85a47588a2f8c,0,1,"Still Face Galleries Probe Videos Bodies Faces Data Preprocessing Face , Body Detector Face Network Body Network Feature Extraction Similarity Computation Graph Inference Training / Testing Output Similarity Pairwise Similarity Cannot-Links Uncertainty-GatedGraph Training Loss Testing Back-prop Faci","Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset.",2019,,,,https://arxiv.org/pdf/1905.02756.pdf
1fe65b671305b38ae320351048cb638288b9941a,0,1,Drone LAMS: A Drone-based Face Detection Dataset with Large Angles and Many Scenarios,"This work presented a new drone-based face detection dataset Drone LAMS in order to solve issues of low performance of drone-based face detection in scenarios such as large angles which was a predominant working condition when a drone flies high. The proposed dataset captured images from 261 videos with over 43k annotations and 4.0k images with pitch or yaw angle in the range of -90° to 90°. Drone LAMS showed significant improvement over currently available drone-based face detection datasets in terms of detection performance, especially with large pitch and yaw angle. Detailed analysis of how key factors, such as duplication rate, annotation method, etc., impact dataset performance was also provided to facilitate further usage of a drone on face detection.",2020,ArXiv,2011.07689,,https://arxiv.org/pdf/2011.07689.pdf
2011d4da646f794456bebb617d1500ddf71989ed,1,0,Transductive Centroid Projection for Semi-supervised Large-Scale Recognition,"Conventional deep semi-supervised learning methods, such as recursive clustering and training process, suffer from cumulative error and high computational complexity when collaborating with Convolutional Neural Networks. To this end, we design a simple but effective learning mechanism that merely substitutes the last fully-connected layer with the proposed Transductive Centroid Projection (TCP) module. It is inspired by the observation of the weights in the final classification layer (called anchors) converge to the central direction of each class in hyperspace. Specifically, we design the TCP module by dynamically adding an ad hoc anchor for each cluster in one mini-batch. It essentially reduces the probability of the inter-class conflict and enables the unlabelled data functioning as labelled data. We inspect its effectiveness with elaborate ablation study on seven public face/person classification benchmarks. Without any bells and whistles, TCP can achieve significant performance gains over most state-of-the-art methods in both fully-supervised and semi-supervised manners.",2018,ECCV,,10.1007/978-3-030-01228-1_5,http://openaccess.thecvf.com/content_ECCV_2018/papers/Yu_Liu_Transductive_Centroid_Projection_ECCV_2018_paper.pdf
20335b4aae78606f3baf04f0bcf3e0cf7490c0d6,0,1,Cascaded Static and Dynamic Local Feature Extractions for Face Sketch to Photo Matching,"The automatic identification of a corresponding photo from a face sketch can assist in criminal investigations. The face sketch is rendered based on the descriptions elicited by the eyewitness. This may cause the face sketch to have some degrees of shape exaggeration that make some parts of the face geometrically misaligned. In this paper, we attempt to address the effect of these influences by a cascaded static and dynamic local feature extraction method so that the constructed feature vectors are built based on the correct patches. In the proposed method, the feature vectors from the local static extraction on a sketch and photo are matched using the nearest neighbors. Then, some <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> most similar photos are shortlisted based on the nearest neighbors. These photos are eventually re-matched using feature vectors from the local dynamic extraction method. The feature vectors are matched using the <inline-formula> <tex-math notation=""LaTeX"">$L_{1}$ </tex-math></inline-formula>-distance measure. The experimental results for The Chinese University of Hong Kong (CUHK) Face Sketch Database (CUFS) and CUHK Face Sketch FERET Database (CUFSF) datasets indicate that the proposed method outperforms the state-of-the-art methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2897599,
203d0eedfc7b9a4ef29afc1a4fd01fe4db6b6e99,0,1,A Review on Face Reenactment Techniques,"Existing Face Re-enactment approaches have two major limitations, first, they require large dataset of images to create photo-realistic face models and second, they do not generalize well if the facial images are not available in training dataset. The generation of a new facial reenactment requires large image dataset and hours are required to train these models. Some progress in Deep Learning has shown quite significant results using Generative Adversarial Networks (GANs). Recent works in GAN have solved the problem of large dataset training dataset by introducing the concept of few-shot learning. This paper reviews existing approaches in Face Re-enactment with few-shot learning techniques and other approaches in Face Re-enactment.",2020,2020 International Conference on Industry 4.0 Technology (I4Tech),,10.1109/I4Tech48345.2020.9102668,
20541068388f9555b0cbb9b5004b4afe56d8ec66,0,1,CMSN: Continuous Multi-stage Network and Variable Margin Cosine Loss for Temporal Action Proposal Generation,"Accurately locating the start and end time of an action in untrimmed videos is a challenging task. One of the important reasons is the boundary of action is not highly distinguishable, and the features around the boundary are difficult to discriminate. To address this problem, we propose a novel framework for temporal action proposal generation, namely Continuous Multi-stage Network (CMSN), which divides a video that contains a complete action instance into six stages, namely Backgroud, Ready, Start, Confirm, End, Follow. To distinguish between Ready and Start, End and Follow more accurately, we propose a novel loss function, Variable Margin Cosine Loss (VMCL), which allows for different margins between different categories. Our experiments on THUMOS14 show that the proposed method for temporal proposal generation performs better than the state-of-the-art methods using the same network architecture and training dataset.",2019,ArXiv,1911.0608,,https://arxiv.org/pdf/1911.06080.pdf
20a64c3d0d2108bc5fc7b0bbd11ad941734d5f2b,1,0,Heterogeneous Face Recognition Using Domain Specific Units,"The task of Heterogeneous Face Recognition consists in matching face images that are sensed in different domains, such as sketches to photographs (visual spectra images), and thermal images to photographs or near-infrared images to photographs. In this paper, we suggest that the high-level features of Deep Convolutional Neural Networks trained in visual spectra images are potentially domain independent and can be used to encode faces sensed in different image domains. A generic framework for Heterogeneous Face Recognition is proposed by adapting Deep Convolutional Neural Networks low-level features in, so-called, Domain Specific Units. The adaptation using the Domain Specific Units allows the learning of shallow feature detectors specific for each new image domain. Furthermore, it handles its transformation to a generic face space shared between all image domains. Experiments carried out with four different face databases covering three different image domains show substantial improvements, in terms of recognition rate, surpassing the state-of-the-art for most of them. This work is made reproducible: all the source code, scores, and trained models of this approach are made publicly available.",2019,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2018.2885284,http://publications.idiap.ch/downloads/papers/2018/deFreitasPereira_IEEET-IFS_2019.pdf
20e4b0eef22624e92254551ae76251ac69a43ba0,1,0,DFQA: Deep Face Image Quality Assessment,"A face image with high quality can be extracted dependable features for further evaluation, however, the one with low quality can’t. Different from the quality assessment algorithms for general images, the face image quality assessment need to consider more practical factors that directly affect the accuracy of face recognition, face verifcation, etc. In this paper, we present a two-stream convolutional neural network (CNN) named Deep Face Quality Assessment (DFQA) specifically for face image quality assessment. DFQA is able to predict the quality score of an input face image quickly and accurately. Specifcally, we design a network with two-stream for increasing the diversity and improving the accuracy of evaluation. Compared with other CNN network architectures and quality assessment methods for similar tasks, our model is smaller in size and faster in speed. In addition, we build a new dataset containing 3000 face images manually marked with objective quality scores. Experiments show that the performance of face recognition is improved by introducing our face image quality assessment algorithm.",2019,ICIG,,10.1007/978-3-030-34110-7_55,
20ef09e7df49bc1839452bd36235d78a7412a6db,1,1,Multiple Transfer Learning and Multi-label Balanced Training Strategies for Facial AU Detection In the Wild,"This paper1 presents SIAT-NTU solution and results of facial action unit (AU) detection in the EmotiNet Challenge 2020. The task aims to detect 23 AUs from facial images in the wild, and its main difficulties lie in the imbalanced AU distribution and discriminative feature learning. We tackle these difficulties from the following aspects. First, to address the unconstrained heterogeneity of in-the-wild images, we detect and align faces with multi-task convolutional neural networks (MTCNN). Second, by using multiple transfer strategies, we pre-train large CNNs on multiple related datasets, e.g. face recognition datasets and facial expression datasets, and fine-tune them on the EmotiNetdataset. Third, we employ a multi-label balanced sampling strategy and a weighted loss to mitigate the imbalance problem. Last but not the least, to further improve performance, we ensemble multiple models and optimize the thresholds for each AU. Our proposed solution achieves an accuracy of 90.13% and F1 of 44.10% in the final test phase. Our Code is available at: https://github.com/kaiwang960112/enc2020_au_detection",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00215,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w29/Ji_Multiple_Transfer_Learning_and_Multi-Label_Balanced_Training_Strategies_for_Facial_CVPRW_2020_paper.pdf
20f87ed94a423b5d8599d85d1f2f80bab8902107,1,0,Pose-Guided Photorealistic Face Rotation,"Face rotation provides an effective and cheap way for data augmentation and representation learning of face recognition. It is a challenging generative learning problem due to the large pose discrepancy between two face images. This work focuses on flexible face rotation of arbitrary head poses, including extreme profile views. We propose a novel Couple-Agent Pose-Guided Generative Adversarial Network (CAPG-GAN) to generate both neutral and profile head pose face images. The head pose information is encoded by facial landmark heatmaps. It not only forms a mask image to guide the generator in learning process but also provides a flexible controllable condition during inference. A couple-agent discriminator is introduced to reinforce on the realism of synthetic arbitrary view faces. Besides the generator and conditional adversarial loss, CAPG-GAN further employs identity preserving loss and total variation regularization to preserve identity information and refine local textures respectively. Quantitative and qualitative experimental results on the Multi-PIE and LFW databases consistently show the superiority of our face rotation method over the state-of-the-art.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00876,http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Pose-Guided_Photorealistic_Face_CVPR_2018_paper.pdf
21117380118ddce47b3c515c5228372c513e61ba,1,1,Deep Face Recognition: A Survey,"Driven by graphics processing units (GPUs), massive amounts of annotated data and more advanced algorithms, deep learning has recently taken the computer vision community by storm and has benefited real-world applications, including face recognition (FR). Deep FR methods leverage deep networks to learn more discriminative representations, significantly improving the state of the art and surpassing human performance (97.53%). In this paper, we provide a comprehensive survey of deep FR methods, including data, algorithms and scenes. First, we summarize the commonly used datasets for training and testing. Then, the data preprocessing methods are categorized into two classes: ""one-to-many augmentation"" and ""many-to-one normalization"". Second, for algorithms, we summarize different network architectures and loss functions used in the state-of-the art methods. Third, we review several scenes in deep FR, such as video FR, 3D FR and cross-age FR. Finally, some potential deficiencies of the current methods and several future directions are highlighted.",2018,ArXiv,1804.06655,,https://arxiv.org/pdf/1804.06655.pdf
21acd7f9d3a36946fb97b914f9b8219038b5c773,0,1,Deep Disentangled Hashing with Momentum Triplets for Neuroimage Search,"Neuroimaging has been widely used in computer-aided clinical diagnosis and treatment, and the rapid increase of neuroimage repositories introduces great challenges for efficient neuroimage search. Existing image search methods often use triplet loss to capture high-order relationships between samples. However, we find that the traditional triplet loss is difficult to pull positive and negative sample pairs to make their Hamming distance discrepancies larger than a small fixed value. This may reduce the discriminative ability of learned hash code and degrade the performance of image search. To address this issue, in this work, we propose a deep disentangled momentum hashing (DDMH) framework for neuroimage search. Specifically, we first investigate the original triplet loss and find that this loss function can be determined by the inner product of hash code pairs. Accordingly, we disentangle hash code norms and hash code directions and analyze the role of each part. By decoupling the loss function from the hash code norm, we propose a unique disentangled triplet loss, which can effectively push positive and negative sample pairs by desired Hamming distance discrepancies for hash codes with different lengths. We further develop a momentum triplet strategy to address the problem of insufficient triplet samples caused by small batch-size for 3D neuroimages. With the proposed disentangled triplet loss and the momentum triplet strategy, we design an end-to-end trainable deep hashing framework for neuroimage search. Comprehensive empirical evidence on three neuroimage datasets shows that DDMH has better performance in neuroimage search compared to several state-of-the-art methods.",2020,MICCAI,,10.1007/978-3-030-59710-8_19,
21dfb2e2438ffee991be47709d55d19f0d15aed3,0,1,Semi-Supervised Contrastive Learning with Generalized Contrastive Loss and Its Application to Speaker Recognition,"This paper introduces a semi-supervised contrastive learning framework and its application to text-independent speaker verification. The proposed framework employs generalized contrastive loss (GCL). GCL unifies losses from two different learning frameworks, supervised metric learning and unsupervised contrastive learning, and thus it naturally determines the loss for semi-supervised learning. In experiments, we applied the proposed framework to text-independent speaker verification on the VoxCeleb dataset. We demonstrate that GCL enables the learning of speaker embeddings in three manners, supervised learning, semi-supervised learning, and unsupervised learning, without any changes in the definition of the loss function.",2020,ArXiv,2006.04326,,https://arxiv.org/pdf/2006.04326.pdf
222ddaf0ebd65869b28d2e1fabcab3f5677a370f,1,1,Personal Guides: Heterogeneous Robots Sharing Personal Tours in Multi-Floor Environments,"GidaBot is an application designed to setup and run a heterogeneous team of robots to act as tour guides in multi-floor buildings. Although the tours can go through several floors, the robots can only service a single floor, and thus, a guiding task may require collaboration among several robots. The designed system makes use of a robust inter-robot communication strategy to share goals and paths during the guiding tasks. Such tours work as personal services carried out by one or more robots. In this paper, a face re-identification/verification module based on state-of-the-art techniques is developed, evaluated offline, and integrated into GidaBot’s real daily activities, to avoid new visitors interfering with those attended. It is a complex problem because, as users are casual visitors, no long-term information is stored, and consequently, faces are unknown in the training step. Initially, re-identification and verification are evaluated offline considering different face detectors and computing distances in a face embedding representation. To fulfil the goal online, several face detectors are fused in parallel to avoid face alignment bias produced by face detectors under certain circumstances, and the decision is made based on a minimum distance criterion. This fused approach outperforms any individual method and highly improves the real system’s reliability, as the tests carried out using real robots at the Faculty of Informatics in San Sebastian show.",2020,Sensors,,10.3390/s20092480,https://pdfs.semanticscholar.org/72cd/545d691a859877bd5e3d68434687dc20c9b4.pdf
224a8232cc04ac0d6e8c9ee363a0893c962b7a3c,0,1,Similarity Attack on Cancelable Biometric Templates using Particle Swarm Optimization,"Biometrics models use behavioral and physiological characteristics for automatically recognizing the identity of individuals. Since the biometric information is immutable, it needs to be stored in a secure way. Cancelable Biometrics (CB) is one such technique that is used for the purpose of biometric template protection. CB schemes typically utilize a transformation function to convert the biometric features into their corresponding cancelable forms. Although the cancelable templates are irreversible, the distance between two feature templates gets preserved during the transformation process. This similarity preserving property of CB causes a Similarity-based Attack (SA), in which the pre-images of the transformed features can be reproduced. In this paper, the applicability of Particle Swarm Optimization (PSO) is analyzed for performing the similarity-based attack on cancelable biometric templates. During our investigation, we have found that not every metaheuristic can be efficiently used for attacking the CB scheme. Furthermore, the performance of the metaheuristic varies depending on the underlying CB scheme. Hence our study provides a detailed outline of the applicability of PSO for launching a SA on a generic CB scheme.",2020,"2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)",,10.1109/GUCON48875.2020.9231257,
226bad1e7ee8b51c6b434c7a451af7ce42cf6a1e,1,0,Access Path Design for Quality Assurance in Crowdsourcing,"In this thesis, we study how the problem of worker group discovery relates to two well-known problems in crowdsourcing: answer aggregation and budget allocation. In contrast to previous studies which assume full independence of worker answers, this work is based on the observation that certain groups of crowd contributors share common behaviors and hence their answers are correlated. Being aware of these clusters of workers can be useful not only to predict more accurately the correct answer from the crowd, but also to wisely distribute the available budget. We investigate various clustering strategies as solutions to the problem of worker group discovery from historical data. Our studies confirm that the most critical challenge to this problem is the high data sparsity that characterizes crowd work. Therefore, we propose to apply such clustering techniques on reduced representations of the historical data based on the heterogeneity of the tasks. In an experimental setup where tasks belong to different categories, worker groups are constructed based on the workers’ average performance in the distinct task categories. We make use of this distinction to introduce a new model which jointly represents: workers, worker groups, and task categories. Next, we also make use of the information on the category of the task to adjust budget allocation. We evaluated our approach on multiple real-world and synthetic datasets which reveal interesting insights on the applicability and the limitations of our approach.",2016,,,10.3929/ethz-a-010725406,https://pdfs.semanticscholar.org/0984/708c99430278e83a2869411ef71a238acf3b.pdf
22784db6e70a9b17dd0220fc6984591690e6f912,0,1,Vein-Based Biometric Verification Using Densely-Connected Convolutional Autoencoder,"In this letter, we propose a vein-based biometric verification system relying on deep learning. A novel approach consisting of a convolutional neural network (CNN), trained in a supervised manner, cascaded with an auto-encoder, trained in an unsupervised way, is here exploited. In more detail, a novel densely-connected convolutional autoencoder is here used on top of backbone CNNs. This architecture aims at increasing the discriminative capability of the features generated from hand vein patterns. Experimental tests on finger, palm, and dorsal veins show that the proposed approach leads to an improvement of the recognition rates with respect to the use of the sole CNNs for feature extraction. The achieved performance are superior to the current state of the art in vein biometric verification.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.3030533,
22b52d3f18eaf43993a3a91053f5efe6267144e7,0,1,Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification,"Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.",2020,ICLR,2001.01526,,https://arxiv.org/pdf/2001.01526.pdf
22dc5c7b2ef5175d8dd8f482c348d37dd17a9171,0,1,Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification,"Although great progress in supervised person re-identification (Re-ID) has been made recently, due to the viewpoint variation of a person, Re-ID remains a massive visual challenge. Most existing viewpoint-based person Re-ID methods project images from each viewpoint into separated and unrelated sub-feature spaces. They only model the identity-level distribution inside an individual viewpoint but ignore the underlying relationship between different viewpoints. To address this problem, we propose a novel approach, called \textit{Viewpoint-Aware Loss with Angular Regularization }(\textbf{VA-reID}). Instead of one subspace for each viewpoint, our method projects the feature from different viewpoints into a unified hypersphere and effectively models the feature distribution on both the identity-level and the viewpoint-level. In addition, rather than modeling different viewpoints as hard labels used for conventional viewpoint classification, we introduce viewpoint-aware adaptive label smoothing regularization (VALSR) that assigns the adaptive soft label to feature representation. VALSR can effectively solve the ambiguity of the viewpoint cluster label assignment. Extensive experiments on the Market1501 and DukeMTMC-reID datasets demonstrated that our method outperforms the state-of-the-art supervised Re-ID methods.",2020,AAAI,1912.013,10.1609/AAAI.V34I07.7014,https://arxiv.org/pdf/1912.01300.pdf
22e06fa904f2022b63954a08543df4a2da63059e,0,1,Multi-Domain Multi-Task Rehearsal for Lifelong Learning,"Rehearsal, seeking to remind the model by storing old knowledge in lifelong learning, is one of the most effective ways to mitigate catastrophic forgetting, i.e., biased forgetting of previous knowledge when moving to new tasks. However, the old tasks of the most previous rehearsal-based methods suffer from the unpredictable domain shift when training the new task. This is because these methods always ignore two significant factors. First, the Data Imbalance between the new task and old tasks that makes the domain of old tasks prone to shift. Second, the Task Isolation among all tasks will make the domain shift toward unpredictable directions; To address the unpredictable domain shift, in this paper, we propose MultiDomain Multi-Task (MDMT) rehearsal to train the old tasks and new task parallelly and equally to break the isolation among tasks. Specifically, a two-level angular margin loss is proposed to encourage the intra-class/task compactness and inter-class/task discrepancy, which keeps the model from domain chaos. In addition, to further address domain shift of the old tasks, we propose an optional episodic distillation loss on the memory to anchor the knowledge for each old task. Experiments on benchmark datasets validate the proposed approach can effectively mitigate the unpredictable domain shift.",2020,,2012.07236,,https://arxiv.org/pdf/2012.07236.pdf
2306b2a8fba28539306052764a77a0d0f5d1236a,1,0,Surveillance Face Recognition Challenge,"Face recognition (FR) is one of the most extensively investigated problems in computer vision. Significant progress in FR has been made due to the recent introduction of the larger scale FR challenges, particularly with constrained social media web images, e.g. high-resolution photos of celebrity faces taken by professional photo-journalists. However, the more challenging FR in unconstrained and low-resolution surveillance images remains largely under-studied. To facilitate more studies on developing FR models that are effective and robust for low-resolution surveillance facial images, we introduce a new Surveillance Face Recognition Challenge, which we call the QMUL-SurvFace benchmark. This new benchmark is the largest and more importantly the only true surveillance FR benchmark to our best knowledge, where low-resolution images are not synthesised by artificial down-sampling of native high-resolution images. This challenge contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time. As a consequence, it presents an extremely challenging FR benchmark. We benchmark the FR performance on this challenge using five representative deep learning face recognition models, in comparison to existing benchmarks. We show that the current state of the arts are still far from being satisfactory to tackle the under-investigated surveillance FR problem in practical forensic scenarios. Face recognition is generally more difficult in an open-set setting which is typical for surveillance scenarios, owing to a large number of non-target people (distractors) appearing open spaced scenes. This is evidently so that on the new Surveillance FR Challenge, the top-performing CentreFace deep learning FR model on the MegaFace benchmark can now only achieve 13.2% success rate (at Rank-20) at a 10% false alarm rate.",2018,ArXiv,1804.09691,,https://arxiv.org/pdf/1804.09691.pdf
23082d047f38cc3c17bf24c761aa052d70ae899d,1,0,SEMANTICADV: GENERATING ADVERSARIAL EXAM-,"Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation"" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate “unrestricted adversarial examples"". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various “adversarial"" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.",2019,,,,https://pdfs.semanticscholar.org/7723/2266b32fdd1c6ad33129bf0e04cfd59a2d88.pdf
2377ba961a270ab0750ee74d3e145f83600ee799,0,1,From an Artificial Neural Network to Teaching,"Aim/Purpose Using Artificial Intelligence with Deep Learning (DL) techniques, which mimic the action of the brain, to improve a student’s grammar learning process. Finding the subject of a sentence using DL, and learning, by way of this computer field, to analyze human learning processes and mistakes. In addition, showing Artificial Intelligence learning processes, with and without a general overview of the problem that it is under examination. Applying the idea of the general perspective that the network gets on the sentences and deriving recommendations from this for teaching processes. Background We looked for common patterns of computer errors and human grammar mistakes. Also deducing the neural network’s learning process, deriving conclusions, and applying concepts from this process to the process of human learning. Methodology We used DL technologies and research methods. After analysis, we built models from three types of complex neuronal networks – LSTM, Bi-LSTM, and GRU – with sequence-to-sequence architecture. After this, we combined the sequence-tosequence architecture model with the attention mechanism that gives a general overview of the input that the network receives. From an Artificial Neural Network to Teaching 2 Contribution The cost of computer applications is cheaper than that of manual human effort, and the availability of a computer program is much greater than that of humans to perform the same task. Thus, using computer applications, we can get many desired examples of mistakes without having to pay humans to perform the same task. Understanding the mistakes of the machine can help us to understand the human mistakes, because the human brain is the model of the artificial neural network. This way, we can facilitate the student learning process by teaching students not to make mistakes that we have seen made by the artificial neural network. We hope that with the method we have developed, it will be easier for teachers to discover common mistakes in students’ work before starting to teach them. In addition, we show that a “general explanation” of the issue under study can help the teaching and learning process. Findings We performed the test case on the Hebrew language. From the mistakes we received from the computerized neuronal networks model we built, we were able to classify common human errors. That is, we were able to find a correspondence between machine mistakes and student mistakes. Recommendations for Practitioners Use an artificial neural network to discover mistakes, and teach students not to make those mistakes. We recommend that before the teacher begins teaching a new topic, he or she gives a general explanation of the problems this topic deals with, and how to solve them. Recommendations for Researchers To use machines that simulate the learning processes of the human brain, and study if we can thus learn about human learning processes. Impact on Society When the computer makes the same mistakes as a human would, it is very easy to learn from those mistakes and improve the study process. The fact that machine and humans make similar mistakes is a valuable insight, especially in the field of education, Since we can generate and analyze computer system errors instead of doing a survey of humans (who make mistakes similar to those of the machine); the teaching process becomes cheaper and more efficient. Future Research We plan to create an automatic grammar-mistakes maker (for instance, by giving the artificial neural network only a tiny data-set to learn from) and ask the students to correct the errors made. In this way, the students will practice on the material in a focused manner. We plan to apply these techniques to other education subfields and, also, to non-educational fields. As far as we know, this is the first study to go in this direction ‒ instead of looking at organisms and building machines, to look at machines and learn about organisms.",2020,,,10.28945/4586,https://pdfs.semanticscholar.org/2377/ba961a270ab0750ee74d3e145f83600ee799.pdf
2391b8cf30f1ec99798672d20203c791b2d0a622,1,1,Deep class-skewed learning for face recognition,"Abstract Face datasets often exhibit highly-skewed class distribution, i.e., rich classes contain a plenty amount of instances, while only few images belong to poor classes. To mitigate this issue, we explore deep class-skewed learning from two aspects in this paper: feature augmentation and feature normalization. To deal with the imbalance distribution problem, we put forward a novel feature augmentation method termed Large Margin Feature Augmentation (LMFA) to augment hard features and equalize class distribution, leading to balanced classification boundaries between rich and poor classes. By considering the distribution gap between training and testing features, A novel feature normalization called Transferable Domain Normalization (TDN) is proposed to normalize domain-specific features to obey an identical Gaussian distribution, and enhance the feature generalization. Extensive experiments are conducted on five popular face recognition datasets including LFW, YTF, CFP, AgeDB and MegaFace. We achieve remarkable results on par with or better than the state-of-the-art methods, which demonstrate the effectiveness of our proposed learning class-balanced features.",2019,Neurocomputing,,10.1016/J.NEUCOM.2019.04.085,
23dd8d17ce09c22d367e4d62c1ccf507bcbc64da,1,0,Deep Density Clustering of Unconstrained Faces ( Supplementary Material ),,2018,,,,https://pdfs.semanticscholar.org/23dd/8d17ce09c22d367e4d62c1ccf507bcbc64da.pdf
23fc584a069c86da5d784da781268cba1c065fc5,0,1,Large image datasets: A pyrrhic win for computer vision?,"In this paper we investigate problematic practices and consequences of large scale vision datasets. We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class-wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both society broadly and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique the pros and cons of these. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation processes.",2020,ArXiv,2006.16923,,https://arxiv.org/pdf/2006.16923.pdf
23fc6825255d9dae460848cea213c0988a964096,1,1,On Improving the Generalization of Face Recognition in the Presence of Occlusions,"In this paper, we address a key limitation of existing 2D face recognition methods: robustness to occlusions. To accomplish this task, we systematically analyzed the impact of facial attributes on the performance of a state-of-the-art face recognition method and through extensive experimentation, quantitatively analyzed the performance degradation under different types of occlusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach learned discriminative facial templates despite the presence of such occlusions. First, an attention mechanism was proposed that extracted local identity-related region. The local features were then aggregated with the global representations to form a single template. Second, a simple, yet effective, training strategy was introduced to balance the non-occluded and occluded facial images. Extensive experiments demonstrated that OREO improved the generalization ability of face recognition under occlusions by 10.17% in a single-image-based setting and outperformed the baseline by approximately 2% in terms of rank-1 accuracy in an image-set-based scenario.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2006.06787,10.1109/CVPRW50498.2020.00407,https://arxiv.org/pdf/2006.06787.pdf
2418160cf74fc4b51577b39c5796c0fc95cc91ce,0,1,Class-Variant Margin Normalized Softmax Loss for Deep Face Recognition.,"In deep face recognition, the commonly used softmax loss and its newly proposed variations are not yet sufficiently effective to handle the class imbalance and softmax saturation issues during the training process while extracting discriminative features. In this brief, to address both issues, we propose a class-variant margin (CVM) normalized softmax loss, by introducing a true-class margin and a false-class margin into the cosine space of the angle between the feature vector and the class-weight vector. The true-class margin alleviates the class imbalance problem, and the false-class margin postpones the early individual saturation of softmax. With negligible computational complexity increment during training, the new loss function is easy to implement in the common deep learning frameworks. Comprehensive experiments on the LFW, YTF, and MegaFace protocols demonstrate the effectiveness of the proposed CVM loss function.",2020,IEEE transactions on neural networks and learning systems,,10.1109/TNNLS.2020.3017528,https://pdfs.semanticscholar.org/1c39/63538bbe32222d1138e37b29ec923d2404b0.pdf
2421bebd09e19e8780745975ac0957183c4f84c7,1,1,Generate to Adapt: Resolution Adaption Network for Surveillance Face Recognition,"Although deep learning techniques have largely improved face recognition, unconstrained surveillance face recognition is still an unsolved challenge, due to the limited training data and the gap of domain distribution. Previous methods mostly match low-resolution and high-resolution faces in different domains, which tend to deteriorate the original feature space in the common recognition scenarios. To avoid this problem, we propose resolution adaption network (RAN) which contains Multi-Resolution Generative Adversarial Networks (MR-GAN) followed by a feature adaption network. MR-GAN learns multi-resolution representations and randomly selects one resolution to generate realistic low-resolution (LR) faces that can avoid the artifacts of down-sampled faces. A novel feature adaption network with translation gate is developed to fuse the discriminative information of LR faces into backbone network, while preserving the discrimination ability of original face representations. The experimental results on IJB-C TinyFace, SCface, QMULSurvFace datasets have demonstrated the superiority of our method compared with state-of-the-art surveillance face recognition methods, while showing stable performance on the common recognition scenarios.",2020,ECCV,,10.1007/978-3-030-58555-6_44,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600732.pdf
2479ab50c9ca4dc316c3f0af27053834b4efa705,1,0,A lighten CNN-LSTM model for speaker verification on embedded devices,"Abstract Augmented by deep learning methods, the performance of speaker recognition pipeline has been drastically boosted. For the scenario of smart home, the algorithms of speaker recognition should be user friendly and has high speed, high precision and low resource demand. However, most of the existing algorithms are designed without considering these four performance requirements simultaneously. To fill this gap, this paper proposes a text-independent speaker verification model. Specifically, the lighten network scheme is constructed using one convolution layer, two bilateral Long Short-term Memory (LSTM) layers and one fully connected layer. Utterance segments are mapped to a hypersphere where cosine similarity is used to measure the degree of difference between speakers. Then we analyze the defects of Additive Angular Margin (AAM) loss and propose a 3-stage training method. Softmax pre-training is used for avoiding divergence. After pre-training, AAM loss is adopted to boost training process. In the end, we use triplet loss to further fine-tune the model. Short-term speech utterances are used in training and testing. The experimental results demonstrate that the proposed model reaches 1.17% Equal Error Rate (EER) on a 200 persons benchmark with real-time inference speed on a generic embedded device.",2019,Future Gener. Comput. Syst.,,10.1016/J.FUTURE.2019.05.057,
24bee4535039570f3a0f7eb1c3e75e689ec202cd,0,1,Vehicle Re-Identification: Pushing the limits of re-identification,"In this paper, we present a series of techniques which help push the limits of vehicle re-identification. First, we establish a strong baseline by using one of the best person re-identification models and applying them to vehicle reidentification. Secondly, we show improvements in four key components of re-identification: 1) detection, 2) tracking, 3) model, 4) loss function. Finally, our improvements lead to the state-of-the-art in the vehicle re-identification dataset VeRi-776, with 85.20 mean Average Precision (mAP) and 96.60% Rank-1 accuracy. This represents a +17.65 mAP and +6.37 Rank-1 improvement over the literature.",2019,CVPR Workshops,,,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Ayala-Acevedo_Vehicle_Re-Identification_Pushing_the_limits_of_re-identification_CVPRW_2019_paper.pdf
24f2e1316c5d6c2042956d19063b4491014307c2,0,1,BSF-RCNN-VFR: Background Subtracted Faster RCNN for Video based Face Recognition,"Surveillance systems are widely deployed in various organization and public palaces to monitor suspicious activities and reduce the crime rate. Recently, visual surveillance systems has gained huge attraction from research community due to their significant impact on monitoring application. Several techniques have been developed which are based on the still image which do not provide efficient solution for real-time application. Hence, video based face recognition is considered as a tedious task. Recently, deep learning based schemes have been adopted widely for video face recognition but these techniques suffer from well-known challenges such as pose and illumination variation. Hence, we present a Convolutional Neural Network (CNN) based approach for video face recognition. According to the proposed approach, we employ background subtraction scheme which helps to reduce the scene complexity and improves the feature extraction process. Later CNN based face detection scheme is developed which uses region proposal generation networks. In this phase, we incorporate bounding box regression model to reduce the face detection error. Finally, RCNN based learning model is applied which uses Joint Bayesian learning to discriminate the classes of detected faces. Based on these stages, the proposed model is named as Background subtracted Faster RCNN for Video based Face Recognition (BSF-RCNN-VFR). An experimental study is carried out based on the proposed method where we use publically available datasets such as YouTube celebrity dataset, Buffy dataset and YouTube face dataset. The experimental study shows a significant improvement in the detection and recognition accuracy process. Keywords— Face recognition, object detection, deep learning,",2019,,,,
24f32f441317764348636912c067ae2e9ab721a8,0,1,A Multi-Modal Approach for Driver Gaze Prediction to Remove Identity Bias,"Driver gaze prediction is an important task in Advanced Driver Assistance System (ADAS). Although the Convolutional Neural Network (CNN) can greatly improve the recognition ability, there are still several unsolved problems due to the challenge of illumination, pose and camera placement. To solve these difficulties, we propose an effective multi-model fusion method for driver gaze estimation. Rich appearance representations, i.e. holistic and eyes regions, and geometric representations, i.e. landmarks and Delaunay angles, are separately learned to predict the gaze, followed by a score-level fusion system. Moreover, pseudo-3D appearance supervision and identity-adaptive geometric normalization are proposed to further enhance the prediction accuracy. Finally, the proposed method achieves state-of-the-art accuracy of 82.5288% on the test data, which ranks 1st at the EmotiW2020 driver gaze prediction sub-challenge.",2020,ICMI,,10.1145/3382507.3417961,
25a199c6544b48120911c5bceaa54068ca1bd51c,1,0,Images as Data for Social Science Research: An Introduction to Convolutional Neural Nets for Image Classification,,2020,,,10.1017/9781108860741,
25a6c167ecc58a8e44242a2168ea4aedc4eab59b,1,0,Improving Face Recognition by Clustering Unlabeled Faces in the Wild,"While deep face recognition has benefited significantly from large-scale labeled data, current research is focused on leveraging unlabeled data to further boost performance, reducing the cost of human annotation. Prior work has mostly been in controlled settings, where the labeled and unlabeled data sets have no overlapping identities by construction. This is not realistic in large-scale face recognition, where one must contend with such overlaps, the frequency of which increases with the volume of data. Ignoring identity overlap leads to significant labeling noise, as data from the same identity is split into multiple clusters. To address this, we propose a novel identity separation method based on extreme value theory. It is formulated as an out-of-distribution detection algorithm, and greatly reduces the problems caused by overlapping-identity label noise. Considering cluster assignments as pseudo-labels, we must also overcome the labeling noise from clustering errors. We propose a modulation of the cosine loss, where the modulation weights correspond to an estimate of clustering uncertainty. Extensive experiments on both controlled and real settings demonstrate our method's consistent improvements over supervised baselines, e.g., 11.6% improvement on IJB-A verification.",2020,ArXiv,2007.06995,,https://arxiv.org/pdf/2007.06995.pdf
25d34a1383eef9682e20c2c36926f0ffb0d4705d,0,1,Margin-Mix: Semi-Supervised Learning for Face Expression Recognition,,2020,ECCV,,10.1007/978-3-030-58592-1_1,
2634c3e57a35609b36c7ee415e3ab98abf6971ea,1,0,Deep Convolutional Neural Network with Independent Softmax for Large Scale Face Recognition,"In this paper, we present our solution to the MS-Celeb-1M Challenge. This challenge aims to recognize 100k celebrities at the same time. The huge number of celebrities is the bottleneck for training a deep convolutional neural network of which the output is equal to the number of celebrities. To solve this problem, an independent softmax model is proposed to split the single classifier into several small classifiers. Meanwhile, the training data are split into several partitions. This decomposes the large scale training procedure into several medium training procedures which can be solved separately. Besides, a large model is also trained and a simple strategy is introduced to merge the two models. Extensive experiments on the MSR-Celeb-1M dataset demonstrate the superiority of the proposed method. Our solution ranks the first and second in two tracks of the final evaluation.",2016,ACM Multimedia,,10.1145/2964284.2984060,http://www1.ece.neu.edu/~yuewu/files/2016/p1063-wu.pdf
26404fbaa8b5f668416e7c49a87e6e151a3816f7,1,0,Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Imbalanced Data,"We propose a novel unsupervised generative model, Elastic-InfoGAN, that learns to disentangle object identity from other low-level aspects in class-imbalanced datasets. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as the signal to learn the latent distribution's parameters. Experiments on both artificial (MNIST) and real-world (YouTube-Faces) datasets demonstrate the effectiveness of our approach in imbalanced data by: (i) better disentanglement of object identity as a latent factor of variation; and (ii) better approximation of class imbalance in the data, as reflected in the learned parameters of the latent distribution.",2019,NeurIPS 2019,,,
264b0ad7094a4a2bd4e781e7272eb85dbe234623,1,1,Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification,"Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.",2020,AIES,2001.03071,10.1145/3375627.3375875,https://arxiv.org/pdf/2001.03071.pdf
266766818dbc5a4ca1161ae2bc14c9e269ddc490,1,0,Boosting a Low-Cost Smart Home Environment with Usage and Access Control Rules,"Smart Home has gained widespread attention due to its flexible integration into everyday life. Pervasive sensing technologies are used to recognize and track the activities that people perform during the day, and to allow communication and cooperation of physical objects. Usually, the available infrastructures and applications leveraging these smart environments have a critical impact on the overall cost of the Smart Home construction, require to be preferably installed during the home construction and are still not user-centric. In this paper, we propose a low cost, easy to install, user-friendly, dynamic and flexible infrastructure able to perform runtime resources management by decoupling the different levels of control rules. The basic idea relies on the usage of off-the-shelf sensors and technologies to guarantee the regular exchange of critical information, without the necessity from the user to develop accurate models for managing resources or regulating their access/usage. This allows us to simplify the continuous updating and improvement, to reduce the maintenance effort and to improve residents’ living and security. A first validation of the proposed infrastructure on a case study is also presented.",2018,Sensors,,10.3390/s18061886,https://pdfs.semanticscholar.org/2667/66818dbc5a4ca1161ae2bc14c9e269ddc490.pdf
2669054ca98a34eeaab12e8c1f59ad67122d3e24,1,0,Adversarial Learning of Privacy-Preserving and Task-Oriented Representations,"Data privacy has emerged as an important issue as data-driven deep learning has been an essential component of modern machine learning systems. For instance, there could be a potential privacy risk of machine learning systems via the model inversion attack, whose goal is to reconstruct the input data from the latent representation of deep networks. Our work aims at learning a privacy-preserving and task-oriented representation to defend against such model inversion attacks. Specifically, we propose an adversarial reconstruction learning framework that prevents the latent representations decoded into original input data. By simulating the expected behavior of adversary, our framework is realized by minimizing the negative pixel reconstruction loss or the negative feature reconstruction (i.e., perceptual distance) loss. We validate the proposed method on face attribute prediction, showing that our method allows protecting visual privacy with a small decrease in utility performance. In addition, we show the utility-privacy trade-off with different choices of hyperparameter for negative perceptual distance loss at training, allowing service providers to determine the right level of privacy-protection with a certain utility performance. Moreover, we provide an extensive study with different selections of features, tasks, and the data to further analyze their influence on privacy protection.",2020,AAAI,1911.10143,10.1609/AAAI.V34I07.6930,https://arxiv.org/pdf/1911.10143.pdf
26d5be8308e3d138dbdbb3adb39bfd4abce0c05f,0,1,Diagonal Symmetric Pattern-Based Illumination Invariant Measure for Severe Illumination Variation Face Recognition,"The center symmetric pattern (CSP) was widely used in the local binary pattern based facial feature, whereas never used to develop the illumination invariant measure in the literature. This paper proposes a novel diagonal symmetric pattern (DSP) to develop the illumination invariant measure for severe illumination variation face recognition. Firstly, the subtraction of two diagonal symmetric pixels is defined as the DSP unit in the face local region, which may be positive or negative. The DSP model is obtained by combining the positive and negative DSP units in the even <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> even block region. Then, the DSP model can be used to generate several DSP images based on the <inline-formula> <tex-math notation=""LaTeX"">$2\times 2$ </tex-math></inline-formula> block or the <inline-formula> <tex-math notation=""LaTeX"">$4\times 4$ </tex-math></inline-formula> block by controlling the proportions of positive and negative DSP units, which results in the DSP2 image or the DSP4 image. The single DSP2 or DSP4 image with the arctangent function can develop the DSP2-face or the DSP4-face. Multi DSP2 or DSP4 images employ the extended sparse representation classification (ESRC) as the classifier that can form the DSP2 images based classification (DSP2C) or the DSP4 images based classification (DSP4C). Further, the DSP model is integrated with the pre-trained deep learning (PDL) model to construct the DSP-PDL model. Finally, the experimental results on the Extended Yale B, CMU PIE, AR, and VGGFace2 face databases indicate that the proposed methods are efficient to tackle severe illumination variations.",2020,IEEE Access,,10.1109/ACCESS.2020.2983837,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09049393.pdf
26d8293b6f94d951f8cd79e2dc6d6ebb8212fd2c,1,1,Mitigating Bias in Face Recognition Using Skewness-Aware Reinforcement Learning,"Racial equality is an important theme of international human rights law, but it has been largely obscured when the overall face recognition accuracy is pursued blindly. More facts indicate racial bias indeed degrades the fairness of recognition system and the error rates on non-Caucasians are usually much higher than Caucasians. To encourage fairness, we introduce the idea of adaptive margin to learn balanced performance for different races based on large margin losses. A reinforcement learning based race balance network (RL-RBN) is proposed. We formulate the process of finding the optimal margins for non-Caucasians as a Markov decision process and employ deep Q-learning to learn policies for an agent to select appropriate margin by approximating the Q-value function. Guided by the agent, the skewness of feature scatter between races can be reduced. Besides, we provide two ethnicity aware training datasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be utilized to study racial bias from both data and algorithm aspects. Extensive experiments on RFW database show that RL-RBN successfully mitigates racial bias and learns more balanced performance.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1911.10692,10.1109/cvpr42600.2020.00934,https://arxiv.org/pdf/1911.10692.pdf
26f0cc02e24418b6fb3ba8866f2028375dc8c63d,0,1,Discriminative Representation Loss (DRL): A More Efficient Approach Than Gradient Re-projection in continual learning.,"The use of episodic memories in continual learning has been shown to be effective in terms of alleviating catastrophic forgetting. In recent studies, several gradient-based approaches have been developed to make more efficient use of compact episodic memories, which constrain the gradients resulting from new samples with those from memorized samples, aiming to reduce the diversity of gradients from different tasks. In this paper, we reveal the relation between diversity of gradients and discriminativeness of representations, demonstrating connections between Deep Metric Learning and continual learning. Based on these findings,we propose a simple yet highly efficient method - Discriminative Representation Loss (DRL) - for continual learning. In comparison with several state-of-the-art methods, DRL shows effectiveness with low computational cost on multiple benchmark experiments in the setting of online continual learning.",2020,,2006.11234,,https://arxiv.org/pdf/2006.11234.pdf
2700adf9af8a6f6f927fab4ee112c44b97843814,0,1,Tensor Linear Regression and Its Application to Color Face Recognition,"Linear regression has achieved the promising preliminary results for face classification. But, most existing methods are incapable of tackling color images classification. The major reason is that they need to transform each color image to a vector or matrix, leading to the loss of multidimensional structure information embedded in color images. To address this problem, we study the tensor linear regression problem, and develop a novel tensor low-rank method, which utilizes tensor-Singular Value Decomposition (t-SVD) based tensor nuclear norm to emphasize the spatial structure embedded in color images. Applying it to color face classification, extensive experiments on three datasets demonstrate that our method is superior to several state-of-the-art methods.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00065,http://openaccess.thecvf.com/content_ICCVW_2019/papers/DFW/Gao_Tensor_Linear_Regression_and_Its_Application_to_Color_Face_Recognition_ICCVW_2019_paper.pdf
270864b41a66bc000812869fde40df10cba91cb7,1,0,Large Scale Face Data Purification based on Correlation Function and Multi-Phase Grouping,"Recent advances in deep learning technologies enable high performance artificial intelligence, which is an equivalence of human capability or higher for various application. However, deep learning is highly resorted to the large scale training data, which typically contains large number of outlier samples that are difficult to remove. In this paper, we proposed a face image purifying algorithm, which combines the correlation function of deep features with multi-phase grouping technique. A correlation function was proposed to determine the principal class by measuring the similarities between all different samples. The principal class was further used as a prior for the multi-phase grouping algorithm to purify the face data by multiple thresholds. The experimental results demonstrate that the proposed algorithm has significant improvement than the primitive cluster algorithm, such as K-Means.",2017,,,10.1051/MATECCONF/201712802021,https://pdfs.semanticscholar.org/2708/64b41a66bc000812869fde40df10cba91cb7.pdf
272b242e6400b6e352ff5d3b39f185a3a48cdb32,1,0,Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning,"We investigate learning a ConvNet classifier with class-imbalanced data. We found that a ConvNet over-fits significantly to the minor classes that do not have sufficient training instances, even if it is trained using vanilla empirical risk minimization (ERM). We conduct a series of analysis and argue that feature deviation between the training and test instances serves as the main cause. We propose to incorporate class-dependent temperatures (CDT) in learning a ConvNet: CDT forces the minor-class instances to have larger decision values in training, so as to compensate for the effect of feature deviation in testing. We validate our approach on several benchmark datasets and achieve promising results. Our studies further suggest that class-imbalanced data affects traditional machine learning and recent deep learning in very different ways. We hope that our insights can inspire new ways of thinking in resolving class-imbalanced deep learning.",2020,ArXiv,2001.01385,,https://arxiv.org/pdf/2001.01385.pdf
27801b41f31aab4746bd957b1c8393f8cbc8caf0,0,1,1-to-N Large Margin Classifier,"Cross entropy with softmax is the standard loss function for classification in neural networks. However, this function can suffer from limitations on discriminative power, lack of generalization, and propensity to overfitting. In order to address these limitations, several approaches propose to enforce a margin on the top of the neural network specifically at the softmax function. In this work, we present a novel formulation that aims to produce generalization and noise label robustness not only by imposing a margin at the top of the neural network, but also by using the entire structure of the mini-batch data. Based on the distance used for SVM to obtain maximal margin, we propose a broader distance definition called 1-to-N distance and an approximated probability function as the basis for our proposed loss function. We perform empirical experimentation on MNIST, CIFAR-10, and ImageNet32 datasets to demonstrate that our loss function has better generalization and noise label robustness properties than the traditional cross entropy method, showing improvements in the following tasks: generalization robustness, robustness in noise label data, and robustness against adversarial examples attacks.",2020,"2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",,10.1109/SIBGRAPI51738.2020.00050,
27a336cc8722744816343d81786be4eb07f53600,1,0,"KANACI, ZHU, GONG: VEHICLE RE-ID BY FINE-GRAINED CROSS-LEVEL DEEP LEARNING1 Vehicle Re-Identification by Fine-Grained Cross-Level Deep Learning","Vehicle re-identification in unconstrained images is a challenging computer vision task due to the subtle visual appearance discrepancy between different vehicle identities and large visual appearance changes of the same vehicle instance in different camera views with uncontrolled illumination, view-angle, low-resolution, and background clutters. Existing methods often rely heavily on the availability of cross-camera identity pairwise annotations collected by exhaustive human labelling. This approach is unscalable to many real-world deployment scenarios with limited access to both labelling budgets and vehicle re-appearance between every camera pair. In this work, we solve these challenges by exploiting the inherent hierarchical structure information of vehicle identity and vehicle model class so to eliminate the need for identity level label collection. Specifically, we propose to transfer the vehicle model discriminative representation for more fine-grained re-id tasks by fully leveraging the strong capacity of existing deep models in learning cross-level representations. This realises “Cross-Level Vehicle Recognition” (CLVR). Extensive comparative experiments demonstrate the superiority of the proposed CLVR method over state-of-the-art approaches to using fine-grained identity pairwise labels on the largest vehicle re-id benchmarking dataset.",2017,,,,http://www.eecs.qmul.ac.uk/~sgg/papers/KanaciEtAl_AMMDS2017.pdf
27a350ad0b4e140a35da14592b28e39d7fbf65c6,1,0,PE-MIU: A Training-Free Privacy-Enhancing Face Recognition Approach Based on Minimum Information Units,"Research on soft-biometrics showed that privacy-sensitive information can be deduced from biometric data. Utilizing biometric templates only, information about a persons gender, age, ethnicity, sexual orientation, and health state can be deduced. For many applications, these templates are expected to be used for recognition purposes only. Thus, extracting this information raises major privacy issues. Previous work proposed two kinds of learning-based solutions for this problem. The first ones provide strong privacy-enhancements, but limited to pre-defined attributes. The second ones achieve more comprehensive but weaker privacy-improvements. In this work, we propose a Privacy-Enhancing face recognition approach based on Minimum Information Units (PE-MIU). PE-MIU, as we demonstrate in this work, is a privacy-enhancement approach for face recognition templates that achieves strong privacy-improvements and is not limited to pre-defined attributes. We exploit the structural differences between face recognition and facial attribute estimation by creating templates in a mixed representation of minimal information units. These representations contain pattern of privacy-sensitive attributes in a highly randomized form. Therefore, the estimation of these attributes becomes hard for function creep attacks. During verification, these units of a probe template are assigned to the units of a reference template by solving an optimal best-matching problem. This allows our approach to maintain a high recognition ability. The experiments are conducted on three publicly available datasets and with five state-of-the-art approaches. Moreover, we conduct the experiments simulating an attacker that knows and adapts to the systems privacy mechanism. The experiments demonstrate that PE-MIU is able to suppress privacy-sensitive information to a significantly higher degree than previous work in all investigated scenarios. At the same time, our solution is able to achieve a verification performance close to that of the unmodified recognition system. Unlike previous works, our approach offers a strong and comprehensive privacy-enhancement without the need of training.",2020,IEEE Access,,10.1109/ACCESS.2020.2994960,
27aae8c3d70b0fec180d5be2c66269adb140c73a,0,1,Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,"Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.07399,10.1109/ICCV.2019.00707,https://arxiv.org/pdf/1904.07399.pdf
27e4916810e16de26179fc0004dd324503e80e9f,1,1,DocFace: Matching ID Document Photos to Selfies*,"Numerous activities in our daily life, including purchases, travels and access to services, require us to verify who we are by showing ID documents containing face images, such as passports and driver licenses. An automatic system for matching ID document photos to live face images in real time with high accuracy would speed up the verification process and reduce the burden on human operators. In this paper, we propose a new method, DocFace, for ID document photo matching using the transfer learning technique. We propose to use a pair of sibling networks to learn domain specific parameters from heterogeneous face pairs. Cross validation testing on an ID-Selfie dataset shows that while the best CNN-based general face matcher only achieves a TAR=61.14% at FAR=0.1% on the problem, the DocFace improves the TAR to 92.77%. Experimental results also indicate that given sufficiently large training data, a viable system for automatic ID document photo matching can be developed and deployed.",2018,"2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)",1805.02283,10.1109/BTAS.2018.8698596,https://arxiv.org/pdf/1805.02283.pdf
2846d6b17415cdcf4fefbd5d34ddd5cb73cb82f3,0,1,Vulnerability Assessment and Detection of Makeup Presentation Attacks,"The accuracy of face recognition systems can be negatively affected by facial cosmetics which have the ability to substantially alter the facial appearance. Recently, it was shown that makeup can also be abused to launch so-called makeup presentation attacks. In such attacks, an attacker might apply heavy makeup to achieve the facial appearance of a target subject for the purpose of impersonation.In this work, we assess the vulnerability of a widely used open-source face recognition system, i.e. ArcFace, to makeup presentation attacks using the publicly available Makeup Induced Face Spoofing (MIFS) and FRGCv2 databases. It is shown that the success rate of makeup presentation attacks in the MIFS database has negligible impact on the security of the face recognition system. Further, we employ image warping to simulate improved makeup presentation attacks which reveal a significantly higher success rate. Moreover, we propose a makeup attack detection scheme which compares face depth data with face depth reconstructions obtained from RGB images of potential makeup presentation attacks. Significant variations between the two sources of information indicate facial shape alterations induced by strong use of makeup, i.e. potential makeup presentation attacks. Conceptual experiments on the MIFS database confirm the soundness of the presented approach.",2020,2020 8th International Workshop on Biometrics and Forensics (IWBF),,10.1109/IWBF49977.2020.9107961,
285f04ad0270e4a7999e688356ccad3fa4e3e688,1,1,A Comprehensive Study on Loss Functions for Cross-Factor Face Recognition,"A significant progress has been made to face recognition in recent years. The progress includes the advancement of the deep learning solutions and the availability of more challenging databases. As the performance on previous benchmark databases, such as MPIE and LFW, saturates, more challenging databases are emerging and keep driving the development of face recognition technology. The loss function considered in a deep face recognition network plays a critical role for the performance. To better evaluate the state-of-the-art loss functions, we define four challenging factors, including pose, age, occlusion and resolution with specific databases and conduct an extensive experimental study on the latest loss functions. We select the IARPA Janus Benchmark-B (IJB- B) and IARPA Janus Benchmark-C (IJB-C) for pose, the FG-Net Aging Database (FG-Net) for age, the AR Face Database (AR Face) for occlusion, and the Surveillance Cameras Face Database (SCface) for low resolution. The loss functions include the Center Loss, the Marginal Loss, the SphereFace, the CosFace and the ArcFace. Although for most factors, the ArcFace outperforms others. However, the best performance against low-resolution is achieved by the SphereFace. Another attractive finding of this study is that the cross-age performance is the lowest among the four factors with a clear margin. This highlight possible directions for future research.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/cvprw50498.2020.00421,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w48/Hsu_A_Comprehensive_Study_on_Loss_Functions_for_Cross-Factor_Face_Recognition_CVPRW_2020_paper.pdf
2894d830cf1494d9f8121cb8ab1ec085b0729653,0,1,"Fighting COVID-19 with Fever Screening, Face Recognition and Tracing","Since the outbreak of COVID-19 corona virus in late 2019, it has become a tremendous threat to the whole world. Driven by the mission to save lives, we develop a fever screening and tracing system which can detect patients with fever symptom, and identify the patients using face recognition. In addition, our big data AI platform enables the tracing of the patients possible. A real-time alert sent to the personnel on duty on a web or mobile app activates the action to trace the patient and the close contacts providing an effective means to control the spread of the virus.",2020,,,10.1088/1742-6596/1634/1/012085,
28a97ccd99d76913180d7f9a8c9c7da48a49893f,1,0,An Implementation of Face Recognition with Deep Learning based on a Container-Orchestration Platform,"As a lightweight alternative to a virtual machine, a container runs applications only with the necessary environmental variables, libraries, etc. Moreover, many more containers can be run on the same computer compared to traditional VMs, which take up a lot of computing resources. Currently, Docker container and Kubernetes (K8s), which is a container-orchestration platform, are very popular tools. In addition, K8s is a high availability (HA) system with many features that can provide containers to implement more applications. In this project, a face recognition application is implemented with deep learning on Kubeflow, which is a machine learning platform running on K8s. Also, the deep learning method output features instead of classifications. This method computes the distance between two images with Triplet loss function and Euclidean distance. K8s runs on the server as a private cloud, on which our face recognition application runs.",2020,"2020 Indo – Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)",,10.1109/Indo-TaiwanICAN48429.2020.9181343,
28cb8b12af8e10275117c9a86f020ab02f0cffaa,1,1,ArcFace for Disguised Face Recognition,"Even though deep face recognition is extensively explored and remarkable advances have been achieved on large-scale in-the-wild dataset, disguised face recognition receives much less attention. Face feature embedding targeting on intra-class compactness and inter-class discrepancy is very challenging as high intra-class diversity and inter-class similarity are very common on the disguised face recognition dataset. In this report, we give the technical details of our submission to the DFW2019 challenge. By using our RetinaFace for face detection and alignment and ArcFace for face feature embedding, we achieve state-of-the-art performance on the DFW2019 challenge.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00061,http://openaccess.thecvf.com/content_ICCVW_2019/papers/DFW/Deng_ArcFace_for_Disguised_Face_Recognition_ICCVW_2019_paper.pdf
28da38c1ac869a40a107f4be3a4492191fb7a2af,1,0,FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization,,2019,ArXiv,1911.1168,,https://arxiv.org/pdf/1911.11680.pdf
28f02e8668f5b9ed1e192c433506b61b0b53b7a9,0,1,Face Memorization Using AIM Model for Mobile Robot and Its Application to Name Calling Function,"We are developing a social mobile robot that has a name calling function using a face memorization system. It is said that it is an important function for a social robot to call to a person by her/his name, and the name calling can make a friendly impression of the robot on her/him. Our face memorization system has the following features: (1) When the robot detects a stranger, it stores her/his face images and name after getting her/his permission. (2) The robot can call to a person whose face it has memorized by her/his name. (3) The robot system has a sleep–wake function, and a face classifier is re-trained in a REM sleep state, or execution frequencies of information processes are reduced when it has nothing to do, for example, when there is no person around the robot. In this paper, we confirmed the performance of these functions and conducted an experiment to evaluate the impression of the name calling function with research participants. The experimental results revealed the validity and effectiveness of the proposed face memorization system.",2020,Sensors,,10.3390/s20226629,https://pdfs.semanticscholar.org/5e8a/1f07229a0e73a81cd48a0ac3a0d16b198d2a.pdf
28f20ff8915fb1f1f6c5dd2a13fc935c8478f978,0,1,Does Adversarial Transferability Indicate Knowledge Transferability?,"Despite the immense success that deep neural networks (DNNs) have achieved, adversarial examples, which are perturbed inputs that aim to mislead DNNs to make mistakes have recently led to great concern. On the other hand, adversarial examples exhibit interesting phenomena, such as adversarial transferability. DNNs also exhibit knowledge transfer, which is critical to improving learning efficiency and learning in domains that lack high-quality training data. In this paper, we aim to turn the existence and pervasiveness of adversarial examples into an advantage. Given that adversarial transferability is easy to measure while it can be challenging to estimate the effectiveness of knowledge transfer, does adversarial transferability indicate knowledge transferability? We first theoretically analyze the relationship between adversarial transferability and knowledge transferability and outline easily checkable sufficient conditions that identify when adversarial transferability indicates knowledge transferability. In particular, we show that composition with an affine function is sufficient to reduce the difference between two models when adversarial transferability between them is high. We provide empirical evaluation for different transfer learning scenarios on diverse datasets, including CIFAR-10, STL-10, CelebA, and Taskonomy-data - showing a strong positive correlation between the adversarial transferability and knowledge transferability, thus illustrating that our theoretical insights are predictive of practice.",2020,ArXiv,2006.14512,,https://arxiv.org/pdf/2006.14512.pdf
291422441c9a37092ce9bcfbbf2ae7a817dff050,1,1,Dataset Cleaning — A Cross Validation Methodology for Large Facial Datasets using Face Recognition,"In recent years, large “in the wild” face datasets have been released in an attempt to facilitate progress in tasks such as face detection, face recognition, and other tasks. Most of these datasets are acquired from webpages with automatic procedures. As a consequence, noisy data are often found. Furthermore, in these large face datasets, the annotation of identities is important as they are used for training face recognition algorithms. But due to the automatic way of gathering these datasets and due to their large size, many identities folder contain mislabeled samples which deteriorates the quality of the datasets. In this work, it is presented a semi-automatic method for cleaning the noisy large face datasets with the use of face recognition. This methodology is applied to clean the CelebA dataset and show its effectiveness. Furthermore, the list with the mislabelled samples in the CelebA dataset is made available.",2020,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),2003.10815,10.1109/QoMEX48832.2020.9123123,https://arxiv.org/pdf/2003.10815.pdf
29485ce08293066bb29f742f4267e33ca27107cf,1,0,Deep Speaker Embedding for Speaker-Targeted Automatic Speech Recognition,"In this work, we investigate three types of deep speaker embedding as text-independent features for speaker-targeted speech recognition in cocktail party environments. The text-independent speaker embedding is extracted from the target speaker's existing speech segment (i-vector and x-vector) or face image (f-vector), which is concatenated with acoustic features of any new speech utterances as input features. Since the proposed model extracts the speaker embedding of the target speaker once and for all, it is computationally more efficient than many prior approaches which estimate the target speaker's characteristics on the fly. Empirical evaluation shows that using speaker embedding along with acoustic features improves Word Error Rate over the audio-only model, from 65.7% to 29.5%. Among the three types of speaker embedding, x-vector and f-vector show robustness against environment variations while i-vector tends to overfit to the specific speaker and environment condition.",2019,ICNLP 2019,,10.1145/3342827.3342847,
2965f2ad6b021554b244e1dd2e3f0097dbf47162,0,1,Authnet: Biometric Authentication Through Adversarial Learning,"We present AuthNet: a generic framework for biometric authentication, based on adversarial neural networks. Differently from other methods, AuthNet maps input biometric traits onto a regularized space in which well-behaved regions, learned by means of an adversarial game, convey the semantic meaning of authorized and unauthorized users. This enables the use of simple boundaries in order to discriminate among the two classes. The novel approach of learning the mapping regularized by target distributions instead of the boundaries further avoids the problem encountered in typical classifiers for which the learnt boundaries may be complex and difficult to analyze. With extensive experiments on publicly available datasets, it is illustrated that the AuthNet performance in terms of security metrics such as accuracy, Equal Error Rate (EER), False Acceptance Rate (FAR) and Genuine Acceptance Rate (GAR) is superior compared to other methods which confirms the effectiveness of the proposed method.",2019,2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP),,10.1109/MLSP.2019.8918810,
296d776efe62a90a7132ce196f67d4f89c92dc27,0,1,Graph-Based Social Relation Reasoning,"Human beings are fundamentally sociable -- that we generally organize our social lives in terms of relations with other people. Understanding social relations from an image has great potential for intelligent systems such as social chatbots and personal assistants. In this paper, we propose a simpler, faster, and more accurate method named graph relational reasoning network (GR2N) for social relation recognition. Different from existing methods which process all social relations on an image independently, our method considers the paradigm of jointly inferring the relations by constructing a social relation graph. Furthermore, the proposed GR2N constructs several virtual relation graphs to explicitly grasp the strong logical constraints among different types of social relations. Experimental results illustrate that our method generates a reasonable and consistent social relation graph and improves the performance in both accuracy and efficiency.",2020,ECCV,2007.07453,10.1007/978-3-030-58555-6_2,https://arxiv.org/pdf/2007.07453.pdf
299ee0aa0d299fcd5f37ffc97e8850952eebec23,1,0,Self-Supervised Representation Learning From Videos for Facial Action Unit Detection,"In this paper, we aim to learn discriminative representation for facial action unit (AU) detection from large amount of videos without manual annotations. Inspired by the fact that facial actions are the movements of facial muscles, we depict the movements as the transformation between two face images in different frames and use it as the self-supervisory signal to learn the representations. However, under the uncontrolled condition, the transformation is caused by both facial actions and head motions. To remove the influence by head motions, we propose a Twin-Cycle Autoencoder (TCAE) that can disentangle the facial action related movements and the head motion related ones. Specifically, TCAE is trained to respectively change the facial actions and head poses of the source face to those of the target face. Our experiments validate TCAE's capability of decoupling the movements. Experimental results also demonstrate that the learned representation is discriminative for AU detection, where TCAE outperforms or is comparable with the state-of-the-art self-supervised learning methods and supervised AU detection methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.01118,http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Self-Supervised_Representation_Learning_From_Videos_for_Facial_Action_Unit_Detection_CVPR_2019_paper.pdf
29a1b2395f38626e3225d214649221f6e599484c,1,1,Significant Feature Based Representation for Template Protection,"The security of biometric templates is of paramount importance. Leakage of biometric information may result in loss of private data and can lead to the compromise of the biometric system. Yet, the security of templates is often overlooked in favour of performance. In this paper, we present a plug-and-play framework for creating secure face templates with negligible degradation in the performance of the system. We propose a significant bit based representation which guarantees security in addition to other biometric aspects such as cancelability and reproducibility. In addition to being scalable, the proposed method does not make unrealistic assumptions regarding the pose or illumination of the face images. We provide experimental results on two unconstrained datasets - IJB-A and IJB-C.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00293,http://openaccess.thecvf.com/content_CVPRW_2019/papers/Biometrics/Mohan_Significant_Feature_Based_Representation_for_Template_Protection_CVPRW_2019_paper.pdf
29aad3d6ae4018381ab90bda636ff8475c584003,0,1,Multi-Modality Matters: A Performance Leap on VoxCeleb,"The information from different modalities usually compensates each other. In this paper, we use the audio and visual data in VoxCeleb dataset to do person verification. We explored different information fusion strategies and loss functions for the audio-visual person verification system at the embedding level. System performance is evaluated using the public trail lists on VoxCeleb1 dataset. Our best system using audio-visual knowledge at the embedding level achieves 0.585%, 0.427% and 0.735% EER on the three official trial lists of VoxCeleb1, which are the best reported results on this dataset. Moreover, to imitate more complex test environment with one modality corrupted or missing, we construct a noisy evaluation set based on VoxCeleb1 dataset. We use a data augmentation strategy at the embedding level to help our audio-visual system to distinguish the noisy and the clean embedding. With such data augmented strategy, the proposed audio-visual person verification system is more robust on the noisy evaluation set.",2020,INTERSPEECH,,10.21437/interspeech.2020-2229,https://isca-speech.org/archive/Interspeech_2020/pdfs/2229.pdf
29abbaef73524207732d2a5091aec2f8712425c9,0,1,The TalTech Systems for the Short-Duration Speaker Verification Challenge 2020,"This paper presents the Tallinn University of Technology systems submitted to the Short-duration Speaker Verification Challenge 2020. The challenge consists of two tasks, focusing on text-dependent and text-independent speaker verification with some cross-lingual aspects. We used speaker embedding models that consist of squeeze-and-attention based residual layers, multi-head attention and either cross-entropy-based or additive angular margin based objective function. In order to encourage the model to produce language-independent embeddings, we trained the models in a multi-task manner, using dataset specific output layers. In the text-dependent task we employed a phrase classifier to reject trials with non-matching phrases. In the text-independent task we used a language classifier to boost the scores of trials where the language of the test and enrollment utterances does not match. Our final primary metric score was 0.075 in Task 1 (ranked as 6th) and 0.118 in Task 2 (rank 8).",2020,INTERSPEECH,,10.21437/interspeech.2020-2233,https://isca-speech.org/archive/Interspeech_2020/pdfs/2233.pdf
29f504e447033fc92eb72a54d7b133b3db519712,0,1,Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse Dataset,"The Google-Landmarks-v2 dataset is the biggest worldwide landmarks dataset characterized by a large magnitude of noisiness and diversity. We present a novel landmark retrieval/recognition system, robust to a noisy and diverse dataset, by our team, smlyaka. Our approach is based on deep convolutional neural networks with metric learning, trained by cosine-softmax based losses. Deep metric learning methods are usually sensitive to noise, and it could hinder to learn a reliable metric. To address this issue, we develop an automated data cleaning system. Besides, we devise a discriminative re-ranking method to address the diversity of the dataset for landmark retrieval. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google Landmark Recognition 2019 challenge on Kaggle.",2019,ArXiv,1906.04087,,https://arxiv.org/pdf/1906.04087.pdf
2a95b6240c13988691f83079f47c2334f4ba3f77,0,1,Consent-based Privacy-preserving Decision Tree Evaluation,"Decision trees are prevalent machine learning models used for data classification. Cloud servers can build their decision tree models and provide users with many classification services, such as remote medical diagnosis. Moreover, users would also like to share the classification results with third-party applications for customized services. For example, the medical diagnosis results can be further utilized by a nutrition application to provide users with dietary recommendations. However, as stringent privacy regulations of personal data, such as GDPR, takes effect, the decision tree evaluation must comply with the following requirements. First, the model parameters and user data (input and output) should be protected from public disclosure. Second, different applications should obtain the classification results with users’ consent in the context of user-customised services. In this paper, we propose a consent-based privacy-preserving decision tree evaluation scheme, named CPDE. Specifically, to achieve model parameter privacy and user data privacy, the original decision tree evaluation is conducted in a private manner in CPDE. As a result, all operations can be performed in the encrypted domain using an additively homomorphic encryption primitive and a secure comparison protocol. In addition, by integrating a proxy re-encryption technique, CDPE enables user-authorized applications to obtain the user’s classification results even if the user is offline. The security analysis shows that CPDE achieves the desirable security properties and performance evaluation demonstrates CPDE is efficient and is suitable for real-world implementations.",2020,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),,10.1109/ICC40277.2020.9149181,
2a9e9a31dd47aadc7d039c0d2aff95cefad20483,1,0,Enhancing Face Recognition from Massive Weakly Labeled Data of New Domains,"Training data are critical in face recognition systems. Labeling a large scale dataset for a particular domain needs lots of manpower. Without dataset related to current face recognition domain, we can’t get a strong face recognition model with existing public datasets. In this paper, we propose a semi-supervised method to automatically construct strong dataset which can be trained to achieve better performance on the target domain from massive weakly labeled data. In the case of Asian face recognition, a well trained VRCN model by CASIA, which achieves 98.63% on LFW and 91.76% on YTF, only achieves 88.53% recognition rate on our test dataset of Asian faces. We collect 530,560 weakly labeled Asian face images of 7962 identities, and get a cleaned dataset of size 285,933. Model trained by the cleaned dataset with VRCN network and same strategy achieves 95.33% recognition rate on the Asian face test dataset (6.8% improved).",2018,Neural Processing Letters,,10.1007/s11063-018-9839-z,
2ad5f836e1d9876fa3fc53cb2c0a704b45988f0f,0,1,Negative Margin Matters: Understanding Margin in Few-shot Classification,"This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles. These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid falsely mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at this https URL.",2020,ECCV,2003.1206,10.1007/978-3-030-58548-8_26,https://arxiv.org/pdf/2003.12060.pdf
2b01e6297137e0eab7d69f4e4f9683e0b06030ae,1,1,Identity Document to Selfie Face Matching Across Adolescence,"Matching live images (``selfies'') to images from ID documents is a problem that can arise in various applications. A challenging instance of the problem arises when the face image on the ID document is from early adolescence and the live image is from later adolescence. We explore this problem using a private dataset called Chilean Young Adult (CHIYA) dataset, where we match live face images taken at age 18-19 to face images on ID documents created at ages 9 to 18. State-of-the-art deep learning face matchers (e.g., ArcFace) have relatively poor accuracy for document-to-selfie face matching. To achieve higher accuracy, we fine-tune the best available open-source model with triplet loss for a few-shot learning. Experiments show that our approach achieves higher accuracy than the DocFace+ model recently developed for this problem. Our fine-tuned model was able to improve the true acceptance rate for the most difficult (largest age span) subset from 62.92% to 96.67% at a false acceptance rate of 0.01%. Our fine-tuned model is available for use by other researchers.",2019,ArXiv,1912.10021,,https://arxiv.org/pdf/1912.10021.pdf
2b26f7b318bc69b353dc8e589a8fbc9ddf85648b,0,1,HASeparator: Hyperplane-Assisted Softmax,"Efficient feature learning with Convolutional Neural Networks (CNNs) constitutes an increasingly imperative property since several challenging tasks of computer vision tend to require cascade schemes and modalities fusion. Feature learning aims at CNN models capable of extracting embeddings, exhibiting high discrimination among the different classes, as well as intra-class compactness. In this paper, a novel approach is introduced that has separator, which focuses on an effective hyperplane-based segregation of the classes instead of the common class centers separation scheme. Accordingly, an innovatory separator, namely the Hyperplane-Assisted Softmax separator (HASeparator), is proposed that demonstrates superior discrimination capabilities, as evaluated on popular image classification benchmarks.",2020,ArXiv,2008.03539,,https://arxiv.org/pdf/2008.03539.pdf
2b2acf2de016f0fb3538ceaaf3a9ba869b466089,1,0,Finding your Lookalike: Measuring Face Similarity Rather than Face Identity,"Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks. Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined. In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces. That is, we predict the perceived similarity between facial images, given that they are not of the same person. Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation. Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another. In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers. We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks. We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1806.05252,10.1109/CVPRW.2018.00311,https://arxiv.org/pdf/1806.05252.pdf
2b87bd8f5d5d6db21103f0cf7a79e3ca85fb1218,1,0,Cross-Domain Face Presentation Attack Detection via Multi-Domain Disentangled Representation Learning,"Face presentation attack detection (PAD) has been an urgent problem to be solved in the face recognition systems. Conventional approaches usually assume the testing and training are within the same domain; as a result, they may not generalize well into unseen scenarios because the representations learned for PAD may overfit to the subjects in the training set. In light of this, we propose an efficient disentangled representation learning for cross-domain face PAD. Our approach consists of disentangled representation learning (DR-Net) and multi-domain learning (MD-Net). DR-Net learns a pair of encoders via generative models that can disentangle PAD informative features from subject discriminative features. The disentangled features from different domains are fed to MD-Net which learns domain-independent features for the final cross-domain face PAD task. Extensive experiments on several public datasets validate the effectiveness of the proposed approach for cross-domain PAD.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.01959,10.1109/cvpr42600.2020.00671,https://arxiv.org/pdf/2004.01959.pdf
2b964fa2028ee01ce279368cb6c29b96e37aa543,1,0,Clustering based Contrastive Learning for Improving Face Representations,"A good clustering algorithm can discover natural groupings in data. These groupings, if used wisely, provide a form of weak supervision for learning representations. In this work, we present Clustering-based Contrastive Learning (CCL), a new clustering-based representation learning approach that uses labels obtained from clustering along with video constraints to learn discriminative face features. We demonstrate our method on the challenging task of learning representations for video face clustering. Through several ablation studies, we analyze the impact of creating pair-wise positive and negative labels from different sources. Experiments on three challenging video face clustering datasets: BBT-0101, BF-0502, and ACCIO show that CCL achieves a new state-of-the-art on all datasets.",2020,ArXiv,2004.02195,,https://arxiv.org/pdf/2004.02195.pdf
2ba3e74f850396292e9e7c6aae62825fa2eda260,0,1,$\sigma^2$R Loss: a Weighted Loss by Multiplicative Factors using Sigmoidal Functions,"In neural networks, the loss function represents the core of the learning process that leads the optimizer to an approximation of the optimal convergence error. Convolutional neural networks (CNN) use the loss function as a supervisory signal to train a deep model and contribute significantly to achieving the state of the art in some fields of artificial vision. Cross-entropy and Center loss functions are commonly used to increase the discriminating power of learned functions and increase the generalization performance of the model. Center loss minimizes the class intra-class variance and at the same time penalizes the long distance between the deep features inside each class. However, the total error of the center loss will be heavily influenced by the majority of the instances and can lead to a freezing state in terms of intra-class variance. To address this, we introduce a new loss function called sigma squared reduction loss ( 2R loss), which is regulated by a sigmoid function to inflate/deflate the error per instance and then continue to reduce the intra-class variance. Our loss has clear intuition and geometric interpretation, furthermore, we demonstrate by experiments the effectiveness of our proposal on several benchmark datasets showing the intra-class variance reduction and overcoming the results obtained with center loss and soft nearest neighbour functions.",2020,,2009.08796,,https://arxiv.org/pdf/2009.08796.pdf
2bbe728c7079f46769990236ead74056df105515,1,0,Dual Adversarial Disentanglement and Deep Representation Decorrelation for NIR-VIS Face Recognition,"The task of near-infrared and visual (NIR-VIS) face recognition refers to matching face data from different modalities, which has broad application prospects in areas such as multimedia information retrieval and criminal investigation. However, it remains a challenging task due to high intra-class variations and small-scale NIR-VIS dataset. In this paper, we propose a novel approach called Dual Adversarial Disentanglement and deep Representation Decorrelation (DADRD) to solve the NIR-VIS matching problem. In order to reduce the gap between NIR-VIS images, three key components are designed for DADRD model, including Cross-modal Margin (CmM) loss, Dual Adversarial Disentangled Variations (DADV) and Deep Representation Decorrelation (DRD). Firstly, the CmM loss captures within- and between-class information of the data, and it further reduces modality difference by a center-variation item. Secondly, the Mixed Facial Representation (MFR) layer of the backbone network is divided into three parts: the identity-related layer, the modality-related layer and the residual-related layer. The DADV is designed to reduce the intra-class variations, which consists of Adversarial Disentangled Modality Variations (ADMV) and Adversarial Disentangled Residual Variations (ADRV). Specifically, the ADMV and ADRV aim at eliminating spectrum variations and residual variations (i.e., lighting, pose, expression, occlusion, etc) respectively via an adversarial mechanism. Finally, we impose a DRD on the three decomposed features to make them irrelevant to each other, which can more effectively separate the three component information and enhance feature representations. In particular, we develop a Joint Three-stage Optimization (JTsO) strategy to effectively optimize the network. The joint formulation leads to the purification of identity information and the disentanglement of within-class variation information. Extensive experiments have been carried out on three challenging datasets, and the results demonstrate the effectiveness of our method.",2021,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.3005314,
2bce4c38f6e1be70d80505671cf44b2b164efd93,0,1,Research on Video Face Retrieval Method Based on Deep Learning and Key Frame,"In order to improve the accuracy and speed of face retrieval in video, this paper proposes a video face retrieval method based on deep learning and key frames. The method firstly uses the inter-frame difference method to extract the key frames of the retrieved video, and then uses MTCNN's face detection and alignment and FaceNet's face recognition function to perform face recognition on key frames to determine whether the retrieved person is included. At the same time, according to the characteristics of video continuity, the retrieval results are specially processed. The experimental results show that the accuracy of the proposed method reaches 99.65%, and the retrieval speed can reach 68FPS.",2020,,,10.1145/3408127.3408199,
2be87b5d2b61ea629e9ffd07a23c9ee8c4b0cc3e,0,1,A Backbone Replaceable Fine-tuning Network for Stable Face Alignment,"Heatmap regression based face alignment has achieved prominent performance on static images. However, the stability and accuracy are remarkably discounted when applying the existing methods on dynamic videos. We attribute the degradation to random noise and motion blur, which are common in videos. The temporal information is critical to address this issue yet not fully considered in the existing works. In this paper, we visit the video-oriented face alignment problem in two perspectives: detection accuracy prefers lower error for a single frame, and detection consistency forces better stability between adjacent frames. On this basis, we propose a Jitter loss function that leverages temporal information to suppress inaccurate as well as jittered landmarks. The Jitter loss is involved in a novel framework with a fine-tuning ConvLSTM structure over a backbone replaceable network. We further demonstrate that accurate and stable landmarks are associated with different regions with overlaps in a canonical coordinate, based on which the proposed Jitter loss facilitates the optimization process during training. The proposed framework achieves at least 40% improvement on stability evaluation metrics while enhancing detection accuracy versus state-of-the-art methods. Generally, it can swiftly convert a landmark detector for facial images to a better-performing one for videos without retraining the entire model.",2020,ArXiv,,,
2bf9cc54e48c58c2dc0394f05215c86895912cef,0,1,WHAT IDENTIFIES A WHALE BY ITS FLUKE? ON THE BENEFIT OF INTERPRETABLE MACHINE LEARNING FOR WHALE IDENTIFICATION,"Abstract. Interpretable and explainable machine learning have proven to be promising approaches to verify the quality of a data-driven model in general as well as to obtain more information about the quality of certain observations in practise. In this paper, we use these approaches for an application in the marine sciences to support the monitoring of whales. Whale population monitoring is an important element of whale conservation, where the identification of whales plays an important role in this process, for example to trace the migration of whales over time and space. Classical approaches use photographs and a manual mapping with special focus on the shape of the whale flukes and their unique pigmentation. However, this is not feasible for comprehensive monitoring. Machine learning methods, especially deep neural networks, have shown that they can efficiently solve the automatic observation of a large number of whales. Despite their success for many different tasks such as identification, further potentials such as interpretability and their benefits have not yet been exploited. Our main contribution is an analysis of interpretation tools, especially occlusion sensitivity maps, and the question of how the gained insights can help a whale researcher. For our analysis, we use images of humpback whale flukes provided by the Kaggle Challenge ”Humpback Whale Identification”. By means of spectral cluster analysis of heatmaps, which indicate which parts of the image are important for a decision, we can show that the they can be grouped in a meaningful way. Moreover, it appears that characteristics automatically determined by a neural network correspond to those that are considered important by a whale expert.",2020,,,10.5194/isprs-annals-v-2-2020-1005-2020,https://pdfs.semanticscholar.org/464e/466bbdf83eda7507208082a38d03ed26b951.pdf
2c2c31cf8ad0d68549a68e856c5ae723ad9482b3,1,1,Learning Meta Face Recognition in Unseen Domains,"Face recognition systems are usually faced with unseen domains in real-world applications and show unsatisfactory performance due to their poor generalization. For example, a well-trained model on webface data cannot deal with the ID vs. Spot task in surveillance scenario. In this paper, we aim to learn a generalized model that can directly handle new unseen domains without any model updating. To this end, we propose a novel face recognition method via meta-learning named Meta Face Recognition (MFR). MFR synthesizes the source/target domain shift with a meta-optimization objective, which requires the model to learn effective representations not only on synthesized source domains but also on synthesized target domains. Specifically, we build domain-shift batches through a domain-level sampling strategy and get back-propagated gradients/meta-gradients on synthesized source/target domains by optimizing multi-domain distributions. The gradients and meta-gradients are further combined to update the model to improve generalization. Besides, we propose two benchmarks for generalized face recognition evaluation. Experiments on our benchmarks validate the generalization of our method compared to several baselines and other state-of-the-arts. The proposed benchmarks and code will be available at https://github.com/cleardusk/MFR.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.07733,10.1109/cvpr42600.2020.00620,https://arxiv.org/pdf/2003.07733.pdf
2cb1b0f03aa2cb4b974e1aa61ada30a2097a0d44,1,0,Training a Multimodal Neural Networkto Determine the Authenticity of Images,"The identification of attempts to substitute images plays an important role in protecting biometric systems (authorization in mobile devices, access control systems for premises, terminals with automatic access by face recognition, etc.). This study presents a new method for detecting falsified images based on processing the multimodal data from a camera. A new neural network architecture is developed that aggregates the features from different modalities at all levels of the model. The separation of the training sample for different types of attacks and the initialization of the model with attributes trained in other tasks that are associated with facial images are considered. Numerical experiments on real data are performed, showing the successful performance of the system. The proposed model won first place in the CASIA-SURF competition for the recognition of falsified facial images.",2020,,,10.1134/s1064230720040073,
2cbd17e6b6ded93c1d80c4a30014113285e1f3c4,0,1,Small footprint Text-Independent Speaker Verification for Embedded Systems,"Deep neural network approaches to speaker verification have proven successful, but typical computational requirements of State-Of-The-Art (SOTA) systems make them unsuited for embedded applications. In this work, we present a two-stage model architecture orders of magnitude smaller than common solutions (237.5K learning parameters, 11.5MFLOPS) reaching a competitive result of 3.31% Equal Error Rate (EER) on the well established VoxCeleb1 verification test set. We demonstrate the possibility of running our solution on small devices typical of IoT systems such as the Raspberry Pi 3B with a latency smaller than 200ms on a 5s long utterance. Additionally, we evaluate our model on the acoustically challenging VOiCES corpus. We report a limited increase in EER of 2.6 percentage points with respect to the best scoring model of the 2019 VOiCES from a Distance Challenge, against a reduction of 25.6 times in the number of learning parameters.",2020,ArXiv,2011.01709,,https://arxiv.org/pdf/2011.01709.pdf
2cc73b3292fe36731d62e88e5fb653b8ace5b1a7,0,1,Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition,"The audio-video based emotion recognition aims to classify a given video into basic emotions. In this paper, we describe our approaches in EmotiW 2019, which mainly explores emotion features and feature fusion strategies for audio and visual modality. For emotion features, we explore audio feature with both speech-spectrogram and Log Mel-spectrogram and evaluate several facial features with different CNN models and different emotion pretrained strategies. For fusion strategies, we explore intra-modal and cross-modal fusion methods, such as designing attention mechanisms to highlights important emotion feature, exploring feature concatenation and factorized bilinear pooling (FBP) for cross-modal feature fusion. With careful evaluation, we obtain 65.5% on the AFEW validation set and 62.48% on the test set and rank third in the challenge.",2019,ICMI,,10.1145/3340555.3355713,
2d5829f085b55c7a5ea7878ffec44dcbe40188f2,1,0,Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition,"A recent trend to recognize facial expressions in the real-world scenario is to deploy attention based convolutional neural networks (CNNs) locally to signify the importance of facial regions and, combine it with global facial features and/or other complementary context information for performance gain. However, in the presence of occlusions and pose variations, different channels respond differently, and further that the response intensity of a channel differ across spatial locations. Also, modern facial expression recognition(FER) architectures rely on external sources like landmark detectors for defining attention. Failure of landmark detector will have a cascading effect on FER. Additionally, there is no emphasis laid on the relevance of features that are input to compute complementary context information. Leveraging on the aforementioned observations, an end-to-end architecture for FER is proposed in this work that obtains both local and global attention per channel per spatial location through a novel spatio-channel attention net (SCAN), without seeking any information from the landmark detectors. SCAN is complemented by a complementary context information (CCI) branch. Further, using efficient channel attention (ECA), the relevance of features input to CCI is also attended to. The representation learnt by the proposed architecture is robust to occlusions and pose variations. Robustness and superior performance of the proposed model is demonstrated on both in-lab and in-the-wild datasets (AffectNet, FERPlus, RAF-DB, FED-RO, SFEW, CK+, Oulu-CASIA and JAFFE) along with a couple of constructed face mask datasets resembling masked faces in COVID-19 scenario. Codes are publicly available at this https URL",2020,ArXiv,2007.10298,,https://arxiv.org/pdf/2007.10298.pdf
2d67c2440abf728433617e266a706bfe94ff63ce,1,1,Discriminative metric learning for face verification using enhanced Siamese neural network,"Although face verification algorithms have made great success under controlled conditions in recent years, there’s plenty of room at its performance under uncontrolled real-world due to lack of discriminative feature representation ability. From the perspective of metric learning, we proposed a context-aware based Siamese neural network (CASNN) to learn a simple yet powerful network for face verification task to enhance its discriminative feature representation ability. Firstly, a context-aware module is used to automatically focus on the key area of the input facial images without irrelevant background area. Then we design a Siamese network equipped with center-classification loss to compress intra-class features and enlarge between-class ones for discriminative metric learning. Finally, we propose a quantitative indicator named “D-score” to show the discriminative representation ability of the learnt features from different methods. The extensive experiments are conducted on LFW dataset, YouTube Face dataset (YTF) and real-world dataset. The results confirm that CASNN outperforms some state-of-the-art deep learning-based face verification methods.",2020,,,10.1007/s11042-020-09784-8,
2d89bc8037bc6148792566b636b94099ef44d5bf,0,1,Person Recognition in Personal Photo Collections,"People nowadays share large parts of their personal lives through social media. Being able to automatically recognise people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task, however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social media photos sets new challenges for computer vision, including non-cooperative subjects (e.g., backward viewpoints, unusual poses) and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1] benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social groups, and events. Compared the conference version of the paper [2] , this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3] ), (2) new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the head viewpoint-wise breakdown of performance, and (5) results on the open-world setup.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2018.2877588,
2dd5b397c86da5bc916b1a0c5b96d9a87ab8e52d,1,1,Caption-Supervised Face Recognition: Training a State-of-the-Art Face Model Without Manual Annotation,,2020,ECCV,,10.1007/978-3-030-58520-4_9,
2e0d56794379c436b2d1be63e71a215dd67eb2ca,1,0,Improving precision and recall of face recognition in SIPP with combination of modified mean search and LSH,"Although face recognition has been improved much as the development of Deep Neural Networks, SIPP(Single Image Per Person) problem in face recognition has not been better solved, especially in practical applications where searching over complicated database. In this paper, a combination of modified mean search and LSH method would be introduced orderly to improve the precision and recall of SIPP face recognition without retrain of the DNN model. First, a modified SVD based augmentation method would be introduced to get more intra-class variations even for person with only one image. Second, an unique rule based combination of modified mean search and LSH method was proposed the first time to help get the most similar personID in a complicated dataset, and some theoretical explaining followed. Third, we would like to emphasize, no need to retrain of the DNN model and would easy to be extended without much efforts. We do some practical testing in competition of Msceleb challenge-2 2017 which was hold by Microsoft Research, great improvement of coverage from 13.39% to 19.25%, 29.94%, 42.11%, 47.52% at precision 99%(P99) would be shown latter, coverage reach 94.2% and 100% at precision 97%(P97) and 95%(P95) respectively. As far as we known, this is the only paper who do not fine-tuning on competition dataset and ranked top-10. A similar test on CASIA WebFace dataset also demonstrated the same improvements on both precision and recall.",2017,,,,https://pdfs.semanticscholar.org/2e0d/56794379c436b2d1be63e71a215dd67eb2ca.pdf
2e2534eb1f1af5c043a009391f0ef955aeed2044,0,1,Switchable Normalization for Learning-to-Normalize Deep Representation,"We address a learning-to-normalize problem by proposing Switchable Normalization (SN), which learns to select different normalizers for different normalization layers of a deep neural network. SN employs three distinct scopes to compute statistics (means and variances) including a channel, a layer, and a minibatch. SN switches between them by learning their importance weights in an end-to-end manner. It has several good properties. First, it adapts to various network architectures and tasks. Second, it is robust to a wide range of batch sizes, maintaining high performance even when small minibatch is presented (eg 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike group normalization that searches the number of groups as a hyper-parameter. Without bells and whistles, SN outperforms its counterparts on various challenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace and Kinetics. Analyses of SN are also presented to answer the following three questions: (a) Is it useful to allow each normalization layer to select its own normalizer? (b) What impacts the choices of normalizers? (c) Do different tasks and datasets prefer different normalizers? We hope SN will help ease the usage and understand the normalization techniques in deep learning. The code of SN has been released at https://github.com/switchablenorms.",2019,IEEE transactions on pattern analysis and machine intelligence,1907.10473,10.1109/TPAMI.2019.2932062,https://arxiv.org/pdf/1907.10473.pdf
2e7124ea9264430a10491cb6f4b57fab62559e70,1,0,A Discriminative Multi‐Channel Facial Shape (MCFS) Representation and Feature Extraction for 3D Human Faces,"Building an effective representation for 3D face geometry is essential for face analysis tasks, that is, landmark detection, face recognition and reconstruction. This paper proposes to use a Multi‐Channel Facial Shape (MCFS) representation that consists of depth, hand‐engineered feature and attention maps to construct a 3D facial descriptor. And, a multi‐channel adjustment mechanism, named filtered squeeze and reversed excitation (FSRE), is proposed to re‐organize MCFS data. To assign a suitable weight for each channel, FSRE is able to learn the importance of each layer automatically in the training phase. MCFS and FSRE blocks collaborate together effectively to build a robust 3D facial shape representation, which has an excellent discriminative ability. Extensive experimental results, testing on both high‐resolution and low‐resolution face datasets, show that facial features extracted by our framework outperform existing methods. This representation is stable against occlusions, data corruptions, expressions and pose variations. Also, unlike traditional methods of 3D face feature extraction, which always take minutes to create 3D features, our system can run in real time.",2020,,,10.1111/cgf.13904,
2e71ead0ca8d5583c99f311e8bf5403995f468ac,1,1,Threat of Adversarial Attacks on Face Recognition: A Comprehensive Survey,"Face recognition (FR) systems have demonstrated outstanding verification performance, suggesting suitability for real-world applications, ranging from photo tagging in social media to automated border control (ABC). In an advanced FR system with deep learning-based architecture, however, promoting the recognition efficiency alone is not sufficient and the system should also withstand potential kinds of attacks designed to target its proficiency. Recent studies show that (deep) FR systems exhibit an intriguing vulnerability to imperceptible or perceptible but natural-looking adversarial input images that drive the model to incorrect output predictions. In this article, we present a comprehensive survey on adversarial attacks against FR systems and elaborate on the competence of new countermeasures against them. Further, we propose a taxonomy of existing attack and defense strategies according to different criteria. Finally, we compare the presented approaches according to techniques' characteristics.",2020,ArXiv,2007.11709,,https://arxiv.org/pdf/2007.11709.pdf
2ea0c9bc4276d3c4a1ce637c012a3f0d855e962d,0,1,Joint Optimization of Classification and Clustering for Deep Speaker Embedding,"This paper proposes a method to train deep speaker embed-dings end-to-end that jointly optimizes classification and clustering. A large margin softmax loss is used to reduce classification errors. A novel large margin Gaussian mixture loss is proposed to improve clustering. With the joint optimization, the learned embeddings capture segment-level acoustic representation from variable-length speech segments to discriminate between speakers and to replicate densities of speaker clusters. We compare performance with alternative methods on large-scale text-independent speaker recognition dataset VoxCeleb1 [1] and observe that it outperforms those methods significantly, achieving new state-of-the-art results on the dataset. Moreover, because of the joint optimization, this method exhibits faster and better convergence than using classification loss alone. Our results suggest great potential of joint optimization of classification and clustering for speaker verification and identification.",2019,2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),,10.1109/ASRU46091.2019.9003860,
2f6105fbf8d6bcee8c25d6a0cca1c6dec0aaa12e,0,1,Mutual Information Maximization on Disentangled Representations for Differential Morph Detection,"In this paper, we present a novel differential morph detection framework, utilizing landmark and appearance disentanglement. In our framework, the face image is represented in the embedding domain using two disentangled but complementary representations. The network is trained by triplets of face images, in which the intermediate image inherits the landmarks from one image and the appearance from the other image. This initially trained network is further trained for each dataset using contrastive representations. We demonstrate that, by employing appearance and landmark disentanglement, the proposed framework can provide state-of-the-art differential morph detection performance. This functionality is achieved by the using distances in landmark, appearance, and ID domains. The performance of the proposed framework is evaluated using three morph datasets generated with different methodologies.",2020,ArXiv,2012.01542,,https://arxiv.org/pdf/2012.01542.pdf
2f75f27ef8f239489437dfe48bc29afc3b24bd71,0,1,Simulation of Print-Scan Transformations for Face Images based on Conditional Adversarial Networks,"In many countries, printing and scanning of face images is frequently performed as part of the issuance process of electronic travel documents, e.g., ePassports. Image alterations induced by such print-scan transformations may negatively effect the performance of various biometric sub-systems, in particular image manipulation detection. Consequently, according training data is needed in order to achieve robustness towards said transformations. However, manual printing and scanning is time-consuming and costly.In this work, we propose a simulation of print-scan transformations for face images based on a Conditional Generative Adversarial Network (cGAN). To this end, subsets of two public face databases are manually printed and scanned using different printer-scanner combinations. A cGAN is then trained to perform an image-to-image translation which simulates the corresponding print-scan transformations. The goodness of simulation is evaluated with respect to image quality, biometric sample quality and performance, as well as human assessment.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
2f861c910d1ce56733c2b2fc299b8d3d443c7c08,1,0,Reliable Age and Gender Estimation from Face Images: Stating the Confidence of Model Predictions,"Automated age and gender estimation became of great importance for many potential applications ranging from forensics to social media. Although previous works reported high increased performances, these solutions tend to mispredict under challenging conditions or when the trained model faces a sample that was underrepresented in the training data. In this work, we propose an age and gender estimation model, as well as a novel reliability measure to quantify the confidence of the model’s prediction. Our solution is based on stochastic forward passes through dropout-reduced neural networks that were theoretically proven to approximate Gaussian processes. By utilizing multiple stochastic forward passes, the centrality and dispersion of these predictions are used to derive a confidence statement about the prediction. Experiments were conducted on the Adience benchmark. We showed that the proposed solution reached and exceeded state-of-the-art performance. Further, we demonstrated that the proposed reliability measure correlates with the prediction performance and thus, is highly successful in quantifying the prediction reliability.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",,10.1109/BTAS46853.2019.9185975,
2fa6cec9a6686468cfe2c0dc6045ee6a940beb83,1,1,SecureFace: Face Template Protection,"It has been shown that face images can be reconstructed from their representations (templates). We propose a randomized CNN to generate protected face biometric templates given the input face image and a user-specific key. The use of user-specific keys introduces randomness to the secure template and hence strengthens the template security. To further enhance the security of the templates, instead of storing the key, we store a secure sketch that can be decoded to generate the key with genuine queries submitted to the system. We have evaluated the proposed protected template generation method using three benchmarking datasets for the face (FRGC v2.0, CFP, and IJB-A). The experimental results justify that the protected template generated by the proposed method are non-invertible and cancellable, while preserving the verification performance.",2021,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.3009590,
2fa8eb9cbcac01b90dbe50cfbc03320d68d30c47,1,0,Mutual Component Convolutional Neural Networks for Heterogeneous Face Recognition,"Heterogeneous face recognition (HFR) aims to identify a person from different facial modalities, such as visible and near-infrared images. The main challenges of HFR lie in the large modality discrepancy and insufficient training samples. In this paper, we propose the mutual component convolutional neural network (MC-CNN), a modal-invariant deep learning framework, to tackle these two issues simultaneously. Our MC-CNN incorporates a generative module, i.e., the mutual component analysis (MCA), into modern deep CNNs by viewing MCA as a special fully connected (FC) layer. Based on deep features, this FC layer is designed to extract modal-independent hidden factors and is updated according to maximum likelihood analytic formulation instead of back propagation which prevents overfitting from limited data naturally. In addition, we develop an MCA loss to update the network for modal-invariant feature learning. Extensive experiments show that our MC-CNN outperforms several fine-tuned baseline models significantly. Our methods achieve the state-of-the-art performance on the CASIA NIR-VIS 2.0, CUHK NIR-VIS, and IIIT-D Sketch datasets.",2019,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2894272,https://pengxj.github.io/files/TIP19-MCCNN.pdf
2fd24578188da5a4d9af66cb6cec4421eaff9932,1,0,Enhanced Face Recognition System: Integrating of Collaborative Representation Based Classification (CRC) _KNN,,2019,,,10.4018/IJECME.2019010104,
300146598b0e447d331d64ca21dded56b7e7ed46,0,1,2nd Place and 2nd Place Solution to Kaggle Landmark Recognition andRetrieval Competition 2019,"We present a retrieval based system for landmark retrieval and recognition challenge.There are five parts in retrieval competition system, including feature extraction and matching to get candidates queue; database augmentation and query extension searching; reranking from recognition results and local feature matching. In recognition challenge including: landmark and non-landmark recognition, multiple recognition results voting and reranking using combination of recognition and retrieval results. All of models trained and predicted by PaddlePaddle framework. Using our method, we achieved 2nd place in the Google Landmark Recognition 2019 and 2nd place in the Google Landmark Retrieval 2019 on kaggle. The source code is available at here.",2019,ArXiv,1906.0399,,https://arxiv.org/pdf/1906.03990.pdf
3002c16de1027be0911ba2642811c68d6059d37a,0,1,Boosting Network Weight Separability via Feed-Backward Reconstruction,"This paper proposes a new evaluation metric and a boosting method for weight separability in neural network design. In contrast to general visual recognition methods designed to encourage both intra-class compactness and inter-class separability of latent features, we focus on estimating linear independence of column vectors in weight matrix and improving the separability of weight vectors. To this end, we propose an evaluation metric for weight separability based on semi-orthogonality of a matrix, Frobenius distance, and the feed-backward reconstruction loss, which explicitly encourages weight separability between the column vectors in the weight matrix. The experimental results on image classification and face recognition demonstrate that the weight separability boosting via minimization of feed-backward reconstruction loss can improve the visual recognition performance, hence universally boosting the performance on various visual recognition tasks.",2020,IEEE Access,1910.09024,10.1109/ACCESS.2020.3041470,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09274406.pdf
3011b5fce49112228711a9e5f92d6f191687c1ea,1,1,Feature Transfer Learning for Deep Face Recognition with Long-Tail Data,"Real-world face recognition datasets exhibit long-tail characteristics, which results in biased classifiers in conventionally-trained deep neural networks, or insufficient data when long-tail classes are ignored. In this paper, we propose to handle long-tail classes in the training of a face recognition engine by augmenting their feature space under a center-based feature transfer framework. A Gaussian prior is assumed across all the head (regular) classes and the variance from regular classes are transferred to the long-tail class representation. This encourages the long-tail distribution to be closer to the regular distribution, while enriching and balancing the limited training data. Further, an alternating training regimen is proposed to simultaneously achieve less biased decision boundaries and a more discriminative feature representation. We conduct empirical studies that mimic long-tail datasets by limiting the number of samples and the proportion of long-tail classes on the MS-Celeb-1M dataset. We compare our method with baselines not designed to handle long-tail classes and also with state-of-the-art methods on face recognition benchmarks. State-of-the-art results on LFW, IJB-A and MS-Celeb-1M datasets demonstrate the effectiveness of our feature transfer approach and training strategy. Finally, our feature transfer allows smooth visual interpolation, which demonstrates disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations.",2018,ArXiv,1803.09014,,https://arxiv.org/pdf/1803.09014.pdf
30b2a0a315f48567981070bab977dc115e554513,1,1,Robust Attacks on Deep Learning Face Recognition in the Physical World,"Deep neural networks (DNNs) have been increasingly used in face recognition (FR) systems. Recent studies, however, show that DNNs are vulnerable to adversarial examples, which can potentially mislead the FR systems using DNNs in the physical world. Existing attacks on these systems either generate perturbations working merely in the digital world, or rely on customized equipments to generate perturbations and are not robust in varying physical environments. In this paper, we propose FaceAdv, a physical-world attack that crafts adversarial stickers to deceive FR systems. It mainly consists of a sticker generator and a transformer, where the former can craft several stickers with different shapes and the latter transformer aims to digitally attach stickers to human faces and provide feedbacks to the generator to improve the effectiveness of stickers. We conduct extensive experiments to evaluate the effectiveness of FaceAdv on attacking 3 typical FR systems (i.e., ArcFace, CosFace and FaceNet). The results show that compared with a state-of-the-art attack, FaceAdv can significantly improve success rate of both dodging and impersonating attacks. We also conduct comprehensive evaluations to demonstrate the robustness of FaceAdv.",2020,ArXiv,2011.13526,,https://arxiv.org/pdf/2011.13526.pdf
30b6eca9585286cc011a56f3c8adbab3bcd33a0b,0,1,VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge,"The VoxCeleb Speaker Recognition Challenge 2019 aimed to assess how well current speaker recognition technology is able to identify speakers in unconstrained or `in the wild' data. It consisted of: (i) a publicly available speaker recognition dataset from YouTube videos together with ground truth annotation and standardised evaluation software; and (ii) a public challenge and workshop held at Interspeech 2019 in Graz, Austria. This paper outlines the challenge and provides its baselines, results and discussions.",2019,ArXiv,1912.02522,,https://arxiv.org/pdf/1912.02522.pdf
314c413abf3b36486d4b6760c9f7cf5024f1f76e,0,1,Cross-Pose LFW : A Database for Studying Cross-Pose Face Recognition in Unconstrained Environments,"Labeled Faces in the Wild (LFW) database has been widely utilized as the benchmark of unconstrained face verification. Recently, due to big data driven machine learning methods, the performance on the database approaches nearly 100%. However, we argue that this accuracy may be too optimistic. Besides illuminations, occlusions and expressions which have been considered as intra-class variations in LFW positive pairs, cross-pose faces of the same individual is another challenge in face recognition. Therefore, we construct a Cross-Pose LFW (CPLFW) database to add pose influence in face recognition. Also, considering that in the psychology literature, the different-identity pairs should show people of the same gender and race, negative pairs in CPLFW are selected using people with the same gender and race. This assures that performance on the database indicates true ability to distinguish individuals using their identities instead of depending on differences in gender and race. We evaluate some deep learning methods on the new database. Compared to the accuracy on LFW, the accuracy drops about 8%-20% on CPLFW.",2018,,,,https://pdfs.semanticscholar.org/314c/413abf3b36486d4b6760c9f7cf5024f1f76e.pdf
31983d2de086a68651622ab6483e5a30981ad357,0,1,Determination of the most representative descriptor among a set of feature vectors for the same object,On an example of solution of the face recognition problem the approach for estimation of the most representative descriptor among a set of feature vectors for the same face is considered in present study. The estimation is based on robust calculation of the mode-median mixture vector for the set as the descriptor by means of Welsch/Leclerc loss function application in case of very sparse filling of the feature space with feature vectors,2020,ArXiv,2007.03021,,https://arxiv.org/pdf/2007.03021.pdf
31a07feb4acedbb1854dcc06befce5fbfc27c24b,1,1,An adversarial learning algorithm for mitigating gender bias in face recognition,"State-of-the-art face recognition networks implicitly encode gender information while being trained for identity classification. Gender is often viewed as an important face attribute to recognize humans. But, the expression of gender information in deep facial features appears to contribute to gender bias in face recognition, i.e. we find a significant difference in the recognition accuracy of DCNNs on male and female faces. We hypothesize that reducing implicitly encoded gender information will help reduce this gender bias. Therefore, we present a novel approach called `Adversarial Gender De-biasing (AGD)' to reduce the strength of gender information in face recognition features. We accomplish this by introducing a bias reducing classification loss $L_{br}$. We show that AGD significantly reduces bias, while achieving reasonable recognition performance. The results of our approach are presented on two state-of-the-art networks.",2020,ArXiv,2006.07845,,https://arxiv.org/pdf/2006.07845.pdf
31f53ee81c91a0c30c02fe8050fe15c8d84d0674,1,0,CNNs Under Attack: On the Vulnerability of Deep Neural Networks Based Face Recognition to Image Morphing,"Facial recognition has become a critical constituent of common automatic border control gates. Despite many advances in recent years, face recognition systems remain susceptible to an ever evolving diversity of spoofing attacks. It has recently been shown that high-quality face morphing or splicing can be employed to deceive facial recognition systems in a border control scenario. Moreover, facial morphs can easily be produced by means of open source software and with minimal technical knowledge. The purpose of this work is to quantify the severeness of the problem using a large dataset of morphed face images. We employ a state-of-the-art face recognition algorithm based on deep convolutional neural networks and measure its performance on a dataset of 7260 high-quality facial morphs with varying blending factor. Using the Inception-ResNet-v1 architecture we train a deep neural model on 4 million images to obtain a validation rate of \(99.96\%\) at \(0.04\%\) false acceptance rate (FAR) on the original, unmodified images. The same model fails to repel \(1.13\%\) of all morphing attacks, accepting both the impostor and the document owner. Based on these results, we discuss the observed weaknesses and possible remedies.",2017,IWDW,,10.1007/978-3-319-64185-0_10,
320527f433d9e2967201560fd32b98183fb08443,0,1,Finger Vein Recognition Based on Multi-Task Learning,"In finger vein recognition, traditional methods for extracting ROI based on edge detection, sliding window detection of joint lines, etc. need to set a fixed threshold, which contains many parameters that need to be adjusted. In the case of large illumination changes or poor image quality, the extracted results are not accurate enough. The existing feature extraction method also has a fixed operator pattern and limited extracted feature patterns. Therefore, a large amount of effective feature information is wasted. In this paper, a multi-task neural network model algorithm is proposed, which uses the multi-task learning method to jointly optimize the ROI extraction task and the feature extraction task. This method not only improves the overall data processing efficiency of finger vein recognition system, but also improves the quality of extracted vein features. At the same time, we explore the use of improved loss function based on softmax to train our model. Our model is better than traditional methods and single task neural network model algorithm in MMCBNU [16] FV-USM [17] and DUMLA-HMT [18] data sets.",2020,,,10.1145/3395260.3395277,
325debdd029c5fc5eb26d2193824d07c625172af,0,1,Adversarial Domain Adaptation for Speaker Verification Using Partially Shared Network,"Speaker verification systems usually suffer from large performance degradation when applied to a new dataset from a different domain. In this work, we will study the domain adaption strategy between datasets with different languages using domain adversarial training. We introduce a partially shared network based domain adversarial training architecture to learn an asymmetric mapping for source and target domain embedding extractor. This architecture can help the embedding extractor learn domain invariant feature without sacrificing the ability on speaker discrimination. When doing the evaluation on crosslingual domain adaption, the source domain data is in English from NIST SRE04-10 and Switchboard, and the target domain data is in Cantonese and Tagalog from NIST SRE16. Our results show that the usual adversarial training mode will indeed harm the speaker discrimination when the source and target domain embedding extractors are fully shared, and in contrast the newly proposed architecture solves this problem and achieves ∼25.0% relative average Equal Error Rate (EER) improvement on SRE16 Cantonese and Tagalog evaluation.",2020,INTERSPEECH,,10.21437/interspeech.2020-2226,https://isca-speech.org/archive/Interspeech_2020/pdfs/2226.pdf
32873f6111963607d3f768f4685fe8137fdd1253,1,1,Learning to Cluster Faces on an Affinity Graph,"Face recognition sees remarkable progress in recent years, and its performance has reached a very high level. Taking it to a next level requires substantially larger data, which would involve prohibitive annotation cost. Hence, exploiting unlabeled data becomes an appealing alternative. Recent works have shown that clustering unlabeled faces is a promising approach, often leading to notable performance gains. Yet, how to effectively cluster, especially on a large-scale (i.e. million-level or above) dataset, remains an open question. A key challenge lies in the complex variations of cluster patterns, which make it difficult for conventional clustering methods to meet the needed accuracy. This work explores a novel approach, namely, learning to cluster instead of relying on hand-crafted criteria. Specifically, we propose a framework based on graph convolutional network, which combines a detection and a segmentation module to pinpoint face clusters. Experiments show that our method yields significantly more accurate face clusters, which, as a result, also lead to further performance gain in face recognition.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.02749,10.1109/CVPR.2019.00240,https://arxiv.org/pdf/1904.02749.pdf
32921da55d7127169e901c4b3e5d6e2333185561,0,1,Show me where the action is!: Automatic capturing and timeline generation for reality TV.,"Reality TV shows have gained popularity, motivating many production houses to bring new variants for us to watch. Compared to traditional TV shows, reality TV shows have spontaneous unscripted footage. Computer vision techniques could partially replace the manual labour needed to record and process this spontaneity. However, automated real-world video recording and editing is a challenging topic. In this paper, we propose a system that utilises state-of-the-art video and audio processing algorithms to, on the one hand, automatically steer cameras, replacing camera operators and on the other hand, detect all audiovisual action cues in the recorded video, to ease the job of the film editor. This publication has hence two main contributions. The first, automating the steering of multiple Pan-Tilt-Zoom PTZ cameras to take aesthetically pleasing medium shots of all the people present. These shots need to comply with the cinematographic rules and are based on the poses acquired by a pose detector. Secondly, when a huge amount of audio-visual data has been collected, it becomes labour intensive for a human editor retrieve the relevant fragments. As a second contribution, we combine state-of-the-art audio and video processing techniques for sound activity detection, action recognition, face recognition, and pose detection to decrease the required manual labour during and after recording. These techniques used during post-processing produce meta-data allowing for footage filtering, decreasing the search space. We extended our system further by producing timelines uniting generated meta-data, allowing the editor to have a quick overview. We evaluated our system on three in-the-wild reality TV recording sessions of 24 hours (× 8 cameras) each taken in real households.",2020,,,10.1007/s11042-020-09616-9,https://link.springer.com/content/pdf/10.1007/s11042-020-09616-9.pdf
32aeb90992f6cf8494b1b5c67f4b912feef05e9c,1,0,Incremental Classifier Learning with Generative Adversarial Networks,"In this paper, we address the incremental classifier learning problem, which suffers from catastrophic forgetting. The main reason for catastrophic forgetting is that the past data are not available during learning. Typical approaches keep some exemplars for the past classes and use distillation regularization to retain the classification capability on the past classes and balance the past and new classes. However, there are four main problems with these approaches. First, the loss function is not efficient for classification. Second, there is unbalance problem between the past and new classes. Third, the size of pre-decided exemplars is usually limited and they might not be distinguishable from unseen new classes. Forth, the exemplars may not be allowed to be kept for a long time due to privacy regulations. To address these problems, we propose (a) a new loss function to combine the cross-entropy loss and distillation loss, (b) a simple way to estimate and remove the unbalance between the old and new classes , and (c) using Generative Adversarial Networks (GANs) to generate historical data and select representative exemplars during generation. We believe that the data generated by GANs have much less privacy issues than real images because GANs do not directly copy any real image patches. We evaluate the proposed method on CIFAR-100, Flower-102, and MS-Celeb-1M-Base datasets and extensive experiments demonstrate the effectiveness of our method.",2018,ArXiv,1802.00853,,https://arxiv.org/pdf/1802.00853.pdf
32b2e1c54c9f13e21611f68241bcb613b5c715ed,1,1,Balanced Alignment for Face Recognition: A Joint Learning Approach,"Face alignment is crucial for face recognition and has been widely adopted. However, current practice is too simple and under-explored. There lacks an understanding of how important face alignment is and how it should be performed, for recognition. This work studies these problems and makes two contributions. First, it provides an in-depth and quantitative study of how alignment strength affects recognition accuracy. Our results show that excessive alignment is harmful and an optimal balanced point of alignment is in need. To strike the balance, our second contribution is a novel joint learning approach where alignment learning is controllable with respect to its strength and driven by recognition. Our proposed method is validated by comprehensive experiments on several benchmarks, especially the challenging ones with large pose.",2020,ArXiv,2003.10168,,https://arxiv.org/pdf/2003.10168.pdf
332548fd2e52b27e062bd6dcc1db0953ced6ed48,1,0,Low-Shot Face Recognition with Hybrid Classifiers,"In this paper, we present our solution to the MS-Celeb-1M Low-shot Face Recognition Challenge. This challenge aims to recognize 21,000 celebrities, in which 20,000 celebrities (Base Set) come with 50-100 images per person for training. But only one training image is provided for each person in the rest 1,000 celebrities (Novel Set). Given the dispersion in the number of training samples between Base Set and Novel Set, it is hard to build a single classifier that works well for both sets. To solve this problem, we propose a framework with multiple classifiers. This decomposes a single classifier for all data into multiple classifiers that each works well for a part of data. To be more specific, a Deep Convolution Neural Network (CNN) is utilized for Base Set and a Nearest Neighbor (NN) model is applied to Novel Set. The final prediction is based on a fusion of CNN results and NN results. Extensive experiments on MS-Celeb-1M Low-shot face dataset demonstrate the superiority of the proposed method. Our solution achieves 92.64% Coverage @Precision=0.99 in Novel Set while maintaining 99.58% top-1 accuracy in Base Set. This result wins the challenge in the track of without external data. Moreover, it is worth to note our result even surpasses some models using external data and can achieve the third place if compared with all participants.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.228,http://www1.ece.neu.edu/~yuewu/files/2017/lowshot2017.pdf
33469a8b2b0f86d0bb3a42e2a17a76d569b37da9,1,0,Cross-Resolution Learning for Face Recognition,"Convolutional Neural Networks have reached extremely high performances on the Face Recognition task. Largely used datasets, such as VGGFace2, focus on gender, pose and age variations trying to balance them to achieve better results. However, the fact that images have different resolutions is not usually discussed and resize to 256 pixels before cropping is used. While specific datasets for very low resolution faces have been proposed, less attention has been payed on the task of cross-resolution matching. Such scenarios are of particular interest for forensic and surveillance systems in which it usually happens that a low-resolution probe has to be matched with higher-resolution galleries. While it is always possible to either increase the resolution of the probe image or to reduce the size of the gallery images, to the best of our knowledge an extensive experimentation of cross-resolution matching was missing in the recent deep learning based literature. In the context of low- and cross-resolution Face Recognition, the contributions of our work are: i) we proposed a training method to fine-tune a state-of-the-art model in order to make it able to extract resolution-robust deep features; ii) we tested our models on the benchmark datasets IJB-B/C considering images at both full and low resolutions in order to show the effectiveness of the proposed training algorithm. To the best of our knowledge, this is the first work testing extensively the performance of a FR model in a cross-resolution scenario; iii) we tested our models on the low resolution and low quality datasets QMUL-SurvFace and TinyFace and showed their superior performances, even though we did not train our model on low-resolution faces only and our main focus was cross-resolution; iv) we showed that our approach can be more effective with respect to preprocessing faces with super resolution techniques.",2020,Image Vis. Comput.,1912.02851,10.1016/j.imavis.2020.103927,https://arxiv.org/pdf/1912.02851.pdf
339d0690f88b01eba162ab1b3244a3db4924d258,0,1,Generating and Validating DSA Private Keys from Online Face Images for Digital Signatures,"Signing digital documents is attracting more attention in recent years, according to the rapidly growing number of digital documents being exchanged online. The digital signature proves the authenticity of the document and the sender’s approval on the contents of the document. However, storing the private keys of users for digital signing imposes threats toward gaining unauthorized access, which can result in producing false signatures. Thus, in this paper, a novel approach is proposed to extract the private component of the key used to produce the digital signature from online face image. Hence, this private component is never stored in any database, so that, false signatures cannot be produced and the sender’s approval cannot be denied. The proposed method uses a convolutional neural network that is trained using a semi-supervised approach, so that, the values used for the training are extracted based on the predictions of the neural network. To avoid the need for training a complex neural network, the proposed neural network makes use of existing pretrained neural networks, that already have the knowledge about the distinctive features in the faces. The use of the MTCNN for face detection and Facenet for face recognition, in addition to the proposed neural network, to achieved the best performance. The performance of the proposed method is evaluated using the Colored FERET Faces Database Version 2 and has achieved robustness rate of 13.48% and uniqueness of 100%.",2019,,,10.18517/IJASEIT.9.3.8950,
3492613fc52dd8ac43e154139b9d024ca8a42e17,0,1,Angular Discriminative Deep Feature Learning for Face Verification,"Thanks to the development of deep Convolutional Neural Network (CNN), face verification has achieved great success rapidly. Specifically, Deep Distance Metric Learning (DDML), as an emerging area, has achieved great improvements in computer vision community. Softmax loss is widely used to supervise the training of most available CNN models. Whereas, feature normalization is often used to compute the pair similarities when testing. In order to bridge the gap between training and testing, we require that the intra-class cosine similarity of the inner-product layer before softmax loss is larger than a margin in the training step, accompanied by the supervision signal of softmax loss. To enhance the discriminative power of the deeply learned features, we extend the intra-class constraint to force the intra-class cosine similarity larger than the mean of nearest neighboring inter-class ones with a margin in the normalized exponential feature projection space. Extensive experiments on Labeled Face in the Wild (LFW) and Youtube Faces (YTF) datasets demonstrate that the proposed approaches achieve competitive performance for the open-set face verification task.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053675,
34a9c544fdf2e306e1743e05c37517b0f041d0c7,0,1,Deep Feature Space: A Geometrical Perspective,"One of the most prominent attributes of Neural Networks (NNs) constitutes their capability of learning to extract robust and descriptive features from high dimensional data, like images. Hence, such an ability renders their exploitation as feature extractors particularly frequent in an abundant of modern reasoning systems. Their application scope mainly includes complex cascade tasks, like multi-modal recognition and deep Reinforcement Learning (RL). However, NNs induce implicit biases that are difficult to avoid or to deal with and are not met in traditional image descriptors. Moreover, the lack of knowledge for describing the intra-layer properties -- and thus their general behavior -- restricts the further applicability of the extracted features. With the paper at hand, a novel way of visualizing and understanding the vector space before the NNs' output layer is presented, aiming to enlighten the deep feature vectors' properties under classification tasks. Main attention is paid to the nature of overfitting in the feature space and its adverse effect on further exploitation. We present the findings that can be derived from our model's formulation, and we evaluate them on realistic recognition scenarios, proving its prominence by improving the obtained results.",2020,ArXiv,2007.00062,,https://arxiv.org/pdf/2007.00062.pdf
34fe617eb4881289eb1d33c4aedc905e9c3f22b9,0,1,Joint Multi-View Face Alignment in the Wild,"The de facto algorithm for facial landmark estimation involves running a face detector with a subsequent deformable model fitting on the bounding box. This encompasses two basic problems: 1) the detection and deformable fitting steps are performed independently, while the detector might not provide the best-suited initialization for the fitting step, and 2) the face appearance varies hugely across different poses, which makes the deformable face fitting very challenging and thus distinct models have to be used (e.g., one for profile and one for frontal faces). In this paper, we propose the first, to the best of our knowledge, joint multi-view convolutional network to handle large pose variations across faces in-the-wild, and elegantly bridge face detection and facial landmark localization tasks. The existing joint face detection and landmark localization methods focus only on a very small set of landmarks. By contrast, our method can detect and align a large number of landmarks for semi-frontal (68 landmarks) and profile (39 landmarks) faces. We evaluate our model on a plethora of datasets including the standard static image datasets such as IBUG, 300W, COFW, and the latest Menpo Benchmark for both semi-frontal and profile faces. A significant improvement over the state-of-the-art methods on deformable face tracking is witnessed on the 300VW benchmark. We also demonstrate state-of-the-art results for face detection on FDDB and MALF datasets.",2019,IEEE Transactions on Image Processing,1708.06023,10.1109/TIP.2019.2899267,https://arxiv.org/pdf/1708.06023.pdf
34ffe6e3285a8b5b1bd4bd24b402a90a84335bc2,0,1,Heimdall: mobile GPU coordination platform for augmented reality applications,"We present Heimdall, a mobile GPU coordination platform for emerging Augmented Reality (AR) applications. Future AR apps impose an explored challenging workload: i) concurrent execution of multiple Deep Neural Networks (DNNs) for physical world and user behavior analysis, and ii) seamless rendering in presence of the DNN execution for immersive user experience. Existing mobile deep learning frameworks, however, fail to support such workload: multi-DNN GPU contention slows down inference latency (e.g., from 59.93 to 1181 ms), and rendering-DNN GPU contention degrades frame rate (e.g., from 30 to ≈12 fps). Multi-tasking for desktop GPUs (e.g., parallelization, preemption) cannot be applied to mobile GPUs as well due to limited architectural support and memory bandwidth. To tackle the challenge, we design a Pseudo-Preemption mechanism which i) breaks down the bulky DNN into smaller units, and ii) prioritizes and flexibly schedules concurrent GPU tasks. We prototyped Heimdall over various mobile GPUs (i.e., recent Adreno series) and multiple AR app scenarios that involve combinations of 8 state-of-the-art DNNs. Our extensive evaluation shows that Heimdall enhances the frame rate from ≈12 to ≈30 fps while reducing the worst-case DNN inference latency by up to ≈15 times compared to the baseline multi-threading approach.",2020,MobiCom,,10.1145/3372224.3419192,
3502d92fd7f7a3ff280d33167f958b7b03717407,0,1,Multi-path x-D recurrent neural networks for collaborative image classification,"With the rapid development of image acquisition and storage, multiple images per class are commonly available for computer vision tasks (e.g., face recognition, object detection, medical imaging, etc.). Recently, the recurrent neural network (RNN) has been widely integrated with convolutional neural networks (CNN) to perform image classification on ordered (sequential) data. In this paper, by permutating multiple images as multiple dummy orders, we generalize the ordered ""RNN+CNN"" design (longitudinal) to a novel unordered fashion, called Multi-path x-D Recurrent Neural Network (MxDRNN) for image classification. To the best of our knowledge, few (if any) existing studies have deployed the RNN framework to unordered intra-class images to leverage classification performance. Specifically, multiple learning paths are introduced in the MxDRNN to extract discriminative features by permutating input dummy orders. Eight datasets from five different fields (MNIST, 3D-MNIST, CIFAR, VGGFace2, and lung screening computed tomography) are included to evaluate the performance of our method. The proposed MxDRNN improves the baseline performance by a large margin across the different application fields (e.g., accuracy from 46.40% to 76.54% in VGGFace2 test pose set, AUC from 0.7418 to 0.8162 in NLST lung dataset). Additionally, empirical experiments show the MxDRNN is more robust to category-irrelevant attributes (e.g., expression, pose in face images), which may introduce difficulties for image classification and algorithm generalizability. The code is publicly available.",2020,Neurocomputing,,10.1016/j.neucom.2020.02.033,
356b34bc02cd92209830cfbe336d39ef78714d1d,0,1,Towards Backward-Compatible Representation Learning,"We propose a way to learn visual features that are compatible with previously computed ones even when they have different dimensions and are learned via different neural network architectures and loss functions. Compatible means that, if such features are used to compare images, then ``new'' features can be compared directly to ``old'' features, so they can be used interchangeably. This enables visual search systems to bypass computing new features for all previously seen images when updating the embedding models, a process known as backfilling. Backward compatibility is critical to quickly deploy new embedding models that leverage ever-growing large-scale training datasets and improvements in deep learning architectures and training methods. We propose a framework to train embedding models, called backward-compatible training (BCT), as a first step towards backward compatible representation learning. In experiments on learning embeddings for face recognition, models trained with BCT successfully achieve backward compatibility without sacrificing accuracy, thus enabling backfill-free model updates of visual embeddings.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.11942,10.1109/cvpr42600.2020.00640,https://arxiv.org/pdf/2003.11942.pdf
3582fe51967f9b5caa7083efe5cedae5af240565,1,0,Methodologies of Audio-Visual Biometric Performance Evaluation for the H2020 SpeechXRays Project,"Biometric recognition is nowadays widely used in different services and applications, making the user authentication easier and more secure than the traditional authentication system. Starting from this idea, the EU SpeechXRays project H2020 developed and evaluated in real-life environments a user recognition platform based on face and voice modalities. Since the proposed biometric solution was evaluated in real-life environments where biometric data recorded was not accessible because of the General Data Protection Regulation GDPR, the ground truth of the conducted evaluation was not available. To correctly report the performance evaluation, some methodologies were proposed to detect the errors caused by the absence of ground truth. This paper describes the biometric solution provided by the project and presents the biometric performance evaluation carried out in three real-life use case pilots on more than 2 000 users.",2020,2020 5th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP),,10.1109/ATSIP49331.2020.9231680,
35937ab3aaf3b8eee89180a1b892c3daa96d2bab,1,1,AdaCos: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations,"The cosine-based softmax losses and their variants achieve great success in deep learning based face recognition. However, hyperparameter settings in these losses have significant influences on the optimization path as well as the final recognition performance. Manually tuning those hyperparameters heavily relies on user experience and requires many training tricks. In this paper, we investigate in depth the effects of two important hyperparameters of cosine-based softmax losses, the scale parameter and angular margin parameter, by analyzing how they modulate the predicted classification probability. Based on these analysis, we propose a novel cosine-based softmax loss, AdaCos, which is hyperparameter-free and leverages an adaptive scale parameter to automatically strengthen the training supervisions during the training process. We apply the proposed AdaCos loss to large-scale face verification and identification datasets, including LFW, MegaFace, and IJB-C 1:1 Verification. Our results show that training deep neural networks with the AdaCos loss is stable and able to achieve high face recognition accuracy. Our method outperforms state-of-the-art softmax losses on all the three datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1905.00292,10.1109/CVPR.2019.01108,https://arxiv.org/pdf/1905.00292.pdf
35ddd4896d9396d5dcff3f32bd79ff7cf3e82777,1,1,ShrinkTeaNet: Million-scale Lightweight Face Recognition via Shrinking Teacher-Student Networks,"Large-scale face recognition in-the-wild has been recently achieved matured performance in many real work applications. However, such systems are built on GPU platforms and mostly deploy heavy deep network architectures. Given a high-performance heavy network as a teacher, this work presents a simple and elegant teacher-student learning paradigm, namely ShrinkTeaNet, to train a portable student network that has significantly fewer parameters and competitive accuracy against the teacher network. Far apart from prior teacher-student frameworks mainly focusing on accuracy and compression ratios in closed-set problems, our proposed teacher-student network is proved to be more robust against open-set problem, i.e. large-scale face recognition. In addition, this work introduces a novel Angular Distillation Loss for distilling the feature direction and the sample distributions of the teacher's hypersphere to its student. Then ShrinkTeaNet framework can efficiently guide the student's learning process with the teacher's knowledge presented in both intermediate and last stages of the feature embedding. Evaluations on LFW, CFP-FP, AgeDB, IJB-B and IJB-C Janus, and MegaFace with one million distractors have demonstrated the efficiency of the proposed approach to learn robust student networks which have satisfying accuracy and compact sizes. Our ShrinkTeaNet is able to support the light-weight architecture achieving high performance with 99.77% on LFW and 95.64% on large-scale Megaface protocols.",2019,ArXiv,1905.1062,,https://arxiv.org/pdf/1905.10620.pdf
35e1120677d1528edd9395e81dd2ca85e7319952,1,0,Investigating Bias in Deep Face Analysis: The KANFace Dataset and Empirical Study,,2020,ArXiv,2005.07302,10.1016/j.imavis.2020.103954,https://arxiv.org/pdf/2005.07302.pdf
3605e0e6ea610576fa85fcf8737b9f20ddd87180,1,0,Self-supervised Face Representation Learning,"This thesis investigates fine-tuning deep face features in a self-supervised manner for discriminative face representation learning, wherein we develop methods to automatically generate pseudo-labels for training a neural network. Most importantly solving this problem helps us to advance the state-of-the-art in representation learning and can be beneficial to a variety of practical downstream tasks. Fortunately, there is a vast amount of videos on the internet that can be used by machines to learn an effective representation. We present methods that can learn a strong face representation from large-scale data be the form of images or video.    However, while learning a good representation using a deep learning algorithm requires a large-scale dataset with manually curated labels, we propose self-supervised approaches to generate pseudo-labels utilizing the temporal structure of the video data and similarity constraints to get supervision from the data itself.    We aim to learn a representation that exhibits small distances between samples from the same person, and large inter-person distances in feature space. Using metric learning one could achieve that as it is comprised of a pull-term, pulling data points from the same class closer, and a push-term, pushing data points from a different class further away. Metric learning for improving feature quality is useful but requires some form of external supervision to provide labels for the same or different pairs. In the case of face clustering in TV series, we may obtain this supervision from tracks and other cues. The tracking acts as a form of high precision clustering (grouping detections within a shot) and is used to automatically generate positive and negative pairs of face images. Inspired from that we propose two variants of discriminative approaches: Track-supervised Siamese network (TSiam) and Self-supervised Siamese network (SSiam). In TSiam, we utilize the tracking supervision to obtain the pair, additional we include negative training pairs for singleton tracks -- tracks that are not temporally co-occurring. As supervision from tracking may not always be available, to enable the use of metric learning without any supervision we propose an effective approach SSiam that can generate the required pairs automatically during training. In SSiam, we leverage dynamic generation of positive and negative pairs based on sorting distances (i.e. ranking) on a subset of frames and do not have to only rely on video/track based supervision.    Next, we present a method namely Clustering-based Contrastive Learning (CCL), a new clustering-based representation learning approach that utilizes automatically discovered partitions obtained from a clustering algorithm (FINCH) as weak supervision along with inherent video constraints to learn discriminative face features. As annotating datasets is costly and difficult, using label-free and weak supervision obtained from a clustering algorithm as a proxy learning task is promising. Through our analysis, we show that creating positive and negative training pairs using clustering predictions help to improve the performance for video face clustering.    We then propose a method face grouping on graphs (FGG), a method for unsupervised fine-tuning of deep face feature representations. We utilize a graph structure with positive and negative edges over a set of face-tracks based on their temporal structure of the video data and similarity-based constraints. Using graph neural networks, the features communicate over the edges allowing each track's feature to exchange information with its neighbors, and thus push each representation in a direction in feature space that groups all representations of the same person together and separates representations of a different person.    Having developed these methods to generate weak-labels for face representation learning, next we propose to learn compact yet effective representation for describing face tracks in videos into compact descriptors, that can complement previous methods towards learning a more powerful face representation. Specifically, we propose Temporal Compact Bilinear Pooling (TCBP) to encode the temporal segments in videos into a compact descriptor. TCBP possesses the ability to capture interactions between each element of the feature representation with one-another over a long-range temporal context. We integrated our previous methods TSiam, SSiam and CCL with TCBP and demonstrated that TCBP has excellent capabilities in learning a strong face representation. We further show TCBP has exceptional transfer abilities to applications such as multimodal video clip representation that jointly encodes images, audio, video and text, and video classification.    All of these contributions are demonstrated on benchmark video clustering datasets: The Big Bang Theory, Buffy the Vampire Slayer and Harry Potter 1. We provide extensive evaluations on these datasets achieving a significant boost in performance over the base features, and in comparison to the state-of-the-art results.",2020,,,10.5445/IR/1000119819,
3608d8b60b5b89804efa72e46e8a0e3509447f7e,0,1,Image Classification to Diagnose Chronic Atrophic Gastritis Based on Squeeze-and-Excitation Block,"Chronic atrophic gastritis (CAG) is an organ-specific autoimmune disease, and its occurrence and development seriously threaten human health. Therefore, CAG is medically characterized as a serious precancerous lesion. We used a convolutional neural network (CNN) to construct a deep learning network, which helps improve the accuracy of diagnosing CAG. In this paper, we used the INPAINT_TELEA algorithm to remove the watermark in gastric antrum images. According to the characteristics of the dataset, a 14 layers’ convolutional neural network (SE-CAG) was designed, in which the BN layer and scale layer are added to prevent overfitting. The designed network is integrated into the Squeeze-and-Excitation (SE) block. Through the feature reuse principle of SE block, the effective features are highlighted, to realize the classification of our datasets. From experimental results on our dataset, the detection rate of chronic atrophic gastritis is 88.15%, which makes it possible to diagnose CAG automatically. Compared with the ResNet (Deep Residual Network) and VGG16, on our own dataset, the proposed model shows a better performance in diagnosing CAG. Then, we used the FP-Growth algorithm to explore the relationship between atrophic gastritis and other gastroscopic symptoms, so that it can establish the relationship between endoscopy and diagnosis results.",2020,2020 IEEE 5th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),,10.1109/ICCCBDA49378.2020.9095753,
361eaef45fccfffd5b7df12fba902490a7d24a8d,1,0,Robust deep learning features for face recognition under mismatched conditions,"In this paper, we addressed the problem of face recognition under mismatched conditions. In the proposed system, for face representation, we leveraged the state-of-the-art deep learning models trained on the VGGFace2 dataset. More specifically, we used pretrained convolutional neural network models to extract 2048 dimensional feature vectors from face images of International Challenge on Biometric Recognition in the Wild dataset, shortly, ICB-RW 2016. In this challenge, the gallery images were collected under controlled, indoor studio settings, whereas probe images were acquired from outdoor surveillance cameras. For classification, we trained a nearest neighbor classifier using correlation as the distance metric. Experiments on the ICB-RW 2016 dataset have shown that the employed deep learning models that were trained on the VGGFace2 dataset provides superior performance. Even using a single model, compared to the ICB-RW 2016 winner system, around 15% absolute increase in Rank-1 correct classification rate has been achieved. Combining individual models at feature level has improved the performance further. The ensemble of four models achieved 91.8% Rank-1, 98.0% Rank-5 identification rate, and 0.997 Area Under the Curve of Cumulative Match Score on the probe set. The proposed method significantly outperforms the Rank-1, Rank-5 identification rates, and Area Under the Curve of Cumulative Match Score of the best approach at the ICB-RW 2016 challenge, which were 69.8%, 85.3%, and 0.954, respectively.",2018,2018 26th Signal Processing and Communications Applications Conference (SIU),,10.1109/SIU.2018.8404319,
362efd9641870f95fc3eba6d49b609673466e3ff,0,1,An efficient deep face matching method for ID and selfie photos: SIRFace,"User authentication using face biometrics has been becoming popular in many administrative activities (e.g., airport check-in, banking security systems, immigration managements, etc.). In this process, the facial image taken from an authorized document (e.g., identity (ID) card, passport, driving license) is compared to the live face that captured directly by stationary camera or human eyes to check whether they are matched as the same person. The problem is really challenging due to cross-domain difference including age changes, low document quality, and accessories. Recently, a few efforts have been made to build automatic face matching systems based on face recognition technologies. In this paper, we proposed a novel and efficient deep face matching system called SIRFace, which is derived from the RetinaFace detection model, InsightFace embedding model, and SVM classifier. SIRFace also contributed an efficient technique using Cosine similarity to mitigate the impacts of the cross-domain difference, which has not been used in any other related works. The experimental results on a dataset of 2,865 subjects with 9,824 images (i.e., ID and selfie photos) using our method achieved an error rate of 2.34%, which is potentially used for real life applications.",2020,International Conference on Digital Image Processing,,10.1117/12.2573327,
36cd62031d7cf516f89cf10b40220d33ca66c87a,0,1,DropClass and DropAdapt: Dropping classes for deep speaker representation learning,"Many recent works on deep speaker embeddings train their feature extraction networks on large classification tasks, distinguishing between all speakers in a training set. Empirically, this has been shown to produce speaker-discriminative embeddings, even for unseen speakers. However, it is not clear that this is the optimal means of training embeddings that generalize well. This work proposes two approaches to learning embeddings, based on the notion of dropping classes during training. We demonstrate that both approaches can yield performance gains in speaker verification tasks. The first proposed method, DropClass, works via periodically dropping a random subset of classes from the training data and the output layer throughout training, resulting in a feature extractor trained on many different classification tasks. Combined with an additive angular margin loss, this method can yield a 7.9% relative improvement in equal error rate (EER) over a strong baseline on VoxCeleb. The second proposed method, DropAdapt, is a means of adapting a trained model to a set of enrolment speakers in an unsupervised manner. This is performed by fine-tuning a model on only those classes which produce high probability predictions when the enrolment speakers are used as input, again also dropping the relevant rows from the output layer. This method yields a large 13.2% relative improvement in EER on VoxCeleb. The code for this paper has been made publicly available.",2020,ArXiv,2002.00453,,https://arxiv.org/pdf/2002.00453.pdf
36e570109b1f14a73fa5ba0a448ce1dd9fbffa92,0,1,"A large amount of attacking methods on generating adversarial examples have been introduced in recent years ( Carlini & Wagner , 2017 a","Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, e.g., CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey inappropriate supervisory signals, which encourage the learned feature points to spread over the space sparsely in training. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly induce dense feature regions in order to benefit robustness. Namely, the MMC loss encourages the model to concentrate on learning ordered and compact representations, which gather around the preset optimal centers for different classes. We empirically demonstrate that applying the MMC loss can significantly improve robustness even under strong adaptive attacks, while keeping high accuracy on clean inputs comparable to the SCE loss with little extra computation.",2019,,,,https://pdfs.semanticscholar.org/36e5/70109b1f14a73fa5ba0a448ce1dd9fbffa92.pdf
372798221bdbe468dbd45e799d7291b389122cc8,0,1,Going Beyond Real Data: A Robust Visual Representation for Vehicle Re-identification,"In this report, we present the Baidu-UTS submission to the AICity Challenge in CVPR 2020. This is the winning solution to the vehicle re-identification (re-id) track. We focus on developing a robust vehicle re-id system for real-world scenarios. In particular, we aim to fully leverage the merits of the synthetic data while arming with real images to learn a robust representation for vehicles in different views and illumination conditions. By comprehensively investigating and evaluating various data augmentation approaches and popular strong baselines, we analyze the bottleneck restricting the vehicle re-id performance. Based on our analysis, we therefore design a vehicle re-id method with better data augmentation, training and post-processing strategies. Our proposed method has achieved the 1st place out of 41 teams, yielding 84.13% mAP on the private test set. We hope that our practice could shed light on using synthetic and real data effectively in training deep re-id networks and pave the way for real-world vehicle re-id systems.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00307,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/Zheng_Going_Beyond_Real_Data_A_Robust_Visual_Representation_for_Vehicle_CVPRW_2020_paper.pdf
377c6563f97e76a4dc836a0bd23d7673492b1aae,1,0,Motion Deblurring of Faces,"Face analysis lies at the heart of computer vision with remarkable progress in the past decades. Face recognition and tracking are tackled by building invariance to fundamental modes of variation such as illumination, 3D pose. A much less standing mode of variation is motion deblurring, which however presents substantial challenges in face analysis. Recent approaches either make oversimplifying assumptions, e.g. in cases of joint optimization with other tasks, or fail to preserve the highly structured shape/identity information. We introduce a two-step architecture tailored to the challenges of motion deblurring: the first step restores the low frequencies; the second restores the high frequencies, while ensuring that the outputs span the natural images manifold. Both steps are implemented with a supervised data-driven method; to train those we devise a method for creating realistic motion blur by averaging a variable number of frames. The averaged images originate from the $$2MF^2$$2MF2 dataset with $$19$$19 million facial frames, which we introduce for the task. Considering deblurring as an intermediate step, we conduct a thorough experimentation on high-level face analysis tasks, i.e. landmark localization and face verification, on blurred images. The experimental evaluation demonstrates the superiority of our method.",2019,Int. J. Comput. Vis.,1803.0333,10.1007/s11263-018-1138-7,https://link.springer.com/content/pdf/10.1007%2Fs11263-018-1138-7.pdf
380493918881ab21f19f9ac68846f77090e28598,1,1,Memory-based Jitter: Improving Visual Recognition on Long-tailed Data with Diversity In Memory,"This paper considers deep visual recognition on long-tailed data, with the majority categories only occupying relatively few samples. The tail categories are prone to lack of within-class diversity, which compromises the representative ability of the learned visual concepts. A radical solution is to augment the tail categories with higher diversity. To this end, we introduce a simple and reliable method named Memory-based Jitter (MBJ) to gain extra diversity for the tail data. We observe that the deep model keeps on jittering from one historical edition to another, even when it already approaches convergence. The ``jitter'' means the small variations between historical models. We argue that such jitter largely originates from the within-class diversity of the overall data and thus encodes the within-class distribution pattern. To utilize such jitter for tail data augmentation, we store the jitter among historical models into a memory bank and get the so-called Memory-based Jitter. With slight modifications, MBJ is applicable for two fundamental visual recognition tasks, \emph{i.e.}, image classification and deep metric learning (on long-tailed data). On image classification, MBJ collects the historical embeddings to learn an accurate classifier. In contrast, on deep metric learning, it collects the historical prototypes of each class to learn a robust deep embedding. Under both scenarios, MBJ enforces higher concentration on tail classes, so as to compensate for their lack of diversity. Extensive experiments on three long-tailed classification benchmarks and two deep metric learning benchmarks (person re-identification, in particular) demonstrate the significant improvement. Moreover, the achieved performance are on par with the state-of-the-art on both tasks.",2020,ArXiv,2008.09809,,https://arxiv.org/pdf/2008.09809.pdf
382574aa71ead1581b5d5a9c41d37ba94901d75b,0,1,CC-Loss: Channel Correlation Loss For Image Classification,"The loss function is a key component in deep learning models. A commonly used loss function for classification is the cross entropy loss, which is a simple yet effective application of information theory for classification problems. Based on this loss, many other loss functions have been proposed,~\emph{e.g.}, by adding intra-class and inter-class constraints to enhance the discriminative ability of the learned features. However, these loss functions fail to consider the connections between the feature distribution and the model structure. Aiming at addressing this problem, we propose a channel correlation loss (CC-Loss) that is able to constrain the specific relations between classes and channels as well as maintain the intra-class and the inter-class separability. CC-Loss uses a channel attention module to generate channel attention of features for each sample in the training stage. Next, an Euclidean distance matrix is calculated to make the channel attention vectors associated with the same class become identical and to increase the difference between different classes. Finally, we obtain a feature embedding with good intra-class compactness and inter-class separability.Experimental results show that two different backbone models trained with the proposed CC-Loss outperform the state-of-the-art loss functions on three image classification datasets.",2020,ArXiv,2010.05469,,https://arxiv.org/pdf/2010.05469.pdf
3827f1cab643a57e3cd22fbffbf19dd5e8a298a8,1,0,One-Shot Face Recognition via Generative Learning,"One-shot face recognition measures the ability to recognize persons with only seeing them once, which is a hallmark of human visual intelligence. It is challenging for existing machine learning approaches to mimic this way, since limited data cannot well represent the data variance. To this end, we propose to build a large-scale face recognizer, which is capable to fight off the data imbalance difficulty. To seek a more effective general classifier, we develop a novel generative model attempting to synthesize meaningful data for one-shot classes by adapting the data variances from other normal classes. Specifically, we formulate conditional generative adversarial networks and the general Softmax classifier into a unified framework. Such a two-player minimax optimization can guide the generation of more effective data, which benefit the classifier learning for one-shot classes. The experimental results on a large-scale face benchmark with 21K persons verify the effectiveness of our proposed algorithm in one-shot classification, as our generative model significantly improves the recognition coverage rate from 25:65% to 94:84% at the precision of 99% for the one-shot classes, while still keeps an overall Top-1 accuracy at 99:80% for the normal classes.",2018,2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018),,10.1109/FG.2018.00011,
38387ec980464898cff48f9f37176f2a80e4359a,0,1,Learning Speaker Embedding with Momentum Contrast,"Speaker verification can be formulated as a representation learning task, where speaker-discriminative embeddings are extracted from utterances of variable lengths. Momentum Contrast (MoCo) is a recently proposed unsupervised representation learning framework, and has shown its effectiveness for learning good feature representation for downstream vision tasks. In this work, we apply MoCo to learn speaker embedding from speech segments. We explore MoCo for both unsupervised learning and pretraining settings. In the unsupervised scenario, embedding is learned by MoCo from audio data without using any speaker specific information. On a large scale dataset with $2,500$ speakers, MoCo can achieve EER $4.275\%$ trained unsupervisedly, and the EER can decrease further to $3.58\%$ if extra unlabelled data are used. In the pretraining scenario, encoder trained by MoCo is used to initialize the downstream supervised training. With finetuning on the MoCo trained model, the equal error rate (EER) reduces $13.7\%$ relative ($1.44\%$ to $1.242\%$) compared to a carefully tuned baseline training from scratch. Comparative study confirms the effectiveness of MoCo learning good speaker embedding.",2020,ArXiv,2001.01986,,https://arxiv.org/pdf/2001.01986.pdf
3850aed233604ff1039d45618d57605a457bb727,0,1,Likelihood Ratio based Loss to finetune CNNs for Very Low Resolution Face Verification,"In this paper, we propose a likelihood ratio based loss for very low-resolution face verification. Existing loss functions either improve the softmax loss to learn large-margin facial features or impose Euclidean margin constraints between image pairs. These methods are proved to be better than traditional softmax, but fail to guarantee the best discrimination features. Therefore, we propose a loss function based on likelihood ratio classifier, an optimal classifier in Neyman-Pearson sense, to give the highest verification rate at a given false accept rate, which is suitable for biometrics verification. To verify the efficacy of the proposed loss function, we apply it to address the very low-resolution face recognition problem. We conduct extensive experiments on the challenging SCface dataset with the resolution of the faces to be recognized below 16 × 16. The results show that the proposed approach outperforms state-of-the-art methods.",2019,2019 International Conference on Biometrics (ICB),,10.1109/ICB45273.2019.8987249,
388d5b1f40559d98d596798963f3ad59f2bccc81,0,1,Visual Explanation for Deep Metric Learning,"This work explores the visual explanation for deep metric learning and its applications. As an important problem for learning representation, metric learning has attracted much attention recently, while the interpretation of such model is not as well studied as classification. To this end, we propose an intuitive idea to show where contributes the most to the overall similarity of two input images by decomposing the final activation. Instead of only providing the overall activation map of each image, we propose to generate point-to-point activation intensity between two images so that the relationship between different regions is uncovered. We show that the proposed framework can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. Furthermore, our experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval. The source code is available at \url{this https URL}.",2019,ArXiv,1909.12977,,https://arxiv.org/pdf/1909.12977.pdf
38b4fa45e1ac1279829475cffa71672a330d3329,1,0,Engagement Intensity Prediction withFacial Behavior Features,"This paper describes an approach for the engagement prediction task, a sub-challenge of the 7th Emotion Recognition in the Wild Challenge (EmotiW 2019). Our method involves three fundamental steps: feature extraction, regression and model ensemble. In the first step, an input video is divided into multiple overlapped segments (instances) and the features extracted for each instance. The combinations of Long short-term memory (LSTM) and Fully connected layers deployed to capture the temporal information and regress the engagement intensity for the features in previous step. In the last step, we performed fusions to achieve better performance. Finally, our approach achieved a mean square error of 0.0597, which is 4.63% lower than the best results last year.",2019,ICMI '19,,10.1145/3340555.3355714,
38d8ff137ff753f04689e6b76119a44588e143f3,1,0,When 3D-Aided 2D Face Recognition Meets Deep Learning: An extended UR2D for Pose-Invariant Face Recognition,"Most of the face recognition works focus on specific modules or demonstrate a research idea. This paper presents a pose-invariant 3D-aided 2D face recognition system (UR2D) that is robust to pose variations as large as 90? by leveraging deep learning technology. The architecture and the interface of UR2D are described, and each module is introduced in detail. Extensive experiments are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset and 3% on the IJB-A dataset on average in face identification tasks. UR2D also achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing the Rank-1 accuracy score from template matching. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques.",2017,ArXiv,1709.06532,,https://arxiv.org/pdf/1709.06532.pdf
3901f177642912be3fd132807a1e7d16796d1011,1,0,Selecting active frames for action recognition with vote fusion method,"Recent applications of Convolutional Neural Networks, especially 3-Dimensional Convolutional Neural Networks (3DCNNs) for human action recognition (HAR) in videos have widely used. In this paper, we use a multi-stream framework which is a combination of separated networks with different kinds of input generated from a unique video dataset. We study various methods for extracting best frames in videos for action representation and find the way to fuse multiple networks for better recognition. We make the following work: first, we propose a method to extract the active frames (called Selected Active Frames - SAF) from videos to build datasets for 3DCNNs in video classification problem. Secondly, we propose a mixed fusing approach called Vote fusion which is considered as an effective fusion method for ensembling multi-stream networks. We evaluate the proposed approach to solving action recognition problem. We carry out this approach on three well-known datasets (KTH, HMDB51, and UCF101). The results are also compared to the state-of-the-art results to illustrate the efficiency and effectiveness in our approach.",2018,2018 7th International Conference on Computer and Communication Engineering (ICCCE),,10.1109/ICCCE.2018.8539313,
391320e73808349b6799e4d5bcbdd95ba274b570,1,0,A Web Application for Glasses Virtual Try-on in 3D Space,"Virtual try-on is a technology that allows people to virtually check the appearance of accessories, makeup, hairstyle, hair color, clothes and potentially more on themselves. The virtual try-on presents many advantages over real try-on, it speeds up the process providing the possibility to test hundreds of products without the need to reach a physical store. In this paper we propose a virtual try-on web application specific for eyeglasses and sunglasses that can be easily used by simply taking a picture of a face and selecting the desired frames. The try-on process is performed on a 3D face reconstructed from the input image allowing the user to see the virtual face and glasses from different viewpoints. The try-on process is fully automated and does not require the user to provide anything else but the picture and selection of the glasses frames to test.",2019,2019 IEEE 23rd International Symposium on Consumer Technologies (ISCT),,10.1109/ISCE.2019.8900979,
3933e323653ff27e68c3458d245b47e3e37f52fd,1,0,Evaluation of a 3 D-aided Pose Invariant 2 D Face Recognition System,"A few well-developed face recognition pipelines have been reported in recent years. Most of the face-related work focuses on a specific module or demonstrates a research idea. In this paper, we present a pose-invariant 3D-aided 2D face recognition system (3D2D-PIFR) that is robust to pose variations as large as 90◦ by leveraging deep learning technology. We describe the architecture and the interface of 3D2D-PIFR, and introduce each module in detail (Code for module algorithms are kindly provided for the authors and their institution). Experiments are conducted on the UHDB31 and IJB-A, demonstrating that 3D2D-PIFR outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on UHDB31 and 3% on IJB-A dataset in average. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques. A video demo of 3D2D-PIFR is available at http://cbl. uh.edu/index.php/pages/research/demos.",2017,,,,https://pdfs.semanticscholar.org/3933/e323653ff27e68c3458d245b47e3e37f52fd.pdf
397689cdbf66e87788d936be0c906fcdc669c409,0,1,THIN: THrowable Information Networks and Application for Facial Expression Recognition In The Wild,"For a number of tasks solved using deep learning techniques, an exogenous variable can be identified such that (a) it heavily influences the appearance of the different classes, and (b) an ideal classifier should be invariant to this variable. An example of such exogenous variable is identity if facial expression recognition (FER) is considered. In this paper, we propose a dual exogenous/endogenous representation. The former captures the exogenous variable whereas the second one models the task at hand (e.g. facial expression). We design a prediction layer that uses a deep ensemble conditioned by the exogenous representation. It employs a differential tree gate that learns an adaptive weak predictor weighting, therefore modeling a partition of the exogenous representation space, upon which the weak predictors specialize. This layer explicitly models the dependency between the exogenous variable and the predicted task (a). We also propose an exogenous dispelling loss to remove the exogenous information from the endogenous representation, enforcing (b). Thus, the exogenous information is used two times in a throwable fashion, first as a conditioning variable for the target task, and second to create invariance within the endogenous representation. We call this method THIN, standing for THrowable Information Networks. We experimentally validate THIN in several contexts where an exogenous information can be identified, such as digit recognition under large rotations and shape recognition at multiple scales. We also apply it to FER with identity as the exogenous variable. In particular, we demonstrate that THIN significantly outperforms state-of-the-art approaches on several challenging datasets.",2020,ArXiv,2010.07614,,https://arxiv.org/pdf/2010.07614.pdf
3998f7d6022f67a1cf6050bccd3131911fec5a8a,1,1,Self Residual Attention Network for Deep Face Recognition,"Discriminative feature embedding is of essential importance in the field of large scale face recognition. In this paper, we propose a self residual attention-based convolutional neural network (SRANet) for discriminative face feature embedding, which aims to learn the long-range dependencies of face images by decreasing the information redundancy among channels and focusing on the most informative components of spatial feature maps. More specifically, the proposed attention module consists of the self channel attention (SCA) block and self spatial attention (SSA) block which adaptively aggregates the feature maps in both channel and spatial domains to learn the inter-channel relationship matrix and the inter-spatial relationship matrix; moreover, matrix multiplications are conducted for a refined and robust face feature. With the attention module we proposed, we can make standard convolutional neural networks (CNNs), such as ResNet-50 and ResNet-101, which have more discriminative power for deep face recognition. The experiments on Labelled Faces in the Wild (LFW), Age Database (AgeDB), Celebrities in Frontal Profile (CFP), and MegaFace Challenge 1 (MF1) show that our proposed SRANet structure consistently outperforms naive CNNs and achieves state-of-the-art performance.",2019,IEEE Access,,10.1109/ACCESS.2019.2913205,
39ac41b539ed423292577e798eeae37b69eb460e,1,0,Machines Learn Appearance Bias in Face Recognition,"We seek to determine whether state-of-the-art, black box face recognition techniques can learn first-impression appearance bias from human annotations. With FaceNet, a popular face recognition architecture, we train a transfer learning model on human subjects' first impressions of personality traits in other faces. We measure the extent to which this appearance bias is embedded and benchmark learning performance for six different perceived traits. In particular, we find that our model is better at judging a person's dominance based on their face than other traits like trustworthiness or likeability, even for emotionally neutral faces. We also find that our model tends to predict emotions for deliberately manipulated faces with higher accuracy than for randomly generated faces, just like a human subject. Our results lend insight into the manner in which appearance biases may be propagated by standard face recognition models.",2020,ArXiv,2002.05636,,https://arxiv.org/pdf/2002.05636.pdf
39b615c73810e13998df3df9d5e73aebd3e67dab,1,0,A Compact Embedding for Facial Expression Similarity,"Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between expressions measured in the action unit space need not correspond to how humans perceive expression similarity. Different from previous work, our goal is to describe facial expressions in a continuous fashion using a compact embedding space that mimics human visual preferences. To achieve this goal, we collect a large-scale faces-in-the-wild dataset with human annotations in the form: Expressions A and B are visually more similar when compared to expression C, and use this dataset to train a neural network that produces a compact (16-dimensional) expression embedding. We experimentally demonstrate that the learned embedding can be successfully used for various applications such as expression retrieval, photo album summarization, and emotion recognition. We also show that the embedding learned using the proposed dataset performs better than several other embeddings learned using existing emotion or action unit datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1811.11283,10.1109/CVPR.2019.00583,https://arxiv.org/pdf/1811.11283.pdf
39c10888a470b92b917788c57a6fd154c97b421c,1,0,Joint multi-feature fusion and attribute relationships for facial attribute prediction,"Predicting facial attributes from wild images is very challenging due to complex face variations. The key to this problem is to construct rich facial representations and take advantage of attribute relationships. In this paper, we propose a novel multi-task convolutional neural network (MTCNN) and a supervision signal called Online Batch Relation Loss (OBRL) for face attribute prediction in the wild. In particular, MTCNN builds informative facial features by embedding identity, age and race features from IdentityNet, AgeNet and RaceNet respectively. In addition, OBRL can diminish distribution shift of attribute relationships by mining attribute correlation within each minibatch, while it penalizes the probability divergence between a pair of attributes. In order to learn discriminative attribute features, we feed AttributeNet with fused facial features and partition attributes into nine groups to share intra-group features and reduce redundant computation. Finally, AttributeNet is optimized with the joint supervision of Cross Entropy Loss and OBRL. Experiments on CelebA and LFWA show that the proposed method outperforms the state-of-the-art methods with a significant margin.",2017,2017 IEEE Visual Communications and Image Processing (VCIP),,10.1109/VCIP.2017.8305036,
39f516aa3b575c9e457ac53ac8013e31f9010982,1,1,Beyond Identity: What Information Is Stored in Biometric Face Templates?,"Deeply-learned face representations enable the success of current face recognition systems. Despite the ability of these representations to encode the identity of an individual, recent works have shown that more information is stored within, such as demographics, image characteristics, and social traits. This threatens the user's privacy, since for many applications these templates are expected to be solely used for recognition purposes. Knowing the encoded information in face templates helps to develop bias-mitigating and privacy-preserving face recognition technologies. This work aims to support the development of these two branches by analysing face templates regarding 113 attributes. Experiments were conducted on two publicly available face embeddings. For evaluating the predictability of the attributes, we trained a massive attribute classifier that is additionally able to accurately state its prediction confidence. This allows us to make more sophisticated statements about the attribute predictability. The results demonstrate that up to 74 attributes can be accurately predicted from face templates. Especially non-permanent attributes, such as age, hairstyles, haircolors, beards, and various accessories, found to be easily-predictable. Since face recognition systems aim to be robust against these variations, future research might build on this work to develop more understandable privacy preserving solutions and build robust and fair face templates.",2020,ArXiv,2009.09918,,https://arxiv.org/pdf/2009.09918.pdf
3a27d164e931c422d16481916a2fa6401b74bcef,1,0,Anti-Makeup: Learning A Bi-Level Adversarial Network for Makeup-Invariant Face Verification,"Makeup is widely used to improve facial attractiveness and is well accepted by the public. However, different makeup styles will result in significant facial appearance changes. It remains a challenging problem to match makeup and non-makeup face images. This paper proposes a learning from generation approach for makeup-invariant face verification by introducing a bi-level adversarial network (BLAN). To alleviate the negative effects from makeup, we first generate non-makeup images from makeup ones, and then use the synthesized non-makeup images for further verification. Two adversarial networks in BLAN are integrated in an end-to-end deep network, with the one on pixel level for reconstructing appealing facial images and the other on feature level for preserving identity information. These two networks jointly reduce the sensing gap between makeup and non-makeup images. Moreover, we make the generator well constrained by incorporating multiple perceptual losses. Experimental results on three benchmark makeup face datasets demonstrate that our method achieves state-of-the-art verification accuracy across makeup status and can produce photo-realistic non-makeup face images.",2018,AAAI,1709.03654,,https://arxiv.org/pdf/1709.03654.pdf
3afcf32596398c04bd734ee6469506522e224e52,0,1,Character Grounding and Re-identification in Story of Videos and Text Descriptions,"We address character grounding and re-identification in multiple story-based videos like movies and associated text descriptions. In order to solve these related tasks in a mutually rewarding way, we propose a model named Character in Story Identification Network (CiSIN). Our method builds two semantically informative representations via joint training of multiple objectives for character grounding, video/text reidentification and gender prediction: Visual Track Embedding from videos and Textual Character Embedding from text context. These two representations are learned to retain rich semantic multimodal information that enables even simple MLPs to achieve the state-of-the-art performance on the target tasks. More specifically, our CiSIN model achieves the best performance in the Fill-in the Characters task of LSMDC 2019 challenges [35]. Moreover, it outperforms previous state-of-the-art models in M-VAD Names dataset [30] as a benchmark of multimodal character grounding and re-identification.",2020,ECCV,,10.1007/978-3-030-58558-7_32,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500528.pdf
3afd41d6489316ad0fc10f39decb7f96cec118ff,1,1,DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition,"Heterogeneous Face Recognition (HFR) refers to matching cross-domain faces, playing a crucial role in public security. Nevertheless, HFR is confronted with the challenges from large domain discrepancy and insufficient heterogeneous data. In this paper, we formulate HFR as a dual generation problem, and tackle it via a novel Dual Variational Generation (DVG-Face) framework. Specifically, a dual variational generator is elaborately designed to learn the joint distribution of paired heterogeneous images. However, the small-scale paired heterogeneous training data may limit the identity diversity of sampling. With this in mind, we propose to integrate abundant identity information of large-scale VIS images into the joint distribution. Furthermore, a pairwise identity preserving loss is imposed on the generated paired heterogeneous images to ensure their identity consistency. As a consequence, massive new diverse paired heterogeneous images with the same identity can be generated from noises. The identity consistency and diversity properties allow us to employ these generated images to train the HFR network via a contrastive learning mechanism, yielding both domain invariant and discriminative embedding features. Concretely, the generated paired heterogeneous images are regarded as positive pairs, and the images obtained from different samplings are considered as negative pairs. Our method achieves superior performances over state-of-the-art methods on seven databases belonging to five HFR tasks, including NIR-VIS, Sketch-Photo, Profile-Frontal Photo, Thermal-VIS, and ID-Camera. The related code will be released at this https URL.",2020,ArXiv,2009.09399,,https://arxiv.org/pdf/2009.09399.pdf
3b06cb17322677395065db08dabb6fd3430a6716,0,1,Deep Cross-Species Feature Learning for Animal Face Recognition via Residual Interspecies Equivariant Network,,2020,ECCV,,10.1007/978-3-030-58583-9_40,
3b1ae606b6376324b339cc7e9466e62c3b85b8fd,1,0,DISCERN: Diversity-based Selection of Centroids for k-Estimation and Rapid Non-stochastic Clustering,"One of the applications of center-based clustering algorithms such as K-means is partitioning data points into K clusters. In some examples, the feature space relates to the underlying problem we are trying to solve, and sometimes we can obtain a suitable feature space. Nevertheless, while K-means is one of the most efficient offline clustering algorithms, it is not equipped to estimate the number of clusters, which is useful in some practical cases. Other practical methods which do are simply too complex, as they require at least one run of K-means for each possible K. In order to address this issue, we propose a K-means initialization similar to K-means++, which would be able to estimate K based on the feature space while finding suitable initial centroids for K-means in a deterministic manner. Then we compare the proposed method, DISCERN, with a few of the most practical K estimation methods, while also comparing clustering results of K-means when initialized randomly, using K-means++ and using DISCERN. The results show improvement in both the estimation and final clustering performance.",2019,ArXiv,1910.05933,10.1007/s13042-020-01193-5,https://arxiv.org/pdf/1910.05933.pdf
3b3941524d97e7f778367a1250ba1efb9205d5fc,0,1,Open Source Face Recognition Performance Evaluation Package,"Biometrics-related research has been accelerated significantly by deep learning technology. However, there are limited open-source resources to help researchers evaluate their deep learning-based biometrics algorithms efficiently, especially for the face recognition tasks. In this work, we design and implement a light-weight, maintainable, scalable, generalizable, and extendable face recognition evaluation toolbox named FaRE that supports both online and offline evaluation to provide feedback to algorithm development and accelerate biometrics-related research. FaRE consists of a set of evaluation metric functions and provides various APIs for commonly-used face recognition datasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be easily extended to include other customized datasets. The package and the pre-trained baseline models will be released for public academic research use after obtaining university approval.",2019,ArXiv,1901.09447,,https://arxiv.org/pdf/1901.09447.pdf
3b4da93fbdf7ae520fa00d39ffa694e850b85162,1,0,Face-Voice Matching using Cross-modal Embeddings,"Face-voice matching is a task to find correspondence between faces and voices. Many researches in cognitive science have confirmed human ability in the face-voice matching tasks. Such ability is useful for creating natural human machine interaction systems and in many other applications. In this paper, we propose a face-voice matching model that learns cross-modal embeddings between face images and voice characteristics. We constructed a novel FVCeleb dataset which consists of face images and utterances from 1,078 persons. These persons were selected from the MS-Celeb-1M face image dataset and the VoxCeleb audio dataset. In two-alternative forced-choice matching task with an audio input and two face-image candidates of the same gender, our model achieved 62.2% and 56.5% accuracy on the FVCeleb and the subset of the GRID corpus, respectively. These results are very similar to human performance reported in cognitive science studies.",2018,MM '18,,10.1145/3240508.3240601,
3b9ceb12d90523cb76d2302d5842a847544adee1,0,1,"Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIX","Existing image inpainting methods often produce artifacts when dealing with large holes in real applications. To address this challenge, we propose an iterative inpainting method with a feedback mechanism. Specifically, we introduce a deep generative model which not only outputs an inpainting result but also a corresponding confidence map. Using this map as feedback, it progressively fills the hole by trusting only high-confidence pixels inside the hole at each iteration and focuses on the remaining pixels in the next iteration. As it reuses partial predictions from the previous iterations as known pixels, this process gradually improves the result. In addition, we propose a guided upsampling network to enable generation of high-resolution inpainting results. We achieve this by extending the Contextual Attention module [39] to borrow high-resolution feature patches in the input image. Furthermore, to mimic real object removal scenarios, we collect a large object mask dataset and synthesize more realistic training data that better simulates user inputs. Experiments show that our method significantly outperforms existing methods in both quantitative and qualitative evaluations. More results and Web APP are available at https://zengxianyu.github.io/iic.",2020,ECCV,,10.1007/978-3-030-58529-7,
3baa3d5325f00c7edc1f1427fcd5bdc6a420a63f,1,0,Enhancing Convolutional Neural Networks for Face Recognition with Occlusion Maps and Batch Triplet Loss,"Despite the recent success of convolutional neural networks for computer vision applications, unconstrained face recognition remains a challenge. In this work, we make two contributions to the field. Firstly, we consider the problem of face recognition with partial occlusions and show how current approaches might suffer significant performance degradation when dealing with this kind of face images. We propose a simple method to find out which parts of the human face are more important to achieve a high recognition rate, and use that information during training to force a convolutional neural network to learn discriminative features from all the face regions more equally, including those that typical approaches tend to pay less attention to. We test the accuracy of the proposed method when dealing with real-life occlusions using the AR face database. Secondly, we propose a novel loss function called batch triplet loss that improves the performance of the triplet loss by adding an extra term to the loss function to cause minimisation of the standard deviation of both positive and negative scores. We show consistent improvement in the Labeled Faces in the Wild (LFW) benchmark by applying both proposed adjustments to the convolutional neural network training.",2018,Image Vis. Comput.,1707.07923,10.1016/j.imavis.2018.09.011,https://arxiv.org/pdf/1707.07923.pdf
3beb1528ce4770ed5b63e424202acc981aeaf149,1,0,Deep convolutional neural networks in the face of caricature,"Real-world face recognition requires us to perceive the uniqueness of a face across variable images. Deep convolutional neural networks (DCNNs) accomplish this feat by generating robust face representations that can be analysed in a multidimensional ‘face space’. We examined the organization of viewpoint, illumination, gender and identity in this space. We found that DCNNs create a highly organized face similarity structure in which identities and images coexist. Natural image variation is organized hierarchically, with face identity nested under gender, and illumination and viewpoint nested under identity. To examine identity, we caricatured faces and found that identification accuracy increased with the strength of identity information in a face, and caricature representations ‘resembled’ their veridical counterparts—mimicking human perception. DCNNs therefore offer a theoretical framework for reconciling decades of behavioural and neural results that emphasized either the image or the face in representations, without understanding how a neural code could seamlessly accommodate both.Human face recognition is robust to changes in viewpoint, illumination, facial expression and appearance. The authors investigated face recognition in deep convolutional neural networks by manipulating the strength of identity information in a face by caricaturing. They found that networks create a highly organized face similarity structure in which identities and images coexist.",2019,,1812.10902,10.1038/s42256-019-0111-7,https://arxiv.org/pdf/1812.10902.pdf
3c5ba48d25fbe24691ed060fa8f2099cc9eba14f,1,1,Racial Faces in-the-Wild: Reducing Racial Bias by Deep Unsupervised Domain Adaptation,"Despite of the progress achieved by deep learning in face recognition (FR), more and more people find that racial bias explicitly degrades the performance in realistic FR systems. Facing the fact that existing training and testing databases consist of almost Caucasian subjects, there are still no independent testing databases to evaluate racial bias and even no training databases and methods to reduce it. To facilitate the research towards conquering those unfair issues, this paper contributes a new dataset called Racial Faces in-the-Wild (RFW) database with two important uses, 1) racial bias testing: four testing subsets, namely Caucasian, Asian, Indian and African, are constructed, and each contains about 3000 individuals with 6000 image pairs for face verification, 2) racial bias reducing: one labeled training subset with Caucasians and three unlabeled training subsets with Asians, Indians and Africans are offered to encourage FR algorithms to transfer recognition knowledge from Caucasians to other races. For we all know, RFW is the first database for measuring racial bias in FR algorithms. After proving the existence of domain gap among different races and the existence of racial bias in FR algorithms, we further propose a deep information maximization adaptation network (IMAN) to bridge the domain gap, and comprehensive experiments show that the racial bias could be narrowed-down by our algorithm.",2018,ArXiv,1812.00194,,https://arxiv.org/pdf/1812.00194.pdf
3c68f95ee183e43b476d33448876979576d5d729,0,1,SimSwap: An Efficient Framework For High Fidelity Face Swapping,"We propose an efficient framework, called Simple Swap (SimSwap), aiming for generalized and high fidelity face swapping. In contrast to previous approaches that either lack the ability to generalize to arbitrary identity or fail to preserve attributes like facial expression and gaze direction, our framework is capable of transferring the identity of an arbitrary source face into an arbitrary target face while preserving the attributes of the target face. We overcome the above defects in the following two ways. First, we present the ID Injection Module (IIM) which transfers the identity information of the source face into the target face at feature level. By using this module, we extend the architecture of an identity-specific face swapping algorithm to a framework for arbitrary face swapping. Second, we propose the Weak Feature Matching Loss which efficiently helps our framework to preserve the facial attributes in an implicit way. Extensive experiments on wild faces demonstrate that our SimSwap is able to achieve competitive identity performance while preserving attributes better than previous state-of-the-art methods.",2020,ACM Multimedia,,10.1145/3394171.3413630,
3d17bd832ca3e3f1fc84624a3093ae84d2bce041,0,1,Hierarchical Feature-Pair Relation Networks for Face Recognition,"We propose a novel face recognition method using a Hierarchical Feature Relational Network (HFRN) which extracts facial part representations around facial landmark points, and predicts hierarchical latent relations between facial part representations. These hierarchical latent relations should be unique relations within the same identity and discriminative relations among different identities for face recognition task. To do this, the HFRN extracts appearance features as facial parts representations around facial landmark points on the feature maps, globally pool these extracted appearance features onto single feature vectors, and captures the relations for the pairs of appearance features. The HFRN captures the locally detailed relations in the low-level layers and the locally abstracted global relations in the high-level layers for the pairs of appearance features extracted around facial landmark points projected on each layer, respectively. These relations from low-level layers to high-level layers are concatenated into a single hierarchical relation feature. To further improve the accuracy of face recognition, we combine the global appearance feature with the hierarchical relation feature. In experiments, the proposed method achieves the comparable performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging IARPA Janus Benchmark A (IJB-A) and IARPA Janus Benchmark B (IJB-B) datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00286,http://openaccess.thecvf.com/content_CVPRW_2019/papers/Biometrics/Kang_Hierarchical_Feature-Pair_Relation_Networks_for_Face_Recognition_CVPRW_2019_paper.pdf
3d73c90a44c1cab72315db02f20bf8e939adb623,0,1,MagNetO: X-vector Magnitude Estimation Network plus Offset for Improved Speaker Recognition,"We present a magnitude estimation network that is combined with a modified ResNet x-vector system to generate embeddings whose inner product is able to produce calibrated scores with increased discrimination. A three-step training procedure is used. First, the network is trained using short segments and a multi-class cross-entropy loss with angular margin softmax. During the second step, only a reduced subset of the DNN parameters are refined using full-length recordings. Finally, the magnitude estimation network is trained using a binary crossentropy loss over pairs of target and non-target trials. The resulting system is evaluated on 4 widely-used benchmarks and provides significant discrimination and calibration gains at multiple operating points.",2020,,,10.21437/odyssey.2020-1,https://www.isca-speech.org/archive/Odyssey_2020/pdfs/65.pdf
3d85cf942efda695347c7d95485fcd1e6796ee3a,1,0,Generating Photo-Realistic Training Data to Improve Face Recognition Accuracy,"In this paper we investigate the feasibility of using synthetic data to augment face datasets. In particular, we propose a novel generative adversarial network (GAN) that can disentangle identity-related attributes from non-identity-related attributes. This is done by training an embedding network that maps discrete identity labels to an identity latent space that follows a simple prior distribution, and training a GAN conditioned on samples from that distribution. Our proposed GAN allows us to augment face datasets by generating both synthetic images of subjects in the training set and synthetic images of new subjects not in the training set. By using recent advances in GAN training, we show that the synthetic images generated by our model are photo-realistic, and that training with augmented datasets can indeed increase the accuracy of face recognition models as compared with models trained with real images alone.",2018,ArXiv,1811.00112,,https://arxiv.org/pdf/1811.00112.pdf
3dc522a6576c3475e4a166377cbbf4ba389c041f,1,0,The iNaturalist Challenge 2017 Dataset,"Existing image classification datasets used in computer vision tend to have an even number of images for each object category. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist Challenge 2017 dataset - an image classification benchmark consisting of 675,000 images with over 5,000 different species of plants and animals. It features many visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, have been verified by multiple citizen scientists, and feature a large class imbalance.  We discuss the collection of the dataset and present baseline results for state-of-the-art computer vision classification models. Results show that current non-ensemble based methods achieve only 64% top one classification accuracy, illustrating the difficulty of the dataset. Finally, we report results from a competition that was held with the data.",2017,ArXiv,1707.06642,,https://arxiv.org/pdf/1707.06642.pdf
3df47644e7aed93e11897d29d48a14a2e99d56f4,1,1,Spherical Feature Transform for Deep Metric Learning,"Data augmentation in feature space is effective to increase data diversity. Previous methods assume that different classes have the same covariance in their feature distributions. Thus, feature transform between different classes is performed via translation. However, this approach is no longer valid for recent deep metric learning scenarios, where feature normalization is widely adopted and all features lie on a hypersphere.  This work proposes a novel spherical feature transform approach. It relaxes the assumption of identical covariance between classes to an assumption of similar covariances of different classes on a hypersphere. Consequently, the feature transform is performed by a rotation that respects the spherical data distributions. We provide a simple and effective training method, and in depth analysis on the relation between the two different transforms. Comprehensive experiments on various deep metric learning benchmarks and different baselines verify that our method achieves consistent performance improvement and state-of-the-art results.",2020,ArXiv,2008.01469,,https://arxiv.org/pdf/2008.01469.pdf
3e134b8c7ddb4aeaac2ac2ceed54875c85adf6ee,0,1,Compositional Few-Shot Recognition with Primitive Discovery and Enhancing,"Few-shot learning (FSL) aims at recognizing novel classes given only few training samples, which still remains a great challenge for deep learning. However, humans can easily recognize novel classes with only few samples. A key component of such ability is the compositional recognition that human can perform, which has been well studied in cognitive science but is not well explored in FSL. Inspired by such capability of humans, to imitate humans' ability of learning visual primitives and composing primitives to recognize novel classes, we propose an approach to FSL to learn a feature representation composed of important primitives, which is jointly trained with two parts, i.e. primitive discovery and primitive enhancing. In primitive discovery, we focus on learning primitives related to object parts by self-supervision from the order of image splits, avoiding extra laborious annotations and alleviating the effect of semantic gaps. In primitive enhancing, inspired by current studies on the interpretability of deep networks, we provide our composition view for the FSL baseline model. To modify this model for effective composition, inspired by both mathematical deduction and biological studies (the Hebbian Learning rule and the Winner-Take-All mechanism), we propose a soft composition mechanism by enlarging the activation of important primitives while reducing that of others, so as to enhance the influence of important primitives and better utilize these primitives to compose novel classes. Extensive experiments on public benchmarks are conducted on both the few-shot image classification and video recognition tasks. Our method achieves the state-of-the-art performance on all these datasets and shows better interpretability.",2020,ACM Multimedia,2005.06047,10.1145/3394171.3413849,https://arxiv.org/pdf/2005.06047.pdf
3e3227c8e9f44593d2499f4d1302575c77977b2e,1,0,Facial Expression Recognition Using a Large Out-of-Context Dataset,"We develop a method for emotion recognition from facial imagery. This problem is challenging in part because of the subjectivity of ground truth labels and in part because of the relatively small size of existing labeled datasets. We use the FER+ dataset [8], a dataset with multiple emotion labels per image, in order to build an emotion recognition model that encompasses a full range of emotions. Since the amount of data in the FER+ dataset is limited, we explore the use of a much larger face dataset, MS-Celeb-1M [41], in conjunction with the FER+ dataset. Specific layers within an Inception-ResNet-v1 [13, 38] model trained for facial recognition are used for the emotion recognition problem. Thus, we leverage the MS-Celeb-1M dataset in addition to the FER+ dataset and experiment with different architectures to assess the overall performance of neural networks to recognize emotion using facial imagery.",2018,2018 IEEE Winter Applications of Computer Vision Workshops (WACVW),,10.1109/WACVW.2018.00012,
3e340dd30ff1e5e935ff9a059d9b02a805bb5b64,1,1,A Multi-Task Comparator Framework for Kinship Verification,"Approaches for kinship verification often rely on cosine distances between face identification features. However, due to gender bias inherent in these features, it is hard to reliably predict whether two opposite-gender pairs are related. Instead of fine tuning the feature extractor network on kinship verification, we propose a comparator network to cope with this bias. After concatenating both features, cascaded local expert networks extract the information most relevant for their corresponding kinship relation. We demonstrate that our framework is robust against this gender bias and achieves comparable results on two tracks of the RFIW Challenge 2020. Moreover, we show how our framework can be further extended to handle partially known or unknown kinship relations.",2020,ArXiv,2006.01615,,https://arxiv.org/pdf/2006.01615.pdf
3e4a49b86c9c34e27e00a0f250b3d82f269cf153,1,1,UniformFace: Learning Deep Equidistributed Representation for Face Recognition,"In this paper, we propose a new supervision objective named uniform loss to learn deep equidistributed representations for face recognition. Most existing methods aim to learn discriminative face features, encouraging large inter-class distances and small intra-class variations. However, they ignore the distribution of faces in the holistic feature space, which may lead to severe locality and unbalance. With the prior that faces lie on a hypersphere manifold, we impose an equidistributed constraint by uniformly spreading the class centers on the manifold, so that the minimum distance between class centers can be maximized through complete exploitation of the feature space. To this end, we consider the class centers as like charges on the surface of hypersphere with inter-class repulsion, and minimize the total electric potential energy as the uniform loss. Extensive experimental results on the MegaFace Challenge I, IARPA Janus Benchmark A (IJB-A), Youtube Faces (YTF) and Labeled Faces in the Wild (LFW) datasets show the effectiveness of the proposed uniform loss.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00353,http://ivg.au.tsinghua.edu.cn/people/Yueqi_Duan/CVPR19_UniformFace%20Learning%20Deep%20Equidistributed%20Representation%20for%20Face%20Recognition.pdf
3e50340855891fcc7284581c0023e108bb9436ae,1,1,Semi-Siamese Training for Shallow Face Learning,"Most existing public face datasets, such as MS-Celeb-1M and VGGFace2, provide abundant information in both breadth (large number of IDs) and depth (sufficient number of samples) for training. However, in many real-world scenarios of face recognition, the training dataset is limited in depth, i.e. only two face images are available for each ID. $\textit{We define this situation as Shallow Face Learning, and find it problematic with existing training methods.}$ Unlike deep face data, the shallow face data lacks intra-class diversity. As such, it can lead to collapse of feature dimension and consequently the learned network can easily suffer from degeneration and over-fitting in the collapsed dimension. In this paper, we aim to address the problem by introducing a novel training method named Semi-Siamese Training (SST). A pair of Semi-Siamese networks constitute the forward propagation structure, and the training loss is computed with an updating gallery queue, conducting effective optimization on shallow training data. Our method is developed without extra-dependency, thus can be flexibly integrated with the existing loss functions and network architectures. Extensive experiments on various benchmarks of face recognition show the proposed method significantly improves the training, not only in shallow face learning, but also for conventional deep face data.",2020,ECCV,2007.08398,10.1007/978-3-030-58548-8_3,https://arxiv.org/pdf/2007.08398.pdf
3e7734186aff2e3ecd4d2e3c70f20ab0143a8e56,0,1,What's in a Loss Function for Image Classification?,"It is common to use the softmax cross-entropy loss to train neural networks on classification datasets where a single class label is assigned to each example. However, it has been shown that modifying softmax cross-entropy with label smoothing or regularizers such as dropout can lead to higher performance. This paper studies a variety of loss functions and output layer regularization strategies on image classification tasks. We observe meaningful differences in model predictions, accuracy, calibration, and out-of-distribution robustness for networks trained with different objectives. However, differences in hidden representations of networks trained with different objectives are restricted to the last few layers; representational similarity reveals no differences among network layers that are not close to the output. We show that all objectives that improve over vanilla softmax loss produce greater class separation in the penultimate layer of the network, which potentially accounts for improved performance on the original task, but results in features that transfer worse to other tasks.",2020,ArXiv,2010.16402,,https://arxiv.org/pdf/2010.16402.pdf
3f22f112443b9028cd0c183474fa1ab27580e816,0,1,AdaSample: Adaptive Sampling of Hard Positives for Descriptor Learning,"Triplet loss has been widely employed in a wide range of computer vision tasks, including local descriptor learning. The effectiveness of the triplet loss heavily relies on the triplet selection, in which a common practice is to first sample intra-class patches (positives) from the dataset for batch construction and then mine in-batch negatives to form triplets. For high-informativeness triplet collection, researchers mostly focus on mining hard negatives in the second stage, while paying relatively less attention to constructing informative batches. To alleviate this issue, we propose AdaSample, an adaptive online batch sampler, in this paper. Specifically, hard positives are sampled based on their informativeness. In this way, we formulate a hardness-aware positive mining pipeline within a novel maximum loss minimization training protocol. The efficacy of the proposed method is evaluated on several standard benchmarks, where it demonstrates a significant and consistent performance gain on top of the existing strong baselines.",2019,ArXiv,1911.1211,,https://arxiv.org/pdf/1911.12110.pdf
3f4a0bb3f031cbe7b0043243116e779c1e75d493,1,0,"FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age","Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups.",2019,ArXiv,1908.04913,,https://arxiv.org/pdf/1908.04913.pdf
3f68de71a79d0f689902f8709a6a6259be30d9b1,0,1,Efficient Low-Resolution Face Recognition via Bridge Distillation,"Face recognition in the wild is now advancing towards light-weight models, fast inference speed and resolution-adapted capability. In this paper, we propose a bridge distillation approach to turn a complex face model pretrained on private high-resolution faces into a light-weight one for low-resolution face recognition. In our approach, such a cross-dataset resolution-adapted knowledge transfer problem is solved via two-step distillation. In the first step, we conduct cross-dataset distillation to transfer the prior knowledge from private high-resolution faces to public high-resolution faces and generate compact and discriminative features. In the second step, the resolution-adapted distillation is conducted to further transfer the prior knowledge to synthetic low-resolution faces via multi-task learning. By learning low-resolution face representations and mimicking the adapted high-resolution knowledge, a light-weight student model can be constructed with high efficiency and promising accuracy in recognizing low-resolution faces. Experimental results show that the student model performs impressively in recognizing low-resolution faces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed reaches up to 14,705, 934 and 763 faces per second on GPU, CPU and mobile phone, respectively.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2020.2995049,
3f6d5913fc03dc08f35b35c91d50ad5bc7a21955,1,1,Analysis of Gender Inequality In Face Recognition Accuracy,"We present a comprehensive analysis of how and why face recognition accuracy differs between men and women. We show that accuracy is lower for women due to the combination of(1) the impostor distribution for women having a skew toward higher similarity scores, and (2) the genuine distribution for women having a skew toward lower similarity scores. We show that this phenomenon of the impostor and genuine distributions for women shifting closer towards each other is general across datasets of African-American, Caucasian, and Asian faces. We show that the distribution of facial expressions may differ between male/female, but that the accuracy difference persists for image subsets rated confidently as neutral expression. The accuracy difference also persists for image subsets rated as close to zero pitch angle. Even when removing images with forehead partially occluded by hair/hat, the same impostor/genuine accuracy difference persists. We show that the female genuine distribution improves when only female images without facial cosmetics are used, but that the female impostor distribution also degrades at the same time. Lastly, we show that the accuracy difference persists even if a state-of-the-art deep learning method is trained from scratch using training data explicitly balanced between male and female images and subjects.",2020,2020 IEEE Winter Applications of Computer Vision Workshops (WACVW),2002.00065,10.1109/WACVW50321.2020.9096947,https://arxiv.org/pdf/2002.00065.pdf
3f701f10241627903118a965b022fbb8b807a022,0,1,Do We Need Depth in State-Of-The-Art Face Authentication?,"Some face recognition methods are designed to utilize geometric features extracted from depth sensors to handle the challenges of single-image based recognition technologies. However, calculating the geometrical data is an expensive and challenging process. Here, we introduce a novel method that learns distinctive geometric features from stereo camera systems without the need to explicitly compute the facial surface or depth map. The raw face stereo images along with coordinate maps allow a CNN to learn geometric features. This way, we keep the simplicity and cost efficiency of recognition from a single image, while enjoying the benefits of geometric data without explicitly reconstructing it. We demonstrate that the suggested method outperforms both existing single-image and explicit depth based methods on large-scale benchmarks. We also provide an ablation study to show that the suggested method uses the coordinate maps to encode more informative features.",2020,ArXiv,2003.10895,,https://arxiv.org/pdf/2003.10895.pdf
3f9fa22f3c751caba28eae49abf5b03680ebdd5f,1,1,Improving ResNet-based Feature Extractor for Face Recognition via Re-ranking and Approximate Nearest Neighbor,"This paper proposes a framework for face recognition based on feature extractor from ResNet, together with other steps for performance improvement, including face detection, face alignment, face verification/identification, and re-ranking via Approximate Nearest Neighbor Search (ANNS). First, we evaluate two face detection algorithms, MTCNN, and FaceBoxes on three common face detection benchmarks, and then summarize the best usage scenario for each approach. Second, with certain preprocessing and postprocessing, our system selects the ResNet-based feature extractor, which achieves 99.33% verification accuracy on the LFW benchmark. Third, we use the penalty curve to determine the best configuration and obtain improved results of face verification. Based on the proposed preprocessing and post-processing, our method not only boosts accuracy from 84.3% to 86.5% in large inter-class variation datasets (CASIA - WebFace) but improves Rank-l accuracy from 86.6% to 87.7% in large intra-class variation datasets (FG-NET).",2019,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2019.8909884,http://mirlab.org/jang/research/paper/2019-avss/avss_final0716.pdf
3fa21735bc0f080601a352505bf543b984443bab,0,1,Compressed Self-Attention for Deep Metric Learning with Low-Rank Approximation,"In this paper, we aim to boost the performance of deep metric learning by using the self-attention (SA) mechanism. However, due to the pairwise similarity measurement, the cost of storing and manipulating the complete attention maps makes it infeasible for large inputs such as images. To solve this problem, we propose a compressed selfattention with low-rank approximation (CSALR) module, which significantly reduces the computation and memory costs without sacrificing the accuracy. In CSALR, the original attention map is decomposed into a landmark attention map and a combination coefficient map with a small number of landmark feature vectors sampled from the input feature map by average pooling. Thanks to the efficiency of CSALR, we can apply CSALR to highresolution shallow convolutional layers and implement a multi-head form of CSALR, which further boosts the performance. We evaluate the proposed CSALR on person re-identification which is a typical metric learning task. Extensive experiments shows the effectiveness and efficiency of CSALR in deep metric learning and its superiority over the baselines.",2020,IJCAI,,10.24963/ijcai.2020/285,https://www.ijcai.org/Proceedings/2020/0285.pdf
3fcecdded1dad3385e06c0e82f21746c24deac89,1,0,Deeply-learned Hybrid Representations for Facial Age Estimation,"In this paper, we propose a novel unified network named Deep Hybrid-Aligned Architecture for facial age estimation. It contains global, local and global-local branches, which are jointly optimized and thus can capture multiple types of features with complementary information. In each sub-network of each branch, we employ a separate loss to extract the independent region features and use a recurrent fusion to explore correlations among them. Considering that pose variations may lead to misalignment in different regions, we design an Aligned Region Pooling operation to generate aligned region features. Moreover, a new large private age dataset named Web-FaceAge owning more than 120K samples is collected under diverse scenes and spanning a large age range. Experiments on five age benchmark datasets, including Web-FaceAge, Morph, FG-NET, CACD and Chalearn LAP 2015, show that the proposed method outperforms the state-of-the-art approaches significantly.",2019,IJCAI,,10.24963/ijcai.2019/492,https://pdfs.semanticscholar.org/4cda/13ec06fc64ff16f0e6bcd13335e20d84959b.pdf
3ff7839a184f0b5d5f98586067ca551540aba1a3,1,0,Are French Really That Different? Recognizing Europeans from Faces Using Data-Driven Learning,"Travel agents and retailers are curious about where their customers come from, which would help them increase their sales and optimize their marketing strategy. In this study, we present a system to predict where people come from in the European region only using their faces. The countries that have been chosen for the study are Russia, Italy, Germany, Spain, and France, based on diversity and representativeness. These countries have been well known for their economy, population, and political impact. First, we implement different neural network classifiers on the dataset of people's faces that we have collected from Twitter. Next, we investigate in more detail 11 different facial features that may help differentiate ethnic groups representative of those five countries. Our system achieves an accuracy of over 50%, more than twice as good as that of humans. Furthermore, we uncover and interpret using genetic anthropological evidences the various differences and similarities between people's faces across geographical distances among different contingents.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545887,http://vietduy.me/European.pdf
40068df11f4b249e7aa85081648e8e5a94224ea1,1,1,Face recognition with dense supervision,"Abstract Recent advances in face recognition mostly concentrate on designing more discriminative loss functions or adding normalization on features/weights to make a single feature more accurate. In this work, inspired by the frequently used multi-patch ensemble method for face recognition and part-based models for person re-identification, we propose a novel training strategy to enhance the discriminability of deeply learned feature from another perspective, namely learning with dense supervision. The main idea is to apply multiple classification losses on top of multiple component features extracted from a single network. Ideally, each component feature is expected to be accurate and have low correlation with the others. To this end, we first design a metric called feature consistency to evaluate the correlation between one component feature and the others, which is defined as the sum of distances between one component feature and the others, where the distance here is measured with KL divergence between corresponding softmax probabilities. Then we use feature consistency to select which component features to sample for one learning pass by importance sampling. The dense supervision significantly outperforms the single supervision baseline and even performs on par with its multi-patch ensemble counterpart which has much more parameters (9×). Our experimental results match state-of-the-art performance on LFW, YTF, MegaFace and surpass the others on LFW BLUFR and VGGFace2 pose protocol, thereby achieving state-of-the-art. Specially, results on VGGFace2 also show the superiority of the dense supervision on cross-pose face matching.",2020,Neurocomputing,,10.1016/j.neucom.2019.12.052,
403c69991bcfd4d37e77da69d264023d25456d5c,0,1,"Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace","Affective computing has been largely limited in terms of available data resources. The need to collect and annotate diverse in-the-wild datasets has become apparent with the rise of deep learning models, as the default approach to address any computer vision task. Some in-the-wild databases have been recently proposed. However: i) their size is small, ii) they are not audiovisual, iii) only a small part is manually annotated, iv) they contain a small number of subjects, or v) they are not annotated for all main behavior tasks (valence-arousal estimation, action unit detection and basic expression classification). To address these, we substantially extend the largest available in-the-wild database (Aff-Wild) to study continuous emotions such as valence and arousal. Furthermore, we annotate parts of the database with basic expressions and action units. As a consequence, for the first time, this allows the joint study of all three types of behavior states. We call this database Aff-Wild2. We conduct extensive experiments with CNN and CNN-RNN architectures that use visual and audio modalities; these networks are trained on Aff-Wild2 and their performance is then evaluated on 10 publicly available emotion databases. We show that the networks achieve state-of-the-art performance for the emotion recognition tasks. Additionally, we adapt the ArcFace loss function in the emotion recognition context and use it for training two new networks on Aff-Wild2 and then re-train them in a variety of diverse expression recognition databases. The networks are shown to improve the existing state-of-the-art. The database, emotion recognition models and source code are available at this http URL.",2019,BMVC,1910.04855,,https://arxiv.org/pdf/1910.04855.pdf
406c5aeca71011fd8f8bd233744a81b53ccf635a,1,0,Scalable softmax loss for face verification,"Thanks to the recent development of deep convolutional neural networks, the performance of face verification has increased significantly. The softmax loss function is used mostly to make CNN models trained well. In order to get more robust face feature by deep CNN model, this paper proposes a new supervision signal based on the regular softmax loss function, namely scalable softmax loss, for face verification task. The scalable softmax loss function adjust the contribution to final loss for different training samples by a learned parameter. And, it's important to note that our proposed scalable softmax loss function can be easily implemented using existing deep learning frameworks. Extensive analysis and experiments on Labeled Face in the Wild(LFW) and YouTube Faces(YTF) show the superiority of the scalable softmax loss function in face verification task. Specially, our proposed scalable achieves comparable results on challenging LFW data set and YTF data set with the accuracy 99% and 95.08% respectively. Codes are released at1.",2017,2017 4th International Conference on Systems and Informatics (ICSAI),,10.1109/ICSAI.2017.8248342,
40bb090a4e303f11168dce33ed992f51afe02ff7,1,0,Marginal Loss for Deep Face Recognition,"Convolutional neural networks have significantly boosted the performance of face recognition in recent years due to its high capacity in learning discriminative features. In order to enhance the discriminative power of the deeply learned features, we propose a new supervision signal named marginal loss for deep face recognition. Specifically, the marginal loss simultaneously minimises the intra-class variances as well as maximises the inter-class distances by focusing on the marginal samples. With the joint supervision of softmax loss and marginal loss, we can easily train a robust CNNs to obtain more discriminative deep features. Extensive experiments on several relevant face recognition benchmarks, Labelled Faces in the Wild (LFW), YouTube Faces (YTF), Cross-Age Celebrity Dataset (CACD), Age Database (AgeDB) and MegaFace Challenge, prove the effectiveness of the proposed marginal loss.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.251,http://spiral.imperial.ac.uk/bitstream/10044/1/71265/7/deng_marginal_loss_for_cvpr_2017_paper.pdf
40c5cfe274590ad2c11fbe4db1beb02132b6d737,0,1,Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment,"Facial landmark localisation in images captured in-the-wild is an important and challenging problem. The current state-of-the-art revolves around certain kinds of Deep Convolutional Neural Networks (DCNNs) such as stacked U-Nets and Hourglass networks. In this work, we innovatively propose stacked dense U-Nets for this task. We design a novel scale aggregation network topology structure and a channel aggregation building block to improve the model's capacity without sacrificing the computational complexity and model size. With the assistance of deformable convolutions inside the stacked dense U-Nets and coherent loss for outside data transformation, our model obtains the ability to be spatially invariant to arbitrary input face images. Extensive experiments on many in-the-wild datasets, validate the robustness of the proposed method under extreme poses, exaggerated expressions and heavy occlusions. Finally, we show that accurate 3D face alignment can assist pose-invariant face recognition where we achieve a new state-of-the-art accuracy on CFP-FP.",2018,BMVC,1812.01936,,https://arxiv.org/pdf/1812.01936.pdf
40e24eb322d19d4f8aa6b2756e9babf88162869a,1,0,Video Face Clustering With Unknown Number of Clusters,"Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded. To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.03381,10.1109/ICCV.2019.00513,https://arxiv.org/pdf/1908.03381.pdf
40eb55fa806391c2f45cf5095e9a204a570f54e8,0,1,FaceLeaks: Inference Attacks against Transfer Learning Models via Black-box Queries,"Transfer learning is a useful machine learning framework that allows one to build task-specific models (student models) without significantly incurring training costs using a single powerful model (teacher model) pre-trained with a large amount of data. The teacher model may contain private data, or interact with private inputs. We investigate if one can leak or infer such private information without interacting with the teacher model directly. We describe such inference attacks in the context of face recognition, an application of transfer learning that is highly sensitive to personal privacy.  Under black-box and realistic settings, we show that existing inference techniques are ineffective, as interacting with individual training instances through the student models does not reveal information about the teacher. We then propose novel strategies to infer from aggregate-level information. Consequently, membership inference attacks on the teacher model are shown to be possible, even when the adversary has access only to the student models.  We further demonstrate that sensitive attributes can be inferred, even in the case where the adversary has limited auxiliary information. Finally, defensive strategies are discussed and evaluated. Our extensive study indicates that information leakage is a real privacy threat to the transfer learning framework widely used in real-life situations.",2020,ArXiv,2010.14023,,https://arxiv.org/pdf/2010.14023.pdf
4118b55c7de7e21b8a197a74c9a2e2a71186e631,1,0,A concept ontology triplet network for learning discriminative representations of fine-grained classes,"Triplet network is an efficient method of metric learning, but with the increase of the number of fine-grained images and sample categories, the training of Triplet network is more and more challengeable. In order to solve this problem, this paper proposes an algorithm that effectively combine Concept Ontology Structure with the Triplet network trained of Two-layer Ontology Loss. It not only utilizes semantic knowledge to guide the Concept Ontology Structure of the network, but also makes use of the relationship between the layers to make the network more effective to see the triplets, which enhances the separability of the learned features. At the same time, we also use the bilinear function jointly trained with the Triplet network to enhance the image details, further improving the performance of the network. Finally, the effectiveness of the proposed algorithm is also proved by the results of classification experiments on the fine-grained image databases - Orchid and Fashion60.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09090-3,
412f7b504244e0843226d0e626691d09f10b8ec6,1,0,Video Face Clustering With Self-Supervised Representation Learning,"Characters are a key component of understanding the story conveyed in TV series and movies. With the rise of advanced deep face models, identifying face images may seem like a solved problem. However, as face detectors get better, clustering and identification need to be revisited to address increasing diversity in facial appearance. In this paper, we propose unsupervised methods for feature refinement with application to video face clustering. Our emphasis is on distilling the essential information, identity, from the representations obtained using deep pre-trained face networks. We propose a self-supervised Siamese network that can be trained without the need for video/track based supervision, that can also be applied to image collections. We evaluate our methods on three video face clustering datasets. Thorough experiments including generalization studies show that our methods outperform current state-of-the-art methods on all datasets. The datasets and code are available at https://github.com/vivoutlaw/SSIAM.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2019.2947264,http://www.cs.toronto.edu/~makarand/papers/TBIOM2019_SelfSupervised.pdf
415efb7b4d9d1e5b64dbaf3fe4229ad462acce71,1,0,"Multimodal Intelligence: Representation Learning, Information Fusion, and Applications","Deep learning methods haverevolutionized speech recognition, image recognition, and natural language processing since 2010. Each of these tasks involves a single modality in their input signals. However, many applications in the artificial intelligence field involve multiple modalities. Therefore, it is of broad interest to study the more difficult and complex problem of modeling and learning across multiple modalities. In this paper, we provide a technical review of available models and learning methods for multimodal intelligence. The main focus of this review is the combination of vision and natural language modalities, which has become an important topic in both the computer vision and natural language processing research communities. This review provides a comprehensive analysis of recent works on multimodal deep learning from three perspectives: learning multimodal representations, fusing multimodal signals at various levels, and multimodal applications. Regarding multimodal representation learning, we review the key concepts of embedding, which unify multimodal signals into a single vector space and thereby enable cross-modality signal processing. We also review the properties of many types of embeddings that are constructed and learned for general downstream tasks. Regarding multimodal fusion, this review focuses on special architectures for the integration of representations of unimodal signals for a particular task. Regarding applications, selected areas of a broad interest in the current literature are covered, including image-to-text caption generation, text-to-image generation, and visual question answering. We believe that this review will facilitate future studies in the emerging field of multimodal intelligence for related communities.",2020,IEEE Journal of Selected Topics in Signal Processing,1911.03977,10.1109/JSTSP.2020.2987728,https://arxiv.org/pdf/1911.03977.pdf
418af4f1aee5e35bb20513a5a7a86de4c524434a,1,1,AirFace:Lightweight and Efficient Model for Face Recognition,"With the development of convolutional neural network, significant progress has been made in computer vision tasks. However, the commonly used loss function softmax loss and highly efficient network architectures for common visual tasks are not as effective for face recognition. In this paper, we propose a novel loss function named Li-ArcFace based on ArcFace. Li-ArcFace takes the value of the angle through a linear function as the target logit rather than through cosine function, which has better convergence and performance on low dimensional embedding feature learning for face recognition. In terms of network architecture, we improved the the perfomance of MobileFaceNet by increasing the network depth, width and adding attention module. Besides, we found some useful training tricks for face recognition. Under all the above effects, we won the second place in the deepglint-light challenge of LFR2019.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),1907.12256,10.1109/ICCVW.2019.00327,https://arxiv.org/pdf/1907.12256.pdf
419251a82508758a5d506584065dc70a065f5043,0,1,APA: Adaptive Pose Alignment for Robust Face Recognition,"In this paper, we propose a new face alignment method, called adaptive pose alignment (APA) which can greatly reduce the intra-class difference and correct the noise caused by the traditional method in the alignment process, especially in unconstrained settings. Instead of aligning all faces to the pre-defined, uniform frontal shape, we adaptively learn the alignment templates according to the facial poses and then align each face of training or testing sets to its related template. To further improve the face recognition performance, we propose a simple, yet effective feature normalization method which can generate more discriminative feature representation of a face or template combined with the APA method. Furthermore, we introduce a poseinvariant face recognition pipeline that sequentially applies APA based alignment, deep representation by Softmax or Arcface, and the effective feature normalization procedure. We empirically show that APA based images can accelerate the training of deep face recognition model by aligning all the images to the optimal templates. Moreover, experiments show that the proposed method achieves the state-of-theart performance on challenging IJB-A, IJB-C and CPLFW datasets.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00032,
419f7aa00cfec364ef3dae9d3aa126e3bbd043fe,0,1,Multi-Scale Thermal to Visible Face Verification via Attribute Guided Synthesis,"Thermal-to-visible face verification is a challenging problem due to the large domain discrepancy between the modalities. Existing approaches either attempt to synthesize visible faces from thermal faces or extract robust features from these modalities for cross-modal matching. In this paper, we use attributes extracted from visible images to synthesize the attributepreserved visible images from thermal imagery for cross-modal matching. A pre-trained VGG-Face network is used to extract the attributes from the visible image. Then, a novel multi-scale generator is proposed to synthesize the visible image from the thermal image guided by the extracted attributes. Finally, a pretrained VGG-Face network is leveraged to extract features from the synthesized image and the input visible image for verification. An extended dataset consisting of polarimetric thermal faces of 121 subjects is also introduced. Extensive experiments evaluated on various datasets and protocols demonstrate that the proposed method achieves state-of-the-art per-formance.",2020,ArXiv,2004.09502,,https://arxiv.org/pdf/2004.09502.pdf
41db7308930ea8bf2fbefcab41294fac6f929721,1,0,Transferability and Hardness of Supervised Classification Tasks,"We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets---CelebA (40 tasks), Animals with Attributes~2 (85 tasks), and Caltech-UCSD Birds~200 (312 tasks)---together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for highly transferable attributes.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.08142,10.1109/ICCV.2019.00148,https://arxiv.org/pdf/1908.08142.pdf
4211b0ebaede85cf7e15867dab1631b0a4da8f03,0,1,Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning,"An increasing number of well-trained deep networks have been released online by researchers and developers, enabling the community to reuse them in a plug-and-play way without accessing the training annotations. However, due to the large number of network variants, such public-available trained models are often of different architectures, each of which being tailored for a specific task or dataset. In this paper, we study a deep-model reusing task, where we are given as input pre-trained networks of heterogeneous architectures specializing in distinct tasks, as teacher models. We aim to learn a multitalented and light-weight student model that is able to grasp the integrated knowledge from all such heterogeneous-structure teachers, again without accessing any human annotation. To this end, we propose a common feature learning scheme, in which the features of all teachers are transformed into a common space and the student is enforced to imitate them all so as to amalgamate the intact knowledge. We test the proposed approach on a list of benchmarks and demonstrate that the learned student is able to achieve very promising performance, superior to those of the teachers in their specialized tasks.",2019,IJCAI,1906.10546,10.24963/ijcai.2019/428,https://arxiv.org/pdf/1906.10546.pdf
421a320c35323a8968e98c8fd85b807abeaba226,1,1,Issues Related to Face Recognition Accuracy Varying Based on Race and Skin Tone,"Face recognition technology has recently become controversial over concerns about possible bias due to accuracy varying based on race or skin tone. We explore three important aspects of face recognition technology related to this controversy. Using two different deep convolutional neural network face matchers, we show that for a fixed decision threshold, the African-American image cohort has a higher false match rate (FMR), and the Caucasian cohort has a higher false nonmatch rate. We present an analysis of the impostor distribution designed to test the premise that darker skin tone causes a higher FMR, and find no clear evidence to support this premise. Finally, we explore how using face recognition for one-to-many identification can have a very low false-negative identification rate and still present concerns related to the false-positive identification rate. Both the ArcFace and VGGFace2 matchers and the MORPH dataset used in our experiments are available to the research community so that others should be able to reproduce or reanalyze our results.",2020,IEEE Transactions on Technology and Society,,10.1109/TTS.2020.2974996,https://ieeexplore.ieee.org/ielx7/8566059/8995808/09001031.pdf
4308e003bf00559a3392d3842790b689df9f1fc7,0,1,Visual Privacy Protection via Mapping Distortion,"Data privacy protection is an important research area, which is especially critical in this big data era. To a large extent, the privacy of visual classification tasks is mainly in the one-to-one mapping between image and its corresponding label, since this relation provides a great amount of information and can be used in other scenarios. In this paper, we propose Mapping Distortion Protection (MDP) and its augmentation-based extension (AugMDP) to protect the data privacy by modifying the original dataset. In MDP, the label of the modified image does not match the ground-truth mapping, yet DNNs can still learn the ground-truth relation even when the provided mapping is distorted. As such, this method protects privacy when the dataset is leaked. Extensive experiments are conducted on CIFAR-10 and restricted CASIA-WebFace dataset, which verifies the effectiveness and feasibility of the method.",2019,ArXiv,1911.01769,,https://arxiv.org/pdf/1911.01769.pdf
4357e752131594302ba91027762884e22d1d9253,0,1,An Examination of Deep-Learning Based Landmark Detection Methods on Thermal Face Imagery,"Thermal-to-visible face recognition is an emerging technology for low-light and nighttime human identification, for which detection of fiducial landmarks is a critical step required for face alignment prior to recognition. However, thermal images with their low contrast, low resolution, and lack of textural information have proven a challenging obstacle for the detection of the fiducial landmarks used for image alignment. This paper analyzes the ability of modern landmark detection algorithms to cope with the adversarial conditions present in the thermal domain by exploring the strengths and weaknesses of three deep-learning based landmark detection architectures originally developed for visible images: the Deep Alignment Network (DAN), Multi-task Convolutional Neural Network (MTCNN), and a Multi-class Patch-based fullyconvolutional neural network (PBC). Our experiments yield a normalized mean squared error of 0.04 at an offset distance of 2.5 meters using the DAN architecture, indicating an ability for cascaded shape regression neural networks to adapt to thermal images. However, we find that even small alignment errors disproportionately reduce correct recognition rates. With images aligned using the best performing model, an 8.2% drop in EER is observed as compared with ground truth alignments, leaving further room for improvement in this area.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00129,http://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Poster_An_Examination_of_Deep-Learning_Based_Landmark_Detection_Methods_on_Thermal_CVPRW_2019_paper.pdf
436d80cc1b52365ed7b2477c0b385b6fbbb51d3b,1,0,Probabilistic Knowledge Transfer for Deep Representation Learning,"Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper a novel knowledge transfer technique, that is capable of training a student model that maintains the same amount of mutual information between the learned representation and a set of (possible unknown) labels as the teacher model, is proposed. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several limitations of existing methods providing new insight into KT as well as novel KT applications, ranging from knowledge transfer from handcrafted feature extractors to {cross-modal} KT from the textual modality into the representation extracted from the visual modality of the data.",2018,ArXiv,,,
4380423c59b292ff42af4be460617e2b665050f6,0,1,Pedestrian Detection with Wearable Cameras for the Blind: A Two-way Perspective,"Blind people have limited access to information about their surroundings, which is important for ensuring one's safety, managing social interactions, and identifying approaching pedestrians. With advances in computer vision, wearable cameras can provide equitable access to such information. However, the always-on nature of these assistive technologies poses privacy concerns for parties that may get recorded. We explore this tension from both perspectives, those of sighted passersby and blind users, taking into account camera visibility, in-person versus remote experience, and extracted visual information. We conduct two studies: an online survey with MTurkers (N=206) and an in-person experience study between pairs of blind (N=10) and sighted (N=40) participants, where blind participants wear a working prototype for pedestrian detection and pass by sighted participants. Our results suggest that both of the perspectives of users and bystanders and the several factors mentioned above need to be carefully considered to mitigate potential social tensions.",2020,CHI,2003.12122,10.1145/3313831.3376398,https://arxiv.org/pdf/2003.12122.pdf
439aca3a3d4497f822b058ccabcbfc94a83c5ee9,0,1,On Learning Disentangled Representations for Gait Recognition,"Gait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and view angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as dynamic gait feature while canonical features are averaged as static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state of the art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/ lower resolutions, cross view angles.",2020,IEEE transactions on pattern analysis and machine intelligence,1909.03051,10.1109/tpami.2020.2998790,https://arxiv.org/pdf/1909.03051.pdf
43c1effcc2bbf90be4b67fbe961beb5c7c08c4af,0,1,Head2Head++: Deep Facial Attributes Re-Targeting,"Facial video re-targeting is a challenging problem aiming to modify the facial attributes of a target subject in a seamless manner by a driving monocular sequence. We leverage the 3D geometry of faces and Generative Adversarial Networks (GANs) to design a novel deep learning architecture for the task of facial and head reenactment. Our method is different to purely 3D model-based approaches, or recent image-based methods that use Deep Convolutional Neural Networks (DCNNs) to generate individual frames. We manage to capture the complex non-rigid facial motion from the driving monocular performances and synthesise temporally consistent videos, with the aid of a sequential Generator and an ad-hoc Dynamics Discriminator network. We conduct a comprehensive set of quantitative and qualitative tests and demonstrate experimentally that our proposed method can successfully transfer facial expressions, head pose and eye gaze from a source video to a target subject, in a photo-realistic and faithful fashion, better than other state-of-the-art methods. Most importantly, our system performs end-to-end reenactment in nearly real-time speed (18 fps).",2020,ArXiv,2006.10199,,https://arxiv.org/pdf/2006.10199.pdf
43dab36cd9caae66ac259401f5f3e02ffa7c2097,1,1,Recognizing Families In the Wild (RFIW): The 4th Edition,"Recognizing Families In the Wild (RFIW): an annual large-scale, multi-track automatic kinship recognition evaluation that supports various visual kin-based problems on scales much higher than ever before. Organized in conjunction with the 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG) as a Challenge, RFIW provides a platform for publishing original work and the gathering of experts for a discussion of the next steps. This paper summarizes the supported tasks (i.e., kinship verification, tri-subject verification, and search & retrieval of missing children) in the evaluation protocols, which include the practical motivation, technical background, data splits, metrics, and benchmark results. Furthermore, top submissions (i.e., leader-board stats) are listed and reviewed as a high-level analysis on the state of the problem. In the end, the purpose of this paper is to describe the 2020 RFIW challenge, end-to-end, along with forecasts in promising future directions.",2020,ArXiv,,,
43eaa31b5e8e9de7ffb48b98a5e1fe338403b6bb,0,1,LS-CNN: Characterizing Local Patches at Multiple Scales for Face Recognition,"Faces in the wild may contain pose variations, age changes, and with different qualities which significantly enlarge the intra-class variations. Although great progresses have been made in face recognition, few existing works could learn local and multi-scale representations together. In this work, we propose a new model, called Local and multi-Scale Convolutional Neural Networks (LS-CNN). First, since similar discriminative face regions may occur at different scales, it is necessary to learn multi-scale features. To this aim, we introduce a new backbone network, namely Harmonious multi-Scale Network (HSNet), which extracts rich multi-scale features from two harmonious perspectives: utilization of different kernel sizes in a single layer, and concatenation of multi-scale feature maps from different layers. Second, identifying similar local patches is important when global face appearances have dramatic changes. Meanwhile, different face regions have different discriminative abilities. To capture critical local similarities and weigh adaptively on different local patches, a spatial attention is proposed. Third, channels have different convolutional kernels which can detect different features with various importance. Besides, hierarchical channels concatenated from different layers contain diverse information: channels from low layers describe local details or small-scale parts, and channels in high layers represent high-level abstraction or large-scale parts. To emphasize important channels and suppress less informative ones automatically, channel attention is used. Due to the complementary characteristics of channel attention and spatial attention, they are fused to form the Dual Face Attentions (DFA). To the best of our knowledge, this is the first effort to employ attentions for the general face recognition task. The LS-CNN is developed by incorporating DFA into HSNet model. Experimental results on various face matching tasks show its capability of learning complex data distributions.",2020,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2019.2946938,
441f74d1b1f35c1c7071ea13e4b99fae4be1a572,0,1,"Robust Discrimination and Generation of Faces using Compact, Disentangled Embeddings","Current solutions to discriminative and generative tasks in computer vision exist separately and often lack interpretability and explainability. Using faces as our application domain, here we present an architecture that is based around two core ideas that address these issues: first, our framework learns an unsupervised, low-dimensional embedding of faces using an adversarial autoencoder that is able to synthesize high-quality face images. Second, a supervised disentanglement splits the low-dimensional embedding vector into four sub-vectors, each of which contains separated information about one of four major face attributes (pose, identity, expression, and style) that can be used both for discriminative tasks and for manipulating all four attributes in an explicit manner. The resulting architecture achieves state-of-the-art image quality, good discrimination and face retrieval results on each of the four attributes, and supports various face editing tasks using a face representation of only 99 dimensions. Finally, we apply the architecture's robust image synthesis capabilities to visually debug label-quality issues in an existing face dataset.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00071,http://openaccess.thecvf.com/content_ICCVW_2019/papers/RSL-CV/Browatzki_Robust_Discrimination_and_Generation_of_Faces_using_Compact_Disentangled_Embeddings_ICCVW_2019_paper.pdf
44681a63e794b72c3bd2653e54a1439c1e89f7f5,1,0,1 Generative Adversarial Network Generative Adversarial Networks,"Face analysis lies at the heart of computer vision with remarkable progress in the past decades. Face recognition and tracking are tackled by building invariance to fundamental modes of variation such as illumination, 3D pose. A much less standing mode of variation is motion deblurring, which however presents substantial challenges in face analysis. Recent approaches either make oversimplifying assumptions, e.g. in cases of joint optimization with other tasks, or fail to preserve the highly structured shape/identity information. We introduce a two-step architecture tailored to the challenges of motion deblurring: the first step restores the low frequencies; the second restores the high frequencies, while ensuring that the outputs span the natural images manifold. Both steps are implemented with a supervised data-driven method; to train those we devise a method for creating realistic motion blur by averaging a variable number of frames. The averaged images originate from the 2MF2 dataset with 19 million facial frames, which we introduce for the task. Considering deblurring as an intermediate step, we conduct a thorough experimentation on high-level face analysis tasks, i.e. landmark localization and face verification, on blurred images. The experimental evaluation demonstrates the superiority of our method.",2018,,,,https://pdfs.semanticscholar.org/4468/1a63e794b72c3bd2653e54a1439c1e89f7f5.pdf
449e5b223b3a3962fa25880665f56d5e175c8cf5,1,0,A Cancellable Face Template Scheme Based on Nonlinear Multi-Dimension Spectral Hashing,"The exposure of face templates potentially leads to severe security and privacy risks. For example, the attacker can utilize the compromised face template to masquerade the template owner. In addition, these concerns are aggravated since face is irreplaceable and irrevocable. In this paper, we propose a cancelable transform, namely nonlinear multi-dimension spectral hashing (NMDSH) to protect face template. Essentially, NMDSH utilizes a many-to-one function to transform real-valued deep face feature vector into binary code. The transformed template thus possesses strong non-invertible property. Next, a highly nonlinear softmod function is further adapted into the scheme to provide an additional layer of protection against similarity-based attack. The accuracy performance of NMDSH is evaluated. Experiment results suggest that NMDSH can preserve the accuracy performance largely. Properties including non-invertibility, revocability and resistance to similarity-based attack are also evaluated.",2019,2019 7th International Workshop on Biometrics and Forensics (IWBF),,10.1109/IWBF.2019.8739179,http://www.eurecom.fr/en/publication/5867/download/sec-publi-5867_1.pdf
44a4465c43cc349ae6b6babe0028090e6a4b9408,1,0,Exploring the Effects of Frontalization and Data Synthesis on Face Recognition,"by Sandipan Banerjee Automatic face recognition performance has improved remarkably in the last decade. Much of this success can be attributed to the development of deep learning techniques like convolutional neural networks (CNNs). But the training process of CNNs requires a large amount of clean and correctly labelled data. In the first part of this work, we try to find the ideal orientation (facial pose, shape, context) of this data for training and testing such CNNs. If a CNN is intended to work with non-frontal face images, should this training data be diverse in terms of facial poses, or should face images be frontalized as a pre-processing step? To answer these questions we evaluate a set of popular facial landmarking and pose frontalization algorithms to understand their effect on facial recognition performance. We also introduce a new landmarking and frontalization scheme that operates over a single image without the need for a subject-specific 3D model, and perform a comparative analysis between the new scheme and other methods in the literature. Secondly, we analyze the usefulness of synthetic images in improving the face recognition pipeline while taking into account its practicality from a computation stand-point. In this regard, we propose a novel face synthesis method for augmentation of existing face image datasets. An augmented dataset reduces overfitting, which in turn, can enhance the face representation capability of a CNN. Our method, start-",2019,,,,
44b827df6c433ca49bcf44f9f3ebfdc0774ee952,1,0,Deep Correlation Feature Learning for Face Verification in the Wild,"Convolutional neural networks (CNNs) commonly uses the softmax loss function as the supervision signal. In order to enhance the discriminative power of the deeply learned features, this letter proposes a new supervision signal, called correlation loss, for face verification task. Specifically, the correlation loss encourages the large correlation between the deep feature vectors and their corresponding weight vectors in softmax loss. With the joint supervision of softmax loss and correlation loss, the deep correlation feature learning (DCFL) network can learn the deep features with both the interclass separability and the intraclass compactness, which are highly discriminative for face verification. More importantly, by applying the weight vector of softmax function as the class prototype, the proposed correlation loss function is easy to be optimized during the backpropatation of CNN. Finally, the DCFL method achieves 99.55% and 96.06% face verification accuracy using a 64-layer ResNet on the labeled face in-the-Wild (LFW) and you-tube face (YTF) benchmark, respectively.",2017,IEEE Signal Processing Letters,,10.1109/LSP.2017.2726105,
44f48a4b1ef94a9104d063e53bf88a69ff0f55f3,1,0,Automatically Building Face Datasets of New Domains from Weakly Labeled Data with Pretrained Models,"Training data are critical in face recognition systems. However, labeling a large scale face data for a particular domain is very tedious. In this paper, we propose a method to automatically and incrementally construct datasets from massive weakly labeled data of the target domain which are readily available on the Internet under the help of a pretrained face model. More specifically, given a large scale weakly labeled dataset in which each face image is associated with a label, i.e. the name of an identity, we create a graph for each identity with edges linking matched faces verified by the existing model under a tight threshold. Then we use the maximal subgraph as the cleaned data for that identity. With the cleaned dataset, we update the existing face model and use the new model to filter the original dataset to get a larger cleaned dataset. We collect a large weakly labeled dataset containing 530,560 Asian face images of 7,962 identities from the Internet, which will be published for the study of face recognition. By running the filtering process, we obtain a cleaned datasets (99.7+% purity) of size 223,767 (recall 70.9%). On our testing dataset of Asian faces, the model trained by the cleaned dataset achieves recognition rate 93.1%, which obviously outperforms the model trained by the public dataset CASIA whose recognition rate is 85.9%.",2016,ArXiv,1611.08107,,https://arxiv.org/pdf/1611.08107.pdf
45527d53a37e6f72eac363f70ba97a2b4073e6e8,0,1,FaceGuard: A Self-Supervised Defense Against Adversarial Face Images,"Prevailing defense schemes against adversarial face images tend to overfit to the perturbations in the training set and fail to generalize to unseen adversarial attacks. We propose a new self-supervised adversarial defense framework, namely FaceGuard, that can automatically detect, localize, and purify a wide variety of adversarial faces without utilizing pre-computed adversarial training samples. During training, FaceGuard automatically synthesizes challenging and diverse adversarial attacks, enabling a classifier to learn to distinguish them from real faces. Concurrently, a purifier attempts to remove the adversarial perturbations in the image space. Experimental results on LFW dataset show that FaceGuard can achieve 99.81% detection accuracy on six unseen adversarial attack types. In addition, the proposed method can enhance the face recognition performance of ArcFace from 34.27% TAR @ 0.1% FAR under no defense to 77.46% TAR @ 0.1% FAR. Code, pre-trained models and dataset will be publicly available.",2020,ArXiv,2011.14218,,https://arxiv.org/pdf/2011.14218.pdf
455a7e03a0c5ab618d0e86a06c9910ac179f0479,1,0,Identity Preserving Face Completion for Large Ocular Region Occlusion,"We present a novel deep learning approach to synthesize complete face images in the presence of large ocular region occlusions. This is motivated by recent surge of VR/AR displays that hinder face-to-face communications. Different from the state-of-the-art face inpainting methods that have no control over the synthesized content and can only handle frontal face pose, our approach can faithfully recover the missing content under various head poses while preserving the identity. At the core of our method is a novel generative network with dedicated constraints to regularize the synthesis process. To preserve the identity, our network takes an arbitrary occlusion-free image of the target identity to infer the missing content, and its high-level CNN features as an identity prior to regularize the searching space of generator. Since the input reference image may have a different pose, a pose map and a novel pose discriminator are further adopted to supervise the learning of implicit pose transformations. Our method is capable of generating coherent facial inpainting with consistent identity over videos with large variations of head motions. Experiments on both synthesized and real data demonstrate that our method greatly outperforms the state-of-the-art methods in terms of both synthesis quality and robustness.",2018,BMVC,1807.08772,,https://arxiv.org/pdf/1807.08772.pdf
4583dba687910d03c714fc4988b3456110a93a38,1,1,ProxylessKD: Direct Knowledge Distillation with Inherited Classifier for Face Recognition,"Knowledge Distillation (KD) refers to transferring knowledge from a large model to a smaller one, which is widely used to enhance model performance in machine learning. It tries to align embedding spaces generated from the teacher and the student model (i.e. to make images corresponding to the same semantics share the same embedding across different models). In this work, we focus on its application in face recognition. We observe that existing knowledge distillation models optimize the proxy tasks that force the student to mimic the teacher's behavior, instead of directly optimizing the face recognition accuracy. Consequently, the obtained student models are not guaranteed to be optimal on the target task or able to benefit from advanced constraints, such as large margin constraints (e.g. margin-based softmax). We then propose a novel method named ProxylessKD that directly optimizes face recognition accuracy by inheriting the teacher's classifier as the student's classifier to guide the student to learn discriminative embeddings in the teacher's embedding space. The proposed ProxylessKD is very easy to implement and sufficiently generic to be extended to other tasks beyond face recognition. We conduct extensive experiments on standard face recognition benchmarks, and the results demonstrate that ProxylessKD achieves superior performance over existing knowledge distillation methods.",2020,ArXiv,2011.00265,,https://arxiv.org/pdf/2011.00265.pdf
45cb03fcc4200116dab6a5c95bae8241b3b2721b,0,1,Face recognition based on improved residual neural network,"For traditional face recognition algorithm based on convolution neural network in the insufficient light, side face of 45° environment, for large-scale face recognition accuracy is not high, and when the network layer increases to a certain layer, the performance of the network tends to saturation, continue to increase the network layer instead lead to performance problems. An improved neural network architecture combined with deep residual neural network and ArcFace Loss is proposed, namely, ArcFace Loss replaces the Loss function Softmax of deep residual neural network. Experimental results show that the improved residual neural network has an accuracy of 97.7% in face recognition on LFW data set, which is higher than the traditional and improved deep learning algorithm.",2019,2019 Chinese Control And Decision Conference (CCDC),,10.1109/CCDC.2019.8833363,
45da14e605ebdf33ed044981446a59e583bafa3f,0,1,Vec2Face: Unveil Human Faces From Their Blackbox Features in Face Recognition,"Unveiling face images of a subject given his/her high-level representations extracted from a blackbox Face Recognition engine is extremely challenging. It is because the limitations of accessible information from that engine including its structure and uninterpretable extracted features. This paper presents a novel generative structure with Bijective Metric Learning, namely Bijective Generative Adversarial Networks in a Distillation framework (DiBiGAN), for synthesizing faces of an identity given that person's features. In order to effectively address this problem, this work firstly introduces a bijective metric so that the distance measurement and metric learning process can be directly adopted in image domain for an image reconstruction task. Secondly, a distillation process is introduced to maximize the information exploited from the blackbox face recognition engine. Then a Feature-Conditional Generator Structure with Exponential Weighting Strategy is presented for a more robust generator that can synthesize realistic faces with ID preservation. Results on several benchmarking datasets including CelebA, LFW, AgeDB, CFP-FP against matching engines have demonstrated the effectiveness of DiBiGAN on both image realism and ID preservation properties.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.06958,10.1109/cvpr42600.2020.00617,https://arxiv.org/pdf/2003.06958.pdf
45e285fdb1512abf2bdd8f06e381162740d0c630,0,1,Domain-Invariant Speaker Vector Projection by Model-Agnostic Meta-Learning,"Domain generalization remains a critical problem for speaker recognition, even with the state-of-the-art architectures based on deep neural nets. For example, a model trained on reading speech may largely fail when applied to scenarios of singing or movie. In this paper, we propose a domain-invariant projection to improve the generalizability of speaker vectors. This projection is a simple neural net and is trained following the Model-Agnostic Meta-Learning (MAML) principle, for which the objective is to classify speakers in one domain if it had been updated with speech data in another domain. We tested the proposed method on CNCeleb, a new dataset consisting of single-speaker multi-condition (SSMC) data. The results demonstrated that the MAML-based domain-invariant projection can produce more generalizable speaker vectors, and effectively improve the performance in unseen domains.",2020,INTERSPEECH,2005.119,10.21437/interspeech.2020-2562,https://arxiv.org/pdf/2005.11900.pdf
4635225773e4e1ae58edc2f0e979e3ebe9a1221b,1,1,Domain Balancing: Face Recognition on Long-Tailed Domains,"Long-tailed problem has been an important topic in face recognition task. However, existing methods only concentrate on the long-tailed distribution of classes. Differently, we devote to the long-tailed domain distribution problem, which refers to the fact that a small number of domains frequently appear while other domains far less existing. The key challenge of the problem is that domain labels are too complicated (related to race, age, pose, illumination, etc.) and inaccessible in real applications. In this paper, we propose a novel Domain Balancing (DB) mechanism to handle this problem. Specifically, we first propose a Domain Frequency Indicator (DFI) to judge whether a sample is from head domains or tail domains. Secondly, we formulate a light-weighted Residual Balancing Mapping (RBM) block to balance the domain distribution by adjusting the network according to DFI. Finally, we propose a Domain Balancing Margin (DBM) in the loss function to further optimize the feature space of the tail domains to improve generalization. Extensive analysis and experiments on several face recognition benchmarks demonstrate that the proposed method effectively enhances the generalization capacities and achieves superior performance.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.13791,10.1109/cvpr42600.2020.00571,https://arxiv.org/pdf/2003.13791.pdf
46702e0127e16a4d6a1feda3ffc5f0f123957e87,1,0,Revisit Multinomial Logistic Regression in Deep Learning: Data Dependent Model Initialization for Image Recognition,"We study in this paper how to initialize the parameters of multinomial logistic regression (a fully connected layer followed with softmax and cross entropy loss), which is widely used in deep neural network (DNN) models for classification problems. As logistic regression is widely known not having a closed-form solution, it is usually randomly initialized, leading to several deficiencies especially in transfer learning where all the layers except for the last task-specific layer are initialized using a pre-trained model. The deficiencies include slow convergence speed, possibility of stuck in local minimum, and the risk of over-fitting. To address those deficiencies, we first study the properties of logistic regression and propose a closed-form approximate solution named regularized Gaussian classifier (RGC). Then we adopt this approximate solution to initialize the task-specific linear layer and demonstrate superior performance over random initialization in terms of both accuracy and convergence speed on various tasks and datasets. For example, for image classification, our approach can reduce the training time by 10 times and achieve 3.2% gain in accuracy for Flickr-style classification. For object detection, our approach can also be 10 times faster in training for the same accuracy, or 5% better in terms of mAP for VOC 2007 with slightly longer training.",2018,ArXiv,1809.06131,,https://arxiv.org/pdf/1809.06131.pdf
46800f04189e5bb57c5ec825b95a2064ed2537df,1,0,Human Attribute Recognition: A Comprehensive Survey,"Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR’s main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.",2020,,,10.20944/preprints202007.0055.v1,https://pdfs.semanticscholar.org/9fc4/56fca45f3c51d32b8dc2b1b6e8e47d7a3dd6.pdf
469f0d26ab35536847de1dd04770a2c9247819fb,1,1,iQIYI Celebrity Video Identification Challenge,"We held the iQIYI Celebrity Video Identification Challenge in ACMMULTIMEDIA 2019. The purpose was to encourage the research on video-based person identification. We released the iQIYI-VID-2019 dataset, which contains 200K videos of 10K celebrities. In this paper, we introduce the organization of the challenge, the dataset, the evaluation process, and the results.",2019,ACM Multimedia,,10.1145/3343031.3356081,
46a8f2a197737894ac400ceb7c76e093c38066e9,0,1,BASN: Enriching Feature Representation Using Bipartite Auxiliary Supervisions for Face Anti-Spoofing,"Face anti-spoofing is an important task to assure the security of face recognition systems. To be applicable to unconstrained real-world environments, generalization capabilities of the face anti-spoofing methods are required. In this work, we present a face anti-spoofing method with robust generalization ability to unseen environments. To achieve our goal, we suggest bipartite auxiliary supervision to properly guide networks to learn generalizable features. We propose a bipartite auxiliary supervision network (BASN) that comprehensively utilizes the suggested supervision to accurately detect presentation attacks. We evaluate our method by conducting experiments on public benchmark datasets and we achieve state-of-the-art performances.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00062,http://openaccess.thecvf.com/content_ICCVW_2019/papers/DFW/Kim_BASN_Enriching_Feature_Representation_Using_Bipartite_Auxiliary_Supervisions_for_Face_ICCVW_2019_paper.pdf
46b968d82c4709e74419828b2767a9218a3ebcc8,0,1,Learning Invariant Representations of Social Media Users,"The evolution of social media users’ behavior over time complicates user-level comparison tasks such as verification, classification, clustering, and ranking. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a mapping from short episodes of user activity on social media to a vector space in which the distance between points captures the similarity of the corresponding users’ invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the mapping may be applied to users not seen at training time and enables efficient comparisons of users in the resulting vector space. We present a comprehensive evaluation to validate the benefits of the proposed approach using data from Reddit, Twitter, and Wikipedia.",2019,EMNLP/IJCNLP,1910.04979,10.18653/v1/D19-1178,https://arxiv.org/pdf/1910.04979.pdf
46cf35e840357437b05a52ac01120ab7ccbcf4d1,0,1,A Novel Generative Model to Synthesize Face Images for Pose-invariant Face Recognition,"Face recognition is an active research area in computer vision, which has been widely used in various applications such as security, video surveillance and personal identification. Although recent studies in this field have achieved great successes, they usually require an enormous amount of data for training and yet still have difficulties in in-the-wild dataset due to large variation of pose, illumination, expression. Among these unconstrained conditions, pose variation is thought to be the factor that harms face recognition accuracy the most. In order to deal with pose variation, one can fulfill the incomplete UV map extracted from in-the-wild faces, then attach the completed UV map to a fitted 3D mesh and finally generate different 2D faces of arbitrary poses, which then can be used for training or testing face recognition models. In this paper, we propose a novel generative model called ResCUNet-GAN to improve UV map completion. Particularly, we improve the original UV-GAN by stacking two U-Nets and enhancing it with multiple-level residual connections and feature fusion. The experiments on the popular Multi-PIE dataset shows that our model outperforms the original UV-GAN model.",2020,2020 International Conference on Multimedia Analysis and Pattern Recognition (MAPR),,10.1109/MAPR49794.2020.9237763,
4739b47af26137ec52bbfb582e6f37e9e9f5aba0,1,1,Hard Example Mining with Auxiliary Embeddings,"Hard example mining is an important part of the deep embedding learning. Most methods perform it at the mini-batch level. However, in the large-scale settings there is only a small chance that proper examples will appear in the same mini-batch and will be coupled into the hard example pairs or triplets. Doppelganger mining was previously proposed to increase this chance by means of class-wise similarity. This method ensures that examples of similar classes are sampled into the same mini-batch together. One of the drawbacks of this method is that it operates only at the class level, while there also might be a way to select appropriate examples within class in a more elaborated way than randomly. In this paper, we propose to use auxiliary embeddings for hard example mining. These embeddings are constructed in such way that similar examples have close embeddings in the cosine similarity sense. With the help of these embeddings it is possible to select new examples for the mini-batch based on their similarity with the already selected examples. We propose several ways to create auxiliary embeddings and use them to increase the number of potentially hard positive and negative examples in each mini-batch. Our experiments on the challenging Disguised Faces in the Wild (DFW) dataset show that hard example mining with auxiliary embeddings improves the discriminative power of learned representations.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00013,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w1/Smirnov_Hard_Example_Mining_CVPR_2018_paper.pdf
47b14a600e6728fb964b3cc964433480560142fa,1,0,Recognizing Disguised Faces in the Wild,"Research in face recognition has seen tremendous growth over the past couple of decades. Beginning from algorithms capable of performing recognition in constrained environments, existing face recognition systems achieve very high accuracies on large-scale unconstrained face datasets. While upcoming algorithms continue to achieve improved performance, many of them are susceptible to reduced performance under disguise variations, one of the most challenging covariate of face recognition. In this paper, the disguised faces in the wild (DFW) dataset is presented, which contains over 11000 images of 1000 identities with variations across different types of disguise accessories (the DFW dataset link: http://iab-rubric.org/resources/dfw.html). The images are collected from the Internet, resulting in unconstrained variations similar to real-world settings. This is a unique dataset that contains impersonator and genuine obfuscated face images for each subject. The DFW dataset has been analyzed in terms of three levels of difficulty: 1) easy; 2) medium; and 3) hard, in order to showcase the challenging nature of the problem. The dataset was released as part of the First International Workshop and Competition on DFW at the Conference on Computer Vision and Pattern Recognition, 2018. This paper presents the DFW dataset in detail, including the evaluation protocols, baseline results, performance analysis of the submissions received as part of the competition, and three levels of difficulties of the DFW challenge dataset.",2019,"IEEE Transactions on Biometrics, Behavior, and Identity Science",1811.08837,10.1109/TBIOM.2019.2903860,https://arxiv.org/pdf/1811.08837.pdf
47cec75f5ce9d8ef191afa42822c59e2076992c8,0,1,"Statistical Language and Speech Processing: 8th International Conference, SLSP 2020, Cardiff, UK, October 14–16, 2020, Proceedings","Automatic speech recognition (ASR) can be deployed in a previously unknown language, in less than 24 h, given just three resources: an acoustic model trained on other languages, a set of language-model training data, and a grapheme-to-phoneme (G2P) transducer to connect them. The LanguageNet G2Ps were created with the goal of being small, fast, and easy to port to a previously unseen language. Data come from pronunciation lexicons if available, but if there are no pronunciation lexicons in the target language, then data are generated from minimal resources: from a Wikipedia description of the target language, or from a one-hour interview with a native speaker of the language. Using such methods, the LanguageNet G2Ps now include simple models in nearly 150 languages, with trained finite state transducers in 122 languages, 59 of which are sufficiently well-resourced to permit measurement of their phone error rates. This paper proposes a measure of the distance between the G2Ps in different languages, and demonstrates that agglomerative clustering of the LanguageNet languages bears some resemblance to a phylogeographic language family tree. The LanguageNet G2Ps proposed in this paper have already been applied in three cross-language ASRs, using both hybrid and end-to-end neural architectures, and further experiments are ongoing.",2020,SLSP,,10.1007/978-3-030-59430-5,
47f6e6a8cb8b95b307f428fd2e267376c3e68033,0,1,A Comparison of Metric Learning Loss Functions for End-To-End Speaker Verification,"Despite the growing popularity of metric learning approaches, very little work has attempted to perform a fair comparison of these techniques for speaker verification. We try to fill this gap and compare several metric learning loss functions in a systematic manner on the VoxCeleb dataset. The first family of loss functions is derived from the cross entropy loss (usually used for supervised classification) and includes the congenerous cosine loss, the additive angular margin loss, and the center loss. The second family of loss functions focuses on the similarity between training samples and includes the contrastive loss and the triplet loss. We show that the additive angular margin loss function outperforms all other loss functions in the study, while learning more robust representations. Based on a combination of SincNet trainable features and the x-vector architecture, the network used in this paper brings us a step closer to a really-end-to-end speaker verification system, when combined with the additive angular margin loss, while still being competitive with the x-vector baseline. In the spirit of reproducible research, we also release open source Python code for reproducing our results, and share pretrained PyTorch models on torch.hub that can be used either directly or after fine-tuning.",2020,SLSP,2003.14021,10.1007/978-3-030-59430-5_11,https://arxiv.org/pdf/2003.14021.pdf
48320c6c156e7e25bfc04171b5ee6003de356a11,0,1,AF-Softmax for Face Recognition,"In order to make facial features more discriminative, some loss functions have been proposed recently. However, these loss functions are not very robust when categories are imbalanced or many easy examples dominate training losses. To overcome these shortcomings, we propose angular focus softmax (AF-Softmax) loss by introducing a focusing factor to lessen the losses attached to those well-classified examples. We refer to our model trained with AF-Softmax loss as AF-Face. To test our approach, we conduct extensive experiments on Youtube Faces (YTF) [8] and Labeled Face in the Wild (LFW) [13] and achieve better performance than SphereFace [9]. These experiments confirm the effectiveness of AF-Softmax on face recognition task.",2018,2018 International Conference on Network Infrastructure and Digital Content (IC-NIDC),,10.1109/ICNIDC.2018.8525505,
483b016c731d4917f3e077ffec1079ecd2241081,0,1,Associative Alignment for Few-shot Image Classification,"Few-shot image classification aims at training a model from only a few examples for each of the ""novel"" classes. This paper proposes the idea of associative alignment for leveraging part of the base data by aligning the novel training instances to the closely related ones in the base training set. This expands the size of the effective novel training set by adding extra ""related base"" instances to the few novel ones, thereby allowing a constructive fine-tuning. We propose two associative alignment strategies: 1) a metric-learning loss for minimizing the distance between related base samples and the centroid of novel instances in the feature space, and 2) a conditional adversarial alignment loss based on the Wasserstein distance. Experiments on four standard datasets and three backbones demonstrate that combining our centroid-based alignment loss results in absolute accuracy improvements of 4.4%, 1.2%, and 6.2% in 5-shot learning over the state of the art for object recognition, fine-grained classification, and cross-domain adaptation, respectively.",2020,ECCV,1912.05094,10.1007/978-3-030-58558-7_2,https://arxiv.org/pdf/1912.05094.pdf
48499deeaa1e31ac22c901d115b8b9867f89f952,1,0,Interim Report of Final Year Project HKU-Face : A Large Scale Dataset for Deep Face Recognition,"Current development of face recognition usually encounters problems with its training dataset because of the small size and human labelling errors. This project proposes a general dataset construction and filtering process in order to tackle the problem efficiently. Several models in the literature are utilized but for the new purpose to filter the original dataset collected from the website. Current results show the impressive effectiveness of automatic filtering and purity enhancement after filtering. Subsequent research and experiment are needed for the further improvement of filtering process with lower false negative rate. After the completion of the project, facial dataset constructions are expected to accelerate with less human effort. Further studies based on it is expected to contribute more to the unsupervised learning in the general object recognition.",2018,,,,https://pdfs.semanticscholar.org/4849/9deeaa1e31ac22c901d115b8b9867f89f952.pdf
484dcbf5eaeeda34b6aca02d7272fd8063fbb9f2,1,0,Face de-identification for privacy protection,"The ability to record, store and analyse images of faces economically, rapidly and on a vast scale brings people’s attention to privacy. The current privacy protection approaches for face images are mainly through masking, blurring or black-out which, however, removes data utilities along with the identifying information. As a result, these ad hoc methods are hardly used for data publishing or in further researches. The technique of de-identification attempts to remove identifying information from a dataset while preserving the data utility as much as possible. The research on de-identify structured data has been established while it remains a challenge to de-identify unstructured data such as face data in images and videos. The k-Same face de-identification was the first method that attempted to use an established de-identification theory, k-anonymity, to de-identify a face image dataset. The k-Same face de-identification is also the starting point of this thesis. Re-identification risk and data utility are two incompatible aspects in face de-identification. The focus of this thesis is to improve the privacy protection performance of a face de-identification system while providing data utility preserving solutions for different application scenarios. This thesis first proposes the k-Samefurthest face de-identification method which introduces the wrong-map protection to the k-Same-M face de-identification, where the identity loss is maximised by replacing an original face with the face that has the least similarity to it. The data utility of face images has been considered from two aspects in this thesis, the dataset-wise data utility such as data distribution of the data set and the individual-wise data utility such as the facial expression in an individual image. With the aim to preserve the diversity of a face image dataset, the k-Diff-furthest face de-identification method is proposed, which extends the k-Same-furthest method and can provide the wrong-map protection. With respect to the data utility of an individual face image, the visual quality and the preservation of facial expression are discussed in this thesis. A method to merge the isolated de-identified face region and its original image background is presented. The described method can increase the visual quality of a de-identified face image in",2018,,,10.18745/th.21270,https://pdfs.semanticscholar.org/484d/cbf5eaeeda34b6aca02d7272fd8063fbb9f2.pdf
4851bcfc0e2ae7b417e38fcb00b7a44ffdd117d0,0,1,MagnifierNet: Towards Semantic Regularization and Fusion for Person Re-identification,"Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this paper, we propose MagnifierNet, a novel network which accurately mines details for each semantic region and selectively fuse all semantic feature representations. Apart from conventional global branch, our proposed network is composed of a Semantic Regularization Branch (SRB) as learning regularizer and a Semantic Fusion Branch (SFB) towards selectively semantic fusion. The SRB learns with limited number of semantic regions randomly sampled in each batch, which forces the network to learn detailed representation for each semantic region, and the SFB selectively fuses semantic region information in a sequential manner, focusing on beneficial information while neglecting irrelevant features or noises. In addition, we introduce a novel loss function ""Semantic Diversity Loss"" (SD Loss) to facilitate feature diversity and improves regularization among all semantic regions. State-of-the-art performance has been achieved on multiple datasets by large margins. Notably, we improve SOTA on CUHK03-Labeled Dataset by 12.6% in mAP and 8.9% in Rank-1. We also outperform existing works on CUHK03-Detected Dataset by 13.2% in mAP and 7.8% in Rank-1 respectively, which demonstrates the effectiveness of our method.",2020,ArXiv,2002.10979,,https://arxiv.org/pdf/2002.10979.pdf
4874daed0f6a42d03011ed86e5ab46f231b02c13,1,0,GridFace: Face Rectification via Learning Local Homography Transformations,"In this paper, we propose a method, called GridFace, to reduce facial geometric variations and improve the recognition performance. Our method rectifies the face by local homography transformations, which are estimated by a face rectification network. To encourage the image generation with canonical views, we apply a regularization based on the natural face distribution. We learn the rectification network and recognition network in an end-to-end manner. Extensive experiments show our method greatly reduces geometric variations, and gains significant improvements in unconstrained face recognition scenarios.",2018,ECCV,1808.0621,10.1007/978-3-030-01270-0_1,https://arxiv.org/pdf/1808.06210.pdf
489936641d9d6033ef4b5ee6524d56e29d8ff24f,1,1,Hierarchical Pyramid Diverse Attention Networks for Face Recognition,"Deep learning has achieved a great success in face recognition (FR), however, few existing models take hierarchical multi-scale local features into consideration. In this work, we propose a hierarchical pyramid diverse attention (HPDA) network. First, it is observed that local patches would play important roles in FR when the global face appearance changes dramatically. Some recent works apply attention modules to locate local patches automatically without relying on face landmarks. Unfortunately, without considering diversity, some learned attentions tend to have redundant responses around some similar local patches, while neglecting other potential discriminative facial parts. Meanwhile, local patches may appear at different scales due to pose variations or large expression changes. To alleviate these challenges, we propose a pyramid diverse attention (PDA) to learn multi-scale diverse local representations automatically and adaptively. More specifically, a pyramid attention is developed to capture multi-scale features. Meanwhile, a diverse learning is developed to encourage models to focus on different local patches and generate diverse local features. Second, almost all existing models focus on extracting features from the last convolutional layer, lacking of local details or small-scale face parts in lower layers. Instead of simple concatenation or addition, we propose to use a hierarchical bilinear pooling (HBP) to fuse information from multiple layers effectively. Thus, the HPDA is developed by integrating the PDA into the HBP. Experimental results on several datasets show the effectiveness of the HPDA, compared to the state-of-the-art methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00835,http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Hierarchical_Pyramid_Diverse_Attention_Networks_for_Face_Recognition_CVPR_2020_paper.pdf
48cb52aa85e55ea00ae3f81f2d80ac75bbe5b6e3,1,0,Multi-algorithmic Fusion for Reliable Age and Gender Estimation from Face Images,"Automated estimation of demographic attributes, such as gender and age, became of great importance for many potential applications ranging from forensics to social media. Although previous works reported performances that closely match human level. These solutions lack of human intuition that allows human beings to state the confidences of their predictions. While the human intuition subconsciously considers surrounding conditions or the lack of experience in a certain task, current algorithmic solutions tend to mispredict with high confidence scores. In this work, we propose a multi-algorithmic fusion approach for age and gender estimation that is able to accurately state the model's prediction reliability. Our solution is based on stochastic forward passes through a dropout-reduced neural network ensemble. By utilizing multiple stochastic forward passes combined from the neural network ensemble, the centrality and dispersion of these predictions are used to derive a confidence statement about the prediction. Our experiments were conducted on the Adience benchmark. We showed that the proposed solution reached and exceeded state-of-the-art performance for the age and gender estimation tasks. Further, we demonstrated that the reliability statements of the predictions of our proposed solution capture challenging conditions and underrepresented training samples.",2019,2019 22th International Conference on Information Fusion (FUSION),,,
48f78da21b07cfd4e95162015be3b37a9584c98b,0,1,Topology-Preserving Class-Incremental Learning,"A well-known issue for class-incremental learning is the catastrophic forgetting phenomenon, where the network’s recognition performance on old classes degrades severely when incrementally learning new classes. To alleviate forgetting, we put forward to preserve the old class knowledge by maintaining the topology of the network’s feature space. On this basis, we propose a novel topology-preserving classincremental learning (TPCIL) framework. TPCIL uses an elastic Hebbian graph (EHG) to model the feature space topology, which is constructed with the competitive Hebbian learning rule. To maintain the topology, we develop the topology-preserving loss (TPL) that penalizes the changes of EHG’s neighboring relationships during incremental learning phases. Comprehensive experiments on CIFAR100, ImageNet, and subImageNet datasets demonstrate the power of the TPCIL for continuously learning new classes with less forgetting. The code will be released.",2020,ECCV,,10.1007/978-3-030-58529-7_16,http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640256.pdf
48fe6bf841aa237593cbd347a7609f16f7e9f48c,0,1,Identity-Preserving Face Hallucination via Deep Reinforcement Learning,"In this paper, we propose an identity-preserving face hallucination (IPFH) method via deep reinforcement learning. Most existing methods ultra-resolve facial visual information in guidance of appearance similarity which rarely attend to recovering the semantic property, undermining further face analysis (e.g., recognition). We present a visual-semantic hallucinator relying on deep reinforcement learning to adaptively repair local details for the restoration of both identity and appearance characteristics. Specifically, we first capture the facial global topology structure to roughly recover the visual information with the pixel-wise similarity constraint. To super-resolve more photo-realistic faces, we explore the contextual interdependency to reconstruct facial local textural details (e.g., over-smoothed edges) with the constraints of visual and identity similarity. In terms of the visual similarity constraint, we develop the dual domain network with bidirectional consistency on both HR domain and LR domain to improve the appearance quality. Moreover, we introduce the identity constraint to encourage hallucinated faces to satisfy the identity property. Experimental results on several benchmarks demonstrate our method achieves promising performance on the recovery of visual and semantic information.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2019.2961629,
49336767aeb78f73235e4bc215d4ea87f0ec019d,0,1,Compressive Hyperspherical Energy Minimization,"Recent work on minimum hyperspherical energy (MHE) has demonstrated its potential in regularizing neural networks and improving their generalization. MHE was inspired by the Thomson problem in physics, where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy. Despite the practical effectiveness, MHE suffers from local minima as their number increases dramatically in high dimensions, limiting MHE from unleashing its full potential in improving network generalization. To address this issue, we propose compressive minimum hyperspherical energy (CoMHE) as an alternative regularization for neural networks. Specifically, CoMHE utilizes a projection mapping to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different constructions for the projection matrix, we propose two major variants: random projection CoMHE and angle-preserving CoMHE. Furthermore, we provide theoretical insights to justify its effectiveness. We show that CoMHE consistently outperforms MHE by a significant margin in comprehensive experiments, and demonstrate its diverse applications to a variety of tasks such as image recognition and point cloud recognition.",2019,ArXiv,,,http://wyliu.com/papers/LinCoMHE19.pdf
49448dc28d67aaf8c73ef8aaa0618e2ea9fbc5a9,0,1,Detecting safety helmet wearing on construction sites with bounding‐box regression and deep transfer learning,,2020,,,10.1111/mice.12579,
498b7a27289e9386e652b4357be2cf66a091adf2,1,0,Spectrum Translation for Cross-Spectral Ocular Matching,"Cross-spectral verification remains a big issue in biometrics, especially for the ocular area due to differences in the reflected features in the images depending on the region and spectrum used.  In this paper, we investigate the use of Conditional Adversarial Networks for spectrum translation between near infra-red and visual light images for ocular biometrics. We analyze the transformation based on the overall visual quality of the transformed images and the accuracy drop of the identification system when trained with opposing data.  We use the PolyU database and propose two different systems for biometric verification, the first one based on Siamese Networks trained with Softmax and Cross-Entropy loss, and the second one a Triplet Loss network. We achieved an EER of 1\% when using a Triplet Loss network trained for NIR and finding the Euclidean distance between the real NIR images and the fake ones translated from the visible spectrum. We also outperform previous results using baseline algorithms.",2020,ArXiv,2002.06228,,https://arxiv.org/pdf/2002.06228.pdf
4a0334ad0a8878465d62332e5975952f544e7efe,0,1,Dynamic Graph Representation for Partially Occluded Biometrics,"The generalization ability of Convolutional neural networks (CNNs) for biometrics drops greatly due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrated the merits of both CNNs and graphical models to learn dynamic graph representations for occlusion problems in biometrics, called Dynamic Graph Representation (DGR). Convolutional features onto certain regions are re-crafted by a graph generator to establish the connections among the spatial parts of biometrics and build Feature Graphs based on these node representations. Each node of Feature Graphs corresponds to a specific part of the input image and the edges express the spatial relationships between parts. By analyzing the similarities between the nodes, the framework is able to adaptively remove the nodes representing the occluded parts. During dynamic graph matching, we propose a novel strategy to measure the distances of both nodes and adjacent matrixes. In this way, the proposed method is more convincing than CNNs-based methods because the dynamic graph method implies a more illustrative and reasonable inference of the biometrics decision. Experiments conducted on iris and face demonstrate the superiority of the proposed framework, which boosts the accuracy of occluded biometrics recognition by a large margin comparing with baseline methods.",2019,ArXiv,1912.00377,,https://arxiv.org/pdf/1912.00377.pdf
4a56d3240a924240c2392c0d5dea8715f6715277,1,0,Under review as a conference paper at ICLR 2019 Sampled Latents D iscrim inator G enerator Real Images Real or Fake ? Generated Images Contrastive Loss Pull Push,,,,,,https://pdfs.semanticscholar.org/4a56/d3240a924240c2392c0d5dea8715f6715277.pdf
4a679f85a96ea26ed1c0daa63edadd880e940bb0,0,1,Finding Missing Children: Aging Deep Face Features,"Given a gallery of face images of missing children, state-of-the-art face recognition systems fall short in identifying a child (probe) recovered at a later age. We propose an age-progression module that can age-progress deep face features output by any commodity face matcher. For time lapses larger than 10 years (the missing child is found after 10 or more years), the proposed age-progression module improves the closed-set identification accuracy of FaceNet from 40% to 49.56% and CosFace from 56.88% to 61.25% on a child celebrity dataset, namely ITWCC. The proposed method also outperforms state-of-the-art approaches with a rank-1 identification rate from 94.91% to 95.91% on a public aging dataset, FG-NET, and from 99.50% to 99.58% on CACD-VS. These results suggest that aging face features enhances the ability to identify young children who are possible victims of child trafficking or abduction.",2019,ArXiv,1911.07538,,https://arxiv.org/pdf/1911.07538.pdf
4a75cda9341bbdcb9f370181e664508c66aee5d4,0,1,Efficient Decision-Based Black-Box Adversarial Attacks on Face Recognition,"Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.04433,10.1109/CVPR.2019.00790,https://arxiv.org/pdf/1904.04433.pdf
4ae3d316748d5f18abc8b13a926ba765a24f647a,0,1,Deep Face Representations for Differential Morphing Attack Detection,"The vulnerability of facial recognition systems to face morphing attacks is well known. Many different approaches for morphing attack detection (MAD) have been proposed in the scientific literature. However, the MAD algorithms proposed so far have mostly been trained and tested on datasets whose distributions of image characteristics are either very limited (e.g., only created with a single morphing tool) or rather unrealistic (e.g., no print-scan transformation). As a consequence, these methods easily overfit on certain image types and the results presented cannot be expected to apply to real-world scenarios. For example, the results of the latest NIST FRVT MORPH show that the majority of submitted MAD algorithms lacks robustness and performance when considering unseen and challenging datasets. In this work, subsets of the FERET and FRGCv2 face databases are used to create a realistic database for training and testing of MAD algorithms, containing a large number of ICAO-compliant bona fide facial images, corresponding unconstrained probe images, and morphed images created with four different face morphing tools. Furthermore, multiple post-processings are applied on the reference images, e.g., print-scan and JPEG2000 compression. On this database, previously proposed differential morphing algorithms are evaluated and compared. In addition, the application of deep face representations for differential MAD algorithms is investigated. It is shown that algorithms based on deep face representations can achieve very high detection performance (less than 3% D-EER) and robustness with respect to various post-processings. Finally, the limitations of the developed methods are analyzed.",2020,IEEE Transactions on Information Forensics and Security,2001.01202,10.1109/TIFS.2020.2994750,https://ieeexplore.ieee.org/ielx7/10206/8833568/09093905.pdf
4ae8642f0f7d36f5f85dd152d661e30909d69212,0,1,Example weighting for deep representation learning,"In gradient-based optimisation, the derivative of the loss of an example can be interpreted as the example’s effect on the update of a model. Consequently, a derivative magnitude function can be considered to provide a weighting scheme from the viewpoint of example weighting. Therefore, example weighting is universal in deep learning. Partially arising from the recent work on the risky memorisation behaviours of deep neural networks (Arpit et al., 2017; Zhang et al., 2017b), example weighting becomes an active research filed (Chang et al., 2017; Toneva et al., 2019). Example weighting has ‘hard’ and ‘soft’ versions: (1) ‘hard’ weighting is well-known as sample selection or mining, i.e., binary weighting; (2) ‘soft’ weighting means example differentiation using a continuous importance score. In this thesis, we study how to learn more robust and discriminative representations using deep supervised learning. Technically, we propose example weighting for better optimisation and regularisation. Example weighting techniques differentiate and weight training data points according to a criteria, which varies in different scenarios. Example weighting improves the generalisation performance a lot, which is proved across multiple network architectures and learning tasks. We focus on two learning tasks in this thesis: learning to rank, and learning to classify. In both tasks, we reveal the importance of example weighting, by which a deep model focuses on more informative patterns, and pays less attention to non-informative (easy) and noisy (usually extremely hard) ones during the learning process. Therefore, example weighting is an important tool for guiding deep models to treat training samples differentially and learn meaningful patterns robustly and effectively. Furthermore, our study on example weighting helps us understand better about the training data and a model’s learning process. When a training dataset is clean, naively assigning higher weights to harder examples works well. However, when the dataset contains both meaningful and wrong information, a model learns meaningful patterns before fitting random errors. The challenge becomes how to differentiate trusted and error patterns as training progresses, and avoid fitting the error transformation. We demonstrate that example weighting is an effective approach for addressing this challenge. Additionally, we empirically justify the effectiveness of our proposed example weighting methods in other adverse cases:",2020,,,,https://pureadmin.qub.ac.uk/ws/portalfiles/portal/216219097/Example_Weighting_for_Deep_Representation_Learning.pdf
4b10a89c611c32fa869bdd093e0fe6a78731d45c,0,1,Learning to Restore a Single Face Image Degraded by Atmospheric Turbulence using CNNs,"Atmospheric turbulence significantly affects imaging systems which use light that has propagated through long atmospheric paths. Images captured under such condition suffer from a combination of geometric deformation and space varying blur. We present a deep learning-based solution to the problem of restoring a turbulence-degraded face image where prior information regarding the amount of geometric distortion and blur at each location of the face image is first estimated using two separate networks. The estimated prior information is then used by a network called, Turbulence Distortion Removal Network (TDRN), to correct geometric distortion and reduce blur in the face image. Furthermore, a novel loss is proposed to train TDRN where first and second order image gradients are computed along with their confidence maps to mitigate the effect of turbulence degradation. Comprehensive experiments on synthetic and real face images show that this framework is capable of alleviating blur and geometric distortion caused by atmospheric turbulence, and significantly improves the visual quality. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method.",2020,ArXiv,2007.08404,,https://arxiv.org/pdf/2007.08404.pdf
4b5173fa234c89271026edadb6237b746a990645,1,0,Low Quality Video Face Recognition: Multi-Mode Aggregation Recurrent Network (MARN),"Face recognition performance deteriorates when face images are of very low quality. For low quality video sequences, however, more discriminative features can be obtained by aggregating the information in video frames. We propose a Multi-mode Aggregation Recurrent Network (MARN) for real-world low-quality video face recognition. Unlike existing recurrent networks (RNNs), MARN is robust against overfitting since it learns to aggregate pre-trained embeddings. Compared with quality-aware aggregation methods, MARN utilizes the video context and learns multiple attention vectors adaptively. Empirical results on three video face recognition datasets, IJB-S, YTF, and PaSC show that MARN significantly boosts the performance on the low quality video dataset while achieves comparable results on high quality video datasets.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00132,http://biometrics.cse.msu.edu/Publications/Fingerprint/Gongetal_ICCVW_2019.pdf
4b560cf6d6155271c4ee18e8ab653eca944402d3,0,1,Adversarial Learning of Mappings Onto Regularized Spaces for Biometric Authentication,"We present AuthNet: a novel framework for generic biometric authentication which, by learning a regularized mapping instead of a classification boundary, leads to higher performance and improved robustness. The biometric traits are mapped onto a latent space in which authorized and unauthorized users follow simple and well-behaved distributions. In turn, this enables simple and tunable decision boundaries to be employed in order to make a decision. We show that, differently from the deep learning and traditional template-based authentication systems, regularizing the latent space to simple target distributions leads to improved performance as measured in terms of Equal Error Rate (EER), accuracy, False Acceptance Rate (FAR) and Genuine Acceptance Rate (GAR). Extensive experiments on publicly available datasets of faces and fingerprints confirm the superiority of AuthNet over existing methods.",2020,IEEE Access,,10.1109/ACCESS.2020.3016599,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09166495.pdf
4b5d42b1f6e3a8924f595ca60a000a96ff047521,0,1,Siamese Networks: The Tale of Two Manifolds,"Siamese networks are non-linear deep models that have found their ways into a broad set of problems in learning theory, thanks to their embedding capabilities. In this paper, we study Siamese networks from a new perspective and question the validity of their training procedure. We show that in the majority of cases, the objective of a Siamese network is endowed with an invariance property. Neglecting the invariance property leads to a hindrance in training the Siamese networks. To alleviate this issue, we propose two Riemannian structures and generalize a well-established accelerated stochastic gradient descent method to take into account the proposed Riemannian structures. Our empirical evaluations suggest that by making use of the Riemannian geometry, we achieve state-of-the-art results against several algorithms for the challenging problem of fine-grained image classification.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00314,http://openaccess.thecvf.com/content_ICCV_2019/papers/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.pdf
4b7f17a0f3f01d2a48a7dcef90cc9dafdbe928d5,1,1,S2LD: Semi-Supervised Landmark Detection in Low Resolution Images and Impact on Face Verification,"Landmark detection algorithms trained on high resolution images perform poorly on datasets containing low resolution images. This degrades the performance of facial verification, recognition and modeling that rely on accurate detection of landmarks. To the best of our knowledge, there is no dataset consisting of low resolution face images along with their annotated landmarks, making supervised training infeasible. In this paper, we present a semi-supervised approach to predict landmarks on low resolution images by learning them from labeled high resolution images. The objective of this work is to show that predicting landmarks directly on low resolution images is more effective than the current practice of aligning images after rescaling or super-resolution. In a two-step process, the proposed approach first learns to generate low resolution images by modeling the distribution of target low resolution images. In the second stage, the model learns to predict landmarks for target low resolution images from generated low resolution images. With extensive experimentation, we study the impact of the various design choices and also show that prediction of landmarks directly in low resolution, improves performance on the critical task of face verification in low resolution images. As a byproduct, the proposed method also achieves competitive land mark detection results for high resolution images, with a single U-Net.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00387,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w45/Kumar_S2LD_Semi-Supervised_Landmark_Detection_in_Low-Resolution_Images_and_Impact_on_CVPRW_2020_paper.pdf
4b806344716949e03ad8a2f5926ee845b7dc3896,0,1,Learning Face Recognition Unsupervisedly by Disentanglement and Self-Augmentation,"As the growth of smart home, healthcare, and home robot applications, learning a face recognition system which is specific for a particular environment and capable of self-adapting to the temporal changes in appearance (e.g., caused by illumination or camera position) is nowadays an important topic. In this paper, given a video of a group of people, which simulates the surveillance video in a smart home environment, we propose a novel approach which unsuper- visedly learns a face recognition model based on two main components: (1) a triplet network that extracts identity-aware feature from face images for performing face recognition by clustering, and (2) an augmentation network that is conditioned on the identity-aware features and aims at synthesizing more face samples. Particularly, the training data for the triplet network is obtained by using the spatiotemporal characteristic of face samples within a video, while the augmentation network learns to disentangle a face image into identity-aware and identity-irrelevant features thus is able to generate new faces of the same identity but with variance in appearance. With taking the richer training data produced by augmentation network, the triplet network is further fine-tuned and achieves better performance in face recognition. Extensive experiments not only show the efficacy of our model in learning an environment- specific face recognition model unsupervisedly, but also verify its adaptability to various appearance changes.",2020,2020 IEEE International Conference on Robotics and Automation (ICRA),,10.1109/ICRA40945.2020.9197348,
4bcdd9b56504127b3024ad63b489c60bc1c02ff2,0,1,Multi-Prototype Networks for Unconstrained Set-based Face Recognition,"In this paper, we study the challenging unconstrained set-based face recognition problem where each subject face is instantiated by a set of media (images and videos) instead of a single image. Naively aggregating information from all the media within a set would suffer from the large intra-set variance caused by heterogeneous factors (e.g., varying media modalities, poses and illuminations) and fail to learn discriminative face representations. A novel Multi-Prototype Network (MPNet) model is thus proposed to learn multiple prototype face representations adaptively from the media sets. Each learned prototype is representative for the subject face under certain condition in terms of pose, illumination and media modality. Instead of handcrafting the set partition for prototype learning, MPNet introduces a Dense SubGraph (DSG) learning sub-net that implicitly untangles inconsistent media and learns a number of representative prototypes. Qualitative and quantitative experiments clearly demonstrate superiority of the proposed model over state-of-the-arts.",2019,IJCAI,1902.04755,10.24963/ijcai.2019/611,https://arxiv.org/pdf/1902.04755.pdf
4bde978ff1616b6a782671d09baf5edf96340eb4,1,0,LEARNING EFFICIENT VISUAL EMBEDDING MODELS UNDER DATA CONSTRAINTS,"LEARNING EFFICIENT VISUAL EMBEDDING MODELS UNDER DATA CONSTRAINTS Mert Bülent Sarıyıldız M.S. in Computer Engineering Advisor: Selim Aksoy Co-Advisor: R. Gökberk Cinbiş September 2019 Deep learning models require large-scale datasets to learn rich sets of low and mid-level patterns and high-level semantics. Therefore, given a high-capacity neural network, one way to improve the performance of a model is increasing the size of the dataset which the model is trained over on. Considering that it is easy to get the amount of computational power required to train a network, data becomes a serious bottleneck in scaling up the existing machine learning pipelines. In this thesis, we look into two main data bottlenecks that rise in computer vision applications: I. the difficulty of finding training data for diverse sets of object categories, II. the complication of utilizing data containing sensitive user information for the purpose of training neural network models. To address these issues, we study zero-shot learning and decentralized learning schemes, respectively. Zero-shot learning (ZSL) is one of the most promising problems where substantial progress can potentially be achieved through unsupervised learning, due to distributional differences between supervised and zero-shot classes. For this reason, several works investigate the incorporation of discriminative domain adaptation techniques into ZSL, which, however, lead to modest improvements in ZSL accuracy. In contrast, we propose a generative model that can naturally learn from unsupervised examples, and synthesize training examples for unseen classes purely based on their class embeddings, and therefore, reduce the zero-shot learning problem into a supervised classification task. The proposed approach consists of two important components: I. a conditional Generative Adversarial Network that learns to produce samples that mimic the characteristics of unsupervised data examples, and II. the Gradient Matching (GM) loss that measures the quality of the gradient signal obtained from the synthesized examples. Using our GM iii",2019,,,,https://pdfs.semanticscholar.org/4bde/978ff1616b6a782671d09baf5edf96340eb4.pdf
4c2a6524b458e7f964a35bf3e888b31d6cf9bf46,1,1,Classical and modern face recognition approaches: a complete review,"Human face recognition have been an active research area for the last few decades. Especially, during the last five years, it has gained significant research attention from multiple domains like computer vision, machine learning and artificial intelligence due to its remarkable progress and broad social applications. The primary goal of any face recognition system is to recognize the human identity from the static images, video data, data-streams and the knowledge of the context in which these data components are being actively used. In this review, we have highlighted major applications, challenges and trends of face recognition systems in social and scientific domains. The prime objective of this research is to sum-up recent face recognition techniques and develop a broad understanding of how these techniques behave on different datasets. Moreover, we discuss some key challenges such as variability in illumination, pose, aging, cosmetics, scale, occlusion, and background. Along with classical face recognition techniques, most recent research directions are deeply investigated, i.e., deep learning, sparse models and fuzzy set theory. Additionally, basic methodologies are briefly discussed, while contemporary research contributions are examined in broader details. Finally, this research presents future aspects of face recognition technologies and its potential significance in the upcoming digital society.",2020,,,10.1007/S11042-020-09850-1,
4c705bb82f0fd13f72a6453071c102143c0a1b0b,1,0,Face re-identification challenge: Are face recognition models good enough?,"Abstract Face re-identification (Re-ID) aims to track the same individuals over space and time with subtle identity class information in automatically detected face images captured by unconstrained surveillance camera views. Despite significant advances of face recognition systems for constrained social media facial images, face Re-ID is more challenging due to poor-quality surveillance face imagery data and remains under-studied. However, solving this problem enables a wide range of practical applications, ranging from law enforcement and information security to business, entertainment and e-commerce. To facilitate more studies on face Re-ID towards practical and robust solutions, a true large scale Surveillance Face Re-ID benchmark (SurvFace) is introduced, characterised by natively low-resolution, motion blur, uncontrolled poses, varying occlusion, poor illumination, and background clutters. This new benchmark is the largest and more importantly the only true surveillance face Re-ID dataset to our best knowledge, where facial images are captured and detected under realistic surveillance scenarios. We show that the current state-of-the-art FR methods are surprisingly poorfor face Re-ID. Besides, face Re-ID is generally more difficult in an open-set setting as naturally required in surveillance scenarios, owing to a large number of non-target people (distractors) appearing in open ended scenes. Moreover, the low-resolution problem inherent to surveillance facial imagery is investigated. Finally, we discuss open research problems that need to be solved in order to overcome the under-studied face Re-ID problem.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107422,https://xiatian-zhu.github.io/papers/ChengEtAl_PR2020.pdf
4c80a6854a72fea53bdfaa637c1cc818b1ff7071,0,1,Tongji University Team for the VoxCeleb Speaker Recognition Challenge 2020,"In this report, we describe the submission of Tongji University team to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020 at Interspeech 2020. We investigate different speaker recognition systems based on the popular ResNet-34 architecture, and train multiple variants via various loss functions. Both Offline and online data augmentation are introduced to improve the diversity of the training set, and score normalization with the exhaustive grid search is applied in the post-processing. Our best fusion of five selected systems for the CLOSE track achieves 0.2800 DCF and 4.7770% EER on the challenge.",2020,ArXiv,2010.08179,,https://arxiv.org/pdf/2010.08179.pdf
4cb5780b87c44bde549420a93b2ae990d0d423b9,1,0,Weighted Feature Pooling Network in Template-Based Recognition,"Many computer vision tasks are template-based learning tasks in which multiple instances of a specific concept (e.g. multiple images of a subject’s face) are available at once to the learning algorithm. The template structure of the input data provides an opportunity for generating a robust and discriminative unified template-level representation that effectively exploits the inherent diversity of feature-level information across instances within a template. In contrast to other feature aggregation methods, we propose a new technique to dynamically predict weights that consider factors such as noise and redundancy in assessing the importance of image-level features and use those weights to appropriately aggregate the features into a single template-level representation. We present extensive experimental results on the MNIST, CIFAR10, UCF101, IJB-A, IJB-B, and Janus CS4 datasets to show that the new technique outperforms statistical feature pooling methods as well as other neural-network-based aggregation mechanisms on a broad set of tasks.",2018,ACCV,,10.1007/978-3-030-20873-8_28,
4cc32db67ff82cf1aa160631c35bb315c5add749,0,1,Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation,"We present a generic image-to-image translation framework, Pixel2Style2Pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. We further introduce a dedicated identity loss which is shown to achieve improved performance in the reconstruction of an input image. We demonstrate pSp to be a simple architecture that, by leveraging a well-trained, fixed generator network, can be easily applied on a wide-range of image-to-image translation tasks. Solving these tasks through the style representation results in a global approach that does not rely on a local pixel-to-pixel correspondence and further supports multi-modal synthesis via the resampling of styles. Notably, we demonstrate that pSp can be trained to align a face image to a frontal pose without any labeled data, generate multi-modal results for ambiguous tasks such as conditional face generation from segmentation maps, and construct high-resolution images from corresponding low-resolution images.",2020,ArXiv,2008.00951,,https://arxiv.org/pdf/2008.00951.pdf
4ccccbe7f347c681a5dd900bbf8736af2f85058c,1,0,IP-GAN: Learning Identity and Pose Disentanglement in Generative Adversarial Networks,"Synthesizing realistic multi-view face images from a single-view input is an effective and cheap way for data augmentation. In addition it is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition. It is a challenging generative learning problem due to the large pose discrepancy between the synthetic and real face images, and the need to preserve identity after generation. We propose IP-GAN, a framework based on Generative Adversarial Networks to disentangle the identity and pose of faces, such that we can generate face images of a specific person with a variety of poses, or images of different identities with a particular pose. To rotate a face, our framework requires one input image of that person to produce an identity vector, and any other input face image to extract a pose embedding vector. Then we recombine the identity vector and the pose vector to synthesize a new face of the person with the extracted pose. Two learning pathways are introduced, the generation and the transformation, where the generation path focuses on learning complete representation in the latent embedding space. While the transformation path focuses on synthesis of new face images with target poses. They collaborate and compete in a parameter-sharing manner, and in an unsupervised settings. The experimental results demonstrate the effectiveness of the proposed framework.",2019,ICANN,,10.1007/978-3-030-30493-5_51,
4cdb6144d56098b819076a8572a664a2c2d27f72,1,0,Face Synthesis for Eyeglass-Robust Face Recognition,"In the application of face recognition, eyeglasses could significantly degrade the recognition accuracy. A feasible method is to collect large-scale face images with eyeglasses for training deep learning methods. However, it is difficult to collect the images with and without glasses of the same identity, so that it is difficult to optimize the intra-variations caused by eyeglasses. In this paper, we propose to address this problem in a virtual synthesis manner. The high-fidelity face images with eyeglasses are synthesized based on 3D face model and 3D eyeglasses. Models based on deep learning methods are then trained on the synthesized eyeglass face dataset, achieving better performance than previous ones. Experiments on the real face database validate the effectiveness of our synthesized data for improving eyeglass face recognition performance.",2018,CCBR,1806.01196,10.1007/978-3-319-97909-0_30,https://arxiv.org/pdf/1806.01196.pdf
4cfb74eae678cd2e8b18b2fa8846fb1a58c5d9fd,1,0,Robust Face Recognition with Assistance of Pose and Expression Normalized Albedo Images,"Facial albedo images are believed to be invariant to external factors of pose, illumination and expression that can greatly affect the appearance of face images and thus face recognition accuracy as well. Unlike most existing face recognition methods that address the impact of one or two of these external factors, we propose an end-to-end network, which consists of De-Light Network (DL-Net) and Normalization Network (N-Net), to generate normalized albedo images with neutral expression and frontal pose for input face images. DL-Net aims to eliminate the effects of illumination and reconstruct a posed albedo image that has the same pose and expression as the input image. N-Net attempts to generate a pose and expression normalized albedo image and extract identity features under the supervision of the normalized albedo images. Our experiments on the Multi-PIE database show that the extracted identity features can effectively assist conventional face recognition methods to improve face recognition accuracy under varying poses, illuminations and expressions.",2019,ICCAI '19,,10.1145/3330482.3330501,
4cfe0b002f34328322f066d8fbe63f3d35898a27,1,0,Conditional Dual-Agent GANs for Photorealistic and Annotation Preserving Image Synthesis,"Conditional and semi-supervised Generative Adversarial Networks (GANs) have been proven to be effective for image synthesis with preserved annotation information. However, learning from GAN generated images may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real images. To narrow this gap, we expand existing generative methods and propose a novel Conditional Dual-Agent GAN (CDA-GAN) model for photorealistic and annotation preserving image synthesis, which significantly benefits object classification and face recognition through Deep Convolutional Neural Networks (DCNNs) learned with such augmented data. Instead of merely distinguishing “real” or “fake” for the generated images, the proposed dual agents of the Discriminator are able to preserve both of realism and annotation information simultaneously through a standard adversarial loss and an auxiliary annotation perception loss. During the training process, the Generator is conditioned on the desired image features learned by a pre-trained CNN sharing the same architecture of the Discriminator yet different weights. Thus, CDA-GAN is flexible in terms of the scalability and able to generate photorealistic image with well preserved class labeling information for learning DCNNs in specific domains. We perform qualitative and quantitative experiments to verify the effectiveness of our proposed method, which outperforms other state-of-the-arts on MNIST hand written digits classification dataset and National Institute of Standards and Technology (NIST) IARPA Janus Benchmark A (IJB-A) face recognition dataset. c © 2017. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. * indicate equal contributions. 2 WANG ET AL.: CONDITIONAL DUAL-AGENT GANS FOR IMAGE SYNTHESIS Furthermore, we also prove that the CDA-GAN generated data represent the distinct class relationships as well as the real data, so adding such data for training DCNN models ends up with impressive improvement in terms of overall accuracy, generalization capacity, and robustness.",2017,,,,https://pdfs.semanticscholar.org/4cfe/0b002f34328322f066d8fbe63f3d35898a27.pdf
4d3fbeb69ee1207d47b182dd88bdd82791f1c17d,0,1,Performance Comparison of Face Detection and Recognition Algorithms,"Recognition of faces is a fundamental cognitive ability that forms an important basis for our social interactions. This paper aims to optimize the existing face recognition system by comparing the results of different algorithms. To achieve this goal, I have analyzed stateof-the-algorithms in both face detection and face recognition. The research for algorithms goes through the analysis of recent benchmarks, two of which (i.e. WIDER FACE [1] and MegaFace [2]) are also used for evaluating those algorithms. The results on these benchmarks allows to determine which algorithms perform better, that is to say SSH [3] for detection and both Dlib-R [4] and ArcFace [5] for recognition. All the tests are performed with algorithm efficiency in mind. And computation time measurements show that the best techniques tend to work slower but that they can achieve practical execution times.",2019,,,,https://pdfs.semanticscholar.org/4d3f/beb69ee1207d47b182dd88bdd82791f1c17d.pdf
4d90a613b280f6e9b52b59afa173b16073681acd,0,1,Semi-Automatic Geometric Normalization of Profile Faces,"This paper proposes a correlation point matching approach, i.e. an efficient methodology for applying geometric normalization for profile face images. This method is used to increase accuracy without imposing a significant increase in face matching computational time when using different feature descriptors. In our work, several such descriptors are tested to compare the accuracy with which low level facial features (edges), useful for profile face image geometric normalization, are extracted. Hence, we determined the most efficient normalization approach that does not substantially increase computational time. Experimental results show that the use of eigenvalues produces a higher than average edge point count, while having a lower increase in computational complexity compared to other similar algorithms. Then, the extracted features are matched using the random sample consensus algorithm (RANSAC). Next, the rotational angles between the pairs of features are calculated and averaged to yield the angle of rotation necessary to achieve a proper profile face image normalization representation. After applying our proposed approach to a deep learning-based profile face recognition algorithm, an increase of 7.2% accuracy is achieved when compared to the baseline (non-normalized profile faces). To the best of our knowledge, this is the first time in the open literature that the impact of automated profile face normalization is being investigated to improve deep learning-based profile face matching performance.",2019,2019 European Intelligence and Security Informatics Conference (EISIC),,10.1109/EISIC49498.2019.9108897,
4da4642ac4c1fcfbecd622dad485307b8d30ed2b,1,1,Deep representation for partially occluded face verification,"By using deep learning-based strategy, the performance of face recognition tasks has been significantly enhanced. However, the verification and discrimination of the faces with occlusions still remain a challenge to most of the state-of-the-art approaches. Bearing this in mind, we propose a novel convolutional neural network which was designed specifically for the verification between the occluded and non-occluded faces for the same identity. It could learn both the shared and unique features based on a multiple network convolutional neural network architecture. The newly presented joint loss function and the corresponding alternating minimization approach were integrated to implement the training and testing of the presented convolutional neural network. Experimental results on the publicly available datasets (LFW 99.73%, YTF 97.30%, CACD 99.12%) show that the proposed deep representation approach outperforms the state-of-the-art face verification techniques.",2018,EURASIP J. Image Video Process.,,10.1186/S13640-018-0379-2,
4e19a1d2b2d28ed2d20cb097513df70e266ea308,1,0,Automatic Group Cohesiveness Detection With Multi-modal Features,"Group cohesiveness is a compelling and often studied composition in group dynamics and group performance. The enormous number of web images of groups of people can be used to develop an effective method to detect group cohesiveness. This paper introduces an automatic group cohesiveness prediction method for the 7th Emotion Recognition in the Wild (EmotiW 2019) Grand Challenge in the category of Group-based Cohesion Prediction. The task is to predict the cohesive level for a group of people in images. To tackle this problem, a hybrid network including regression models which are separately trained on face features, skeleton features, and scene features is proposed. Predicted regression values, corresponding to each feature, are fused for the final cohesive intensity. Experimental results demonstrate that the proposed hybrid network is effective and makes promising improvements. A mean squared error (MSE) of 0.444 is achieved on the testing sets which outperforms the baseline MSE of 0.5.",2019,ICMI '19,1910.01197,10.1145/3340555.3355716,https://arxiv.org/pdf/1910.01197.pdf
4e6ac0f8dfa5592458e100bc0bb5bac69f7912c1,0,1,Robust RGB-D Face Recognition Using Attribute-Aware Loss,"Existing convolutional neural network (CNN) based face recognition algorithms typically learn a discriminative feature mapping, using a loss function that enforces separation of features from different classes and/or aggregation of features within the same class. However, they may suffer from bias in the training data such as uneven sampling density, because they optimize the adjacency relationship of the learned features without considering the proximity of the underlying faces. Moreover, since they only use facial images for training, the learned feature mapping may not correctly indicate the relationship of other attributes such as gender and ethnicity, which can be important for some face recognition applications. In this paper, we propose a new CNN-based face recognition approach that incorporates such attributes into the training process. Using an attribute-aware loss function that regularizes the feature mapping using attribute proximity, our approach learns more discriminative features that are correlated with the attributes. We train our face recognition model on a large-scale RGB-D data set with over 100K identities captured under real application conditions. By comparing our approach with other methods on a variety of experiments, we demonstrate that depth channel and attribute-aware loss greatly improve the accuracy and robustness of face recognition.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,1811.09847,10.1109/TPAMI.2019.2919284,http://orca.cf.ac.uk/122840/1/GuidedRecognition.pdf
4e727474c8b28bcac124d33f2ad6b9eb151cf9fc,1,1,APA: Adaptive Pose Alignment for Pose-Invariant Face Recognition,"Face alignment is a crucial step in face recognition. Through making the position of face consistent, face alignment reduces intra-class variability due to factors such as lighting, background, pose, and perspective transformation, and further facilitating the recognition tasks. In this paper, we propose a new face alignment method for pose-invariant face recognition, called adaptive pose alignment (APA), which can greatly reduce the intra-class difference and correct the noise caused by the traditional method in the alignment process, especially in unconstrained settings. Instead of aligning all faces to the pre-defined, uniform frontal shape, we adaptively learn the alignment templates according to the facial poses and then align each face of training or testing sets to its related template. To further improve face recognition performance, we propose a simple, yet effective feature normalization method which can generate more discriminative feature representation of a face or a set of faces (template) combined with the APA method. Furthermore, we introduce a pose-invariant face recognition pipeline that sequentially applies APA-based alignment, deep representation by softmax or ArcFace loss function, and the effective feature normalization procedure. We empirically show that APA-based images can accelerate the training of deep face recognition model by aligning all the images to the optimal templates. Moreover, experimental results show that the proposed method achieves the state-of-the-art performance on popular challenging face datasets including IJB-A, IJB-C, and CPLFW datasets.",2019,IEEE Access,,10.1109/ACCESS.2019.2894162,
4ebb8e13b68c1676d197b0ebde7c5d63fbfcd11a,0,1,Deep Speaker Recognition: Modular or Monolithic?,"Speaker recognition has made extraordinary progress with the advent of deep neural networks. In this work, we analyze the performance of end-to-end deep speaker recognizers on two popular text-independent tasks NIST-SRE 2016 and VoxCeleb. Through a combination of a deep convolutional feature extractor, self-attentive pooling and large-margin loss functions, we achieve state-of-the-art performance on VoxCeleb. Our best individual and ensemble models show a relative improvement of 70% an 82% respectively over the best reported results on this task. On the challenging NIST-SRE 2016 task, our proposed endto-end models show good performance but are unable to match a strong i-vector baseline. State-of-the-art systems for this task use a modular framework that combines neural network embeddings with a probabilistic linear discriminant analysis (PLDA) classifier. Drawing inspiration from this approach we propose to replace the PLDA classifier with a neural network. Our modular neural network approach is able to outperform the i-vector baseline using cosine distance to score verification trials.",2019,INTERSPEECH,,10.21437/interspeech.2019-3146,https://pdfs.semanticscholar.org/4ebb/8e13b68c1676d197b0ebde7c5d63fbfcd11a.pdf
4ebc747e7bafafe1551ec0873f621bd870c6a632,0,1,Hyperparameter-Free Out-of-Distribution Detection Using Softmax of Scaled Cosine Similarity,"The ability to detect out-of-distribution (OOD) samples is vital to secure the reliability of deep neural networks in real-world applications. Considering the nature of OOD samples, detection methods should not have hyperparameters that need to be tuned depending on incoming OOD samples. However, most of the recently proposed methods do not meet this requirement, leading to compromised performance in real-world applications. In this paper, we propose a simple, hyperparameter-free method based on softmax of scaled cosine similarity. It resembles the approach employed by modern metric learning methods, but it differs in details; the differences are essential to achieve high detection performance. We show through experiments that our method outperforms the existing methods on the evaluation test recently proposed by Shafaei et al., which takes the above issue of hyperparameter dependency into account. We also show that it achieves at least comparable performance to other methods on the conventional test, where their hyperparameters are chosen using explicit OOD samples. Furthermore, it is computationally more efficient than most of the previous methods, since it needs only a single forward pass.",2019,ArXiv,1905.10628,,https://arxiv.org/pdf/1905.10628.pdf
4f0b641860d90dfa4c185670bf636149a2b2b717,1,1,Improve Cross-Domain Face Recognition with IBN-block,"Domain adaptation is one of the major challenges for face recognition (FR). Most large-scale FR training datasets are built from massive images crawled from the Internet, while in practical applications face images come from specific scenarios. Especially, for applications like ID card verification, registered face images are taken in controlled environments while probe face images are not. There are different distributions between source domain and target domain. In this paper, we propose to use Instance-Batch-Normalization (IBN) block to improve cross-domain FR performance. A million-scale cross-domain test set named IDCard-Scene-1M is used for evaluation. CNN models are trained with an improved loss function which we call L2-ASoftmax Loss. Without using any data from the target domain, IBN-block increased recall rates (@FPR = 10−6) on IDCard-Scene-1M by 1 percentage point for different CNN models. Besides, experiments show that the proposed IBN-CNN models trained with L2-ASoftmax Loss made state-of-the-art performance on MegaFace evaluation.",2018,2018 IEEE International Conference on Big Data (Big Data),,10.1109/BigData.2018.8622251,
4f10a7697fb2a2c626d1190db2afba83c4ffe856,1,0,Cartoon-to-Photo Facial Translation with Generative Adversarial Networks,"Cartoon-to-photo facial translation could be widely used in different applications, such as law enforcement and anime remaking. Nevertheless, current general-purpose imageto-image models usually produce blurry or unrelated results in this task. In this paper, we propose a Cartoon-to-Photo facial translation with Generative Adversarial Networks (CP-GAN) for inverting cartoon faces to generate photo-realistic and related face images. In order to produce convincing faces with intact facial parts, we exploit global and local discriminators to capture global facial features and three local facial regions, respectively. Moreover, we use a specific content network to capture and preserve face characteristic and identity between cartoons and photos. As a result, the proposed approach can generate convincing high-quality faces that satisfy both the characteristic and identity constraints of input cartoon faces. Compared with recent works on unpaired image-to-image translation, our proposed method is able to generate more realistic and correlative images.",2018,ACML,,,https://pdfs.semanticscholar.org/4f10/a7697fb2a2c626d1190db2afba83c4ffe856.pdf
4f686309f5a34d5a5c687539b71bac0bafd8476f,1,0,The Devil of Face Recognition is in the Noise,"The growing scale of face recognition datasets empowers us to train strong convolutional networks for face recognition. While a variety of architectures and loss functions have been devised, we still have a limited understanding of the source and consequence of label noise inherent in existing datasets. We make the following contributions: (1) We contribute cleaned subsets of popular face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new large-scale noise-controlled IMDb-Face dataset. (2) With the original datasets and cleaned subsets, we profile and analyze label noise properties of MegaFace and MS-Celeb-1M. We show that a few orders more samples are needed to achieve the same accuracy yielded by a clean subset. (3) We study the association between different types of noise, i.e., label flips and outliers, with the accuracy of face recognition models. (4) We investigate ways to improve data cleanliness, including a comprehensive user study on the influence of data labeling strategies to annotation accuracy. The IMDb-Face dataset has been released on https://github.com/fwang91/IMDb-Face.",2018,ECCV,1807.11649,10.1007/978-3-030-01240-3_47,https://arxiv.org/pdf/1807.11649.pdf
4f804888768e8fd1c07e74868da8257d7f439f1a,0,1,Metric Classification Network in Actual Face Recognition Scene,"In order to make facial features more discriminative, some new models have recently been proposed. However, almost all of these models use the traditional face verification method, where the cosine operation is performed using the features of the bottleneck layer output. However, each of these models needs to change a threshold each time it is operated on a different test set. This is very inappropriate for application in real-world scenarios. In this paper, we train a validation classifier to normalize the decision threshold, which means that the result can be obtained directly without replacing the threshold. We refer to our model as validation classifier, which achieves best result on the structure consisting of one convolution layer and six fully connected layers. To test our approach, we conduct extensive experiments on Labeled Face in the Wild (LFW) and Youtube Faces (YTF), and the relative error reduction is 25.37% and 26.60% than traditional method respectively. These experiments confirm the effectiveness of validation classifier on face recognition task.",2019,ArXiv,1910.11563,,https://arxiv.org/pdf/1910.11563.pdf
4f842a92c110f40e9187bc9e6544b5dcb1daa5f3,0,1,Advancing High Fidelity Identity Swapping for Forgery Detection,"In this work, we study various existing benchmarks for deepfake detection researches. In particular, we examine a novel two-stage face swapping algorithm, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, FaceShifter generates the swapped face with high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. FaceShifter can handle facial occlusions with a second synthesis stage consisting of a Heuristic Error Acknowledging Refinement Network (HEAR-Net), which is trained to recover anomaly regions in a self-supervised way without any manual annotations. Experiments show that existing deepfake detection algorithm performs poorly with FaceShifter, since it achieves advantageous quality over all existing benchmarks. However, our newly developed Face X-Ray method can reliably detect forged images created by FaceShifter.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00512,https://pdfs.semanticscholar.org/4f84/2a92c110f40e9187bc9e6544b5dcb1daa5f3.pdf
4fc1d766e1376f45ee3d8699f215242ada3a69a0,1,0,An Identity-Preserved Model for Face Sketch-Photo Synthesis,"Face sketch-photo synthesis can be regarded as an image-to-image translation problem. Although many generative models achieve good translations from sketches to photos, they still have limitations in preserving face identity due to the huge modality gap of the two domains. To this end, we propose an identity-preserved adversarial model (IPAM), which includes an extended U-Net to increase the weight of the original sketch in translation, two discriminators focusing on the real or fake image concatenation of two domains to learn more styles of the target domain, and an identity constraint to request the fakes and the real targets to have zero cosine distance in feature space. We evaluate our method on two face sketch databases with face recognition. The results demonstrate our translation method is superior to the existing methods in maintaining face identity information.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.3005039,
4fd6b60f3f92e13a3924a64aae416fd6d0d0ed5d,0,1,High PE Utilization CNN Accelerator with Channel Fusion Supporting Pattern-Compressed Sparse Neural Networks,"Recently CNN-based methods have made remarkable progress in broad fields. Both network pruning algorithms and hardware accelerators have been introduced to accelerate CNN. However, existing pruning algorithms have not fully studied the pattern pruning method, and current index storage scheme of sparse CNN is not efficient. Furthermore, the performance of existing accelerators suffers from no-load PEs on sparse networks. This work proposes a software-hardware co-design to address these problems. The software includes an ADMM-based method which compresses the patterns of convolution kernels with acceptable accuracy loss, and a Huffman encoding method which reduces index storage overhead. The hardware is a fusion-enabled systolic architecture, which can reduce PEs’ no-load rate and improve performance by supporting the channel fusion. On CIFAR-10, this work achieves 5.63x index storage reduction with 2-7 patterns among different layers with 0.87% top-1 accuracy loss. Compared with the state-of-art accelerator, this work achieves 1.54x-1.79x performance and 25%-34% reduction of no-load rate with reasonable area and power overheads.",2020,2020 57th ACM/IEEE Design Automation Conference (DAC),,10.1109/DAC18072.2020.9218630,
4ff32a608eab344ca188fdb4f3e2a789d2b5c981,0,1,Smooth Proxy-Anchor Loss for Noisy Metric Learning,"Many industrial applications use Metric Learning as a way to circumvent scalability issues when designing systems with a high number of classes. Because of this, this field of research is attracting a lot of interest from the academic and non-academic communities. Such industrial applications require large-scale datasets, which are usually generated with web data and, as a result, often contain a high number of noisy labels. While Metric Learning systems are sensitive to noisy labels, this is usually not tackled in the literature, that relies on manually annotated datasets.  In this work, we propose a Metric Learning method that is able to overcome the presence of noisy labels using our novel Smooth Proxy-Anchor Loss. We also present an architecture that uses the aforementioned loss with a two-phase learning procedure. First, we train a confidence module that computes sample class confidences. Second, these confidences are used to weight the influence of each sample for the training of the embeddings. This results in a system that is able to provide robust sample embeddings.  We compare the performance of the described method with current state-of-the-art Metric Learning losses (proxy-based and pair-based), when trained with a dataset containing noisy labels. The results showcase an improvement of 2.63 and 3.29 in Recall@1 with respect to MultiSimilarity and Proxy-Anchor Loss respectively, proving that our method outperforms the state-of-the-art of Metric Learning in noisy labeling conditions.",2020,ArXiv,2006.05142,,https://arxiv.org/pdf/2006.05142.pdf
504144a77eb3c8ede75fa528a1a74c180ada9c8f,0,1,Deep Normalization for Speaker Vectors,"Deep speaker embedding has demonstrated state-of-the-art performance in audio speaker recognition (SRE). However, one potential issue with this approach is that the speaker vectors derived from deep embedding models tend to be non-Gaussian for each individual speaker, and non-homogeneous for distributions of different speakers. These irregular distributions can seriously impact SRE performance, especially with the popular PLDA scoring method, which assumes homogeneous Gaussian distribution. In this paper, we argue that deep speaker vectors require deep normalization, and propose a deep normalization approach based on a novel discriminative normalization flow (DNF) model. We demonstrate the effectiveness of the proposed approach with experiments using the widely used SITW and CNCeleb corpora. In these experiments, the DNF-based normalization delivered substantial performance gains and also showed strong generalization capability in out-of-domain tests.",2020,ArXiv,2004.04095,,https://arxiv.org/pdf/2004.04095.pdf
50b6a8123a821e8c48b129eed466ed6c57f33546,1,0,Dual-Structure Disentangling Variational Generation for Data-Limited Face Parsing,"Deep learning based face parsing methods have attained state-of-the-art performance in recent years. Their superior performance heavily depends on the large-scale annotated training data. However, it is expensive and time-consuming to construct a large-scale pixel-level manually annotated dataset for face parsing. To alleviate this issue, we propose a novel Dual-Structure Disentangling Variational Generation (D2VG) network. Benefiting from the interpretable factorized latent disentanglement in VAE, D2VG can learn a joint structural distribution of facial image and its corresponding parsing map. Owing to these, it can synthesize large-scale paired face images and parsing maps from a standard Gaussian distribution. Then, we adopt both manually annotated and synthesized data to train a face parsing model in a supervised way. Since there are inaccurate pixel-level labels in synthesized parsing maps, we introduce a coarseness-tolerant learning algorithm, to effectively handle these noisy or uncertain labels. In this way, we can significantly boost the performance of face parsing. Extensive quantitative and qualitative results on HELEN, CelebAMask-HQ and LaPa demonstrate the superiority of our methods.",2020,ACM Multimedia,,10.1145/3394171.3413919,
5114e46720e3eeee3def2e6d73aad4049e2c2a78,0,1,Interpretable and Generalizable Deep Image Matching with Adaptive Convolutions,"For image matching tasks, like face recognition and person re-identification, existing deep networks often focus on representation learning. However, without domain adaptation or transfer learning, the learned model is fixed as is, which is not adaptable to handle various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and result is interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of such an image matching architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. The proposed method is preliminarily validated on the person re-identification task. Through direct cross-dataset evaluation without further transfer learning, it achieves better results than many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method is proposed, which improves the performance to a further extent, resulting in state-of-the-art results in cross-dataset evaluation.",2019,ArXiv,,,
5121f42de7cb9e41f93646e087df82b573b23311,1,0,Classifying Online Dating Profiles on Tinder using FaceNet Facial Embeddings,"A method to produce personalized classification models to automatically review online dating profiles on Tinder is proposed, based on the user's historical preference. The method takes advantage of a FaceNet facial classification model to extract features which may be related to facial attractiveness. The embeddings from a FaceNet model were used as the features to describe an individual's face. A user reviewed 8,545 online dating profiles. For each reviewed online dating profile, a feature set was constructed from the profile images which contained just one face. Two approaches are presented to go from the set of features for each face, to a set of profile features. A simple logistic regression trained on the embeddings from just 20 profiles could obtain a 65% validation accuracy. A point of diminishing marginal returns was identified to occur around 80 profiles, at which the model accuracy of 73% would only improve marginally after reviewing a significant number of additional profiles.",2018,ArXiv,1803.04347,,https://arxiv.org/pdf/1803.04347.pdf
5137bba5570653b6da97848d8ee6ab1e7d2855d6,0,1,Hybrid Dictionary Learning and Matching for Video-based Face Verification,"We propose a hybrid dictionary learning and matching approach using deep features for unconstrained videobased face verification. Popular off-the-shelf image-based deep neural networks often fail to effectively exploit multiple frames for video-based verification. Unlike recurrent neural network-based approaches which require an external large-scale annotated data for training, the proposed unsupervised approach can effectively model both structural and temporal information of face features in target videos using structural and dynamical dictionaries, respectively. We propose an iterative optimization procedure to learn the dynamical dictionaries from videos. Using the learned dictionaries, we model video-to-video similarity as subspace-to-subspace similarity which is not only more robust but also utilizes the information in multiple frames better than the widely used reconstruction error-based measures, where the subspaces are spanned by the learned dictionaries. Experiments on challenging video-based face verification datasets, including Multiple Biometric Grand Challenge (MBGC), Face and Ocular Challenge Series (FOCS) and IARPA JANUS Benchmark A (IJB-A) datasets, demonstrate that the proposed method can effectively learn robust and discriminative representations for videos and improve the face verification performance.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",,10.1109/BTAS46853.2019.9185988,
515e89d34ac95449c9708cbbef0405ee0f585767,0,1,Learning mappings onto regularized latent spaces for biometric authentication,"We propose a novel architecture for generic biometric authentication based on deep neural networks: RegNet. Differently from other methods, RegNet learns a mapping of the input biometric traits onto a target distribution in a well-behaved space in which users can be separated by means of simple and tunable boundaries. More specifically, authorized and unauthorized users are mapped onto two different and well behaved Gaussian distributions. The novel approach of learning the mapping instead of the boundaries further avoids the problem encountered in typical classifiers for which the learnt boundaries may be complex and difficult to analyze. RegNet achieves high performance in terms of security metrics such as Equal Error Rate (EER), False Acceptance Rate (FAR) and Genuine Acceptance Rate (GAR). The experiments we conducted on publicly available datasets of face and fingerprint confirm the effectiveness of the proposed system.",2019,2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP),1911.08764,10.1109/MMSP.2019.8901698,https://arxiv.org/pdf/1911.08764.pdf
51795affcecc6e86feea381c4ee34e93464554b4,1,0,Visual Data Augmentation through Learning,"The rapid progress in machine learning methods has been empowered by i) huge datasets that have been collected and annotated, ii) improved engineering (e.g. data pre-processing/normalization). The existing datasets typically include several million samples, which constitutes their extension a colossal task. In addition, the state-of-the-art data-driven methods demand a vast amount of data, hence a standard engineering trick employed is artificial data augmentation for instance by adding into the data cropped and (affinely) transformed images. However, this approach does not correspond to any change in the natural 3D scene.  We propose instead to perform data augmentation through learning realistic local transformations. We learn a forward and an inverse transformation that maps an image from the high-dimensional space of pixel intensities to a latent space which varies (approximately) linearly with the latent space of a realistically transformed version of the image. Such transformed images can be considered two successive frames in a video. Next, we utilize these transformations to learn a linear model that modifies the latent spaces and then use the inverse transformation to synthesize a new image. We argue that the this procedure produces powerful invariant representations. We perform both qualitative and quantitative experiments that demonstrate our proposed method creates new realistic images.",2018,ArXiv,1801.06665,,https://arxiv.org/pdf/1801.06665.pdf
518cdc7cdfe65d12e4f58ae8312e637e82d1cc11,1,1,Disguised Face Verification using Inverse Disguise Quality,"Research in face recognition has evolved over the past few decades. With initial research focusing heavily on constrained images, recent research has focused more on unconstrained images captured inthe-wild settings. Faces captured in unconstrained settings with disguise accessories persist to be a challenge for automated face verification. To this effect, this research proposes a novel deep learning framework for disguised face verification. A novel Inverse Disguise Quality metric is proposed for evaluating amount of disguise in the input image, which is utilized in likelihood ratio as a quality score for enhanced verification performance. The proposed framework is model-agnostic and can be applied in conjunction with existing state-of-the-art face verification models for obtaining improved performance. Experiments have been performed on the Disguised Faces in Wild (DFW) 2018 and DFW 2019 datasets, with three state-of-the-art deep learning models, where it demonstrates substantial improvement compared to the base model.",2020,,,,http://iab-rubric.org/papers/2020_ECCVworkshop_DisguisedFaceVerification.pdf
5234caf3bcc1edb0b38554b4ffddf5a46877dd45,0,1,Occlusion Robust Face Recognition Based on Mask Learning With Pairwise Differential Siamese Network,"Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of face recognition over past years. However, existing CNN models are far less accurate when handling partially occluded faces. These general face models generalize poorly for occlusions on variable facial areas. Inspired by the fact that human visual system explicitly ignores the occlusion and only focuses on the non-occluded facial areas, we propose a mask learning strategy to find and discard corrupted feature elements from recognition. A mask dictionary is firstly established by exploiting the differences between the top conv features of occluded and occlusion-free face pairs using innovatively designed pairwise differential siamese network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements from recognition. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed algorithm significantly outperforms the state-of-the-art systems.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1908.0629,10.1109/ICCV.2019.00086,https://arxiv.org/pdf/1908.06290.pdf
52859a7eda649233405ce0b01121391cf9715376,0,1,Human Activity Classification Using mm-Wave FMCW Radar by Improved Representation Learning,"The paper proposes a novel Euclidean distance softmax layer for radar-based human activity classification. The method aims to overcome the angular dependency of classical softmax approaches. Through the freedoms thus gained, the activity classes can be distributed freely within the entire embedded feature space, due to which the dimension of the embeddings and the whole neural network size can be reduced. The performance of our novel deep learning architecture is evaluated for 60 GHz mm-wave radar sensor-based human activity classification. The results show that the proposed approach increases the robustness against random and unknown movements compared to state-of-art representation learning techniques.",2020,,,10.1145/3412060.3418430,
529143209b4063da3db7253368a30aa4b5624da0,1,0,Scalable Object Detection for Stylized Objects,"Following recent breakthroughs in convolutional neural networks and monolithic model architectures, state-of-the-art object detection models can reliably and accurately scale into the realm of up to thousands of classes. Things quickly break down, however, when scaling into the tens of thousands, or, eventually, to millions or billions of unique objects. Further, bounding box-trained end-to-end models require extensive training data. Even though - with some tricks using hierarchies - one can sometimes scale up to thousands of classes, the labor requirements for clean image annotations quickly get out of control. In this paper, we present a two-layer object detection method for brand logos and other stylized objects for which prototypical images exist. It can scale to large numbers of unique classes. Our first layer is a CNN from the Single Shot Multibox Detector family of models that learns to propose regions where some stylized object is likely to appear. The contents of a proposed bounding box is then run against an image index that is targeted for the retrieval task at hand. The proposed architecture scales to a large number of object classes, allows to continously add new classes without retraining, and exhibits state-of-the-art quality on a stylized object detection task such as logo recognition.",2017,ArXiv,1711.09822,,https://arxiv.org/pdf/1711.09822.pdf
53088cde6d68e20706a2128e93461b28858ae8d7,0,1,HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss,"Recent works show that local descriptor learning benefits from the use of L2 normalisation, however, an in-depth analysis of this effect lacks in the literature. In this paper, we investigate how L2 normalisation affects the back-propagated descriptor gradients during training. Based on our observations, we propose HyNet, a new local descriptor that leads to state-of-the-art results in matching. HyNet introduces a hybrid similarity measure for triplet margin loss, a regularisation term constraining the descriptor norm, and a new network architecture that performs L2 normalisation of all intermediate feature maps and the output descriptors. HyNet surpasses previous methods by a significant margin on standard benchmarks that include patch matching, verification, and retrieval, as well as outperforming full end-to-end methods on 3D reconstruction tasks.",2020,NeurIPS,2006.10202,,https://arxiv.org/pdf/2006.10202.pdf
5309dfee51cad8a280a66a14a7c0466ad161d251,1,1,EagleEye: wearable camera-based person identification in crowded urban spaces,"We present EagleEye, an AR-based system that identifies missing person (or people) in large, crowded urban spaces. Designing EagleEye involves critical technical challenges for both accuracy and latency. Firstly, despite recent advances in Deep Neural Network (DNN)-based face identification, we observe that state-of-the-art models fail to accurately identify Low-Resolution (LR) faces. Accordingly, we design a novel Identity Clarification Network to recover missing details in the LR faces, which enhances true positives by 78% with only 14% false positives. Furthermore, designing EagleEye involves unique challenges compared to recent continuous mobile vision systems in that it requires running a series of complex DNNs multiple times on a high-resolution image. To tackle the challenge, we develop Content-Adaptive Parallel Execution to optimize complex multi-DNN face identification pipeline execution latency using heterogeneous processors on mobile and cloud. Our results show that EagleEye achieves 9.07X faster latency compared to naive execution, with only 108 KBytes of data offloaded.",2020,MobiCom,,10.1145/3372224.3380881,
5313367e0a350f3e0b632dfb2e588f63fc272b2b,1,1,Feature Transfer Learning for Face Recognition With Under-Represented Data,"Despite the large volume of face recognition datasets, there is a significant portion of subjects, of which the samples are insufficient and thus under-represented. Ignoring such significant portion results in insufficient training data. Training with under-represented data leads to biased classifiers in conventionally-trained deep networks. In this paper, we propose a center-based feature transfer framework to augment the feature space of under-represented subjects from the regular subjects that have sufficiently diverse samples. A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones. This encourages the under-represented distribution to be closer to the regular distribution. Further, an alternating training regimen is proposed to simultaneously achieve less biased classifiers and a more discriminative feature representation. We conduct ablative study to mimic the under-represented datasets by varying the portion of under-represented classes on the MS-Celeb-1M dataset. Advantageous results on LFW, IJB-A and MS-Celeb-1M demonstrate the effectiveness of our feature transfer and training strategy, compared to both general baselines and state-of-the-art methods. Moreover, our feature transfer successfully presents smooth visual interpolation, which conducts disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations such as pose and lighting.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00585,http://cseweb.ucsd.edu/~mkchandraker/pdf/cvpr19_featuretransfer.pdf
53829cd71916382dd9dc8cbf1ec369ce2bd7f0a4,0,1,A New Discriminative Feature Learning for Person Re-Identification Using Additive Angular Margin Softmax Loss,"In this paper, a new end-to-end framework is proposed for person re-identification (re-ID) by combining metric learning and classification. In this new framework, the Additive Angular Margin Softmax is used which imposes an additive angular margin constraint to the target logit on hypersphere manifold. This is aimed to improve the similarity of the intra-class features and the dissimilarity of the inter-class features simultaneously. Compard with the three popular used softmax-based-loss methods, the experiments show that the proposed approach has achieved improved performance on Market1501 and DukeMTMC-reID datasets for person re-ID.",2019,2019 UK/ China Emerging Technologies (UCET),,10.1109/UCET.2019.8881838,
53840c83f7b6ae78d4310c5b84ab3fde1a33bc4f,1,0,Accelerated Training for Massive Classification via Dynamic Class Selection,"Massive classification, a classification task defined over a vast number of classes (hundreds of thousands or even millions), has become an essential part of many real-world systems, such as face recognition. Existing methods, including the deep networks that achieved remarkable success in recent years, were mostly devised for problems with a moderate number of classes. They would meet with substantial difficulties, e.g. excessive memory demand and computational cost, when applied to massive problems. We present a new method to tackle this problem. This method can efficiently and accurately identify a small number of ""active classes"" for each mini-batch, based on a set of dynamic class hierarchies constructed on the fly. We also develop an adaptive allocation scheme thereon, which leads to a better tradeoff between performance and cost. On several large-scale benchmarks, our method significantly reduces the training cost and memory demand, while maintaining competitive performance.",2018,AAAI,1801.01687,,https://arxiv.org/pdf/1801.01687.pdf
53a6a5a46f685190c5eb56726394a854201368dc,1,1,BroadFace: Looking at Tens of Thousands of People at Once for Face Recognition,"The datasets of face recognition contain an enormous number of identities and instances. However, conventional methods have difficulty in reflecting the entire distribution of the datasets because a mini-batch of small size contains only a small portion of all identities. To overcome this difficulty, we propose a novel method called BroadFace, which is a learning process to consider a massive set of identities, comprehensively. In BroadFace, a linear classifier learns optimal decision boundaries among identities from a large number of embedding vectors accumulated over past iterations. By referring more instances at once, the optimality of the classifier is naturally increased on the entire datasets. Thus, the encoder is also globally optimized by referring the weight matrix of the classifier. Moreover, we propose a novel compensation method to increase the number of referenced instances in the training stage. BroadFace can be easily applied on many existing methods to accelerate a learning process and obtain a significant improvement in accuracy without extra computational burden at inference stage. We perform extensive ablation studies and experiments on various datasets to show the effectiveness of BroadFace, and also empirically prove the validity of our compensation method. BroadFace achieves the state-of-the-art results with significant improvements on nine datasets in 1:1 face verification and 1:N face identification tasks, and is also effective in image retrieval.",2020,ArXiv,2008.06674,,https://arxiv.org/pdf/2008.06674.pdf
53bc5fdfea0fa3ca59b7792aabe567f7e18f4a12,0,1,A Proposed Framework: Face Recognition With Deep Learning,"Face recognition is the capability to ascertain the identification of a person solitary or amidst multitudes of individuals. In lieu to this, deep learning has dominated and it has been used in recent years due to its momentous performance to solve the face recognition challenges using convolutional neural networks (CNN). It is a technology with enormous capabilities and diversities used in computer vison problems such as modelling and saliency detection, semantic segmentation, handwriting digital recognition, emotion recognition and many more. CNN architectures such has Alex Net, VGG are the practically known architectures that have immensely prompt new dataset for CNN model designs. This paper contributes to actualization of a propose CNN based on a pre-trained VGG Face for face recognition from set of faces tracked in video or image capture achieving a 97% accuracy. Also, implementing the use of metric learning to actualized a discriminative feature from our instances.",2020,,,,http://www.ijstr.org/final-print/jul2020/A-Proposed-Framework-Face-Recognition-With-Deep-Learning.pdf
53ea629b5933430eafa98692edf39ab2bd737d09,1,1,Probabilistic Face Embeddings,"Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.09658,10.1109/ICCV.2019.00700,https://arxiv.org/pdf/1904.09658.pdf
53fa57765ce5f3aa6f770077adc0e877ede1d98c,0,1,FEGAN: Flexible and Efficient Face Editing With Pre-Trained Generator,"Since generative adversarial network (GAN) was first proposed, the processing of face images, especially the research of facial attribute editing, has attracted much interest. It not only can alleviate the problems associated with data deficiency, but also has great applications in the field of entertainment. However, existing approaches have limited scalability in the processing of newly-added face attributes, and the quality of generated images is poor. To solve these problems, FEGAN is proposed in this paper to achieve the accurate editing of multi-attribute faces by modifying feature vectors in the latent space. Firstly, a trained generator is used, which greatly reduces the training difficulty of GANs, and the inverse of the generator is used to establish the unique correspondence between the input image and the latent code. Secondly, a linear guide is applied to the latent code, and thus the same distribution as the target image in the latent space is assured. Finally, a generator is used to generate a face image from the guided latent code. The proposed method is utilized for a large number of attribute editing experiments, and the results show that FEGAN can flexibly perform accurate attribute editing while guaranteeing that other areas are not changed. Both qualitative and quantitative results demonstrate its advantages over existing methods.",2020,IEEE Access,,10.1109/ACCESS.2020.2985086,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09055004.pdf
54a6da19283bf3a10bc518c52d8ae1dccc75585b,1,1,"Is Face Recognition Sexist? No, Gendered Hairstyles and Biology Are","Recent news articles have accused face recognition of being ""biased"", ""sexist"" or ""racist"". There is consensus in the research literature that face recognition accuracy is lower for females, who often have both a higher false match rate and a higher false non-match rate. However, there is little published research aimed at identifying the cause of lower accuracy for females. For instance, the 2019 Face Recognition Vendor Test that documents lower female accuracy across a broad range of algorithms and datasets also lists ""Analyze cause and effect"" under the heading ""What we did not do"". We present the first experimental analysis to identify major causes of lower face recognition accuracy for females on datasets where previous research has observed this result. Controlling for equal amount of visible face in the test images reverses the apparent higher false non-match rate for females. Also, principal component analysis indicates that images of two different females are inherently more similar than of two different males, potentially accounting for a difference in false match rates.",2020,ArXiv,2008.06989,,https://arxiv.org/pdf/2008.06989.pdf
54ac0938649d70d3ce4a45028475b07fcdc9cbd5,1,0,Gotta Adapt 'Em All: Joint Pixel and Feature-Level Domain Adaptation for Recognition in the Wild,"Recent developments in deep domain adaptation have allowed knowledge transfer from a labeled source domain to an unlabeled target domain at the level of intermediate features or input pixels. We propose that advantages may be derived by combining them, in the form of different insights that lead to a novel design and complementary properties that result in better performance. At the feature level, inspired by insights from semi-supervised learning, we propose a classification-aware domain adversarial neural network that brings target examples into more classifiable regions of source domain. Next, we posit that computer vision insights are more amenable to injection at the pixel level. In particular, we use 3D geometry and image synthesis based on a generalized appearance flow to preserve identity across pose transformations, while using an attribute-conditioned CycleGAN to translate a single source into multiple target images that differ in lower-level properties such as lighting. Besides standard UDA benchmark, we validate on a novel and apt problem of car recognition in unlabeled surveillance images using labeled images from the web, handling explicitly specified, nameable factors of variation through pixel-level and implicit, unspecified factors through feature-level adaptation.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00278,
54bf900782bc7b37663ea3bfda0c6cf34f0420a7,1,0,Characterizing Bias in Classifiers using Generative Models,"Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the ""blind spots"" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.",2019,NeurIPS,1906.11891,,https://arxiv.org/pdf/1906.11891.pdf
54e1daa2c01ef8907796867a1d23b52acb2573ef,1,0,Finding Person Relations in Image Data of the Internet Archive,"The amount of multimedia content in the World Wide Web is rapidly growing and contains valuable information for many applications in different domains. The Internet Archive initiative has gathered billions of time-versioned web pages since the mid-nineties. However, the huge amount of data is rarely labeled with appropriate metadata and automatic approaches are required to enable semantic search. Normally, the textual content of the Internet Archive is used to extract entities and their possible relations across domains such as politics and entertainment, whereas image and video content is usually disregarded. In this paper, we introduce a system for person recognition in image content of web news stored in the Internet Archive. Thus, the system complements entity recognition in text and allows researchers and analysts to track media coverage and relations of persons more precisely. Based on a deep learning face recognition approach, we suggest a system that detects persons of interest and gathers sample material, which is subsequently used to identify them in the image data of the Internet Archive. We evaluate the performance of the face recognition system on an appropriate standard benchmark dataset and demonstrate the feasibility of the approach with two use cases.",2018,TPDL,1806.08246,10.1007/978-3-030-00066-0_20,https://arxiv.org/pdf/1806.08246.pdf
555c2b2e952eef9feb603bf72954f5926af846fe,1,0,Early warning system: From face recognition by surveillance cameras to social media analysis to detecting suspicious people,"Abstract Surveillance security cameras are increasingly deployed in almost every location for monitoring purposes, including watching people and their actions for security purposes. For criminology, images collected from these cameras are usually used after an incident occurs to analyze who could be the people involved. While this usage of the cameras is important for a post crime action, there exists the need for real time monitoring to act as an early warning to prevent or avoid an incident before it occurs. In this paper, we describe the development and implementation of an early warning system that recognizes people automatically in a surveillance camera environment and then use data from various sources to identify these people and build their profile and network. The current literature is still missing a complete workflow from identifying people/criminals from a video surveillance to building a criminal information extraction framework and identifying those people and their interactions with others We train a feature extraction model for face recognition using convolutional neural networks to get a good recognition rate on the Chokepoint dataset collected using surveillance cameras. The system also provides the function to record people appearance in a location, such that unknown people passing through a scene excessive number of times (above a threshold decided by a security expert) will then be further analyzed to collect information about them. We implemented a queue based system to record people entrance. We try to avoid missing relevant individuals passing through as in some cases it is not possible to add every passing person to the queue which is maintained using some cache handling techniques. We collect and analyze information about unknown people by comparing their images from the cameras to a list of social media profiles collected from Facebook and intelligent services archives. After locating the profile of a person, traditional news and other social media platforms are crawled to collect and analyze more information about the identified person. The analyzed information is then presented to the analyst where a list of keywords and verb phrases are shown. We also construct the person’s network from individuals mentioned with him/her in the text. Further analysis will allow security experts to mark this person as a suspect or safe. This work shows that building a complete early warning system is feasible to tackle and identify criminals so that authorities can take the required actions on the spot.",2020,,,10.1016/j.physa.2019.123151,
556c4bd00ef2a9a13a8db74847da3ce047782251,1,1,Context-Aware Based Discriminative Siamese Neural Network for Face Verification,,2021,,,10.1007/978-3-030-56178-9_9,
55762d0bc8d79f99ae601ad5cd33550ccc4f8dc3,0,1,Training Deep Face Recognition for Efficient Inference by Distillation and Mutual Learning,"Currently most of deep face recognition algorithms utilize heavy networks to achieve the state-of-the-art performance. In most scenarios, the more challenging task is to achieve the relative high accuracy with low computational cost especially for embedded devices. In this paper, we propose a lightweight network for face recognition using distillation and deep mutual learning. In proposed methods a new indicator is designed to monitor the model convergence and an assessment criteria is developed to evaluate the Labeled Faces in the Wild(LFW) dataset. Experiments show that our models work better than networks trained directly and other mobile face recognition solutions.",2018,2018 IEEE International Conference on Progress in Informatics and Computing (PIC),,10.1109/PIC.2018.8706137,
5598c9519bbece7cc158cfbd46ea9c7074fb1ab2,0,1,Make the Best of Face Clues in iQIYI Celebrity VideoIdentification Challenge 2019,"iQIYI-VID-2019 is the largest video dataset for multi-modal person identification. It is composed of more than 200k video clips of 10,034 celebrities. Face is a critical clue for person identification when the face is visible in video. However face quality in a video may not always be good, and it also contains a lot of noise caused by detection and feature extraction. Meanwhile, conventional multi-modal person classification methods do not fully exploit the ability of face modality. They do not make full use of face detection confidence and quality evaluation indicators, which are key information in face modality. To address these issues, we develop a quality-based video face feature fusion method in inference with a quality-based face feature denoising and augmentation method in training. Our approach is only based on 512-dimensional face features provided by iQIYI-VID-2019 dataset. Utilizing our proposed novel method, we have achieved the mAP score of 89.83% which is the 4th place in iQIYI Celebrity Video Identification Challenge 2019.",2019,ACM Multimedia,,10.1145/3343031.3356056,
559ef2cc4218b6d190c76fc681091bc038807721,1,1,Feature Aggregation Network for Video Face Recognition,"This paper aims to learn a compact representation of a video for video face recognition task. We make the following contributions: first, we propose a meta attention-based aggregation scheme which adaptively and fine-grained weighs the feature along each feature dimension among all frames to form a compact and discriminative representation. It makes the best to exploit the valuable or discriminative part of each frame to promote the performance of face recognition, without discarding or despising low quality frames as usual methods do. Second, we build a feature aggregation network comprised of a feature embedding module and a feature aggregation module. The embedding module is a convolutional neural network used to extract a feature vector from a face image, while the aggregation module consists of cascaded two meta attention blocks which adaptively aggregate the feature vectors into a single fixed-length representation. The network can deal with arbitrary number of frames, and is insensitive to frame order. Third, we validate the performance of proposed aggregation scheme. Experiments on publicly available datasets, such as YouTube face dataset and IJB-A dataset, show the effectiveness of our method, and it achieves competitive performances on both the verification and identification protocols.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00128,
55a51f80d3c1140f37974715f4b94037f583f5ce,0,1,The ins and outs of speaker recognition: lessons from VoxSRC 2020,"The VoxCeleb Speaker Recognition Challenge (VoxSRC) at Interspeech 2020 offers a challenging evaluation for speaker recognition systems, which includes celebrities playing different parts in movies. The goal of this work is robust speaker recognition of utterances recorded in these challenging environments. We utilise variants of the popular ResNet architecture for speaker recognition and perform extensive experiments using a range of loss functions and training parameters. To this end, we optimise an efficient training framework that allows powerful models to be trained with limited time and resources. Our trained models demonstrate improvements over most existing works with lighter models and a simple pipeline. The paper shares the lessons learned from our participation in the challenge.",2020,ArXiv,2010.15809,,https://arxiv.org/pdf/2010.15809.pdf
55fad18dfd6d15d87e286b129bdf07f8c873b4ff,1,0,Multi-Process Training GAN for Identity-Preserving Face Synthesis,"Recently, the advent of generative adversarial networks (GANs) in synthesizing identity-preserving faces has aroused the considerable interest of many scholars. However, face attribute representation learning, which is explicitly disentangled from identity feature and synthesizes identity-preserving face images with high diversity and quality in other datasets, still remains challenging. To cope with that, this paper proposes multi-process training GAN, or MP-GAN for short, which significantly improves the disentangled representation, diversity, and quality. Unlike other existing single-process models that map noise to a final output resolution image in a single training process, MP-GAN divides training into multiple processes. The main idea is to first generate lower resolution images that contain lower frequency feature information through competition and then extract their disentangled facial features to generate a higher resolution image. Furthermore, an identity-preserving image with real identity feature and disentangled facial feature could be generated at the final output resolution training process. The distinct benefits are not only getting diverse facial feature generation but also achieving disentangled representation from the lower resolution training processes and rendering a photo-realistic image that contains high diversity but preserves identity at the final output resolution training process. The high performance of this method is highlighted by quantitative and qualitative comparisons. We conclude that MP-GAN can generate face images featuring high diversity and quality while efficiently preserving identity, thereby significantly outperforming most modern advanced methods.",2019,IEEE Access,,10.1109/ACCESS.2019.2930203,
566a2ede36a6493010ea42a7df49916739e00c9d,1,0,Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis,"Photorealistic frontal view synthesis from a single face image has a wide range of applications in the field of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoderdecoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from profiles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-theart results on large pose face recognition.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1704.04086,10.1109/ICCV.2017.267,https://arxiv.org/pdf/1704.04086.pdf
566c7c18639934a0a20293d8e58956742e1c735e,1,0,Mutual variation of information on transfer-CNN for face recognition with degraded probe samples,"Abstract Learning based on convolutional neural networks (CNNs) or deep learning has been a major research area with applications in face recognition (FR). Under degraded conditions, performance of FR algorithms severely degrade. The work presented in this paper has three contributions. First, it proposes a transfer-CNN architecture of deep learning tailor-made for domain adaptation (DA), to overcome the difference in feature distributions between the gallery and probe samples. The proposed architecture consists of three units: base convolution (BCM), transfer (TM) and linear (LM) modules. Secondly, a novel 3-stage algorithm for Mutually Exclusive Training (3-MET) based on stochastic gradient descent, has been proposed. The initial stage of 3-MET involves updating the parameters of the BCM and LM units using samples from gallery. The second stage involves updating the parameters of TM, to bridge the disparity between the source and target distributions, based on mutual variation of information (MVI). The final stage of training in 3-MET freezes the layers of the BCM and TM, for updating (fine-tuning) only the parameters of the LM using a few probe (as target) samples. This helps the proposed transfer-CNN to provide enhanced domain-invariant representation for efficient deep-DA learning and classification. The third contribution comes from rigorous experimentations performed on three benchmark real-world degraded face datasets captured using surveillance cameras, one real-world dataset with non-uniform motion blur and three synthetically degraded benchmark face datasets. This exhibits superior performance of the proposed transfer-CNN architecture with 3-MET training, using Rank-1 recognition rates and ROC and CMC metrics, over many recent state-of-the-art techniques of CNN and DA. Experiments also include performance analysis under unbiased training with two large-scale chimeric face datasets.",2018,Neurocomputing,,10.1016/j.neucom.2018.05.038,
567fcd10911111737dc489ec04ad6495bacc359e,0,1,Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation,"Many real-world applications today like video surveillance and urban governance need to address the recognition of masked faces, where content replacement by diverse masks often brings in incomplete appearance and ambiguous representation, leading to a sharp drop in accuracy. Inspired by recent progress on amodal perception, we propose to migrate the mechanism of amodal completion for the task of masked face recognition with an end-to-end de-occlusion distillation framework, which consists of two modules. The de-occlusion module applies a generative adversarial network to perform face completion, which recovers the content under the mask and eliminates appearance ambiguity. The distillation module takes a pre-trained general face recognition model as the teacher and transfers its knowledge to train a student for completed faces using massive online synthesized face pairs. Especially, the teacher knowledge is represented with structural relations among instances in multiple orders, which serves as a posterior regularization to enable the adaptation. In this way, the knowledge can be fully distilled and transferred to identify masked faces. Experiments on synthetic and realistic datasets show the efficacy of the proposed approach.",2020,ACM Multimedia,,10.1145/3394171.3413960,
568435e4a2d161c68808a35250be147202867ce4,1,1,Deep Learning For Face Recognition: A Critical Analysis,"Face recognition is a rapidly developing and widely applied aspect of biometric technologies. Its applications are broad, ranging from law enforcement to consumer applications, and industry efficiency and monitoring solutions. The recent advent of affordable, powerful GPUs and the creation of huge face databases has drawn research focus primarily on the development of increasingly deep neural networks designed for all aspects of face recognition tasks, ranging from detection and preprocessing to feature representation and classification in verification and identification solutions. However, despite these improvements, real-time, accurate face recognition is still a challenge, primarily due to the high computational cost associated with the use of Deep Convolutions Neural Networks (DCNN), and the need to balance accuracy requirements with time and resource constraints. Other significant issues affecting face recognition relate to occlusion, illumination and pose invariance, which causes a notable decline in accuracy in both traditional handcrafted solutions and deep neural networks. This survey will provide a critical analysis and comparison of modern state of the art methodologies, their benefits, and their limitations. It provides a comprehensive coverage of both deep and shallow solutions, as they stand today, and highlight areas requiring future development and improvement. This review is aimed at facilitating research into novel approaches, and further development of current methodologies by scientists and engineers, whilst imparting an informative and analytical perspective on currently available solutions to end users in industry, government and consumer contexts.",2019,ArXiv,1907.12739,,https://arxiv.org/pdf/1907.12739.pdf
5697ae2b337a771956684b6f8b26e0d6f13097cc,0,1,Real-time detection tracking and recognition algorithm based on multi-target faces,"At present, face recognition algorithms are facing some problems with poor face tracking and low real-time performance in multi-target recognition scenarios. This paper details a multi-target face real-time detection tracking and recognition algorithm, including three methods of fast-tracking, fast detection, and quick recognition. The first step offers a new network based on GOTURN for achieving fast face tracking. The prior information of the previous frame image used to predict the position of the face boxes at the current frame. The second step is based on MTCNN for face detection, using the prior information of the present structure to avoid generating massive of invalid candidate boxes, thereby achieving rapid detection of faces. Finally, fast face recognition realized by reduced MobileFaceNet. By avoiding repeated exposure and repeated identification of the same target, the algorithm successfully transforms a multi-target scene into a single-target scene. On the OTB2015 and 300_VW test sets, the evaluation trackers tracked faces with an accuracy rate of 92.2% and 99.6% respectively. On the Xiph test set, multi-target detection and tracking face speed reached 102fps on the CPU. Compared with the original MobileFaceNet, the streamlined network has an accuracy rate of 99.1% on LFW, the feature extraction speed increased by 25%, and the model size reduced by 45%. Experimental results show that the algorithm has high recognition accuracy and real-time performance in multi-target recognition scenes.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09601-2,
5714183a5ef9fa40a5ad0bdff4908fb447a144a6,1,1,NPCFace: A Negative-Positive Cooperation Supervision for Training Large-scale Face Recognition,"Deep face recognition has made remarkable advances in the last few years, while the training scheme still remains challenging in the large-scale data situation where many hard cases occur. Especially in the range of low false accept rate (FAR), there are various hard cases in both positives ($\textit{i.e.}$ intra-class) and negatives ($\textit{i.e.}$ inter-class). In this paper, we study how to make better use of these hard samples for improving the training. The existing training methods deal with the challenge by adding margins in either the positive logit (such as SphereFace, CosFace, ArcFace) or the negative logit (such as MV-softmax, ArcNegFace, CurricularFace). However, the correlation between hard positive and hard negative is overlooked, as well as the relation between the margin in positive logit and the margin in negative logit. We find such correlation is significant, especially in the large-scale dataset, and one can take advantage from it to boost the training via relating the positive and negative margins for each training sample. To this end, we propose an explicit cooperation between positive and negative margins sample-wisely. Given a batch of hard samples, a novel Negative-Positive Cooperation loss, named NPCFace, is formulated, which emphasizes the training on both the negative and positive hard cases via a cooperative-margin mechanism in the softmax logits, and also brings better interpretation of negative-positive hardness correlation. Besides, the negative emphasis is implemented with an improved formulation to achieve stable convergence and flexible parameter setting.We validate the effectiveness of our approach on various benchmarks of large-scale face recognition and outperform the previous methods especially in the low FAR range.",2020,ArXiv,2007.10172,,https://arxiv.org/pdf/2007.10172.pdf
5736e42fe3ec4220a446100dbce6dc57f93a37a5,1,0,Face Feature Recovery via Temporal Fusion for Person Search,"Searching actors from videos by a single portrait image is a challenging task, due to large variations of video scenes and intra-person appearance. To tackle this problem, most recent works apply deep neural networks for detecting and extracting robust facial features for matching. However, when the face of an actor is not detected due to occlusion, such image-matching based strategies would not be applicable. To address the issue, we propose a unique framework of ""Face Feature Recovery via Temporal Fusion"" to synthesize virtual facial features by observing both temporal and contextual information. Once such face features are extracted, a simple extension to the k-nearest neighbors for re-ranking, ""Iterative k-nearest Multi-fusion"", is presented to utilize both face and body features for improved person search. We conduct extensive experiments to evaluate the performance of our framework on the challenging extended version of the Cast Search in Movies (ECSM) dataset [1]. Without utilizing tracklet information during training, the proposed approach still performs favorably against recent works in searching actors of interest from movie videos. Besides, we also show the proposed approach can be fused with them to further improve the performance.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053779,
576d65c9bee41af096d1b46282ed07ab86161fa5,1,0,Visual BMI estimation from face images using a label distribution based method,"Abstract Body mass index (BMI) analysis from face images is an interesting and challenging topic in machine learning and computer vision. Recent research shows that facial adiposity is associated with BMI prediction. In this work, we investigate the problem of visual BMI estimation from face images by a two-stage learning framework. BMI-related facial features are learned from the first stage. Then a label distribution based BMI estimator is learned by an optimization procedure that is implemented by projecting the features and assigned labels to a new domain which maximizing the correlation between them. Two label assignment strategies are analyzed for modeling the single BMI value as a discrete probability distribution over a range of BMIs. Extensive experiments are conducted on FIW-BMI, Morph II and VIP_attribute datasets. The experimental results show that the two-stage learning framework improves the performance step by step. More importantly, the proposed BMI estimator efficiently reduces the error. It outperforms regression based methods, two label distribution methods and two deep learning methods in most cases.",2020,Comput. Vis. Image Underst.,,10.1016/j.cviu.2020.102985,
57a807f65e61bbba0ea3ba02572cc5843ecef2b6,1,0,L2-constrained Softmax Loss for Discriminative Face Verification,"In recent years, the performance of face verification systems has significantly improved using deep convolutional neural networks (DCNNs). A typical pipeline for face verification includes training a deep network for subject classification with softmax loss, using the penultimate layer output as the feature descriptor, and generating a cosine similarity score given a pair of face images. The softmax loss function does not optimize the features to have higher similarity score for positive pairs and lower similarity score for negative pairs, which leads to a performance gap. In this paper, we add an L2-constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. Specifically, we achieve state-of-the-art results on the challenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept Rate 0.0001 on the face verification protocol. Additionally, we achieve state-of-the-art performance on LFW dataset with an accuracy of 99.78%, and competing performance on YTF dataset with accuracy of 96.08%.",2017,ArXiv,1703.09507,,https://arxiv.org/pdf/1703.09507.pdf
584386f5d83b40bdc8236cf2d9b1408bf64d25d5,0,1,Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval,"We propose an efficient pipeline for large-scale landmark image retrieval that addresses the diversity of the dataset through two-stage discriminative re-ranking. Our approach is based on embedding the images in a feature-space using a convolutional neural network trained with a cosine softmax loss. Due to the variance of the images, which include extreme viewpoint changes such as having to retrieve images of the exterior of a landmark from images of the interior, this is very challenging for approaches based exclusively on visual similarity. Our proposed re-ranking approach improves the results in two steps: in the sort-step, k-nearest neighbor search with soft-voting to sort the retrieved results based on their label similarity to the query images, and in the insert-step, we add additional samples from the dataset that were not retrieved by image-similarity. This approach allows overcoming the low visual diversity in retrieved images. In-depth experimental results show that the proposed approach significantly outperforms existing approaches on the challenging Google Landmarks Datasets. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge on Kaggle. Our code is publicly available here: https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2003.11211,10.1109/CVPRW50498.2020.00514,https://arxiv.org/pdf/2003.11211.pdf
590b2c92f59a90bd17e4dbcaf417d6a6eaf81a5f,0,1,Deep Representation Learning With Full Center Loss for Credit Card Fraud Detection,"Credit card fraud detection is an important study in the current era of mobile payment. Improving the performance of a fraud detection model and keeping its stability are very challenging because users’ payment behaviors and criminals’ fraud behaviors are often changing. In this article, we focus on obtaining deep feature representations of legal and fraud transactions from the aspect of the loss function of a deep neural network. Our purpose is to obtain better separability and discrimination of features so that it can improve the performance of our fraud detection model and keep its stability. We propose a new kind of loss function, full center loss (FCL), which considers both distances and angles among features and, thus, can comprehensively supervise the deep representation learning. We conduct lots of experiments on two big data sets of credit card transactions, one is private and another is public, to demonstrate the detection performance of our model by comparing FCL with other state-of-the-art loss functions. The results illustrate that FCL outperforms others. We also conduct experiments to show that FCL can ensure a more stable model than others.",2020,IEEE Transactions on Computational Social Systems,,10.1109/TCSS.2020.2970805,
5991adeac3641690e6cb1d785d276e10ec013160,1,0,"Multi-Channel Biometrics for eHealth Combining Acoustic and Machine Vision Analysis of Speech, Lip Movement and Face: a Case Study","The purpose of this work is to present a solution combining user-friendliness and cost-effectiveness use of audio (speech) & visual (video/image) biometrics, for eHealth, able to achieve better accuracy and increase the ability to avoid counterfeiting. This work shows the evaluation results for an eHealth pilot study that tested the security, privacy, usability and cost-effective features of a user authentication platform for the management of sensitive heterogeneous multi-scale medical data (i.e. medical imaging such as MRI/CT scans, physical reports, and laboratory results), through easy acquisition of biometric data via laptops, and tablets equipped with cameras and microphones. Regarding the user enrollment and verification, audio-visual biometric information from an individual is captured, processed and stored as a biometric template. In subsequent uses, biometric information is captured and compared with the biometric templates. If the comparison is successful the verified user could be allowed to sign in to a medical collaboration platform of the hospitals infrastructure. In this work we present the biometric platform developed, the testing methodology and the administrative framework and legal processes, related to GDPR, for the eHealth pilot study and the results from our quantitative and qualitative analysis that was performed.",2019,2019 IEEE International Conference on Imaging Systems and Techniques (IST),,10.1109/IST48021.2019.9010457,
59b6e9320a4e1de9216c6fc49b4b0309211b17e8,1,0,Robust Representations for unconstrained Face Recognition and its Applications,"Title of dissertation: ROBUST REPRESENTATIONS FOR UNCONSTRAINED FACE RECOGNITION AND ITS APPLICATIONS Jun-Cheng Chen, Doctor of Philosophy, 2016 Dissertation directed by: Professor Rama Chellappa Department of Computer Science Face identification and verification are important problems in computer vision and have been actively researched for over two decades. There are several applications including mobile authentication, visual surveillance, social network analysis, and video content analysis. Many algorithms have shown to work well on images collected in controlled settings. However, the performance of these algorithms often degrades significantly on images that have large variations in pose, illumination and expression as well as due to aging, cosmetics, and occlusion. How to extract robust and discriminative feature representations from face images/videos is an important problem to achieve good performance in uncontrolled settings. In this dissertation, we present several approaches to extract robust feature representation from a set of images/video frames for face identification and verification problems. We first present a dictionary approach with dense facial landmark features. Each face video is segmented into K partitions first, and the multi-scale features are extracted from patches centered at detected facial landmarks. Then, compact and representative dictionaries are learned from dense features for each partition of a video and then concatenated together into a video dictionary representation for the video. Experiments show that the representation is effective for the unconstrained video-based face identification task. Secondly, we present a landmark-based Fisher vector approach for video-based face verification problems. This approach encodes over-complete local features into a highdimensional feature representation followed by a learned joint Bayesian metric to project the feature vector into a low-dimensional space and to compute the similarity score. We then present an automated system for face verification which exploits features from deep convolutional neural networks (DCNN) trained using the CASIA-WebFace dataset. Our experimental results show that the DCNN model is able to characterize the face variations from the large-scale source face dataset and generalizes well to another smaller one. Finally, we also demonstrate that the model pre-trained for face identification and verification tasks encodes rich face information which benefit other face-related tasks with scarce annotated training data. We use apparent age estimation as an example and develop a cascade convolutional neural network framework which consists of age group classification and age regression, and a deep networks is fine-tuned using the target data. ROBUST REPRESENTATIONS FOR UNCONSTRAINED FACE RECOGNITION AND ITS APPLICATIONS",2016,,,10.13016/M22835,
59c47e49d8211953b1acd68984650b807ce69a71,1,1,Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network,"Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00078,
59ed3740b1c4b681d629128dc07ad492cb5da86e,0,1,Regularizing Class-Wise Predictions via Self-Knowledge Distillation,"Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.13964,10.1109/cvpr42600.2020.01389,https://arxiv.org/pdf/2003.13964.pdf
59fb2a2898decc1d2f8f6259af8036500c95c422,1,0,Neural Aggregation Network for Video Face Recognition,"This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),1603.05474,10.1109/CVPR.2017.554,https://arxiv.org/pdf/1603.05474.pdf
59fc69b3bc4759eef1347161e1248e886702f8f7,1,1,Final Report of Final Year Project HKU-Face : A Large Scale Dataset for Deep Face Recognition,"Current research and development of face recognition mainly focus on three perspectives, the scale and purity of facial dataset, the effectiveness of loss functions and employment of effective backbone network structures[2, 6, 9, 13, 19, 34]. The scale and labelling errors of training dataset is usually the problem encountered by many researchers in institutions for face recognition research. This project introduces a dataset filtering and construction procedures for solving the problem with less human effort. Pre-trained face recognition models like Sphereface[19] and Facenet[28] are employed to extract deep features of face images. Then these representational features are used for noisy image filtering. Impressive result of automatic filtering and purity level have been achieved compared with the raw dataset. Except for the self-constructed dataset, another filtered dataset named CASIAVGG merged from CASIA-Webface[40] dataset and VGG[23] Dataset is also researched and analyzed in this project.",2018,,,,https://pdfs.semanticscholar.org/59fc/69b3bc4759eef1347161e1248e886702f8f7.pdf
5a564d108b43c6ff006a86d2fc981cd36c6c54dd,1,0,Deep Learning for Understanding Faces,"Recent developments in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on various object detection/recognition problems. This has been made possible due to the availability of large annotated data, a better understanding of the non-linear mapping between images and class labels as well as the affordability of powerful GPUs. These developments in deep learning have also improved the capabilities of machines in understanding faces and automatically executing the tasks of face detection, pose estimation, landmark localization, and face recognition from unconstrained images and videos. In this paper, we provide an overview of deep learning methods used for face recognition. We discuss different modules involved in designing an automatic face recognition system and the role of deep learning for each of them. Some open issues regarding DCNNs for face recognition problems are then discussed. The paper should prove valuable to scientists, engineers and end users working in the fields of face recognition, security, visual surveillance, and biometrics.",2017,,,,https://pdfs.semanticscholar.org/5a56/4d108b43c6ff006a86d2fc981cd36c6c54dd.pdf
5a803d9334246cfb7762eb210c19459febdf4b22,1,0,Rethinking Feature Discrimination and Polymerization for Large-scale Recognition,"Feature matters. How to train a deep network to acquire discriminative features across categories and polymerized features within classes has always been at the core of many computer vision tasks, specially for large-scale recognition systems where test identities are unseen during training and the number of classes could be at million scale. In this paper, we address this problem based on the simple intuition that the cosine distance of features in high-dimensional space should be close enough within one class and far away across categories. To this end, we proposed the congenerous cosine (COCO) algorithm to simultaneously optimize the cosine similarity among data. It inherits the softmax property to make inter-class features discriminative as well as shares the idea of class centroid in metric learning. Unlike previous work where the center is a temporal, statistical variable within one mini-batch during training, the formulated centroid is responsible for clustering inner-class features to enforce them polymerized around the network truncus. COCO is bundled with discriminative training and learned end-to-end with stable convergence. Experiments on five benchmarks have been extensively conducted to verify the effectiveness of our approach on both small-scale classification task and large-scale human recognition problem.",2017,ArXiv,1710.0087,,https://arxiv.org/pdf/1710.00870.pdf
5a8fb050f3127fd33292a720878ff0d38567caae,0,1,Margin-Based Deep Learning Networks for Human Activity Recognition,"Human activity recognition (HAR) is a popular and challenging research topic, driven by a variety of applications. More recently, with significant progress in the development of deep learning networks for classification tasks, many researchers have made use of such models to recognise human activities in a sensor-based manner, which have achieved good performance. However, sensor-based HAR still faces challenges; in particular, recognising similar activities that only have a different sequentiality and similarly classifying activities with large inter-personal variability. This means that some human activities have large intra-class scatter and small inter-class separation. To deal with this problem, we introduce a margin mechanism to enhance the discriminative power of deep learning networks. We modified four kinds of common neural networks with our margin mechanism to test the effectiveness of our proposed method. The experimental results demonstrate that the margin-based models outperform the unmodified models on the OPPORTUNITY, UniMiB-SHAR, and PAMAP2 datasets. We also extend our research to the problem of open-set human activity recognition and evaluate the proposed method’s performance in recognising new human activities.",2020,Sensors,,10.3390/s20071871,https://pdfs.semanticscholar.org/c63f/d596503061fc6de92c50dadb8fe5db88b271.pdf
5b65716709a2a7a4da2e2aeb611f82e7aacfbbf0,0,1,Large Margin Softmax Loss for Speaker Verification,"In neural network based speaker verification, speaker embedding is expected to be discriminative between speakers while the intra-speaker distance should remain small. A variety of loss functions have been proposed to achieve this goal. In this paper, we investigate the large margin softmax loss with different configurations in speaker verification. Ring loss and minimum hyperspherical energy criterion are introduced to further improve the performance. Results on VoxCeleb show that our best system outperforms the baseline approach by 15\% in EER, and by 13\%, 33\% in minDCF08 and minDCF10, respectively.",2019,INTERSPEECH,1904.03479,10.21437/interspeech.2019-2357,https://arxiv.org/pdf/1904.03479.pdf
5b9c6ca84268cb283941ae28b73989c0cf7e2ac2,1,0,A Pipeline to Improve Face Recognition Datasets and Applications,"Face recognition has a wide practical applicability in various contexts, for example, detecting students attending a lecture at university, identifying members in a gym or monitoring people in an airport. Recent methods based on Convolutional Neural Network (CNN), such as FaceNet, achieved state-of-the-art performance in face recognition. Inspired from this work, we propose a pipeline to improve face recognition systems based on Center loss. The main advantage is that our approach does not suffer from data expansion as in Triplet loss. Our pipeline is capable of cleaning an existing face dataset to improve the recognition performance or creating one from scratch. We present detailed experiments to show characteristics and performance of the pipeline. In addition, a small-scale application for face recognition that makes use of the proposed cleaning process is presented.",2018,2018 International Conference on Image and Vision Computing New Zealand (IVCNZ),,10.1109/IVCNZ.2018.8634724,
5bb5b3c7b42614111a907392739642ddf093db3f,0,1,Robust Face Recognition in the Deep Learning Era,"Modern facial recognition systems rely on deep neural networks which are known to be susceptible to adversarial attacks. We evaluate the robustness of a state-of-the-art facial recognition system, FaceNet, under four modern attacks and two defenses. We find several surprising results – for instance, that uninformed attackers are extremely ineffective against even the most basic defense – and conclude our work with several recommendations for practitioners. We publish all code for this project on Github. 2",2019,,,,https://pdfs.semanticscholar.org/5bb5/b3c7b42614111a907392739642ddf093db3f.pdf
5bf85a60cf7506b0c14d484a2a50f553ae9a45a9,1,0,Conditional Expression Synthesis with Face Parsing Transformation,"Facial expression synthesis with various intensities is a challenging synthesis task due to large identity appearance variations and a paucity of efficient means for intensity measurement. This paper advances the expression synthesis domain by the introduction of a Couple-Agent Face Parsing based Generative Adversarial Network (CAFP-GAN) that unites the knowledge of facial semantic regions and controllable expression signals. Specially, we employ a face parsing map as a controllable condition to guide facial texture generation with a special expression, which can provide a semantic representation of every pixel of facial regions. Our method consists of two sub-networks: face parsing prediction network (FPPN) uses controllable labels (expression and intensity) to generate a face parsing map transformation that corresponds to the labels from the input neutral face, and facial expression synthesis network (FESN) makes the pretrained FPPN as a part of it to provide the face parsing map as a guidance for expression synthesis. To enhance the reality of results, couple-agent discriminators are served to distinguish fake-real pairs in both two sub-nets. Moreover, we only need the neutral face and the labels to synthesize the unknown expression with different intensities. Experimental results on three popular facial expression databases show that our method has the compelling ability on continuous expression synthesis.",2018,MM '18,,10.1145/3240508.3240647,
5c2481d33218ae2fe119bf74b0e6b9ac8074a655,0,1,Interpretable Inference Graphs for Face Recognition,"Convolutional Neural Networks with Adaptive Inference Graphs (ConvNet-AIG) use adaptive network topologies through on/off gating on network layers for individual images to achieve improved computational efficiency and classification accuracy. Face recognition is a more difficult task than object classification due to fine-grained differences in facial features. We evaluate the performance of ConvNet-AIG for the task of Face Recognition on the IMDb-Face dataset. We analyse the interpretability of inference graphs for different facial features and show that facial features like gender, skin color and race can be interpreted corresponding to individual images. We propose a novel loss function to force interpretable inference graphs(ConvNet-IIG) and empirically show an improvement in classification accuracy.",2019,2019 International Conference on Image and Vision Computing New Zealand (IVCNZ),,10.1109/IVCNZ48456.2019.8960990,
5c30f836c18c8b4c02ef5405593c5818df86e460,0,1,Incremental Training for Face Recognition,"Many applications require the identification of persons in video. However, the set of persons of interest is not always known in advance, e.g., in applications for media production and archiving. Additional training samples may be added during the analysis, or groups of faces of one person may need to be identified retrospectively. In order to avoid re-running the face recognition, we propose an approach that supports fast incremental training based on a state of the art face detection and recognition pipeline using CNNs and an online random forest as a classifier. We also describe an algorithm to use the incremental training approach to automatically train classifiers for unknown persons, including safeguards to avoid noise in the training data. We show that the approach reaches state of the art performance on two datasets when using all training samples, but performs better with few or even only one training sample.",2019,MMM,,10.1007/978-3-030-05710-7_24,
5c54e0f46330787c4fac48aecced9a8f8e37658a,1,0,Simple Triplet Loss Based on Intra/Inter-Class Metric Learning for Face Verification,"Recently, benefiting from the advances of the deep convolution neural networks (CNNs), significant progress has been made in the field of the face verification and face recognition. Specially, the performance of the FaceNet has overpassed the human level performance in terms of the accuracy on the datasets ""Labeled Faces in the Wild (LFW)"" and ""Youtube Faces in the Wild (YTF)"". The triplet loss used in the FaceNet has proved its effectiveness for face verification. However, the number of the possible triplets is explosive when using a large scale dataset to train the model. In this paper, we propose a simple class-wise triplet loss based on the intra/inter-class distance metric learning which can largely reduce the number of the possible triplets to be learned. However the simplification of the classic triplet loss function has not degraded the performance of the proposed approach. The experimental evaluations on the most widely used benchmarks LFW and YTF show that the model with the proposed class-wise simple triplet loss can reach the state-of-the-art performance. And the visualization of the distribution of the learned features based on the MNIST dataset has also shown the effectiveness of the proposed method to better separate the classes and make the features more discriminative in comparison with the other state-of-the-art loss function.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.194,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w23/Ming_Simple_Triplet_Loss_ICCV_2017_paper.pdf
5c930ad1665ef5a1da37e1bcadcf97bb7412b793,0,1,Differential 3D Facial Recognition: Adding 3D to Your State-of-the-Art 2D Method,"Active illumination is a prominent complement to enhance 2D face recognition and make it more robust, e.g., to spoofing attacks and low-light conditions. In the present work we show that it is possible to adopt active illumination to enhance state-of-the-art 2D face recognition approaches with 3D features, while bypassing the complicated task of 3D reconstruction. The key idea is to project over the test face a high spatial frequency pattern, which allows us to simultaneously recover real 3D information plus a standard 2D facial image. Therefore, state-of-the-art 2D face recognition solution can be transparently applied, while from the high frequency component of the input image, complementary 3D facial features are extracted. Experimental results on ND-2006 dataset show that the proposed ideas can significantly boost face recognition performance and dramatically improve the robustness to spoofing attacks.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,2004.03385,10.1109/TPAMI.2020.2986951,https://arxiv.org/pdf/2004.03385.pdf
5cdc7e9bd040d11bafc5aa39642b1630bb5ec637,0,1,State-of-the-art speaker recognition with neural network embeddings in NIST SRE18 and Speakers in the Wild evaluations,"Abstract We present a thorough analysis of the systems developed by the JHU-MIT consortium in the context of NIST speaker recognition evaluation 2018. In the previous NIST evaluation, in 2016, i-vectors were the speaker recognition state-of-the-art. However now, neural network embeddings (a.k.a. x-vectors) rise as the best performing approach. We show that in some conditions, x-vectors’ detection error reduces by 2 w.r.t. i-vectors. In this work, we experimented on the Speakers In The Wild evaluation (SITW), NIST SRE18 VAST (Video Annotation for Speech Technology), and SRE18 CMN2 (Call My Net 2, telephone Tunisian Arabic) to compare network architectures, pooling layers, training objectives, back-end adaptation methods, and calibration techniques. x-Vectors based on factorized and extended TDNN networks achieved performance without parallel on SITW and CMN2 data. However for VAST, performance was significantly worse than for SITW. We noted that the VAST audio quality was severely degraded compared to the SITW, even though they both consist of Internet videos. This degradation caused strong domain mismatch between training and VAST data. Due to this mismatch, large networks performed just slightly better than smaller networks. This also complicated VAST calibration. However, we managed to calibrate VAST by adapting SITW scores distribution to VAST, using a small amount of in-domain development data. Regarding pooling methods, learnable dictionary encoder performed the best. This suggests that representations learned by x-vector encoders are multi-modal. Maximum margin losses were better than cross-entropy for in-domain data but not in VAST mismatched data. We also analyzed back-end adaptation methods in CMN2. PLDA semi-supervised adaptation and adaptive score normalization (AS-Norm) yielded significant improvements. However, results were still worse than in English in-domain conditions like SITW. We conclude that x-vectors have become the new state-of-the-art in speaker recognition. However, their advantages reduce in cases of strong domain mismatch. We need to investigate domain adaptation and domain invariant training approaches to improve performance in all conditions. Also, speech enhancement techniques with a focus on improving the speaker recognition performance could be of great help.",2020,Comput. Speech Lang.,,10.1016/j.csl.2019.101026,
5ced2bc1c3815bc8ef2def1a8b28424004747db2,0,1,MagnifierNet: Towards Semantic Adversary and Fusion for Person Re-identification,"Although person re-identification (ReID) has achieved significant improvement recently by enforcing part alignment, it is still a challenging task when it comes to distinguishing visually similar identities or identifying the occluded person. In these scenarios, magnifying details in each part features and selectively fusing them together may provide a feasible solution. In this work, we propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts. Firstly, the holistic salient features are encoded by a global branch. Secondly, to enhance detailed representation for each semantic region, the ""Semantic Adversarial Branch"" is designed to learn from dynamically generated semantic-occluded samples during training. Meanwhile, we introduce ""Semantic Fusion Branch"" to filter out irrelevant noises by selectively fusing semantic region information sequentially. To further improve feature diversity, we introduce a novel loss function ""Semantic Diversity Loss"" to remove redundant overlaps across learned semantic representations. State-of-the-art performance has been achieved on three benchmarks by large margins. Specifically, the mAP score is improved by 6% and 5% on the most challenging CUHK03-L and CUHK03-D benchmarks.",2020,,,,
5cf3b29a998e12542584560d253012808345ad7f,1,0,Trustworthy Face Recognition: Improving Generalization of Deep Face Presentation Attack Detection,"The extremely high recognition accuracy achieved by modern, convolutional neural network (CNN) based face recognition (FR) systems has contributed significantly to the adoption of such systems in a variety of applications, from mundane activities like unlocking phones to high-security applications such as border-control. Nonetheless, they have been shown to be highly vulnerable to presentation attacks (PA), also known as spoof-attacks. A face PA is said to have occurred when a face biometric-sample is presented to the camera of an FR system with the intention of interfering with the operation of biometric recognition. An example PA is when someone tries to illicitly access an FR system by presenting a printed face photo of an authorized person to the camera. State-of-the-art face presentation attack detection (PAD) systems which are based on CNNs as well offer counter-measures to PAs. Over the past decade, several datasets have been collected and publicly shared by different research groups, for face PAD experiments. It has been shown that most face PAD systems do not generalize well. That is, PAD systems show satisfactory classification performance when they are trained and evaluated on disjoint subsets of a dataset (known as an intradataset evaluation). However, their performance degrades significantly when they are trained using data from one dataset and evaluated using data from another dataset (a cross-dataset evaluation). The poor generalization of PAD systems precludes FR systems from deployment in many real-world applications. In this thesis, I address generalization issues in face PAD systems in three ways: 1. Although many CNN architectures have been proposed for face PAD, no systematic evaluation of their classification performance has been done before. Here, I evaluate six different CNN architectures on four face PAD datasets in terms of both intra-dataset and cross-dataset performance, and show that patch-based CNN architectures generalize better. Moreover, I propose a novel CNN that analyzes the face images at different scales. This multi-scale analysis allows the proposed CNN to generalize better compared to baseline CNNs. 2. I formulate the low cross-dataset performance of PAD as a domain shift problem and investigate domain adaptation methods as a solution. I propose a novel domain adaptation method based on the hypothesis that some learned filters in CNNs are domain",2020,,,10.5075/EPFL-THESIS-7635,http://publications.idiap.ch/downloads/papers/2020/Mohammadi_THESIS_2020.pdf
5d01486a097ed38d033338089dbd9be85d6c4d17,1,0,Domain Private and Agnostic Feature for Modality Adaptive Face Recognition,"Heterogeneous face recognition is a challenging task due to the large modality discrepancy and insufficient cross-modal samples. Most existing works focus on discriminative feature transformation, metric learning and cross-modal face synthesis. However, the fact that cross-modal faces are always coupled by domain (modality) and identity information has received little attention. Therefore, how to learn and utilize the domain-private feature and domain-agnostic feature for modality adaptive face recognition is the focus of this work. Specifically, this paper proposes a Feature Aggregation Network (FAN), which includes disentangled representation module (DRM), feature fusion module (FFM) and adaptive penalty metric (APM) learning session. First, in DRM, two subnetworks, i.e. domain-private network and domain-agnostic network are specially designed for learning modality features and identity features, respectively. Second, in FFM, the identity features are fused with domain features to achieve cross-modal bi-directional identity feature transformation, which, to a large extent, further disentangles the modality information and identity information. Third, considering that the distribution imbalance between easy and hard pairs exists in cross-modal datasets, which increases the risk of model bias, the identity preserving guided metric learning with adaptive hard pairs penalization is proposed in our FAN. The proposed APM also guarantees the cross-modality intra-class compactness and inter-class separation. Extensive experiments on benchmark cross-modal face datasets show that our FAN outperforms SOTA methods.",2020,,2008.03848,,https://arxiv.org/pdf/2008.03848.pdf
5d8d9dc887210b4487e99c95f5e9184036885d41,1,0,Recurrent Embedding Aggregation Network for Video Face Recognition,"Recurrent networks have been successful in analyzing temporal data and have been widely used for video analysis. However, for video face recognition, where the base CNNs trained on large-scale data already provide discriminative features, using Long Short-Term Memory (LSTM), a popular recurrent network, for feature learning could lead to overfitting and degrade the performance instead. We propose a Recurrent Embedding Aggregation Network (REAN) for set to set face recognition. Compared with LSTM, REAN is robust against overfitting because it only learns how to aggregate the pre-trained embeddings rather than learning representations from scratch. Compared with quality-aware aggregation methods, REAN can take advantage of the context information to circumvent the noise introduced by redundant video frames. Empirical results on three public domain video face recognition datasets, IJB-S, YTF, and PaSC show that the proposed REAN significantly outperforms naive CNN-LSTM structure and quality-aware aggregation methods.",2019,ArXiv,1904.12019,,https://arxiv.org/pdf/1904.12019.pdf
5d973cd1a383b6bed1bf7a42a7cef21524d87d29,0,1,AdvFaces: Adversarial Face Synthesis,"Face recognition systems have been shown to be vulnerable to adversarial examples resulting from adding small perturbations to probe images. Such adversarial images can lead state-of-the-art face recognition systems to falsely reject a genuine subject (obfuscation attack) or falsely match to an impostor (impersonation attack). Current approaches to crafting adversarial face images lack perceptual quality and take an unreasonable amount of time to generate them. We propose, AdvFaces, an automated adversarial face synthesis method that learns to generate minimal perturbations in the salient facial regions via Generative Adversarial Networks. Once AdvFaces is trained, it can automatically generate imperceptible perturbations that can evade state-of-the-art face matchers with attack success rates as high as 97.22% and 24.30% for obfuscation and impersonation attacks, respectively.",2019,ArXiv,1908.05008,,https://arxiv.org/pdf/1908.05008.pdf
5d9945b1c65dfba54de804533ce6671aa7bc8c5d,0,1,Benchmarking Image Retrieval for Visual Localization,"Visual localization, i.e., camera pose estimation in a known scene, is a core component of technologies such as autonomous driving and augmented reality. State-of-the-art localization approaches often rely on image retrieval techniques for one of two tasks: (1) provide an approximate pose estimate or (2) determine which parts of the scene are potentially visible in a given query image. It is common practice to use state-of-the-art image retrieval algorithms for these tasks. These algorithms are often trained for the goal of retrieving the same landmark under a large range of viewpoint changes. However, robustness to viewpoint changes is not necessarily desirable in the context of visual localization. This paper focuses on understanding the role of image retrieval for multiple visual localization tasks. We introduce a benchmark setup and compare state-of-the-art retrieval representations on multiple datasets. We show that retrieval performance on classical landmark retrieval/recognition tasks correlates only for some but not all tasks to localization performance. This indicates a need for retrieval approaches specifically designed for localization tasks. Our benchmark and evaluation protocols are available at https://github.com/ naver/kapture-localization.",2020,ArXiv,2011.11946,,https://arxiv.org/pdf/2011.11946.pdf
5e053fbe96616bcc71959de434b4319333f03be3,1,0,Task-oriented learning of structured probability distributions,"Machine learning models automatically learn from historical data to predict unseen events. Such events are often represented as complex multi-dimensional structures. In many cases there is high uncertainty in the prediction process. Research has developed probabilistic models to capture distributions of complex objects, but their learning objective is often agnostic of the evaluation loss. In this thesis, we address the aforementioned defficiency by designing probabilistic methods for structured object prediction that take into account the task at hand. First, we consider that the task at hand is explicitly known, but there is ambiguity in the prediction due to an unobserved (latent) variable. We develop a framework for latent structured output prediction that unifies existing empirical risk minimisation methods. We empirically demonstrate that for large and ambiguous latent spaces, performing prediction by minimising the uncertainty in the latent variable provides more accurate results. Empirical risk minimisation methods predict only a pointwise estimate of the output, however there can be uncertainty on the output value itself. To tackle this deficiency, we introduce a novel type of model to perform probabilistic structured output prediction. Our training objective minimises a dissimilarity coefficient between the data distribution and the model's distribution. This coefficient is defined according to a loss of choice, thereby our objective can be tailored to the task loss. We empirically demonstrate the ability of our model to capture distributions over complex objects. Finally, we tackle a setting where the task loss is implicitly expressed. Specifically, we consider the case of grouped observations. We propose a new model for learning a representation of the data that decomposes according to the semantics behind this grouping, while allowing efficient test-time inference. We experimentally demonstrate that our model learns a disentangled and controllable representation, leverages grouping information when available, and generalises to unseen observations.",2017,,,,https://pdfs.semanticscholar.org/5e05/3fbe96616bcc71959de434b4319333f03be3.pdf
5e0675694ccaed1beeae9607ab90935f412ba019,1,0,"Re-Training StyleGAN - A First Step Towards Building Large, Scalable Synthetic Facial Datasets","StyleGAN is a state-of-art generative adversarial network architecture that generates random 2D high-quality synthetic facial data samples. In this paper we recap the StyleGAN architecture and training methodology and present our experiences of retraining it on a number of alternative public datasets. Practical issues and challenges arising from the retraining process are discussed. Tests and validation results are presented and a comparative analysis of several different re-trained StyleGAN weightings is provided. The role of this tool in building large, scalable datasets of synthetic facial data is also discussed.",2020,2020 31st Irish Signals and Systems Conference (ISSC),2003.10847,10.1109/ISSC49989.2020.9180189,https://arxiv.org/pdf/2003.10847.pdf
5e127324c272c2dd3ac3f11f91da7716380eadf0,0,1,Template-Instance Loss for Offline Handwritten Chinese Character Recognition,"The long-standing challenges for offline handwritten Chinese character recognition (HCCR) are twofold: Chinese characters can be very diverse and complicated while similarly looking, and cursive handwriting (due to increased writing speed and infrequent pen lifting) makes strokes and even characters connected together in a flowing manner. In this paper, we propose the template and instance loss functions for the relevant machine learning tasks in offline handwritten Chinese character recognition. First, the character template is designed to deal with the intrinsic similarities among Chinese characters. Second, the instance loss can reduce category variance according to classification difficulty, giving a large penalty to the outlier instance of handwritten Chinese character. Trained with the new loss functions using our deep network architecture HCCR14Layer model consisting of simple layers, our extensive experiments show that it yields state-of-the-art performance and beyond for offline HCCR.",2019,2019 International Conference on Document Analysis and Recognition (ICDAR),1910.05545,10.1109/icdar.2019.00058,https://arxiv.org/pdf/1910.05545.pdf
5e2b070e4f401093b2a8a6eb330f0e545ee23bd5,1,0,GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection,"The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. In this study, we focus on the synthesis of entire facial images, which is a specific type of facial manipulation. The main contributions of this study are four-fold: i) a novel strategy to remove GAN “fingerprints” from synthetic fake images based on autoencoders is described, in order to spoof facial manipulation detection systems while keeping the visual quality of the resulting images; ii) an in-depth analysis of the recent literature in facial manipulation detection; iii) a complete experimental assessment of this type of facial manipulation, considering the state-of-the-art fake detection systems (based on holistic deep networks, steganalysis, and local artifacts), remarking how challenging is this task in unconstrained scenarios; and finally iv) we announce a novel public database, named iFakeFaceDB, yielding from the application of our proposed GAN-fingerprint Removal approach (GANprintR) to already very realistic synthetic fake images. The results obtained in our empirical evaluation show that additional efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques, such as the one proposed in this study.",2020,IEEE Journal of Selected Topics in Signal Processing,1911.05351,10.1109/JSTSP.2020.3007250,https://arxiv.org/pdf/1911.05351.pdf
5e3787abeffb6d540f692ab73b517841919743b1,1,0,CtrlFaceNet: Framework for geometric-driven face image synthesis,"Abstract In this work, we introduce a novel framework based on Generative Adversarial Networks to control the pose, expression and facial features of a given face image using another face image. It can then be used for data augmentation, pose invariant face identification, face verification, and lightweight image editing. Generating new realistic face images with controllable poses, facial features, and expressions is a challenging generative learning problem due to skin tone variations, the identity preservation problem, necessity to deal with unseen large poses, and the absence of ground truth images in the training process. We make the following contributions. First, we present a network, CtrlFaceNet that can control a source face image while preserving the identity and skin tone. Second, we introduce a method for training the framework in fully self-supervised mode using a large-scale dataset of unconstrained face images. Third, we show that the style loss function can be used to preserve the skin tone of the source image. The experimental results show that our approach outperforms all other baselines. Furthermore, to the best of our knowledge, we are the first to train such a model using large-scale dataset of unconstrained face images.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.08.026,
5e393f107578d250bec23c2c5d076b692001f128,0,1,Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment,"Human face reenactment aims at transferring motion patterns from one face (from a source-domain video) to an-other (in the target domain with the identity of interest).While recent works report impressive results, they are notable to handle multiple identities in a unified model. In this paper, we propose a unique network of CrossID-GAN to perform multi-ID face reenactment. Given a source-domain video with extracted facial landmarks and a target-domain image, our CrossID-GAN learns the identity-invariant motion patterns via the extracted landmarks and such information to produce the videos whose ID matches that of the target domain. Both supervised and unsupervised settings are proposed to train and guide our model during training.Our qualitative/quantitative results confirm the robustness and effectiveness of our model, with ablation studies confirming our network design.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00711,http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Learning_Identity-Invariant_Motion_Representations_for_Cross-ID_Face_Reenactment_CVPR_2020_paper.pdf
5e868741bc96920a7eddb2f2bed3cc22f9c2fffc,1,1,Rotate-and-Render: Unsupervised Photorealistic Face Rotation From Single-View Images,"Though face rotation has achieved rapid progress in recent years, the lack of high-quality paired training data remains a great hurdle for existing methods. The current generative models heavily rely on datasets with multi-view images of the same person. Thus, their generated results are restricted by the scale and domain of the data source. To overcome these challenges, we propose a novel unsupervised framework that can synthesize photo-realistic rotated faces using only single-view image collections in the wild. Our key insight is that rotating faces in the 3D space back and forth, and re-rendering them to the 2D plane can serve as a strong self-supervision. We leverage the recent advances in 3D face modeling and high-resolution GAN to constitute our building blocks. Since the 3D rotation-and-render on faces can be applied to arbitrary angles without losing details, our approach is extremely suitable for in-the-wild scenarios (i.e. no paired data are available), where existing methods fall short. Extensive experiments demonstrate that our approach has superior synthesis quality as well as identity preservation over the state-of-the-art methods, across a wide range of poses and domains. Furthermore, we validate that our rotate-and-render framework naturally can act as an effective data augmentation engine for boosting modern face recognition systems even on strong baseline models",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.08124,10.1109/cvpr42600.2020.00595,https://arxiv.org/pdf/2003.08124.pdf
5ed0b724be802bfef7d47e3afb58213f97d6b30e,0,1,Novel Analytical Models of Face Recognition Accuracy in Terms of Video Capturing and Encoding Parameters,"To fit the tight resource constraints, including network bandwidth, the video streams in Computer Vision systems are adapted dynamically by changing the video capturing and encoding parameters. We propose two novel analytical models that characterize the face recognition accuracy in terms of these parameters, specifically resolution, quantization, and actual bitrate. We find that the accuracy is a logistic function of the video quantization parameter, with the value of the Sigmoid’s midpoint being a function of the resolution. Alternatively, we find that the accuracy is equal to the sum of two exponentials of the actual video bitrate, with the resolution as a multiplicative factor with one exponential. We develop an evaluation framework to validate the models using two distinct video datasets with 99 videos and the widely used Labeled Faces in the Wild (LFW) dataset with 13, 233 images. We conduct 1, 668 experiments that involve varying combinations of encoding parameters. We show that both models hold true for the deep-learning and statistical-based face recognition. The developed models achieve an average coefficient of determination $(R^{2})$ of 98.7% to 99.8%.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/icme46284.2020.9102791,http://nabil.eng.wayne.edu/_resources/pdf/ICME2020_hayder.pdf
5f9eb88409bfc6e171792668fecc83d1d9e3c8cf,0,1,Practical No-box Adversarial Attacks against DNNs,"The study of adversarial vulnerabilities of deep neural networks (DNNs) has progressed rapidly. Existing attacks require either internal access (to the architecture, parameters, or training set of the victim model) or external access (to query the model). However, both the access may be infeasible or expensive in many scenarios. We investigate no-box adversarial examples, where the attacker can neither access the model information or the training set nor query the model. Instead, the attacker can only gather a small number of examples from the same problem domain as that of the victim model. Such a stronger threat model greatly expands the applicability of adversarial attacks. We propose three mechanisms for training with a very small dataset (on the order of tens of examples) and find that prototypical reconstruction is the most effective. Our experiments show that adversarial examples crafted on prototypical auto-encoding models transfer well to a variety of image classification and face verification models. On a commercial celebrity recognition system held by clarifai.com, our approach significantly diminishes the average prediction accuracy of the system to only 15.40%, which is on par with the attack that transfers adversarial examples from a pre-trained Arcface model. Our code is publicly available at: https://github.com/qizhangli/nobox-attacks.",2020,NeurIPS,2012.02525,,https://arxiv.org/pdf/2012.02525.pdf
5fe49a1aac79752199e6f33ecc62165f35a05915,1,0,Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Imbalanced Data,"We propose a novel unsupervised generative model, Elastic-InfoGAN, that learns to disentangle object identity from other low-level aspects in class-imbalanced datasets. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as the signal to learn the latent distribution's parameters. Experiments on both artificial (MNIST) and real-world (YouTube-Faces) datasets demonstrate the effectiveness of our approach in imbalanced data by: (i) better disentanglement of object identity as a latent factor of variation; and (ii) better approximation of class imbalance in the data, as reflected in the learned parameters of the latent distribution.",2019,ArXiv,1910.01112,,https://arxiv.org/pdf/1910.01112.pdf
5feff3090edb002d3a5d74700fc4415f2f00619d,1,0,When 3 D-Aided 2 D Face Recognition Meets Deep Learning : An extended UR 2 D for Pose-Invariant Face Recognition,"Most of the face recognition works focus on specific modules or demonstrate a research idea. This paper presents a pose-invariant 3D-aided 2D face recognition system (UR2D) that is robust to pose variations as large as 90◦ by leveraging deep learning technology. The architecture and the interface of UR2D are described, and each module is introduced in detail. Extensive experiments are conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms existing 2D face recognition systems such as VGG-Face, FaceNet, and a commercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset and 3% on the IJB-A dataset on average in face identification tasks. UR2D also achieves state-of-the-art performance of 85% on the IJB-A dataset by comparing the Rank-1 accuracy score from template matching. It fills a gap by providing a 3D-aided 2D face recognition system that has compatible results with 2D face recognition systems using deep learning techniques.",2017,,,,https://pdfs.semanticscholar.org/5fef/f3090edb002d3a5d74700fc4415f2f00619d.pdf
605dd24753c9cad5e45e1a9415583c9e6188b293,1,1,Margin based knowledge distillation for mobile face recognition,"With the rapid progress of face recognition it has more and more applications in everyday life. Although its backbone, very deep neural networks, also show improvement both in terms of accuracy and efficiency their computational cost and memory usage is still a limiting factor for deploying these models on a hardware with limited computational and power resources, such as mobile or embedded devices. Here arises the task of learning fast and compact deep neural networks which have a comparable accuracy to the complex model as requirement of real-life applications. Another issue is that sometimes face recognition system may run models of different complexity depending of the devices used for biometric template extraction (i.e. desktop with GPU or mobile phone), so the compatibility between the face descriptors is desirable. Our paper considers both this cases: we propose a new method for learning fast and compact face recognition model which has a similar performance to a much more complex model used for transferring its knowledge and we also show that both these models can be used for verification in a single face recognition system. To the best of our knowledge such evaluation of a compatibility between 2 different models for face recognition was never done before our work.",2020,International Conference on Machine Vision,,10.1117/12.2557244,
606d159ca979d352cecd9343faefa4ad5a2a7f24,1,1,Diving into Optimization of Topology in Neural Networks,"Seeking effective networks has become one of the most crucial and practical areas in deep learning. The architecture of a neural network can be represented as a directed acyclic graph, whose nodes denote the transformation of layers and edges represent information flow. Despite the selection of micro node operations, macro connections among the whole network, noted as topology, largely affect the optimization process. We first rethink the residual connections via a new topological view and observe the benefits provided by dense connections to the optimization. Motivated by this, we propose an novel method to optimize the topology of a neural network. The optimization space is defined as a complete graph, and through assigning learnable weights which reflect the importance of connections, the optimization of topology is transformed into learning a set of continuous variables of edges. To extend the optimization to larger search spaces, a new series of networks, called TopoNet, are designed. To further focus on critical edges and promote generalization ability in dense topologies, an auxiliary sparsity constraint is adopted to constrain the distribution of edges. Experiments on classical networks prove the effectiveness of the optimization of topology. Experiments with TopoNets further verify both availability and transferability of the proposed method in different tasks e.g. image classification, object detection, and face recognition.",2019,,,,https://pdfs.semanticscholar.org/606d/159ca979d352cecd9343faefa4ad5a2a7f24.pdf
60b992139e5cf9a1683ea640d43aa7b1f24bdf3c,1,0,Fairness in Biometrics: a figure of merit to assess biometric verification systems,"Machine learning-based (ML) systems are being largely deployed since the last decade in a myriad of scenarios impacting several instances in our daily lives. With this vast sort of applications, aspects of fairness start to rise in the spotlight due to the social impact that this can get in minorities. In this work aspects of fairness in biometrics are addressed. First, we introduce the first figure of merit that is able to evaluate and compare fairness aspects between multiple biometric verification systems, the so-called Fairness Discrepancy Rate (FDR). A use case with two synthetic biometric systems is introduced and demonstrates the potential of this figure of merit in extreme cases of fair and unfair behavior. Second, a use case using face biometrics is presented where several systems are evaluated compared with this new figure of merit using three public datasets exploring gender and race demographics.",2020,ArXiv,2011.02395,,https://arxiv.org/pdf/2011.02395.pdf
60c15cd8973872f79cd3bb16e9f0254beb800f2e,1,1,How Does Gender Balance In Training Data Affect Face Recognition Accuracy?,"Deep learning methods have greatly increased the accuracy of face recognition, but an old problem still persists: accuracy is usually higher for men than women. It is often speculated that lower accuracy for women is caused by under-representation in the training data. This work investigates female under-representation in the training data is truly the cause of lower accuracy for females on test data. Using a state-of-the-art deep CNN, three different loss functions, and two training datasets, we train each on seven subsets with different male/female ratios, totaling forty two trainings, that are tested on three different datasets. Results show that (1) gender balance in the training data does not translate into gender balance in the test accuracy, (2) the ""gender gap"" in test accuracy is not minimized by a gender-balanced training set, but by a training set with more male images than female images, and (3) training to minimize the accuracy gap does not result in highest female, male or average accuracy",2020,ArXiv,2002.02934,,https://arxiv.org/pdf/2002.02934.pdf
61352f9287a18fb61624fe1daf3d00a62d4974e8,1,1,RDCFace: Radial Distortion Correction for Face Recognition,"The effects of radial lens distortion often appear in wide-angle cameras of surveillance and safeguard systems, which may severely degrade performances of previous face recognition algorithms. Traditional methods for radial lens distortion correction usually employ line features in scenarios that are not suitable for face images. In this paper, we propose a distortion-invariant face recognition system called RDCFace, which directly and only utilize the distorted images of faces, to alleviate the effects of radial lens distortion. RDCFace is an end-to-end trainable cascade network, which can learn rectification and alignment parameters to achieve a better face recognition performance without requiring supervision of facial landmarks and distortion parameters. We design sequential spatial transformer layers to optimize the correction, alignment, and recognition modules jointly. The feasibility of our method comes from implicitly using the statistics of the layout of face features learned from the large-scale face data. Extensive experiments indicate that our method is distortion robust and gains significant improvements on LFW, YTF, CFP, and RadialFace, a real distorted face benchmark compared with state-of-the-art methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00774,https://pdfs.semanticscholar.org/6135/2f9287a18fb61624fe1daf3d00a62d4974e8.pdf
616a3bde185c5e304334903e4256417ebfe07a5a,1,0,Multi-task Deep Face Recognition,"In recent years, deep learning has become one of the most representative and effective techniques in face recognition. Due to the high expense of labelling data, it is costly to collect a large-scale face dataset with accurate label information. For the tasks without sufficient data, deep models cannot be well trained. Generally, parameters of deep models are usually initialized with a pre-trained model, and then fine-tuned on a small dataset of specific task. However, by straightforward fine-tuning, the final model usually does not generalize well. In this paper, we propose a multi-task deep learning (MTDL) method for face recognition. The superiority of the proposed multi-task method is demonstrated by experiments on LFW and CCFD.",2017,CCBR,,10.1007/978-3-319-69923-3_20,
618c9e7009b666ad11350bd646becdf828d0fdb5,1,0,Cyclic Generative Neural Networks for Improved Face Recognition in Nonstandard Domains,"A system of methods for improving the quality of face recognition from infrared images is described. For testing the recognition algorithm in a multidomain environment, a database of ordinary and infrared face images is collected. An algorithm based on cyclic generative neural networks is developed. This algorithm makes it possible to transform images from the color domain into the infrared domain, which significantly increases the size of the training sample. It is shown that fine-tuning the recognition algorithm using the generated infrared images improves the recognition result on the test sample.",2018,,,10.1134/S1064230718040093,
6195045b122e3bee1fc4af37a86ae1b98e550845,0,1,Recognition Oriented Iris Image Quality Assessment in the Feature Space,"A large portion of iris images captured in real world scenarios are poor quality due to the uncontrolled environment and the non-cooperative subject. To ensure that the recognition algorithm is not affected by low-quality images, traditional hand-crafted factors based methods discard most images, which will cause system timeout and disrupt user experience. In this paper, we propose a recognition-oriented quality metric and assessment method for iris image to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factors based iris quality assessment methods. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. Compared with hand-crafted factors based methods, the proposed method is a trial to bridge the gap between the image quality assessment and biometric recognition. The code is available at this https URL.",2020,ArXiv,2009.00294,,https://arxiv.org/pdf/2009.00294.pdf
61c0f341adc938531f40fc73ad08159c4d132f33,0,1,Multi-face Recognition,"When facing the challenge of multi-face recognition, the existing face recognition algorithms have the defects of low accuracy and too slow speed. This paper proposes a multi-face recognition algorithm based on the InsightFace algorithm, which is applied in the scene of large human flow throughput. First, an improved multi-task cascaded convolutional network (Multi-face-MTCNN) algorithm is proposed to accurately realize face detection and feature alignment in a multi-face environment. Moreover, we combine the MobilNetv3 structure and the GhostNet structure to propose a lightweight network structure MobileNetv3-GhostNet that is faster and can extract more facial features. The amount of parameters is greatly reduced, whose size is only 6.7Mb. This algorithm takes into account the accuracy and speed of the multi-face recognition, has good performance, and proposes new ideas for multi-face recognition.",2020,"2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",,10.1109/CISP-BMEI51763.2020.9263565,
6201cf3773a2ed565ae7f60271b1824536a245f4,1,1,Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces,"Margin-based deep face recognition methods (e.g. SphereFace, CosFace, and ArcFace) have achieved remarkable success in unconstrained face recognition. However, these methods are susceptible to the massive label noise in the training data and thus require laborious human effort to clean the datasets. In this paper, we relax the intra-class constraint of ArcFace to improve the robustness to label noise. More specifically, we design K sub-centers for each class and the training sample only needs to be close to any of the K positive subcenters instead of the only one positive center. The proposed sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Extensive experiments confirm the robustness of sub-center ArcFace under massive real-world noise. After the model achieves enough discriminative power, we directly drop non-dominant sub-centers and high-confident noisy samples, which helps recapture intra-compactness, decrease the influence from noise, and achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. By taking advantage of the large-scale raw web faces (Celeb500K), sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT.",2020,ECCV,,10.1007/978-3-030-58621-8_43,https://ibug.doc.ic.ac.uk/media/uploads/documents/eccv_1445.pdf
621db5ccdcdd9bc58fab2e256f74bb295bb568d3,0,1,Advancing Deep Learning for Automatic Autonomous Vision-based Power Line Inspection,"Electricity is fundamental to the ability to function of almost all modern-day societies. To maintain the reliability, availability, and sustainability of electricity supply, electric utilities are usually required to perform visual inspections on their electrical grids regularly. These inspections have been typically carried out using a combination of airborne surveys via low-flying helicopters and field surveys via foot patrol and tower climb. The primary purpose of these visual inspections is to plan for necessary repair or replacement works before any major damage that may lead to a power outage. These traditional inspection methods are not only slow and expensive but also potentially dangerous. In the past few years, numerous efforts have been made to automate these visual inspections. However, due to the high accuracy requirements of the task and its unique challenges, automatic vision-based inspection has not yet been widely adopted in this field. In this dissertation, we exploit recent advances in Deep Learning (DL), especially deep Convolutional Neural Networks (CNNs), and Unmanned Aerial Vehicle (UAV) technologies for facilitating automatic autonomous vision-based power line inspection. We propose a novel automatic autonomous vision-based power line inspection concept that uses UAV inspection as the main inspection method, optical images as the primary data source, and deep learning as the backbone of data analysis. Next, we conduct an extensive literature review on automatic vision-based power line inspection. Based on that, we identify the possibilities and six main challenges of DL vision-based UAV inspection: (i) the lack of training data; (ii) class imbalance; (iii) the detection of small power line components and defects; (iv) the detection of power lines in cluttered backgrounds; (v) the detection of previously unseen power line components and defects; and (vi) the lack of metrics for evaluating inspection performance. We address the first three challenges by creating four medium-sized datasets for training component detection and classification models, by applying a series of effective data augmentation techniques to balance out the imbalanced classes, and by utilizing multistage component detection and classification based on Single Shot multibox Detector (SDD) and deep Residual Networks (ResNets) to detect small power line components",2019,,,,
6220cac2e0fbebea5c28d9e8b7d399eb5c375357,0,1,GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction,"In the past few years, a lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the most recent works, differentiable renderers were employed in order to learn the relationship between the facial identity features and the parameters of a 3D morphable model for shape and texture. The texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction of the state-of-the-art methods is still not capable of modeling textures in high fidelity. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful generator of facial texture in UV space. Then, we revisit the original 3D Morphable Models (3DMMs) fitting approaches making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. We optimize the parameters with the supervision of pretrained deep identity features through our end-to-end differentiable framework. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1902.05978,10.1109/CVPR.2019.00125,https://eprints.mdx.ac.uk/26523/1/Kotsia_ganfit.pdf
623a401f8a3a772573436302b3f86a34c0f7b73d,0,1,Child Face Age-Progression via Deep Feature Aging,"Given a gallery of face images of missing children, state-of-the-art face recognition systems fall short in identifying a child (probe) recovered at a later age. We propose a feature aging module that can age-progress deep face features output by a face matcher. In addition, the feature aging module guides age-progression in the image space such that synthesized aged faces can be utilized to enhance longitudinal face recognition performance of any face matcher without requiring any explicit training. For time lapses larger than 10 years (the missing child is found after 10 or more years), the proposed age-progression module improves the closed-set identification accuracy of FaceNet from 16.53% to 21.44% and CosFace from 60.72% to 66.12% on a child celebrity dataset, namely ITWCC. The proposed method also outperforms state-of-the-art approaches with a rank-1 identification rate of 95.91%, compared to 94.91%, on a public aging dataset, FG-NET, and 99.58%, compared to 99.50%, on CACD-VS. These results suggest that aging face features enhances the ability to identify young children who are possible victims of child trafficking or abduction.",2020,ArXiv,2003.08788,,https://arxiv.org/pdf/2003.08788.pdf
628a3f027b7646f398c68a680add48c7969ab1d9,1,0,Plan for Final Year Project : HKU-Face : A Large Scale Dataset for Deep Face Recognition,"Face recognition has been one of the most successful techniques in the field of artificial intelligence because of its surpassing human-level performance in academic experiments and broad application in the industrial world. Gaussian-face[1] and Facenet[2] hold state-of-the-art record using statistical method and deep-learning method respectively. What’s more, face recognition has been applied in various areas like authority checking and recording, fostering a large number of start-ups like Face.",2017,,,,https://pdfs.semanticscholar.org/628a/3f027b7646f398c68a680add48c7969ab1d9.pdf
629d5af2878f79d4503a177e3bbc166a127f9e40,0,1,"Efficient Facial Representations for Age, Gender and Identity Recognition in Organizing Photo Albums using Multi-output CNN","This paper is focused on the automatic extraction of persons and their attributes (gender, year of born) from album of photos and videos. We propose the two-stage approach, in which, firstly, the convolutional neural network simultaneously predicts age/gender from all photos and additionally extracts facial representations suitable for face identification. We modified the MobileNet, which is preliminarily trained to perform face recognition, in order to additionally recognize age and gender. In the second stage of our approach, extracted faces are grouped using hierarchical agglomerative clustering techniques. The born year and gender of a person in each cluster are estimated using aggregation of predictions for individual photos. We experimentally demonstrated that our facial clustering quality is competitive with the state-of-the-art neural networks, though our implementation is much computationally cheaper. Moreover, our approach is characterized by more accurate video-based age/gender recognition when compared to the publicly available models.",2019,PeerJ Comput. Sci.,1807.07718,10.7717/peerj-cs.197,https://arxiv.org/pdf/1807.07718.pdf
62c777ea4d71197a0c6151d1961ca0fd9afca8e6,1,1,Applying a Novel Feature Set Fusion Technique to Facial Recognition,"An important use of facial recognition is the Take Me Home project. In this project, people with disabilities (PWD) are voluntarily registered so that law enforcement officers can identify them and bring them home safely when they are lost. In an application like Take me Home, optimization of person recognition is of prime importance. While facial recognition models have seen huge performance gains in recent years through improvements to the training process, we show that accuracy can be improved by combining models trained for different recognition objectives. Specifically, we find that the accuracy of facial recognition model is higher when its output is fused with the output of model trained to recognize specific attributes such as hair color, age, lighting, and picture quality. The fusion is performed with a linear regression that can be applied to countless other machine learning tasks. The main contribution of our methodology is the mathematical formulation and a neural network using the Inception Net architecture that enables the recognition of the person using up to 40 attributes. In addition, we designed a framework that uses a joint linear regression scheme to combine the facial feature vectors produced by the facial recognition module and the attribute vectors produced by the attribute recognition module. The result is an efficient solution in which a lost person is more accurately identified by police officers even under unideal conditions.",2019,2019 2nd International Conference on Data Intelligence and Security (ICDIS),,10.1109/ICDIS.2019.00019,
62ce48ff409cd4d334dac639d04191531e6a25ff,0,1,Robust Online Multi-target Visual Tracking using a HISP Filter with Discriminative Deep Appearance Learning,"We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like multiple hypothesis tracking (MHT) and point-process-based approaches like probability hypothesis density (PHD) filter, and it has a linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep convolutional neural networks (CNN) appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal (motion) information that can improve the tracker's performance. In addition, we solve the problem of two or more targets having identical label taking into account the weight propagated with each confirmed hypothesis. Finally, we carry out extensive experiments on Multiple Object Tracking 2016 (MOT16) and 2017 (MOT17) benchmark data sets and find out that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy.",2019,ArXiv,1908.03945,,https://arxiv.org/pdf/1908.03945.pdf
64028cca88d1124e9463bb57b557d1285255190d,1,0,A Semi-Automatic Method of Collecting Samples for Learning a Face Identification Algorithm,"A method for the semi-automatic collection of samples for learning face identification algorithms is proposed. In the experimental evaluation, the operation of the face identification algorithm on ethnically diverse data is considered. The algorithm operation is also evaluated on the data with a wide variation of ages. The proposed method makes it possible to expand the training sample by indexing new data.",2019,Programming and Computer Software,,10.1134/S0361768819030022,
6468a271a94633221dc2b4c896f3be365c11ad2b,0,1,Face Verification using Convolutional Neural Networks,"You will train your model on a dataset with a few thousand images of labelled ID’s (i.e., a set of images, each labeled by an ID that uniquely identifies the person). You will learn more about embeddings (in this case, embeddings for face information), several loss functions, and, of course, convolutional layers as effective shift-invariant feature extractors. You will also develop skills necessary for processing and training neural networks with big data, which is often the scale at which deep neural networks demonstrate excellent performance in practice.",2020,,,,http://deeplearning.cs.cmu.edu/F20/document/homework/Homework_2_2.pdf
648aa7a1cd861ff42d1d5e8450f52bb91127e6f4,0,1,FoggySight: A Scheme for Facial Lookup Privacy,"Advances in deep learning algorithms have enabled better-than-human performance on face recognition tasks. In parallel, private companies have been scraping social media and other public websites that tie photos to identities and have built up large databases of labeled face images. Searches in these databases are now being offered as a service to law enforcement and others and carry a multitude of privacy risks for social media users. In this work, we tackle the problem of providing privacy from such face recognition systems. We propose and evaluate FoggySight, a solution that applies lessons learned from the adversarial examples literature to modify facial photos in a privacy-preserving manner before they are uploaded to social media. FoggySight’s core feature is a community protection strategy where users acting as protectors of privacy for others upload decoy photos generated by adversarial machine learning algorithms. We explore different settings for this scheme and find that it does enable protection of facial privacy – including against a facial recognition service with unknown internals.",2020,,2012.08588,,https://arxiv.org/pdf/2012.08588.pdf
64993fd5e6fead52722fc6189ab1184482c12a44,0,1,Person Re-Identification Using Additive Distance Constraint With Similar Labels Loss,"Despite the promising progress made in recent years, person re-identification (Re-ID) remains a challenging task due to the intra-class variations. Most of the current studies used the traditional Softmax loss for solutions, but its discriminative capability encounters a bottleneck. Therefore, how to improve person Re-ID performance is still a challenging task. To address this problem, we proposed a novel loss function, namely additive distance constraint with similar labels loss (ADCSLL). Specifically, we reformulated the Softmax loss by adding a distance constraint to the ground truth label, based on which similar labels were introduced to enhance the learned features to be much more stable and centralized. Experimental evaluations were conducted on two popular datasets (Market-1501 and DukeMTMC-reID) to examine the effectiveness of our proposed method. The results showed that our proposed ADCSLL was more discriminative than most of the other compared state-of-the-art methods. The rank-1 accuracy and the mAP on Market-1501 were 95.0% and 87.0%, respectively. The numbers were 88.6% and 77.2% on DukeMTMC-reID, respectively.",2020,IEEE Access,,10.1109/ACCESS.2020.3023948,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195852.pdf
64cec1a426296a28f789eaccff9512125b1f5aac,1,0,Generating Master Faces for Use in Performing Wolf Attacks on Face Recognition Systems,"Due to its convenience, biometric authentication, especial face authentication, has become increasingly mainstream and thus is now a prime target for attackers. Presentation attacks and face morphing are typical types of attack. Previous research has shown that finger-vein- and fingerprint-based authentication methods are susceptible to wolf attacks, in which a wolf sample matches many enrolled user templates. In this work, we demonstrated that wolf (generic) faces, which we call ""master faces,"" can also compromise face recognition systems and that the master face concept can be generalized in some cases. Motivated by recent similar work in the fingerprint domain, we generated high-quality master faces by using the state-of-the-art face generator StyleGAN in a process called latent variable evolution. Experiments demonstrated that even attackers with limited resources using only pre-trained models available on the Internet can initiate master face attacks. The results, in addition to demonstrating performance from the attacker's point of view, can also be used to clarify and improve the performance of face recognition systems and harden face authentication systems.",2020,ArXiv,2006.08376,,https://arxiv.org/pdf/2006.08376.pdf
65251e9715c740fb5f1a86a98615867e9eb634af,0,1,Video Based Face Recognition by Using Discriminatively Learned Convex Models,"A majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual’s face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generatively learned convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on six of the eight tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX, IJB-C and ESOGU video datasets.",2020,International Journal of Computer Vision,,10.1007/s11263-020-01356-5,
65295fe3f34afc21d07fda5a9a349041d56f0a9c,0,1,A Survey On Anti-Spoofing Methods For Face Recognition with RGB Cameras of Generic Consumer Devices,"The widespread deployment of face recognition-based biometric systems has made face Presentation Attack Detection (face anti-spoofing) an increasingly critical issue. This survey thoroughly investigates the face Presentation Attack Detection (PAD) methods, that only require RGB cameras of generic consumer devices, over the past two decades. We present an attack scenario-oriented typology of the existing face PAD methods and we provide a review of over 50 of the most recent face PAD methods and their related issues. We adopt a comprehensive presentation of the methods that have most influenced face PAD following the proposed typology, and in chronological order. By doing so, we depict the main challenges, evolutions and current trends in the field of face PAD, and provide insights on its future research. From an experimental point of view, this survey paper provides a summarized overview of the available public databases and extensive comparative experimental results of different PAD methods.",2020,ArXiv,2010.04145,10.3390/jimaging6120139,https://arxiv.org/pdf/2010.04145.pdf
6556b3a45c7627586fc1e2d3db449ce742648993,0,1,Investigation of Large-Margin Softmax in Neural Language Modeling,"To encourage intra-class compactness and inter-class separability among trainable feature vectors, large-margin softmax methods are developed and widely applied in the face recognition community. The introduction of the large-margin concept into the softmax is reported to have good properties such as enhanced discriminative power, less overfitting and well-defined geometric intuitions. Nowadays, language modeling is commonly approached with neural networks using softmax and cross entropy. In this work, we are curious to see if introducing large-margins to neural language models would improve the perplexity and consequently word error rate in automatic speech recognition. Specifically, we first implement and test various types of conventional margins following the previous works in face recognition. To address the distribution of natural language data, we then compare different strategies for word vector norm-scaling. After that, we apply the best norm-scaling setup in combination with various margins and conduct neural language models rescoring experiments in automatic speech recognition. We find that although perplexity is slightly deteriorated, neural language models with large-margin softmax can yield word error rate similar to that of the standard softmax baseline. Finally, expected margins are analyzed through visualization of word vectors, showing that the syntactic and semantic relationships are also preserved.",2020,INTERSPEECH,2005.10089,10.21437/interspeech.2020-1849,https://arxiv.org/pdf/2005.10089.pdf
659815bc880a2fa6d617cf1545ab5f0bcbcb5eda,1,0,Defending Black Box Facial Recognition Classifiers Against Adversarial Attacks,"Defending adversarial attacks is a critical step towards reliable deployment of deep learning empowered solutions for biometrics verification. Current approaches for defending Black box models use the classification accuracy of the Black box as a performance metric for validating their defense. However, classification accuracy by itself is not a reliable metric to determine if the resulting image is ""adversarial-free"". This is a serious problem for online biometrics verification applications where the ground-truth of the incoming image is not known and hence we cannot compute the accuracy of the classifier or know if the image is ""adversarial-free"" or not. This paper proposes a novel framework for defending Black box systems from adversarial attacks using an ensemble of iterative adversarial image purifiers whose performance is continuously validated in a loop using Bayesian uncertainties. The proposed approach is (i) model agnostic, (ii) can convert single step black box defenses into an iterative defense and (iii) has the ability to reject adversarial examples. This paper uses facial recognition as a test case for validating the defense and experimental results on the MS-Celeb dataset show that the proposed approach can consistently detect adversarial examples and purify/reject them against a variety of adversarial attacks with different ranges of perturbations.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00414,https://www.vislab.ucr.edu/PUBLICATIONS/pubs/Journal%20and%20Conference%20Papers/after10-1-1997/Conference/2020/defending%20black%20box%20facial.pdf
65984ea40c3b17bb8965c215b61972cd660f61a7,1,0,Doppelganger Mining for Face Representation Learning,"In this paper we present Doppelganger mining - a method to learn better face representations. The main idea of this method is to maintain a list with the most similar identities for each identity in the training set. This list is used to generate better mini-batches by sampling pairs of similar-looking identities (""doppelgangers"") together. It is especially useful for methods, based on exemplar-based supervision. Usually hard example mining comes with a price of necessity to use large mini-batches or substantial extra computation and memory cost, particularly for datasets with large numbers of identities. Our method needs only a negligible extra computation and memory. In our experiments on a benchmark dataset with 21,000 persons we show that Doppelganger mining, being inserted in the face representation learning process with joint prototype-based and exemplar-based supervision, significantly improves the discriminative power of learned face representations.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.226,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Smirnov_Doppelganger_Mining_for_ICCV_2017_paper.pdf
65e62791fc8df7d578991937533e41d5c4dc5263,1,1,Disguised Faces in the Wild 2019,"Disguised face recognition has wide-spread applicability in scenarios such as law enforcement, surveillance, and access control. Disguise accessories such as sunglasses, masks, scarves, or make-up modify or occlude different facial regions which makes face recognition a challenging task. In order to understand and benchmark the state-of-the-art on face recognition in the presence of disguise variations, the Disguised Faces in the Wild 2019 (DFW2019) competition has been organized. This paper summarizes the outcome of the competition in terms of the dataset used for evaluation, a brief review of the algorithms employed by the participants for this task, and the results obtained. The DFW2019 dataset has been released with four evaluation protocols and baseline results obtained from two deep learning-based state-of-the-art face recognition models. The DFW2019 dataset has also been analyzed with respect to degrees of difficulty: (i) easy, (ii) medium, and (iii) hard. The dataset has been released as part of the International Workshop on Disguised Faces in the Wild at International Conference on Computer Vision (ICCV), 2019.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00067,http://iab-rubric.org/papers/2019_ICCVW_DFW2019.pdf
6600cd7c59199589ce72785908934a417a80f9cb,0,1,Jhu-HLTCOE System for the Voxsrc Speaker Recognition Challenge,"The VoxSRC speaker recognition challenge comprises data obtained from YouTube videos of celebrity interviews in a wide range of recording environments. The challenge provides FIXED and OPEN training conditions to allow cross-system comparisons and to characterize the effects of additional amounts of training data on system performance. This paper describes our submission to this challenge where we have explored x-vector extractor topologies, classification head alternatives, data augmentation, and angular margin penalty. Our final entry to the FIXED condition (which achieved 2nd place) is the score average of 4 diverse systems. We find that this system outperforms a large single DNN with similar number of parameters.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053209,
661ee1178a17631313774023a5b0c9ea7d918474,0,1,Reverse-auction-based crowdsourced labeling for active learning,"In the past few years, Machine Learning (ML) has aroused great interest in both academic and industrial societies. ML is booming because of its huge application potential in many areas, such as facial recognition, natural language processing, self-driving car, and so on. Nevertheless, one of the key problems is the scarcity of labeled data. Fortunately, mobile crowdsourcing makes it possible to recruit mobile workers to label large-scale data by offering them small payments. In this paper, we use crowdsourcing to tackle the scarcity of training data in active learning, and then propose an approximately truthful, individually rational, privacy-preserving incentive mechanism with a guaranteed approximate performance , based on the single-minded reverse auction for data labeling in active learning. Different from prior works, we take crowd workers’ reliability into consideration when selecting data to be labeled which can improve the labeling quality and the model performance. In addition, we employ differential privacy to preserve workers’ bid privacy because a worker’s bid usually contains sensitive information. The simulation results demonstrate that the learning model is much accurate compared with the traditional active learning without the consideration of reliability in the case of the same number of iterations.",2019,World Wide Web,,10.1007/s11280-019-00744-3,
666657f92b0968a2967154a0f86327c9ef55fff0,0,1,Cross-resolution face recognition adversarial attacks,"Abstract Face Recognition is among the best examples of computer vision problems where the supremacy of deep learning techniques compared to standard ones is undeniable. Unfortunately, it has been shown that they are vulnerable to adversarial examples - input images to which a human imperceptible perturbation is added to lead a learning model to output a wrong prediction. Moreover, in applications such as biometric systems and forensics, cross-resolution scenarios are easily met with a non-negligible impact on the recognition performance and adversary’s success. Despite the existence of such vulnerabilities set a harsh limit to the spread of deep learning-based face recognition systems to real-world applications, a comprehensive analysis of their behavior when threatened in a cross-resolution setting is missing in the literature. In this context, we posit our study, where we harness several of the strongest adversarial attacks against deep learning-based face recognition systems considering the cross-resolution domain. To craft adversarial instances, we exploit attacks based on three different metrics, i.e., L1, L2, and L∞, and we study the resilience of the models across resolutions. We then evaluate the performance of the systems against the face identification protocol, open- and close-set. In our study, we find that the deep representation attacks represents a much dangerous menace to a face recognition system than the ones based on the classification output independently from the used metric. Furthermore, we notice that the input image’s resolution has a non-negligible impact on an adversary’s success in deceiving a learning model. Finally, by comparing the performance of the threatened networks under analysis, we show how they can benefit from a cross-resolution training approach in terms of resilience to adversarial attacks.",2020,,,10.1016/j.patrec.2020.10.008,
66678231dc26afc999dd6a8e2694412db34354c1,0,1,Gicoface: Global Information-Based Cosine Optimal Loss for Deep Face Recognition,"Loss function plays an important role in CNNs. However, the recent loss functions either do not apply weight and feature normalisation or do not explicitly follow the two targets of improving discriminative ability: minimising intra-class variance and maximising inter-class variance. Besides, all of them consider only the feedback information from the current mini-batch instead of the distribution information from the whole training set. In this paper, we propose a novel loss function – Global Information-based Cosine Optimal loss (Gico loss). Gico loss is applied with weight and feature normalisation, designed explicitly following the aforementioned two targets of improving discriminative ability, and is guided by the distribution information from the whole training set. Extensive experiments are conducted on multiple public datasets, which confirms the effectiveness of the proposed Gico loss and shows that we achieve state-of-the-art performance.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803410,
66cc9155885feda88a961ad8185710ee3c9a61bf,1,1,Factorizing and Reconstituting Large-Kernel MBConv for Lightweight Face Recognition,"In the past few years, Neural Architecture Search (NAS) has exhibited remarkable advances in terms of neural architecture design, especially on mobile devices. NAS normally use hand-craft MBConv as building block. However, they mainly searched for block-related hyperparameters, and the structure of MBConv itself was largely overlooked. This paper investigates that factorization and reconstitution can promote the efficiency of large-kernel MBConv and thus proposes FR-MBConv (Factorizing and Reconstituting large-kernel MBConv). Compared to large-kernel MBConv with the same receptive field, our FR-MBConv has fewer number of parameters and less computational cost, dramatically increased depth and nonlinearity. In addition, from the perspective of feature generation mechanism, FR-MBConv can be equivalent to more regular convolutions. We combine FR-MBConv with MobileNetV3 to build a lightweight face recognition model. Extensive experiments on face recognition benchmark demonstrate that our lightweight face recognition model outperforms the state-of-the-art lightweight model. Even on large scale face recognition benchmark IJB-B, IJB-C and MegaFace, our lightweight model also achieves comparable performance with large models.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00329,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Lyu_Factorizing_and_Reconstituting_Large-Kernel_MBConv_for_Lightweight_Face_Recognition_ICCVW_2019_paper.pdf
674cb344fb6172282f0e19f2458202e46f671ac8,0,1,Shape My Face: Registering 3D Face Scans by Surface-to-Surface Translation,"Existing surface registration methods focus on fitting in-sample data with little to no generalization ability, and require both heavy pre-processing and careful hand-tuning. In this paper, we cast the registration task as a surface-tosurface translation problem, and design a model to reliably capture the latent geometric information directly from raw 3D face scans. We introduce Shape-My-Face (SMF), a powerful encoder-decoder architecture based on an improved point cloud encoder, a novel visual attention mechanism, graph convolutional decoders with skip connections, and a specialized mouth model that we smoothly integrate with the mesh convolutions. Compared to the previous state-of-the-art learning algorithms for non-rigid registration of face scans, SMF only requires the raw data to be rigidly aligned (with scaling) with a pre-defined face template. Additionally, our model provides topologically-sound meshes with minimal supervision, offers faster training time, has orders of magnitude fewer trainable parameters, is more robust to noise, and can generalize to previously unseen datasets. We extensively evaluate the quality of our registrations on diverse data. We demonstrate the robustness and generalizability of our model with in-the-wild face scans across different modalities, sensor types, and resolutions. Finally, we show that, by learning to M. Bahri, E. O’ Sullivan, S. Gong, M. M. Bronstein, and S. Zafeiriou are with Department of Computing Imperial College London, London, UK E-mail: {m.bahri, e.o-sullivan16, shunwang.gong16, m.bronstein, s.zafeiriou}@imperial.ac.uk M. M. Bronstein is also with Twitter, UK F. Liu and X. Liu are with Department of Computer Science and Engineering Michigan State University, East Lansing, MI, USA E-mail: isliuf1990@gmail.com, liuxm@cse.msu.edu Corresponding author: M. Bahri (ORCID 0000-0002-2409-0261) register scans, SMF produces a hybrid linear and non-linear morphable model that can be used for generation, shape morphing, and expression transfer through manipulation of the latent space, including in-the-wild. We train SMF on a dataset of human faces comprising 9 large-scale databases on commodity hardware.",2020,,2012.09235,,https://arxiv.org/pdf/2012.09235.pdf
6765fdfc5fbe6ac2a7043b94774ff801a0817bdf,0,1,An Online Face Clustering Algorithm for Face Monitoring and Retrieval in Real-Time Videos,"Facial recognition in real-time videos will output a feature vector for each face tracking process, namely Euclidean embedding. The task of face retrieval is to find similar faces from the historical face database. Clustering historical face Euclidean embedding can improve retrieval performance. The data collected by the facial recognition system from real-time video is not all available. Since the traditional clustering algorithms operate under the condition that all face embedding data is available, it is categorized as an offline clustering method and offline clustering methods are not suitable for real-time scenarios. Based on real-time video surveillance scenarios, we propose an online clustering algorithm which can cluster the face embedding and achieve an accuracy of 84% on the benchmark video dataset. Our algorithm achieves better results than state-of-the-art online methods, and it is also comparable and even superior to the offline clustering algorithms.",2019,"2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",,10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00122,
679a837da9ae5b43fb8213c473e2c4576b855a3b,0,1,Training Speaker Enrollment Models by Network Optimization,"In this paper, we present a new approach for the enrollment process in a deep neural network (DNN) system which learns the speaker model by an optimization process. Most Speaker Verification (SV) systems extract representations for both the enrollment and test utterances called embeddings, and then, these systems usually apply a similarity metric or complex back-ends to carry out the verification process. Unlike previous works, we propose to take advantage of the knowledge acquired by a DNN to model the speakers from the training set since the last layer of the DNN can be seen as an embedding dictionary which represents train speakers. Thus, after the initial training phase, we introduce a new learnable vector for each enrollment speaker. Furthermore, to lead this training process, we employ a loss function more appropriate for verification, the approximated Detection Cost Function (aDCF ) loss function. The new strategy to produce enrollment models for each target speaker was tested on the RSR-Part II database for text-dependent speaker verification, where the proposed approach outperforms the reference system based on directly averaging of the embeddings extracted from the enroll data using the network and the application of cosine similarity.",2020,INTERSPEECH,,10.21437/interspeech.2020-2325,https://isca-speech.org/archive/Interspeech_2020/pdfs/2325.pdf
67a9659de0bf671fafccd7f39b7587f85fb6dfbd,1,0,Ring Loss: Convex Feature Normalization for Face Recognition,"We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.0013,10.1109/CVPR.2018.00534,https://arxiv.org/pdf/1803.00130.pdf
67c6afafc43f7fda5e2aea7db77e86aa95955517,0,1,FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping,"In this work, we propose a novel two-stage framework, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, our framework, in its first stage, generates the swapped face in high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. We propose a novel attributes encoder for extracting multi-level target face attributes, and a new generator with carefully designed Adaptive Attentional Denormalization (AAD) layers to adaptively integrate the identity and the attributes for face synthesis. To address the challenging facial occlusions, we append a second stage consisting of a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is trained to recover anomaly regions in a self-supervised way without any manual annotations. Extensive experiments on wild faces demonstrate that our face swapping results are not only considerably more perceptually appealing, but also better identity preserving in comparison to other state-of-the-art methods.",2019,ArXiv,1912.13457,,https://arxiv.org/pdf/1912.13457.pdf
6834b6a529c969e5feb1fb77713eff8f19704b31,1,1,Linkage Based Face Clustering via Graph Convolution Network,"In this paper, we present an accurate and scalable approach to the face clustering task. We aim at grouping a set of faces by their potential identities. We formulate this task as a link prediction problem: a link exists between two faces if they are of the same identity. The key idea is that we find the local context in the feature space around an instance (face) contains rich information about the linkage relationship between this instance and its neighbors. By constructing sub-graphs around each instance as input data, which depict the local context, we utilize the graph convolution network (GCN) to perform reasoning and infer the likelihood of linkage between pairs in the sub-graphs. Experiments show that our method is more robust to the complex distribution of faces than conventional methods, yielding favorably comparable results to state-of-the-art methods on standard face clustering benchmarks, and is scalable to large datasets. Furthermore, we show that the proposed method does not need the number of clusters as prior, is aware of noises and outliers, and can be extended to a multi-view version for more accurate clustering accuracy.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.11306,10.1109/CVPR.2019.00121,https://arxiv.org/pdf/1903.11306.pdf
6891c6efdeeb1eff4456f5a507223e804a164bd7,0,1,An efficient global representation constrained by Angular Triplet loss for vehicle re-identification,"Vehicle re-identification is becoming an increasingly important problem in modern intelligent transportation systems. Substantial results have been achieved with methods based on deep metric learning. Most of the previous works tend to design complicated neural network models or utilize extra information. In this work, we introduce a simple Angular Triplet loss on the basis of analysis of different feature representations constrained by softmax loss and triplet loss. A batch normalization layer with zero bias is adopted to pass through the embedded feature before loss calculation. Then, triplet loss is calculated in cosine metric space instead of Euclidean space. In this way, triplet loss can cooperate with softmax consistently. By unifying the metric space of these two types of losses, the proposed method achieves 77.3% and 95.9% in rank-1 on VehicleID and VeRi-776 datasets, respectively. With only global features utilized, the proposed model can be seen as an effective baseline for vehicle re-identification task.",2020,Pattern Analysis and Applications,,10.1007/s10044-020-00900-w,
68cf5f416518617d7050cce9c9175db36b5b629e,1,0,COSONet: Compact Second-Order Network for Video Face Recognition,"In this paper, we study the task of video face recognition. The face images in the video typically cover large variations in expression, lighting, or pose, and also suffer from video-type noises such as motion blur, out-of-focus blur and low resolution. To tackle these two types of challenges, we propose an extensive framework which contains three aspects: neural network design, training data augmentation, and loss function. First, we devise an expressive COmpact Second-Order network (COSONet) to extract features from faces with large variations. The network manages to encode the correlation (e.g. sample covariance matrix) of local features in a spatial invariant way, which is useful to model the global texture and appearance of face images. To further handle the curse of high-dimensional problem in the sample covariance matrix, we apply a layer named 2D fully connected (2D-FC) layer with few parameters to reduce the dimension. Second, due to no video-type noises in still face datasets and small inter-frame variation in video face datasets, we augment a large dataset with both large face variations and video-type noises from existing still face dataset. Finally, to get a discriminative face descriptor while balancing the effect of images with various quality, a mixture loss function which encourages the discriminability and simultaneously regularizes the feature is elaborately designed. Detailed experiments show that the proposed framework can achieve very competitive accuracy over state-of-the-art approaches on IJB-A and PaSC datasets.",2018,ACCV,,10.1007/978-3-030-20893-6_4,http://vipl.ict.ac.cn/homepage/rpwang/publications/COSONet_Compact%20Second-Order%20Network%20for%20Video%20Face%20Recognition_ACCV2018.pdf
69adf2f122ff18848ff85e8de3ee3b2bc495838e,1,0,Arbitrary Facial Attribute Editing: Only Change What You Want,"Facial attribute editing aims to modify either single or multiple attributes on a face image. Since it is practically infeasible to collect images with arbitrarily specified attributes for each person, the generative adversarial net (GAN) and the encoder-decoder architecture are usually incorporated to handle this task. With the encoder-decoder architecture, arbitrary attribute editing can then be conducted by decoding the latent representation of the face image conditioned on the specified attributes. A few existing methods attempt to establish attribute-independent latent representation for arbitrarily changing the attributes. However, since the attributes portray the characteristics of the face image, the attribute-independent constraint on the latent representation is excessive. Such constraint may result in information loss and unexpected distortion on the generated images (e.g. over-smoothing), especially for those identifiable attributes such as gender, race etc. Instead of imposing the attribute-independent constraint on the latent representation, we introduce an attribute classification constraint on the generated image, just requiring the correct change of the attributes. Meanwhile, reconstruction learning is introduced in order to guarantee the preservation of all other attribute-excluding details on the generated image, and adversarial learning is employed for visually realistic generation. Moreover, our method can be naturally extended to attribute intensity manipulation. Experiments on the CelebA dataset show that our method outperforms the state-of-the-arts on generating realistic attribute editing results with facial details well preserved.",2017,ArXiv,,,
69af7e9c522e327c835917bc9f03756cb8f1989a,0,1,ChildFace: Gender Aware Child Face Aging,"Child face aging and rejuvenation has amassed considerable active research interest due to its immense impact on monitoring applications especially for finding lost/abducted children with childhood photos and hence protect children. Prior studies are primarily motivated to enhance the generation quality and aging of face images, rather than quantifying face recognition performance. To address this challenge we propose ChildFace model. Our model does child face aging and rejuvenation while using gender as condition. Our model uses Conditional Generative Adversarial Nets (cGANs), VGG19 based perceptual loss and LightCNN29 age classifier and produces impressive results. Intense quantitative study based on verification, identification and age estimation proves that our model is competent to existing state-of-art models and can make a significant contribution in identifying missing children.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
69edac26cf26c09420a3c96b3d98cabcc699696d,0,1,EqCo: Equivalent Rules for Self-supervised Contrastive Learning,"In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the infomax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that for the first time, we can perform self-supervised contrastive training using only a few negative pairs (e.g.smaller than 256 per query) on large-scale vision tasks like ImageNet, while with little accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.",2020,ArXiv,2010.01929,,https://arxiv.org/pdf/2010.01929.pdf
6a133eb625cee083db848cc2f51a5cb54e28f67e,1,1,Noise-Tolerant Paradigm for Training Face Recognition CNNs,"Benefit from large-scale training datasets, deep Convolutional Neural Networks(CNNs) have achieved impressive results in face recognition(FR). However, tremendous scale of datasets inevitably lead to noisy data, which obviously reduce the performance of the trained CNN models. Kicking out wrong labels from large-scale FR datasets is still very expensive, although some cleaning approaches are proposed. According to the analysis of the whole process of training CNN models supervised by angular margin based loss(AM-Loss) functions, we find that the distribution of training samples implicitly reflects their probability of being clean. Thus, we propose a novel training paradigm that employs the idea of weighting samples based on the above probability. Without any prior knowledge of noise, we can train high performance CNN models with largescale FR datasets. Experiments demonstrate the effectiveness of our training paradigm. The codes are available at https://github.com/huangyangyu/NoiseFace.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.10357,10.1109/CVPR.2019.01216,https://arxiv.org/pdf/1903.10357.pdf
6a4c1acce95f7a74e39808099ccf6c32eea9d91a,0,1,Domain-adversarial training of multi-speaker TTS,"Multi-speaker TTS has to learn both linguistic embedding and text embedding to generate speech of desired linguistic content in desired voice. However, it is unclear which characteristic of speech results from speaker and which part from linguistic content. In this paper, text embedding is forced to unlearn speaker dependent characteristic using gradient reversal layer to auxiliary speaker classifier that we introduce. We train a speaker classifier using angular margin softmax loss. In subjective evaluation, it is shown that the adversarial training of text embedding for unilingual multi-speaker TTS results in 39.9% improvement on similarity MOS and 40.1% improvement on naturalness MOS.",2020,,2006.06942,,https://arxiv.org/pdf/2006.06942.pdf
6a6535dda813b9b72dfe82ca652c8eacdf6eb196,0,1,A Technical Report for VIPriors Image Classification Challenge,"Image classification has always been a hot and challenging task. This paper is a brief report to our submission to the VIPriors Image Classification Challenge. In this challenge, the difficulty is how to train the model from scratch without any pretrained weight. In our method, several strong backbones and multiple loss functions are used to learn more representative features. To improve the models' generalization and robustness, efficient image augmentation strategies are utilized, like autoaugment and cutmix. Finally, ensemble learning is used to increase the performance of the models. The final Top-1 accuracy of our team DeepBlueAI is 0.7015, ranking second in the leaderboard.",2020,ArXiv,2007.08722,,https://arxiv.org/pdf/2007.08722.pdf
6a85e5b62499dc26f848009c0e980182fa92aab7,1,0,"Identity, Gender, and Age Recognition Convergence System for Robot Environments","This paper proposes a new dentity, gender, and age recognition convergence system for robot environments. In a robot environment, it is difficult to apply deep learning based methods because of various limitations. To overcome the limitations, we propose a shallow deep-learning fusion model that can calculate identity, gender, and age at once, and a technique for improving recognition performance. Using convergence network, we can obtain three pieces of information from a single input through a single operation. In addition, we propose a 2D / 3D augmentation method to generate virtual additional datasets for learning data. The proposed method has a smaller model size and faster computation time than existing methods and uses a very small number of parameters. Through the proposed method, we finally achieved 99.35%, 90.0%, and 60.9% / 94.5% of performance in identity recognition, gender recognition, and age recognition. In all experiments, we did not exceed the state-of-the-art results, but compared to other studies, we obtained performance similar to the previous study using only less than 10% parameters. In some experiments, we also achieved state-of-the-art result.",2019,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),,10.1109/RO-MAN46459.2019.8956313,
6afe1f668eea8dfdd43f0780634073ed4545af23,1,0,Deep learning for content-based video retrieval in film and television production,"While digitization has changed the workflow of professional media production, the content-based labeling of image sequences and video footage, necessary for all subsequent stages of film and television production, archival or marketing is typically still performed manually and thus quite time-consuming. In this paper, we present deep learning approaches to support professional media production. In particular, novel algorithms for visual concept detection, similarity search, face detection, face recognition and face clustering are combined in a multimedia tool for effective video inspection and retrieval. The analysis algorithms for concept detection and similarity search are combined in a multi-task learning approach to share network weights, saving almost half of the computation time. Furthermore, a new visual concept lexicon tailored to fast video retrieval for media production and novel visualization components are introduced. Experimental results show the quality of the proposed approaches. For example, concept detection achieves a mean average precision of approximately 90% on the top-100 video shots, and face recognition clearly outperforms the baseline on the public Movie Trailers Face Dataset.",2017,Multimedia Tools and Applications,,10.1007/s11042-017-4962-9,
6b155f84e7d0cd5ba98b24dafb6015339bdcdcc0,0,1,Speaker Recognition Based on Deep Learning: An Overview,"Speaker recognition is a task of identifying persons from their voices. Recently, deep learning has dramatically revolutionized speaker recognition. However, there is lack of comprehensive reviews on the exciting progress. In this paper, we review several major subtasks of speaker recognition, including speaker verification, identification, diarization, and robust speaker recognition, with a focus on deep-learning-based methods. Because the major advantage of deep learning over conventional methods is its representation ability, which is able to produce highly abstract embedding features from utterances, we first pay close attention to deep-learning-based speaker feature extraction, including the inputs, network structures, temporal pooling strategies, and objective functions respectively, which are the fundamental components of many speaker recognition subtasks. Then, we make an overview of speaker diarization, with an emphasis of recent supervised, end-to-end, and online diarization. Finally, we survey robust speaker recognition from the perspectives of domain adaptation and speech enhancement, which are two major approaches of dealing with domain mismatch and noise problems. Popular and recently released corpora are listed at the end of the paper.",2020,,2012.00931,,https://arxiv.org/pdf/2012.00931.pdf
6b1e01c99e26efd1a9d1daf218ba4edcbe8daddd,0,1,Dual-View Normalization for Face Recognition,"Face normalization refers to a family of approaches that rotate a non-frontal face to the frontal pose for better handling of face recognition. While a great majority of face normalization methods focus on frontal pose only, we proposed a framework for dual-view normalization that generates a frontal pose and an additional yaw-45° pose to an input face of an arbitrary pose. The proposed Dual-View Normalization (DVN) framework is designed to learn the transformation from a source set to two normal sets. The source set contains faces collected in the wild and covers a wide scope of variables. One normal set contains face images taken under controlled conditions and all faces are in frontal pose and balanced in illumination. The other normal set contains faces also taken under controlled conditions and balanced in illumination, but in 45° pose. The DVN framework is composed of one face encoder, two layers of generators, and two sets of discriminators. The encoder is made of a state-of-the-art face recognition network, which is not updated during training, and it acts as a facial feature extractor. The Layer-1 generators are trained on both the source and normal sets, aiming at learning the transformation from the source set to both normal sets. The trained generators can transform an arbitrary face into a pair of normalized faces, one in frontal pose and the other in 45° pose. The Layer-2 generators are trained to enhance the identity preservation of the faces made by the Layer-1 generators by minimizing the cross-pose identity loss. The discriminators are trained to ensure the photo-realistic quality of the dual-view normalized face images generated by the generators. The loss functions employed in the generators and the discriminators are designed to achieve satisfactory dual-view normalization outcomes and identity preservation. We verify the DVN framework on benchmark databases and compare with other state-of-the-art approaches for tackling face recognition.",2020,IEEE Access,,10.1109/ACCESS.2020.3014877,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09162013.pdf
6b2e5f18e890d3ef905c09f5b649478b73df5a65,0,1,A Deep Learning Approach for Dog Face Verification and Recognition,"Recently, deep learning methods for biometrics identification have mainly focused on human face identification and have proven their efficiency. However, little research have been performed on animal biometrics identification. In this paper, a deep learning approach for dog face verification and recognition is proposed and evaluated. Due to the lack of available datasets and the complexity of dog face shapes this problem is harder than human identification. The first publicly available dataset is thus composed, and a deep convolutional neural network coupled with the triplet loss is trained on this dataset. The model is then evaluated on a verification problem, on a recognition problem and on clustering dog faces. For an open-set of 48 different dogs, it reaches an accuracy of 92% on a verification task and a rank-5 accuracy of 88% on a one-shot recognition task. The model can additionally cluster pictures of these unknown dogs. This work could push zoologists to further investigate these new kinds of techniques for animal identification or could help pet owners to find their lost animal. The code and the dataset of this project are publicly available (https://github.com/GuillaumeMougeot/DogFaceNet).",2019,PRICAI,,10.1007/978-3-030-29894-4_34,
6b3ce02d967af862c751ce769bec2afe96dcc67c,1,1,Beyond Disentangled Representations: An Attentive Angular Distillation Approach to Large-scale Lightweight Age-Invariant Face Recognition,"Disentangled representations have been commonly adopted to Age-invariant Face Recognition (AiFR) tasks. However, these methods have reached some limitations with (1) the requirement of large-scale face recognition (FR) training data with age labels, which is limited in practice; (2) heavy deep network architecture for high performance; and (3) their evaluations are usually taken place on age-related face databases while neglecting the standard large-scale FR databases to guarantee its robustness. This work presents a novel Attentive Angular Distillation (AAD) approach to Large-scale Lightweight AiFR that overcomes these limitations. Given two high-performance heavy networks as teachers with different specialized knowledge, AAD introduces a learning paradigm to efficiently distill the age-invariant attentive and angular knowledge from those teachers to a lightweight student network making it more powerful with higher FR accuracy and robust against age factor. Consequently, AAD approach is able to take the advantages of both FR datasets with and without age labels to train an AiFR model. Far apart from prior distillation methods mainly focusing on accuracy and compression ratios in closed-set problems, our AAD aims to solve the open-set problem, i.e. large-scale face recognition. Evaluations on LFW, IJB-B and IJB-C Janus, AgeDB and MegaFace-FGNet with one million distractors have demonstrated the efficiency of the proposed approach. This work also presents a new longitudinal face aging (LogiFace) database for further studies in age-related facial problems in future.",2020,ArXiv,2004.05085,,https://arxiv.org/pdf/2004.05085.pdf
6b6a78db8c7cd69b4242760e1825863cbc3d3977,1,0,Deep Poisoning: Towards Robust Image Data Sharing against Visual Disclosure.,"Due to respectively limited training data, different entities addressing the same vision task based on certain sensitive images may not train a robust deep network. This paper introduces a new vision task where various entities share task-specific image data to enlarge each other's training data volume without visually disclosing sensitive contents (e.g. illegal images). Then, we present a new structure-based training regime to enable different entities learn task-specific and reconstruction-proof image representations for image data sharing. Specifically, each entity learns a private Deep Poisoning Module (DPM) and insert it to a pre-trained deep network, which is designed to perform the specific vision task. The DPM deliberately poisons convolutional image features to prevent image reconstructions, while ensuring that the altered image data is functionally equivalent to the non-poisoned data for the specific vision task. Given this equivalence, the poisoned features shared from one entity could be used by another entity for further model refinement. Experimental results on image classification prove the efficacy of the proposed method.",2020,,1912.06895,,https://arxiv.org/pdf/1912.06895.pdf
6b8472d2c77c4879c87640eedbc301f6ee4fee2e,1,0,Survey on the Analysis and Modeling of Visual Kinship: A Decade in the Making.,"Kinship recognition is a challenging problem with many practical applications. With much progress and milestones having been reached after ten years - we are now able to survey the research and create new milestones. We review the public resources and data challenges that enabled and inspired many to hone-in on the views of automatic kinship recognition in the visual domain. The different tasks are described in technical terms and syntax consistent across the problem domain and the practical value of each discussed and measured. State-of-the-art methods for visual kinship recognition problems, whether to discriminate between or generate from, are examined. As part of such, we review systems proposed as part of a recent data challenge held in conjunction with the 2020 IEEE Conference on Automatic Face and Gesture Recognition. We establish a stronghold for the state of progress for the different problems in a consistent manner. This survey will serve as the central resource for the work of the next decade to build upon. For the tenth anniversary, the demo code is provided for the various kin-based tasks. Detecting relatives with visual recognition and classifying the relationship is an area with high potential for impact in research and practice.",2020,,2006.16033,,https://arxiv.org/pdf/2006.16033.pdf
6bc3cb384e44b2a65f99fc7ba396ae7f94f73830,1,1,How (Not) to Measure Bias in Face Recognition Networks,"Within the last years Face Recognition (FR) systems have achieved human-like (or better) performance, leading to extensive deployment in large-scale practical settings. Yet, especially for sensible domains such as FR we expect algorithms to work equally well for everyone, regardless of somebody’s age, gender, skin colour and/or origin. In this paper, we investigate a methodology to quantify the amount of bias in a trained Convolutional Neural Network (CNN) model for FR that is not only intuitively appealing, but also has already been used in the literature to argue for certain debiasing methods. It works by measuring the “blindness” of the model towards certain face characteristics in the embeddings of faces based on internal cluster validation measures. We conduct experiments on three openly available FR models to determine their bias regarding race, gender and age, and validate the computed scores by comparing their predictions against the actual drop in face recognition performance for minority cases. Interestingly, we could not link a crisp clustering in the embedding space to a strong bias in recognition rates—it is rather the opposite. We therefore offer arguments for the reasons behind this observation and argue for the need of a less naive clustering approach to develop a working measure for bias in FR models.",2020,,,10.21256/ZHAW-20277,
6bf20e8dc79df9fd6e395e9b6ae1b960ff4519e1,0,1,Taking Modality-free Human Identification as Zero-shot Learning,"Human identification is an important topic in event detection, person tracking, and public security. There have been numerous methods proposed for human identification, such as face identification, person re-identification, and gait identification. Typically, existing methods predominantly classify a queried image to a specific identity in an image gallery set (I2I). This is seriously limited for the scenario where only a textual description of the query or an attribute gallery set is available in a wide range of video surveillance applications (A2I or I2A). However, very few efforts have been devoted towards modality-free identification, i.e., identifying a query in a gallery set in a scalable way. In this work, we take an initial attempt, and formulate such a novel Modality-Free Human Identification (named MFHI) task as a generic zero-shot learning model in a scalable way. Meanwhile, it is capable of bridging the visual and semantic modalities by learning a discriminative prototype of each identity. In addition, the semantics-guided spatial attention is enforced on visual modality to obtain representations with both high global category-level and local attribute-level discrimination. Finally, we design and conduct an extensive group of experiments on two common challenging identification tasks, including face identification and person re-identification, demonstrating that our method outperforms a wide variety of state-of-the-art methods on modality-free human identification.",2020,ArXiv,2010.00975,,https://arxiv.org/pdf/2010.00975.pdf
6c23646a0172cd1dcdda97298aaefd23cd00df39,1,1,Jointly De-Biasing Face Recognition and Demographic Attribute Estimation,"We address the problem of bias in automated face recognition and demographic attribute estimation algorithms, where errors are lower on certain cohorts belonging to specific demographic groups. We present a novel de-biasing adversarial network (DebFace) that learns to extract disentangled feature representations for both unbiased face recognition and demographics estimation. The proposed network consists of one identity classifier and three demographic classifiers (for gender, age, and race) that are trained to distinguish identity and demographic attributes, respectively. Adversarial learning is adopted to minimize correlation among feature factors so as to abate bias influence from other factors. We also design a new scheme to combine demographics with identity features to strengthen robustness of face representation in different demographic groups. The experimental results show that our approach is able to reduce bias in face recognition as well as demographics estimation while achieving state-of-the-art performance.",2020,ECCV,1911.0808,10.1007/978-3-030-58526-6_20,https://arxiv.org/pdf/1911.08080.pdf
6c3f88f290107ebfefe2f3d1bf68668d74491c6a,0,1,Non-Probabilistic Cosine Similarity Loss for Few-Shot Image Classification,"A few-shot image classification problem aims to recognize previously unseen objects with a small amount of data. Many works have been offered to solve the problem, while a simple transfer learning method with the cosine similarity based cross-entropy loss is still powerful compared with other methods. To improve the performance, we propose a novel Non-Probabilistic Cosine similarity (NPC) loss for few-shot classification that can replace the cross-entropy loss with the cosine similarity. A key difference of NPC loss is that it uses values of inputs instead of their probabilities. By simply changing the loss function, our model avoids overfitting on a training set and performs well on fewshot tasks. Experimental results show that the model with NPC loss clearly outperforms those with other loss functions and also achieves excellent performance compared with state-of-the-art algorithms on Mini-Imagenet and CUB-200-2011 datasets.",2020,,,,https://pdfs.semanticscholar.org/d6c1/593b979891fb83ff382e900f0e7b7c4212c8.pdf
6c55bcc205b24c7aaf39680d71716e598a3cc536,0,1,The General Pair-based Weighting Loss for Deep Metric Learning,"Deep metric learning aims at learning the distance metric between pair of samples, through the deep neural networks to extract the semantic feature embeddings where similar samples are close to each other while dissimilar samples are farther apart. A large amount of loss functions based on pair distances have been presented in the literature for guiding the training of deep metric learning. In this paper, we unify them in a general pair-based weighting loss function, where the minimizing objective loss is just the distances weighting of informative pairs. The general pair-based weighting loss includes two main aspects, (1) samples mining and (2) pairs weighting. Samples mining aims at selecting the informative positive and negative pair sets to exploit the structured relationship of samples in a mini-batch and also reduce the number of non-trivial pairs. Pair weighting aims at assigning different weights for different pairs according to the pair distances for discriminatively training the network. We detailedly review those existing pair-based losses inline with our general loss function, and explore some possible methods from the perspective of samples mining and pairs weighting. Finally, extensive experiments on three image retrieval datasets show that our general pair-based weighting loss obtains new state-of-the-art performance, demonstrating the effectiveness of the pair-based samples mining and pairs weighting for deep metric learning.",2019,ArXiv,1905.12837,,https://arxiv.org/pdf/1905.12837.pdf
6c7d264d9cc7f801c483fba374048b0354d427a1,1,0,Robust face recognition via hierarchical collaborative representation,"Abstract Collaborative representation-based classification (CRC) is currently attracting the attention of researchers because it is more effective than conventional representation-based classifiers in recognition tasks. CRC has shown high face recognition accuracy; however, its accuracy is degraded significantly if the number of training faces in each class is small. This is because the accuracy of CRC is only dependent on the results of minimizing the Euclidean distance between a testing face and its approximator in the collaborative subspace of training faces. In this research, we proved that the accuracy of CRC can be improved substantially by minimizing not only the Euclidean distance between a testing face and its approximator but also the Euclidean distances from the approximator to training faces in each class. Consequently, we presented a hierarchical collaborative representation-based classification (HCRC) in which a two-stage classifier is applied for training faces, and the recognition accuracy of the second-stage classifier is significantly improved in comparison to that of the first-stage classifier. Moreover, the recognition rate of our classifier can be considerably increased by using models of discriminative feature extraction. Since noise and illumination are the main factors that cause CRC to be less accurate, we propose combining HCRC with a wide model of local ternary patterns (LTP). This combination enhances the efficiency of face recognition under different illumination and noisy conditions. For dealing with face recognition under variations in pose, expression and illumination, we present a deep convolutional neural network (DCNN) model of discriminative feature learning, which transforms face images into a common set of distinct features. The combination of HCRC with this deep model achieves high recognition rates on challenging face databases. Furthermore both models are optimized to reduce computational costs so that they can be successfully applied for real-world applications of face recognition that are required to run reliably in real time. In addition, we also prove that combining state-of-the-art DCNN models with HCRC results in an significant improvement in face recognition performance. We demonstrate several experiments with challenging face recognition datasets. Our results show that the hierarchical collaborative representation-based classifier with the models significantly outperforms state-of-the-art methods.",2018,Inf. Sci.,,10.1016/j.ins.2017.12.014,
6ca6ade6c9acb833790b1b4e7ee8842a04c607f7,1,0,Deep Transfer Network for Unconstrained Face Verification,"Face verification has achieved remarkable success on constrained settings where faces have frontal bias because of the limitation of face detection algorithms. However, unconstrained face verification is still a challenging problem and IJB-A dataset is an important benchmark in this setting. In this paper, we propose the Deep Transfer Network (DTN) which integrates transfer learning and attention-based feature aggregation mechanism together. First, we train a resnet with all its convolutions replaced by shufflenet-like convolutions supervised by A-softmax loss function. Then, we apply metric learning, feature aggregation and template adaptation successively to improve the performance. Our DTN is more efficient than previous methods thanks to its compact representation (256-d). And our DTN can produce results comparable to the state-of-the-art in the challenging face dataset, IJB-A.",2018,ICDLT '18,,10.1145/3234804.3234805,
6cacda04a541d251e8221d70ac61fda88fb61a70,1,0,One-shot Face Recognition by Promoting Underrepresented Classes,"In this paper, we study the problem of training large-scale face identification model with imbalanced training data. This problem naturally exists in many real scenarios including large-scale celebrity recognition, movie actor annotation, etc. Our solution contains two components. First, we build a face feature extraction model, and improve its performance, especially for the persons with very limited training samples, by introducing a regularizer to the cross entropy loss for the multi-nomial logistic regression (MLR) learning. This regularizer encourages the directions of the face features from the same class to be close to the direction of their corresponding classification weight vector in the logistic regression. Second, we build a multi-class classifier using MLR on top of the learned face feature extraction model. Since the standard MLR has poor generalization capability for the one-shot classes even if these classes have been oversampled, we propose a novel supervision signal called underrepresented-classes promotion loss, which aligns the norms of the weight vectors of the one-shot classes (a.k.a. underrepresented-classes) to those of the normal classes. In addition to the original cross entropy loss, this new loss term effectively promotes the underrepresented classes in the learned model and leads to a remarkable improvement in face recognition performance.  We test our solution on the MS-Celeb-1M low-shot learning benchmark task. Our solution recognizes 94.89% of the test images at the precision of 99\% for the one-shot classes. To the best of our knowledge, this is the best performance among all the published methods using this benchmark task with the same setup, including all the participants in the recent MS-Celeb-1M challenge at ICCV 2017.",2017,ArXiv,1707.05574,,https://arxiv.org/pdf/1707.05574.pdf
6d8275a62e95f875ad3a3c2204a8d462aa36fdd1,0,1,Parametric Instance Classification for Unsupervised Visual Feature Learning,"This paper presents parametric instance classification (PIC) for unsupervised visual feature learning. Unlike the state-of-the-art approaches which do instance discrimination in a dual-branch non-parametric fashion, PIC directly performs a one-branch parametric instance classification, revealing a simple framework similar to supervised classification and without the need to address the information leakage issue. We show that the simple PIC framework can be as effective as the state-of-the-art approaches, i.e. SimCLR and MoCo v2, by adapting several common component settings used in the state-of-the-art approaches. We also propose two novel techniques to further improve effectiveness and practicality of PIC: 1) a sliding-window data scheduler, instead of the previous epoch-based data scheduler, which addresses the extremely infrequent instance visiting issue in PIC and improves the effectiveness; 2) a negative sampling and weight update correction approach to reduce the training time and GPU memory consumption, which also enables application of PIC to almost unlimited training images. We hope that the PIC framework can serve as a simple baseline to facilitate future study.",2020,NeurIPS,2006.14618,,https://arxiv.org/pdf/2006.14618.pdf
6d91da37627c05150cb40cac323ca12a91965759,1,0,Gender Politics in the 2016 U.S. Presidential Election: A Computer Vision Approach,"Gender plays an important role in the 2016 U.S. presidential election, especially with Hillary Clinton becoming the first female presidential nominee and Donald Trump being frequently accused of sexism. In this paper, we introduce computer vision to the study of gender politics and present an image-driven method that can measure the effects of gender in an accurate and timely manner. We first collect all the profile images of the candidates’ Twitter followers. Then we train a convolutional neural network using images that contain gender labels. Lastly, we classify all the follower and unfollower images. Through a case study of the ‘woman card’ controversy, we demonstrate how gender is informing the 2016 presidential election. Our framework of analysis can be readily generalized to other case studies and elections.",2017,SBP-BRiMS,1611.02806,10.1007/978-3-319-60240-0_4,https://arxiv.org/pdf/1611.02806.pdf
6d972b4f48bacf745c2422119270ab841e14c020,0,1,AQD: Towards Accurate Quantized Object Detection,"Network quantization aims to lower the bitwidth of weights and activations and hence reduce the model size and accelerate the inference of deep networks. Even though existing quantization methods have achieved promising performance on image classification, applying aggressively low bitwidth quantization on object detection while preserving the performance is still a challenge. In this paper, we demonstrate that the poor performance of the quantized network on object detection comes from the inaccurate batch statistics of batch normalization. To solve this, we propose an accurate quantized object detection (AQD) method. Specifically, we propose to employ multi-level batch normalization (multi-level BN) to estimate the batch statistics of each detection head separately. We further propose a learned interval quantization method to improve how the quantizer itself is configured. To evaluate the performance of the proposed methods, we apply AQD to two one-stage detectors (i.e., RetinaNet and FCOS). Experimental results on COCO show that our methods achieve near-lossless performance compared with the full-precision model by using extremely low bitwidth regimes such as 3-bit. In particular, we even outperform the full-precision counterpart by a large margin with a 4-bit detector, which is of great practical value.",2020,ArXiv,2007.06919,,https://arxiv.org/pdf/2007.06919.pdf
6de33bca3a7a98bf55a78f12192022bd98e67cfb,0,1,Analysis of ABC Submission to NIST SRE 2019 CMN and VAST Challenge,"We present a condensed description and analysis of the joint submission of ABC team for NIST SRE 2019, by BUT, CRIM, Phonexia, Omilia and UAM. We concentrate on challenges that arose during development and we analyze the results obtained on the evaluation data and on our development sets. The conversational telephone speech (CMN2) condition is challenging for current state-of-the-art systems, mainly due to the language mismatch between training and test data. We show that a combination of adversarial domain adaptation, backend adaptation and score normalization can mitigate this mismatch. On the VAST condition, we demonstrate the importance of deploying diarization when dealing with multi-speaker utterances and the drastic improvements that can be obtained by combining audio and visual modalities.",2020,,,10.21437/odyssey.2020-41,https://pdfs.semanticscholar.org/2198/1231bc09fe0c8410a8a57edf5dd37f802233.pdf
6df2e75289e7a257bf338dd58d1054cd1776337b,0,1,Improve L2-normalized Softmax with Exponential Moving Average,"In this paper, we propose an effective training method to improve the performance of L2-normalized softmax for convolutional neural networks. Recent studies of deep learning show that by L2-normalizing the input features of softmax, the accuracy of CNN can be increased. Several works proposed novel loss functions based on the L2-normalized softmax. A common property shared by these modified normalized softmax models is that an extra set of parameters is introduced as the class centers. Although the physical meaning of this parameter is clear, few attentions have been paid to how to learn these class centers, which limits futher improvement.In this paper, we address the problem of learning the class centers in the L2-normalized softmax. By treating the CNN training process as a time series, we propose a novel learning algorithm that combines the generally used gradient descent with the exponential moving average. Extensive experiments show that our model not only achieves better performance but also has a higher tolerance to the imbalance data.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8851850,
6ec3cf99b76ed19fea41538e1f3dfc4ccec660a4,1,1,Accelerating Deep Learning with Millions of Classes,"Deep learning has achieved remarkable success in many classification tasks because of its great power of representation learning for complex data. However, it remains challenging when extending to classification tasks with millions of classes. Previous studies are focused on solving this problem in a distributed fashion or using a sampling-based approach to reduce the computational cost caused by the softmax layer. However, these approaches still need high GPU memory in order to work with large models and it is non-trivial to extend them to parallel settings. To address these issues, we propose an e cient training framework to handle extreme classification tasks based on Random Projection. The key idea is that we first train a slimmed model with a random projected softmax classifier and then we recover it to the original version. We also show a theoretical guarantee that this recovered classifier can approximate the original classifier with a small error. Later, we extend our framework to parallel scenarios by adopting a communication reduction technique. In our experiment, we demonstrate that the proposed framework is able to train deep learning models with millions of classes and achieve above 10⇥ speedup compared to existing approaches.",2020,ECCV,,10.1007/978-3-030-58592-1_42,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680698.pdf
6edb8eca8dd359fd8c446ab4efa490424f8fd4e6,0,1,Improving Automatic Speech Recognition and Speech Translation via Word Embedding Prediction,"In this article, we target speech translation (ST). We propose lightweight approaches that generally improve either ASR or end-to-end ST models. We leverage continuous representations of words, known as word embeddings, to improve ASR in cascaded systems as well as end-to-end ST models. The benefit of using word embedding is that word embedding can be obtained easily by training on pure textual data, which alleviates data scarcity issue. Also, word embedding provides additional contextual information to speech models. We motivate to distill the knowledge from word embedding into speech models. In ASR, we use word embeddings as a regularizer to reduce the WER, and further propose a novel decoding method to fuse the semantic relations among words for further improvement. In the end-to-end ST model, we propose leveraging word embeddings as an intermediate representation to enhance translation performance. Our analysis shows that it is possible to map speech signals to semantic space, which motivates future work on applying the proposed methods in spoken language processing tasks.",2021,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",,10.1109/TASLP.2020.3037543,
6ef4245b61572b3fcee18a8e6cbe4f78f6103a41,0,1,A Novel Soft Margin Loss Function for Deep Discriminative Embedding Learning,"Deep embedding learning aims to learn discriminative feature representations through a deep convolutional neural network model. Commonly, such a model contains a network architecture and a loss function. The architecture is responsible for hierarchical feature extraction, while the loss function supervises the training procedure with the purpose of maximizing inter-class separability and intra-class compactness. By considering that loss function is crucial for the feature performance, in this article we propose a new loss function called soft margin loss (SML) based on a classification framework for deep embedding learning. Specifically, we first normalize the learned features and the classification weights to map them into the hypersphere. After that, we construct our loss with the difference between the maximum intra-class distance and minimum inter-class distance. By constraining the distance difference with a soft margin that is inherent in the proposed loss, both the inter-class discrepancy and intra-class compactness of learned features can be effectively improved. Finally, under the joint training with an improved softmax loss, the model can learn features with strong discriminability. Toy experiments on MNIST dataset are conducted to show the effectiveness of the proposed method. Additionally, experiments on re-identification tasks are also provided to demonstrate the superior performance of embedding learning. Specifically, 65.48% / 62.68% mAP on CUHK03 labeled / detected dataset (person re-id) and 74.36% mAP on VeRi-776 dataset (vehicle re-id) are achieved respectively.",2020,IEEE Access,,10.1109/ACCESS.2020.3036185,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09249024.pdf
6efa92deb42ff10c7d96d239b3e7c41637a3f06b,0,1,Rodlike nanoparticle parameter measurement method based on improved Mask R-CNN segmentation,,2020,,,10.1007/s11760-020-01779-0,
6f4ab58e523eb3360cf2695467c009d484e1e2f6,0,1,Persistent Mixture Model Networks for Few-Shot Image Classification,"We introduce Persistent Mixture Model (PMM) networks for representation learning in the few-shot image classification context. While previous methods represent classes with a single centroid or rely on post hoc clustering methods, our method learns a mixture model for each base class jointly with the data representation in an end-to-end manner. The PMM training algorithm is organized into two main stages: 1) initial training and 2) progressive following. First, the initial estimate for multi-component mixtures is learned for each class in the base domain using a combination of two loss functions (competitive and collaborative). The resulting network is then progressively refined through a leaderfollower learning procedure, which uses the current estimate of the learner as a fixed “target” network. This target network is used to make a consistent assignment of instances to mixture components, in order to increase performance while stabilizing the training. The effectiveness of our joint representation/mixture learning approach is demonstrated with extensive experiments on four standard datasets and four backbones. In particular, we demonstrate that when we combine our robust representation with recent alignmentand margin-based approaches, we achieve new state-of-theart results in the inductive setting, with an absolute accuracy for 5-shot classification of 82.45% on miniImageNet, 88.20% with tieredImageNet, and 60.70% in FC100, all using the ResNet-12 backbone.",2020,ArXiv,2011.11872,,https://arxiv.org/pdf/2011.11872.pdf
6f5309d8cc76d3d300b72745887addd2a2480ba8,1,0,KinNet: Fine-to-Coarse Deep Metric Learning for Kinship Verification,"Automatic kinship verification has attracted increasing attentions as it holds promise to an abundance of applications. However, existing kinship verification methods suffer from the lack of large scale real-world data. Without enough training data, it is difficult to learn proper features that are discriminant for blood-related peoples. In this work, we propose KinNet, a fine-to-coarse deep metric learning framework for kinship verification. In the framework, we transfer knowledge from the large-scale-data-driven face recognition task, which is a fine-grained version of kinship recognition, by pre-training the network with massive data for face recognition. Then, the network is fine-tuned to find a metric space where kin-related peoples are discriminant. The metric space is learned by minimizing a soft triplet loss on the augmented kinship dataset. An augmented strategy is proposed to balance the amount of images per family member. Finally, we ensemble four networks to further boost the performance. The experimental results on the 1st Large-Scale Kinship Recognition Data Challenge (Track 1) demonstrate that our KinNet achieves the state-of-the-art performance in kinship verification.",2017,RFIW '17,,10.1145/3134421.3134425,
6f94bea2bcc93eb3e4a389aea857ba1d74713def,0,1,Statistical Insights into DNN Learning in Subspace Classification,"Deep learning has benefited almost every aspect of modern big data applications. Yet its statistical properties still remain largely unexplored. It is commonly believed nowadays that deep neural nets (DNNs) benefit from representational learning. To gain some statistical insights into this, we design a simple simulation setting where we generate data from some latent subspace structure with each subspace regarded as a cluster. We empirically demonstrate that the performance of DNN is very similar to the two-step procedure of clustering followed by classification (unsupervised plus supervised). This motivates us to ask: does DNN indeed mimic the two-step procedure statistically? That is, do bottom layers in DNN try to cluster first and then top layers classify within each cluster? To answer this question, we conduct a series of simulation studies and to our surprise, none of the hidden layers in DNN conducts successful clustering. In some sense, our results provide an important complement to the common belief of representational learning, suggesting that at least in some model settings, although the performance of DNN is comparable to the ideal two-step procedure knowing the true latent cluster information a priori, it does not really do clustering in any of its layers. We also provide some statistical insights and heuristic arguments to support our empirical discoveries and further demonstrate the revealed phenomenon on the real data application of traffic sign recognition.",2020,,,,http://faculty.marshall.usc.edu/yingying-fan/publications/Stat-WFL20.pdf
6fb142dc1bf764baf9e85208d9cbfa1ed0d12258,1,0,Deep face recognition with weighted center loss,"ID face photos are widely used for identity verification in many real-world applications. In these cases, the face from ID photo is expected to compare with the face obtained from the daily life image. To improve the ID/life face verification performance, the datasets containing various ID face photos and daily life face images are always needed to train an available CNN model for feature extraction. However, it’s really hard to get uniform distributed data for both the ID photos and daily life images. In most face datasets, each subject only contains one ID photo (for example, obtained from the identity card) while contains varies of life images. This imbalanced distribution causes the difficulty of learning a general deep feature representation for both the ID and life faces, since the daily life images tend to have a higher impact on the feature learning than the ID photos. To address this challenge, we propose a weighted center loss which aims to learn a center of deep features for each class and penalizes the distance between the image features and their corresponding class centers with different weights. By emphasizing the ID photo features in the loss computation, our weighted center loss can train a general CNN which obviously narrows the distance between the ID and life faces. Final experiments demonstrate that with the joint supervision of softmax and weighted center loss, our proposed framework can minimize the intra-class variations caused by the imbalanced ID/life data distribution and significantly improve the ID face based verification accuracy.",2019,International Conference on Graphic and Image Processing,,10.1117/12.2524146,
6fce879d4aa39369d4edb9cb0564f09b79398c27,0,1,Point-to-Set Similarity Based Deep Metric Learning for Offline Signature Verification,"Offline signature verification is a challenging task, where the scarcity of the signature data per writer makes it a few-shot problem. We found that previous deep metric learning based methods, whether in pairs or triplets, are unaware of intra-writer variations and have low training efficiency because only point-to-point (P2P) distances are considered. To address this issue, we present a novel point-to-set (P2S) metric for offline signature verification in this paper. By dividing a training batch into a support set and a query set, our optimization goal is to pull each query to its belonging support set. To further strengthen the P2S metric, a hard mining scheme and a margin strategy are introduced. Experiments conducted on three datasets show the effectiveness of our proposed method.",2020,2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR),,10.1109/ICFHR2020.2020.00059,
6fd885e4927f3a42c6d962753cbc1b5ec470425a,0,1,Momentum Contrast Speaker Representation Learning,"Unsupervised representation learning has shown remarkable achievement by reducing the performance gap with supervised feature learning, especially in the image domain. In this study, to extend the technique of unsupervised learning to the speech domain, we propose the Momentum Contrast for VoxCeleb (MoCoVox) as a form of learning mechanism. We pre-trained the MoCoVox on the VoxCeleb1 by implementing instance discrimination. Applying MoCoVox for speaker verification revealed that it outperforms the state-of-the-art metric learning-based approach by a large margin. We also empirically demonstrate the features of contrastive learning in the speech domain by analyzing the distribution of learned representations. Furthermore, we explored which pretext task is adequate for speaker verification. We expect that learning speaker representation without human supervision helps to address the open-set speaker recognition.",2020,ArXiv,2010.11457,,https://arxiv.org/pdf/2010.11457.pdf
700831b6e7eaf17a00a4973bca167f31d115e01f,0,1,Comparative Study on various Losses for Vehicle Re-identification,"In this paper, we tackle the problem of vehicle reidentification, which has extensive applications in traffic analysis such as anomaly detection, congestion pricing and tolling. While previous methods extract visual features from the images and then use spatio-temporal regularization to further refine the results, our method focuses on extracting purely visual features from vehicle images and then further employs a re-ranking technique to improve results. We evaluate the proposed pipeline on the VeRi and CityFlow (NVIDIA AI City Challenge 2019) datasets. Experiments show that our pipeline achieves state of the art performance on the VeRi dataset. We also perform extensive analysis on each step of the pipeline and demonstrate how they increase overall performance.",2019,CVPR Workshops,,,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Shankar_Comparative_Study_on_various_Losses_for_Vehicle_Re-identification_CVPRW_2019_paper.pdf
7060003d09260c0522fbab47b8757ff3ca1c1873,0,1,FeatherNets: Convolutional Neural Networks as Light as Feather for Face Anti-Spoofing,"Face Anti-spoofing gains increased attentions recently in both academic and industrial fields. With the emergence of various CNN based solutions, the multi-modal(RGB, depth and IR) methods based CNN showed better performance than single modal classifiers. However, there is a need for improving the performance and reducing the complexity. Therefore, an extreme light network architecture(FeatherNet A/B) is proposed with a streaming module which fixes the weakness of Global Average Pooling and uses less parameters. Our single FeatherNet trained by depth image only, provides a higher baseline with 0.00168 ACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure with ""ensemble + cascade"" structure is presented to satisfy the performance preferred use cases. Meanwhile, the MMFD dataset is collected to provide more attacks and diversity to gain better generalization. We use the fusion method in the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the result of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and 0.9814(TPR@FPR=10e-4).",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1904.0929,10.1109/CVPRW.2019.00199,https://arxiv.org/pdf/1904.09290.pdf
70c59dc3470ae867016f6ab0e008ac8ba03774a1,1,0,VGGFace2: A Dataset for Recognising Faces across Pose and Age,"In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimise the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS-Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on the IJB-A and IJB-B face recognition benchmarks, exceeding the previous state-of-the-art by a large margin. The dataset and models are publicly available.",2018,2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018),1710.08092,10.1109/FG.2018.00020,https://arxiv.org/pdf/1710.08092.pdf
7124176c1a9711bd535afb7e00fe18957af3aefe,1,0,Virtual Class Enhanced Discriminative Embedding Learning,"Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method Virtual Softmax to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.",2018,NeurIPS,1811.12611,,https://arxiv.org/pdf/1811.12611.pdf
716af324a2a36cfc464d7fd5b59114e6ce5ceadf,0,1,Lipreading with LipsID,This paper presents an approach for adaptation of the current visual speech recognition systems. The adaptation technique is based on LipsID features. These features represent a processed area of lips ROI. The features are extracted in a classification task by neural network pre-trained on the dataset-specific to the lip-reading system used for visual speech recognition. The training procedure for LipsID implements ArcFace loss to separate different speakers in the dataset and to provide distinctive features for every one of them. The network uses convolutional layers to extract features from input sequences of speaker images and is designed to take the same input as the lipreading system. Parallel processing of input sequence by LipsID network and lipreading network is followed by a combination of both feature sets and final recognition by Connectionist Temporal Classification (CTC) mechanism. This paper presents results from experiments with the LipNet network by re-implementing the system and comparing it with and without LipsID features. The results show a promising path for future experiments and other systems. The training and testing process of neural networks used in this work utilizes Tensorflow/Keras implementations [4].,2020,SPECOM,,10.1007/978-3-030-60276-5_18,
7170a829f92d9a23116a5cb58015eab1a03f2386,0,1,Accurate and Reliable Facial Expression Recognition Using Advanced Softmax Loss With Fixed Weights,"An important challenge for facial expression recognition (FER) is that real-world training data are usually imbalanced. Although many deep learning approaches have been proposed to enhance the discriminative power of deep expression features and enable a good predictive effect, few works have focused on the multiclass imbalance problem. When supervised by softmax loss (SL), which is widely used in FER, the classifier is often biased against minority categories (i.e., smaller interclass angular distances). In this letter, we present advanced softmax loss (ASL) to mitigate the bias induced by data imbalance and hence increase accuracy and reliability. The proposed ASL essentially magnifies the interclass diversity in the angular space to enhance discriminative power in every category. The proposed loss can easily be implemented in any deep network. Extensive experiments on the FER2013 and real-world affective faces (RAF) databases demonstrate that ASL is significantly more accurate and reliable than many state-of-the-art approaches and that it can easily be plugged into other methods and improves their performance.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.2989670,
7183c15ff7920266bb45f664c0948f29b7577844,1,1,Face Verification Between ID Document Photos and Partial Occluded Spot Photos,"ID-spot face verification is an important problem in face verification area, which aims to identify whether the spotted face is the same to the ID photo. Although some face verification systems have been deployed in many application scenarios, most of them are used in a constrained environment and many key problems need to be addressed furthermore. In this paper, we focus on a challenging ID-spot face verification task, in which the spot photo is partially occluded. Toward this end, a two-stream network is employed to learn more discriminative feature for distinguishing different ID-Spot face pairs. In addition, to suppress the negative effect of background and occlusion, a global weight pooling method is proposed, which makes the available face area more significant than the background and occlusion. The experimental results show that the proposed method obtains 10% improvements on FAR@0.01 compared with previous schemes.",2019,ICIG,,10.1007/978-3-030-34110-7_9,
71b7fc715e2f1bb24c0030af8d7e7b6e7cd128a6,1,0,The Do’s and Don’ts for CNN-Based Face Verification,"While the research community appears to have developed a consensus on the methods of acquiring annotated data, design and training of CNNs, many questions still remain to be answered. In this paper, we explore the following questions that are critical to face recognition research: (i) Can we train on still images and expect the systems to work on videos? (ii) Are deeper datasets better than wider datasets? (iii) Does adding label noise lead to improvement in performance of deep networks? (iv) Is alignment needed for face recognition? We address these questions by training CNNs using CASIA-WebFace, UMD-Faces, and a new video dataset and testing on YouTube-Faces, IJB-A and a disjoint portion of UMDFaces datasets. Our new data set, which will be made publicly available, has 22,075 videos and 3,735,476 human annotated frames extracted from them.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),1705.07426,10.1109/ICCVW.2017.299,https://arxiv.org/pdf/1705.07426.pdf
71b97bf34bfbaf8fb5b1fd074290e6899781737c,0,1,Utilizing VOiCES Dataset for Multichannel Speaker Verification with Beamforming,"VOiCES from a Distance Challenge 2019 aimed at the evaluation of speaker verification (SV) systems using single-channel trials based on the Voices Obscured in Complex Environmental Settings (VOiCES) corpus. Since it comprises recordings of the same utterances captured simultaneously by multiple microphones in the same environments, it is also suitable for multichannel experiments. In this work, we design a multichannel dataset as well as development and evaluation trials for SV inspired by the VOiCES challenge. Alternatives discarding harmful microphones are presented as well. We asses the utilization of the created dataset for x-vector based SV with beamforming as a front end. Standard fixed beamforming and NN-supported beamforming using simulated data and ideal binary masks (IBM) are compared with another variant of NNsupported beamforming that is trained solely on the VOiCES data. Lack of data revealed by experiments with VOiCESdata trained beamformer was tackled by means of a variant of SpecAugment applied to magnitude spectra. This approach led to as much as 10% relative improvement in EER pushing results closer to those obtained by a good beamformer based on IBMs.",2020,,,10.21437/odyssey.2020-27,https://www.isca-speech.org/archive/Odyssey_2020/pdfs/80.pdf
71d39d08707e71a6d233e85e864dd9ed4e22110c,0,1,CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition,"As an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, the idea of mining-based strategies is adopted to emphasize the misclassified samples, achieving promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited; or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issue. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.00288,10.1109/cvpr42600.2020.00594,https://arxiv.org/pdf/2004.00288.pdf
71f2937c665b5a42372b3cb98e9f8297cd810ea4,1,1,Towards Flops-Constrained Face Recognition,"Large scale face recognition is challenging especially when the computational budget is limited. Given a flops upper bound, the key is to find the optimal neural network architecture and optimization method. In this article, we introduce the solutions of team 'trojans' for the ICCV19 - Lightweight Face Recognition Challenge. Our team mainly focuses on the two 'large' tracks, image-based and video-based, respectively. The submissions of these two tracks are required to be one single model with computational budget no higher than 30 GFlops. We introduce a network architecture 'Efficient PolyFace', a novel loss function 'ArcNegFace', a novel frame aggregation method 'QAN++', together with a bag of useful tricks in our implementation (augmentations, regular face, label smoothing, anchor finetuning, etc.). Our basic model, 'Efficient PolyFace', takes 28.25 Gflops for the 'deepglint-large' image-based track, and the 'PolyFace+QAN++' solution takes 24.12 Gflops for the 'iQiyi-large' video-based track. These two solutions achieve 94.198% @ 1e-8 and 72.981% @ 1e-4 in the two tracks respectively, which are the state-of-the-art results.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),1909.00632,10.1109/ICCVW.2019.00330,https://arxiv.org/pdf/1909.00632.pdf
71fcfab6fe35ae525e6ba2e27c4ff7b6396cbac9,0,1,Open Cross-Domain Visual Search,"This paper addresses cross-domain visual search, where visual queries retrieve category samples from a different domain. For example, we may want to sketch an airplane and retrieve photographs of airplanes. Despite considerable progress, the search occurs in a closed setting between two pre-defined domains. In this paper, we make the step towards an open setting where multiple visual domains are available. This notably translates into a search between any pair of domains, from a combination of domains or within multiple domains. We introduce a simple -- yet effective -- approach. We formulate the search as a mapping from every visual domain to a common semantic space, where categories are represented by hyperspherical prototypes. Open cross-domain visual search is then performed by searching in the common semantic space, regardless of which domains are used as source or target. Domains are combined in the common space to search from or within multiple domains simultaneously. A separate training of every domain-specific mapping function enables an efficient scaling to any number of domains without affecting the search performance. We empirically illustrate our capability to perform open cross-domain visual search in three different scenarios. Our approach is competitive with respect to existing closed settings, where we obtain state-of-the-art results on several benchmarks for three sketch-based search tasks.",2020,Comput. Vis. Image Underst.,1911.08621,10.1016/j.cviu.2020.103045,https://arxiv.org/pdf/1911.08621.pdf
7232c88607067648ec726caaa4c02f58a9808424,1,1,Audio-Visual Speaker Recognition with a Cross-Modal Discriminative Network,"Audio-visual speaker recognition is one of the tasks in the recent 2019 NIST speaker recognition evaluation (SRE). Studies in neuroscience and computer science all point to the fact that vision and auditory neural signals interact in the cognitive process. This motivated us to study a cross-modal network, namely voice-face discriminative network (VFNet) that establishes the general relation between human voice and face. Experiments show that VFNet provides additional speaker discriminative information. With VFNet, we achieve 16.54% equal error rate relative reduction over the score level fusion audio-visual baseline on evaluation set of 2019 NIST SRE.",2020,INTERSPEECH,2008.03894,10.21437/interspeech.2020-1814,https://arxiv.org/pdf/2008.03894.pdf
72489f4f058b1fe77aa94213565d1595e0f435ee,0,1,Face Video Retrieval Based on the Deep CNN With RBF Loss,"This paper presents a novel framework to extract highly compact and discriminative features for face video retrieval tasks using the deep convolutional neural network (CNN). The face video retrieval task is to find the videos containing the face of a specific person from a database with a face image or a face video of the same person as a query. A key challenge is to extract discriminative features with small storage space from face videos with large intra-class variations caused by different angle, illumination, and facial expression. In recent years, the CNN-based binary hashing and metric learning methods showed notable progress in image/video retrieval tasks. However, the existing CNN-based binary hashing and metric learning have limitations in terms of inevitable information loss and storage inefficiency, respectively. To cope with these problems, the proposed framework consists of two parts: first, a novel loss function using a radial basis function kernel (RBF Loss) is introduced to train a neural network to generate compact and discriminative high-level features, and secondly, an optimized quantization using a logistic function (Logistic Quantization) is suggested to convert a real-valued feature to a 1-byte integer with the minimum information loss. Through the face video retrieval experiments on a challenging TV series data set (ICT-TV), it is demonstrated that the proposed framework outperforms the existing state-of-the-art feature extraction methods. Furthermore, the effectiveness of RBF loss was also demonstrated through the image classification and retrieval experiments on the CIFAR-10 and Fashion-MNIST data sets with LeNet-5.",2021,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3040847,
725c81c956b574df77fc0cbe0b9856e17ca88e2c,0,1,A Performance Comparison of Loss Functions,"Generally, the deep neural network learns by way of a loss function, which is an approach to evaluate how well given dataset is predicted on a particular network architecture (or network model). If the prediction deviates too far from real data, a loss function would generate a very large value. Progressively, with the help of some optimization function, the loss function lowers the prediction error by providing the network architecture with information that can control the weights of the network architecture. Thus, the loss functions plays an important role in training the network architecture.Recently, several researchers have studied various loss functions such as Softmax, Modified softmax, Angular softmax, Additive-Margin softmax, Arcface, Center, and Focal losses. In this manuscript, we propose a new and simple loss function that just adds the existing loss functions. In addition, we conduct experiments with the MNIST dataset in order to compare the performance between all loss functions including the proposed and the existing loss functions. Resultingly, the experiments show that the proposed loss function is visibly superior to the ability to classify digit images. The experimental results also indicate that Arcface loss and Additive-Margin loss functions satisfy predefined test accuracy most quickly under two and three dimensional embedding, respectively. The fast learning ability of the both loss functions has the advantage of providing relatively high accuracy even when the number of train data is small.",2019,2019 International Conference on Information and Communication Technology Convergence (ICTC),,10.1109/ictc46691.2019.8939902,
726f76f11e904d7fcb12736c276a0b00eb5cde49,1,1,A Performance Comparison of Loss Functions for Deep Face Recognition,"Face recognition is one of the most widely publicized feature in the devices today and hence represents an important problem that should be studied with the utmost priority. As per the recent trends, the Convolutional Neural Network (CNN) based approaches are highly successful in many tasks of Computer Vision including face recognition. The loss function is used on the top of CNN to judge the goodness of any network. In this paper, we present a performance comparison of different loss functions such as Cross-Entropy, Angular Softmax, Additive-Margin Softmax, ArcFace and Marginal Loss for face recognition. The experiments are conducted with two CNN architectures namely, ResNet and MobileNet. Two widely used face datasets namely, CASIA-Webface and MS-Celeb-1M are used for the training and benchmark Labeled Faces in the Wild (LFW) face dataset is used for the testing.",2019,ArXiv,1901.05903,,https://arxiv.org/pdf/1901.05903.pdf
727d03100d4a8e12620acd7b1d1972bbee54f0e6,1,0,von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification,"A number of pattern recognition tasks, \textit{e.g.}, face verification, can be boiled down to classification or clustering of unit length directional feature vectors whose distance can be simply computed by their angle. In this paper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical foundation for an effective deep-learning of such directional features and derive a novel vMF Mixture Loss and its corresponding vMF deep features. The proposed vMF feature learning achieves the characteristics of discriminative learning, \textit{i.e.}, compacting the instances of the same class while increasing the distance of instances from different classes. Moreover, it subsumes a number of popular loss functions as well as an effective method in deep learning, namely normalization. We conduct extensive experiments on face verification using 4 different challenging face datasets, \textit{i.e.}, LFW, YouTube faces, CACD and IJB-A. Results show the effectiveness and excellent generalization ability of the proposed approach as it achieves state-of-the-art results on the LFW, YouTube faces and CACD datasets and competitive results on the IJB-A dataset.",2017,ArXiv,1706.04264,,https://arxiv.org/pdf/1706.04264.pdf
72acb3fc7147a8cb65e2baed4df1ce2c29ca059c,0,1,A Privacy-Preserving Filter for Oblique Face Images Based on Adaptive Hopping Gaussian Mixtures,"Photographs taken in public places often contain faces of bystanders thus leading to a perceived or actual violation of privacy. To address this issue, we propose to pseudo-randomly modify the appearance of face regions in the images using a privacy filter that prevents a human or a face recogniser from inferring the identity of people. The filter, which is applied only when the resolution is high enough for a face to be recognisable, adaptively distorts the face appearance as a function of its resolution. Moreover, the proposed filter locally changes the values of its parameters to counter attacks that attempt to estimate them. The filter exploits both global adaptiveness to reduce distortion and local parameter hopping to make their estimation difficult for an attacker. In order to evaluate the efficiency of the proposed approach, we consider an important scenario of oblique face images: photographs taken with low altitude Micro Aerial Vehicles (MAVs). We use a state-of-the-art face recognition algorithm and synthetically generated face data with 3D geometric image transformations that mimic faces captured from an MAV at different heights and pitch angles. Experimental results show that the proposed filter protects privacy while reducing distortion, and is also robust against attacks.",2019,IEEE Access,,10.1109/ACCESS.2019.2944861,
72ccaf2a39a297db03a3f7b6b70d1a0abe7b9f1d,1,0,Range Loss for Deep Face Recognition with Long-Tailed Training Data,"Deep convolutional neural networks have achieved significant improvements on face recognition task due to their ability to learn highly discriminative features from tremendous amounts of face images. Many large scale face datasets exhibit long-tail distribution where a small number of entities (persons) have large number of face images while a large number of persons only have very few face samples (long tail). Most of the existing works alleviate this problem by simply cutting the tailed data and only keep identities with enough number of examples. Unlike these work, this paper investigated how long-tailed data impact the training of face CNNs and develop a novel loss function, called range loss, to effectively utilize the tailed data in training process. More specifically, range loss is designed to reduce overall intrapersonal variations while enlarge interpersonal differences simultaneously. Extensive experiments on two face recognition benchmarks, Labeled Faces in the Wild (LFW) [11] and YouTube Faces (YTF) [33], demonstrate the effectiveness of the proposed range loss in overcoming the long tail effect, and show the good generalization ability of the proposed methods.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1611.08976,10.1109/ICCV.2017.578,http://www.public.asu.edu/~zfang29/papers/rangeloss_poster.pdf
730c1f9b703dfb36f2f8138e36fd0c9dde77b025,1,1,FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition,"This work summarizes the 2020 ChaLearn Looking at People Fair Face Recognition and Analysis Challenge and provides a description of the top-winning solutions and analysis of the results. The aim of the challenge was to evaluate accuracy and bias in gender and skin colour of submitted algorithms on the task of 1:1 face verification in the presence of other confounding attributes. Participants were evaluated using an in-the-wild dataset based on reannotated IJB-C, further enriched by 12.5K new images and additional labels. The dataset is not balanced, which simulates a real world scenario where AI-based models supposed to present fair outcomes are trained and evaluated on imbalanced data. The challenge attracted 151 participants, who made more than 1.8K submissions in total. The final phase of the challenge attracted 36 active teams out of which 10 exceeded 0.999 AUC-ROC while achieving very low scores in the proposed bias metrics. Common strategies by the participants were face pre-processing, homogenization of data distributions, the use of bias aware loss functions and ensemble models. The analysis of top-10 teams shows higher false positive rates (and lower false negative rates) for females with dark skin tone as well as the potential of eyeglasses and young age to increase the false positive rates too.",2020,ArXiv,2009.07838,,https://arxiv.org/pdf/2009.07838.pdf
730e906ff2f3ce3da874435298b844f9a763cfe0,1,1,P2SGrad: Refined Gradients for Optimizing Deep Face Models,"Cosine-based softmax losses significantly improve the performance of deep face recognition networks. However, these losses always include sensitive hyper-parameters which can make training process unstable, and it is very tricky to set suitable hyper parameters for a specific dataset. This paper addresses this challenge by directly designing the gradients for training in an adaptive manner. We first investigate and unify previous cosine softmax losses from the perspective of gradients. This unified view inspires us to propose a novel gradient called P2SGrad (Probability-to-Similarity Gradient), which leverages a cosine similarity instead of classification probability to control the gradients for updating neural network parameters. P2SGrad is adaptive and hyper-parameter free, which makes training process more efficient and faster. We evaluate our P2SGrad on three face recognition benchmarks, LFW, MegaFace, and IJB-C. The results show that P2SGrad is stable in training, robust to noise, and achieves state-of-the-art performance on all the three benchmarks.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1905.02479,10.1109/CVPR.2019.01014,https://arxiv.org/pdf/1905.02479.pdf
730f9c7f88cda4d3a97b8a444e88af3fe939d13d,1,0,Face-to-Music Translation Using a Distance-Preserving Generative Adversarial Network with an Auxiliary Discriminator,"Learning a mapping between two unrelated domains-such as image and audio, without any supervision is a challenging task. In this work, we propose a distance-preserving generative adversarial model to translate images of human faces into an audio domain. The audio domain is defined by a collection of musical note sounds recorded by 10 different instrument families (NSynth \cite{nsynth2017}) and a distance metric where the instrument family class information is incorporated together with a mel-frequency cepstral coefficients (MFCCs) feature. To enforce distance-preservation, a loss term that penalizes difference between pairwise distances of the faces and the translated audio samples is used. Further, we discover that the distance preservation constraint in the generative adversarial model leads to reduced diversity in the translated audio samples, and propose the use of an auxiliary discriminator to enhance the diversity of the translations while using the distance preservation constraint. We also provide a visual demonstration of the results and numerical analysis of the fidelity of the translations. A video demo of our proposed model's learned translation is available in this https URL.",2020,ArXiv,2006.13469,,https://arxiv.org/pdf/2006.13469.pdf
732be2eeda341df5d5b057b05c6d2f6e0eee1171,1,0,Extended YouTube Faces: a Dataset for Heterogeneous Open-Set Face Identification,"In this paper, we propose an extension of the famous YouTube Faces (YTF) dataset. In the YTF dataset, the goal was to state whether two videos contained the same subject or not (video-based face verification). We enrich YTF with still images and an identification protocol. In the classic face identification, given a probe image (or video), the correct identity has to be retrieved among the gallery ones; the main peculiarity of such protocol is that each probe identity has a correspondent in the gallery (closed-set). To resemble a realistic and practical scenario, we devised a protocol in which probe identities are not guaranteed to be in the gallery (open-set). Compared to a closed-set identification, the latter is definitely more challenging in as much as the system needs firstly to reject impostors (i.e., probe identities missing from the gallery), and subsequently, if the probe is accepted as genuine, retrieve the correct identity. In our case, the probe set is composed of full-length videos from the original dataset, while the gallery is composed of templates, i.e., sets of still images. To collect the images, an automatic application was developed. The main motivations behind this work can be found in both the lack of open-set identification protocols defined in the literature and the undeniable complexity of such. We also argued that extending an existing and widely used dataset could make its distribution easier and that data heterogeneity would make the problem even more challenging and realistic. We named the dataset Extended YTF (E-YTF). Finally, we report baseline recognition results using two well known DCNN architectures.11The dataset, metadata and protocols are available at https://www.micc.unifi.it/resources/datasets/e-ytf/",2018,2018 24th International Conference on Pattern Recognition (ICPR),1902.03804,10.1109/ICPR.2018.8545642,https://arxiv.org/pdf/1902.03804.pdf
73587f97500203b94a9f312b0b86891f62326679,0,1,How are attributes expressed in face DCNNs?,"As deep networks become increasingly accurate at recognizing faces, it is vital to understand how these networks process faces. While these networks are solely trained to recognize identities, they also contain face related information such as sex, age, and pose of the face. The networks are not trained to learn these attributes. We introduce expressivity as a measure of how much a feature vector informs us about an attribute, where a feature vector can be from internal or final layers of a network. Expressivity is computed by a second neural network whose inputs are features and attributes. The output of the second neural network approximates the mutual information between feature vectors and an attribute. We investigate the expressivity for two different deep convolutional neural network (DCNN) architectures: a Resnet-101 and an Inception Resnet v2. In the final fully connected layer of the networks, we found the order of expressivity for facial attributes to be Age > Sex > Yaw. Additionally, we studied the changes in the encoding of facial attributes over training iterations. We found that as training progresses, expressivities of yaw, sex, and age decrease. Our technique can be a tool for investigating the sources of bias in a network and a step towards explaining the network's identity decisions.",2019,ArXiv,1910.05657,,https://arxiv.org/pdf/1910.05657.pdf
7366b471bec0fe3cba441d7a15582c90d179b624,0,1,On Parameter Adaptation in Softmax-Based Cross-Entropy Loss for Improved Convergence Speed and Accuracy in DNN-Based Speaker Recognition,"In various classification tasks the major challenge is in generating discriminative representation of classes. By proper selection of deep neural network (DNN) loss function we can encourage it to produce embeddings with increased inter-class separation and smaller intra-class distances. In this paper, we develop softmax-based cross-entropy loss function which adapts its parameters to the current training phase. The proposed solution improves accuracy up to 24% in terms of Equal Error Rate (EER) and minimum Detection Cost Function (minDCF). In addition, our proposal also accelerates network convergence compared with other state-of-the-art softmax-based losses. As an additional contribution of this paper, we adopt and subsequently modify the ResNet DNN structure for the speaker recognition task. The proposed ResNet network achieves relative gains of up to 32% and 15% in terms of EER and minDCF respectively, compared with the well-established Time Delay Neural Network (TDNN) architecture for x-vector extraction.",2020,INTERSPEECH,,10.21437/interspeech.2020-2264,https://isca-speech.org/archive/Interspeech_2020/pdfs/2264.pdf
736968afc1b0312938d1298303cbadb04a58b700,0,1,Occlusion-guided compact template learning for ensemble deep network-based pose-invariant face recognition,"Concatenation of the deep network representations extracted from different facial patches helps to improve face recognition performance. However, the concatenated facial template increases in size and contains redundant information. Previous solutions aim to reduce the dimensionality of the facial template without considering the occlusion pattern of the facial patches. In this paper, we propose an occlusion-guided compact template learning (OGCTL) approach that only uses the information from visible patches to construct the compact template. The compact face representation is not sensitive to the number of patches that are used to construct the facial template and is more suitable for incorporating the information from different view angles for image-set based face recognition. Instead of using occlusion masks in face matching (e.g., DPRFS [38]), the proposed method uses occlusion masks in template construction and achieves significantly better image-set based face verification performance on a challenging database with a template size that is an order-of-magnitude smaller than DPRFS.",2019,ArXiv,1903.04752,,https://arxiv.org/pdf/1903.04752.pdf
737b5bc8adb28b0848b35ccb4a0e768b8a9620b0,0,1,Dysarthric Speech Recognition Based on Deep Metric Learning,"We present in this paper an automatic speech recognition (ASR) system for a person with an articulation disorder resulting from athetoid cerebral palsy. Because their utterances are often unstable or unclear, speech recognition systems have difficulty recognizing the speech of those with this disorder. For example, their speech styles often fluctuate greatly even when they are repeating the same sentences. For this reason, their speech tends to have great variation even within recognition classes. To alleviate this intra-class variation problem, we propose an ASR system based on deep metric learning. This system learns an embedded representation that is characterized by a small distance between input utterances of the same class, while the distance of the input utterances of different classes is large. Therefore, our method makes it easy for the ASR system to distinguish dysarthric speech. Experimental results show that our proposed approach using deep metric learning improves the word-recognition accuracy consistently. Moreover, we also evaluate the combination of our proposed method and transfer learning from unimpaired speech to alleviate the low-resource problem associated with impaired speech.",2020,INTERSPEECH,,10.21437/interspeech.2020-2267,https://isca-speech.org/archive/Interspeech_2020/pdfs/2267.pdf
7389b5776195f41bd681192c44313772c4dd3572,1,1,Dual Variational Generation for Low-Shot Heterogeneous Face Recognition,"Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. The related code is available at this https URL.",2019,NeurIPS,1903.10203,,https://arxiv.org/pdf/1903.10203.pdf
73c07e0a998576bb9d9409e5eed713788c0be037,1,1,Large-Scale Long-Tailed Recognition in an Open World,"Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale OLTR datasets we curate from object-centric ImageNet, scene-centric Places, and face-centric MS1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future OLTR research and are publicly available at \url{https://liuziwei7.github.io/projects/LongTail.html}.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.0516,10.1109/CVPR.2019.00264,https://arxiv.org/pdf/1904.05160.pdf
73e301ac549cce16e32553da96110e143d461142,0,1,Face representation by deep learning: a linear encoding in a parameter space?,"Recently, Convolutional Neural Networks (CNNs) have achieved tremendous performances on face recognition, and one popular perspective regarding CNNs' success is that CNNs could learn discriminative face representations from face images with complex image feature encoding. However, it is still unclear what is the intrinsic mechanism of face representation in CNNs. In this work, we investigate this problem by formulating face images as points in a shape-appearance parameter space, and our results demonstrate that: (i) The encoding and decoding of the neuron responses (representations) to face images in CNNs could be achieved under a linear model in the parameter space, in agreement with the recent discovery in primate IT face neurons, but different from the aforementioned perspective on CNNs' face representation with complex image feature encoding; (ii) The linear model for face encoding and decoding in the parameter space could achieve close or even better performances on face recognition and verification than state-of-the-art CNNs, which might provide new lights on the design strategies for face recognition systems; (iii) The neuron responses to face images in CNNs could not be adequately modelled by the axis model, a model recently proposed on face modelling in primate IT cortex. All these results might shed some lights on the often complained blackbox nature behind CNNs' tremendous performances on face recognition.",2019,ArXiv,1910.09768,,https://arxiv.org/pdf/1910.09768.pdf
73f38254a39b0dfa7447477100d580a766847d60,0,1,Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agenda,"Participation on social media platforms has many benefits but also poses substantial threats. Users often face an unintended loss of privacy, are bombarded with mis-/disinformation, or are trapped in filter bubbles due to over-personalized content. These threats are further exacerbated by the rise of hidden AI-driven algorithms working behind the scenes to shape users' thoughts, attitudes, and behaviour. We investigate how multimedia researchers can help tackle these problems to level the playing field for social media users. We perform a comprehensive survey of algorithmic threats on social media and use it as a lens to set a challenging but important research agenda for effective and real-time user nudging. We further implement a conceptual prototype and evaluate it with experts to supplement our research agenda. This paper calls for solutions that combat the algorithmic threats on social media by utilizing machine learning and multimedia content analysis techniques but in a transparent manner and for the benefit of the users.",2020,ACM Multimedia,2009.07632,10.1145/3394171.3414692,https://arxiv.org/pdf/2009.07632.pdf
7465f26142c243e95cbf9adbfd6544c1a704e51f,0,1,A Kernel Density Estimation Based Loss Function and its Application to ASV-Spoofing Detection,"Biometric systems are exposed to spoofing attacks which may compromise their security, and voice biometrics, also known as automatic speaker verification (ASV), is no exception. Replay, synthesis and voice conversion attacks cause false acceptances that can be detected by anti-spoofing systems. Recently, deep neural networks (DNNs) which extract embedding vectors have shown superior performance than conventional systems in both ASV and anti-spoofing tasks. In this work, we develop a new concept of loss function for training DNNs which is based on kernel density estimation (KDE) techniques. The proposed loss functions estimate the probability density function (pdf) of every training class in each mini-batch, and compute a log likelihood matrix between the embedding vectors and pdfs of all training classes within the mini-batch in order to obtain the KDE-based loss. To evaluate our proposal for spoofing detection, experiments were carried out on the recent ASVspoof 2019 corpus, including both logical and physical access scenarios. The experimental results show that training a DNN based anti-spoofing system with our proposed loss functions clearly outperforms the performance of the same system being trained with other well-known loss functions. Moreover, the results also show that the proposed loss functions are effective for different types of neural network architectures.",2020,IEEE Access,,10.1109/ACCESS.2020.3000641,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09110494.pdf
74bfaacd4e86a1304d2b5e7340591cffb38d84dd,0,1,SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification,"Abstract Many current successful Person Re-Identification (ReID) methods train a model with the softmax loss function to classify images of different persons and obtain the feature vectors at the same time. However, the underlying feature embedding space is ignored. In this paper, we use a modified softmax function, termed Sphere Softmax, to solve the classification problem and learn a hypersphere manifold embedding simultaneously. A balanced sampling strategy is also introduced. Finally, we propose a convolutional neural network called SphereReID adopting Sphere Softmax and training a single model end-to-end with a new warming-up learning rate schedule on four challenging datasets including Market-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results demonstrate that this single model outperforms the state-of-the-art methods on all four datasets without fine-tuning or re-ranking. For example, it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID. The code and trained weights of our model will be released.",2019,J. Vis. Commun. Image Represent.,1807.00537,10.1016/j.jvcir.2019.01.010,https://arxiv.org/pdf/1807.00537.pdf
750486b2278ba542987655eede1b4da8c10f7f2e,1,1,Softmax Dissection: Towards Understanding Intra- and Inter-clas Objective for Embedding Learning,"The softmax loss and its variants are widely used as objectives for embedding learning, especially in applications like face recognition. However, the intra- and inter-class objectives in the softmax loss are entangled, therefore a well-optimized inter-class objective leads to relaxation on the intra-class objective, and vice versa. In this paper, we propose to dissect the softmax loss into independent intra- and inter-class objective (D-Softmax). With D-Softmax as objective, we can have a clear understanding of both the intra- and inter-class objective, therefore it is straightforward to tune each part to the best state. Furthermore, we find the computation of the inter-class objective is redundant and propose two sampling-based variants of D-Softmax to reduce the computation cost. Training with regular-scale data, experiments in face verification show D-Softmax is favorably comparable to existing losses such as SphereFace and ArcFace. Training with massive-scale data, experiments show the fast variants of D-Softmax significantly accelerates the training process (such as 64x) with only a minor sacrifice in performance, outperforming existing acceleration methods of softmax in terms of both performance and efficiency.",2020,AAAI,1908.01281,10.1609/AAAI.V34I07.6729,https://arxiv.org/pdf/1908.01281.pdf
7522679b432649785f1355a7013d448b837a08c2,0,1,Facial Emotion Recognition using Neighborhood Features,"We present a new method for human facial emotions recognition. For this purpose, initially, we detect faces in the images by using the famous cascade classifiers. Subsequently, we then extract a localized regional descriptor (LRD) which represents the features of a face based on regional appearance encoding. The LRD formulates and models various spatial regional patterns based on the relationships between local areas themselves instead of considering only raw and unprocessed intensity features of an image. To classify facial emotions into various classes of facial emotions, we train a multiclass support vector machine (M-SVM) classifier which recognizes these emotions during the testing stage. Our proposed method takes into account robust features and is independent of gender and facial skin color for emotion recognition. Moreover, our method is illumination and orientation invariant. We assessed our method on two benchmark datasets and compared it with four reference methods. Our proposed method outperformed them considering both the datasets.",2020,,,10.14569/ijacsa.2020.0110137,https://pdfs.semanticscholar.org/7522/679b432649785f1355a7013d448b837a08c2.pdf
75928919724534e242b23f603b23347eb1fad919,0,1,Plastic Surgery: An Obstacle for Deep Face Recognition?,"The impacts of plastic surgery on face recognition systems have been investigated in the past decade by many researchers. Diverse well-known face recognition approaches, e.g. based on PCA or LBP, have been bench-marked mostly on the web-collected IIITD plastic surgery face database. Generally, significant performance drops were reported when comparing facial images taken before and after plastic surgeries. On the one side, some researchers reported problems with said plastic surgery database, i.e. the presence of low quality images. On the other side, the applied methods no longer reflect the state-of-the-art in face recognition. This calls for evaluating the impact of plastic surgery on state-of-the-art deep face recognition systems anew considering high quality imagery of most relevant plastic surgeries.This work introduces the new Hochschule Darmstadt (HDA) plastic surgery database of facial images taken before and after surgery. This database vastly complies with the quality requirements defined by the International Civil Aviation Organization (ICAO) for electronic travel documents and comprises face images of the five most frequently applied facial plastic surgeries. The HDA plastic surgery database, the IIITD plastic surgery database, and a non-surgery database, i.e. ICAO-compliant subsets of the FRGCv2 and FERET datasets, are used for comparative verification and identification evaluations which are conducted using the commercial Cognitec FaceVACS system and the open-source ArcFace system. The obtained results suggest that the impact of plastic surgery on deep face recognition systems is less significant than that observed for previously benchmarked methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00411,
75976bdad3cbdfe41f1d9852cb5a7bafca19a06c,1,1,Long-Tailed Recognition Using Class-Balanced Experts,"Classic deep learning methods achieve impressive results in image recognition over large-scale artificially-balanced datasets. However, real-world datasets exhibit highly class-imbalanced distributions. In this work we address the problem of long tail recognition wherein the training set is highly imbalanced and the test set is kept balanced. The key challenges faced by any long tail recognition technique are relative imbalance amongst the classes and data scarcity or unseen concepts for mediumshot or fewshot classes. Existing techniques rely on data-resampling, cost sensitive learning, online hard example mining, reshaping the loss objective and complex memory based models to address this problem. We instead propose an ensemble of experts technique that decomposes the imbalanced problem into multiple balanced classification problems which are more tractable. Our ensemble of experts reaches close to state-of-the-art results and an extended ensemble establishes new state-of-the-art on two benchmarks for long tail recognition. We conduct numerous experiments to analyse the performance of the ensemble, and show that in modern datasets relative imbalance is a harder problem than data scarcity.",2020,ArXiv,2004.03706,,https://arxiv.org/pdf/2004.03706.pdf
75d5b4a493d83320f83dd1c942f4e889792d9c3e,1,0,Adversarial Graph Representation Adaptation for Cross-Domain Facial Expression Recognition,"Data inconsistency and bias are inevitable among different facial expression recognition (FER) datasets due to subjective annotating process and different collecting conditions. Recent works resort to adversarial mechanisms that learn domain-invariant features to mitigate domain shift. However, most of these works focus on holistic feature adaptation, and they ignore local features that are more transferable across different datasets. Moreover, local features carry more detailed and discriminative content for expression recognition, and thus integrating local features may enable fine-grained adaptation. In this work, we propose a novel Adversarial Graph Representation Adaptation (AGRA) framework that unifies graph representation propagation with adversarial learning for cross-domain holistic-local feature co-adaptation. To achieve this, we first build a graph to correlate holistic and local regions within each domain and another graph to correlate these regions across different domains. Then, we learn the per-class statistical distribution of each domain and extract holistic-local features from the input image to initialize the corresponding graph nodes. Finally, we introduce two stacked graph convolution networks to propagate holistic-local feature within each domain to explore their interaction and across different domains for holistic-local feature co-adaptation. In this way, the AGRA framework can adaptively learn fine-grained domain-invariant features and thus facilitate cross-domain expression recognition. We conduct extensive and fair experiments on several popular benchmarks and show that the proposed AGRA framework achieves superior performance over previous state-of-the-art methods.",2020,ACM Multimedia,2008.00859,10.1145/3394171.3413822,https://arxiv.org/pdf/2008.00859.pdf
768f6a14a7903099729872e0db231ea814eb05e9,1,0,De-Mark GAN: Removing Dense Watermark with Generative Adversarial Network,"This paper mainly considers the MeshFace verification problem with dense watermarks. A dense watermark often covers the crucial parts of face photo, thus degenerating the performance in the existing face verification system. The key to solving it is to preserve the ID information while removing the dense watermark. In this paper, we propose an improved GAN model, named De-mark GAN, for MeshFace verification. It consists of one generator and one global-internal discriminator. The generator is an encoderdecoder architecture with a pixel reconstruction loss and a feature loss. It maps a MeshFace photo to a representation vector, and then decodes the vector to a RGB ID photo. The succedent global-internal discriminator integrates a global discriminator and an internal discriminator with a global loss and internal loss, respectively. It can ensure the generated image quality and preserve the the ID information of recovered ID photos. Experimental results show that the verification benefits well from the recovered ID photos with high quality and our proposed De-mark GAN can achieve a competitive result in both image quality and verification.",2018,2018 International Conference on Biometrics (ICB),,10.1109/ICB2018.2018.00021,http://www.cbsr.ia.ac.cn/users/zlei/papers/JLWU-ICB-2018.pdf
769b03fca1aeceafdd2cbfb82216c6e616f69c3d,0,1,Long-Term Face Tracking for Crowded Video-Surveillance Scenarios,"Most current multi-object trackers focus on short-term tracking, and are based on deep and complex systems that do not operate in real-time, often making them impractical for video-surveillance. In this paper, we present a long-term multi-face tracking architecture conceived for working in crowded contexts, particularly unconstrained in terms of movement and occlusions, and where the face is often the only visible part of the person. Our system benefits from advances in the fields of face detection and face recognition to achieve long-term tracking. It follows a tracking-by-detection approach, combining a fast short-term visual tracker with a novel online tracklet reconnection strategy grounded on face verification. Additionally, a correction module is included to correct past track assignments with no extra computational cost. We present a series of experiments introducing novel, specialized metrics for the evaluation of long-term tracking capabilities and a video dataset that we publicly release. Findings demonstrate that, in this context, our approach allows to obtain up to 50% longer tracks than state-of-the-art deep learning trackers.",2020,ArXiv,2010.08675,,https://arxiv.org/pdf/2010.08675.pdf
76beca47e3face28c1e008414870d8a5230f84b6,1,1,Co-Mining: Deep Face Recognition With Noisy Labels,"Face recognition has achieved significant progress with the growing scale of collected datasets, which empowers us to train strong convolutional neural networks (CNNs). While a variety of CNN architectures and loss functions have been devised recently, we still have a limited understanding of how to train the CNN models with the label noise inherent in existing face recognition datasets. To address this issue, this paper develops a novel co-mining strategy to effectively train on the datasets with noisy labels. Specifically, we simultaneously use the loss values as the cue to detect noisy labels, exchange the high-confidence clean faces to alleviate the errors accumulated issue caused by the sample-selection bias, and re-weight the predicted clean faces to make them dominate the discriminative model training in a mini-batch fashion. Extensive experiments by training on three popular datasets (\textit{i.e.}, CASIA-WebFace, MS-Celeb-1M and VggFace2) and testing on several benchmarks, including LFW, AgeDB, CFP, CALFW, CPLFW, RFW, and MegaFace, have demonstrated the effectiveness of our new approach over the state-of-the-art alternatives.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00945,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf
76bf211f21db11aee9fac5377cf7ccdfd22a570f,0,1,Learning Clusterable Visual Features for Zero-Shot Recognition,"In zero-shot learning (ZSL), conditional generators have been widely used to generate additional training features. These features can then be used to train the classifiers for testing data. However, some testing data are considered ""hard"" as they lie close to the decision boundaries and are prone to misclassification, leading to performance degradation for ZSL. In this paper, we propose to learn clusterable features for ZSL problems. Using a Conditional Variational Autoencoder (CVAE) as the feature generator, we project the original features to a new feature space supervised by an auxiliary classification loss. To further increase clusterability, we fine-tune the features using Gaussian similarity loss. The clusterable visual features are not only more suitable for CVAE reconstruction but are also more separable which improves classification accuracy. Moreover, we introduce Gaussian noise to enlarge the intra-class variance of the generated features, which helps to improve the classifier's robustness. Our experiments on SUN,CUB, and AWA2 datasets show consistent improvement over previous state-of-the-art ZSL results by a large margin. In addition to its effectiveness on zero-shot classification, experiments show that our method to increase feature clusterability benefits few-shot learning algorithms as well.",2020,ArXiv,2010.03245,,https://arxiv.org/pdf/2010.03245.pdf
76cd5e43df44e389483f23cb578a9015d1483d70,1,0,Face Verification from Depth using Privileged Information,"In this paper, a deep Siamese architecture for depth-based face verification is presented. The proposed approach efficiently verifies if two face images belong to the same person while handling a great variety of head poses and occlusions. The architecture, namely JanusNet, consists in a combination of a depth, a RGB and a hybrid Siamese network. During the training phase, the hybrid network learns to extract complementary mid-level convolutional features which mimic the features of the RGB network, simultaneously leveraging on the light invariance of depth images. At testing time, the model, relying only on depth data, achieves state-of-art results and real time performance, despite the lack of deep-oriented depth-based datasets.",2018,BMVC,,,http://bmvc2018.org/contents/papers/0410.pdf
76e333e5941a91da1fc4c5ddf9a06d02fea01496,1,0,FaceGenderID: Exploiting Gender Information in DCNNs Face Recognition Systems,"This paper addresses the effect of gender as a covariate in face verification systems. Even though pre-trained models based on Deep Convolutional Neural Networks (DCNNs), such as VGG-Face or ResNet-50, achieve very high performance, they are trained on very large datasets comprising millions of images, which have biases regarding demographic aspects like the gender and the ethnicity among others. In this work, we first analyse the separate performance of these state-of-the-art models for males and females. We observe a gap between face verification performances obtained by both gender classes. These results suggest that features obtained by biased models are affected by the gender covariate. We propose a gender-dependent training approach to improve the feature representation for both genders, and develop both: i) gender specific DCNNs models, and ii) a gender balanced DCNNs model. Our results show significant and consistent improvements in face verification performance for both genders, individually and in general with our proposed approach. Finally, we announce the availability (at GitHub) of the FaceGenderID DCNNs models proposed in this work, which can support further experiments on this topic.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00278,http://openaccess.thecvf.com/content_CVPRW_2019/papers/BEFA/Vera-Rodriguez_FaceGenderID_Exploiting_Gender_Information_in_DCNNs_Face_Recognition_Systems_CVPRW_2019_paper.pdf
76ee3aae926dc2ccd5446342d59682cb35e3d0ca,1,0,Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation,"Facial attributes (e.g., age and attractiveness) estimation performance has been greatly improved by using convolutional neural networks. However, existing methods have an inconsistency between the training objectives and the evaluation metric, so they may be suboptimal. In addition, these methods always adopt image classification or face recognition models with a large amount of parameters, which carry expensive computation cost and storage overhead. In this paper, we firstly analyze the essential relationship between two state-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking method is in fact learning label distribution implicitly. This result thus firstly unifies two existing popular state-of-the-art methods into the DLDL framework. Second, in order to alleviate the inconsistency and reduce resource consumption, we design a lightweight network architecture and propose a unified framework which can jointly learn facial attribute distribution and regress attribute value. The effectiveness of our approach has been demonstrated on both facial age and attractiveness estimation tasks. Our method achieves new state-of-the-art results using the single model with 36$\times$(6$\times$) fewer parameters and 2.6$\times$(2.1$\times$) faster inference speed on facial age (attractiveness) estimation. Moreover, our method can achieve comparable results as the state-of-the-art even though the number of parameters is further reduced to 0.9M (3.8MB disk storage).",2020,ArXiv,2007.01771,,https://arxiv.org/pdf/2007.01771.pdf
76ef93ff9172867f8dea04b3266a782e12cc5a70,0,1,Google Landmark Recognition 2020 Competition Third Place Solution,"We present our third place solution to the Google Landmark Recognition 2020 competition. It is an ensemble of global features only Sub-center ArcFace models. We introduce dynamic margins for ArcFace loss, a family of tune-able margin functions of class size, designed to deal with the extreme imbalance in GLDv2 dataset. Progressive finetuning and careful postprocessing are also key to the solution. Our two submissions scored 0.6344 and 0.6289 on private leaderboard, both ranking third place out of 736 teams.",2020,ArXiv,2010.0535,,https://arxiv.org/pdf/2010.05350.pdf
773391cbaa5295d4e0ea1a3ee004ea25d2da7052,0,1,Enhancing Open-Set Face Recognition by Closing It with Cluster-Inferred Gallery Augmentation,,2019,ACPR,,10.1007/978-3-030-41299-9_2,
7749009cfcc05b096811e10696dcee424409c39f,1,0,Residual Compensation Networks for Heterogeneous Face Recognition,"Heterogeneous Face Recognition (HFR) is a challenging task due to large modality discrepancy as well as insufficient training images in certain modalities. In this paper, we propose a new two-branch network architecture, termed as Residual Compensation Networks (RCN), to learn separated features for different modalities in HFR. The RCN incorporates a residual compensation (RC) module and a modality discrepancy loss (MD loss) into traditional convolutional neural networks. The RC module reduces modal discrepancy by adding compensation to one of the modalities so that its representation can be close to the other modality. The MD loss alleviates modal discrepancy by minimizing the cosine distance between different modalities. In addition, we explore different architectures and positions for the RC module, and evaluate different transfer learning strategies for HFR. Extensive experiments on IIIT-D Viewed Sketch, Forensic Sketch, CASIA NIR-VIS 2.0 and CUHK NIR-VIS show that our RCN outperforms other state-of-the-art methods significantly.",2019,AAAI,,10.1609/AAAI.V33I01.33018239,https://pdfs.semanticscholar.org/7749/009cfcc05b096811e10696dcee424409c39f.pdf
77501e770f6b45a949fd9ea41f8f1775da821bba,0,1,Automatic Face Aging in Videos via Deep Reinforcement Learning,"This paper presents a novel approach for synthesizing automatically age-progressed facial images in video sequences using Deep Reinforcement Learning. The proposed method models facial structures and the longitudinal face-aging process of given subjects coherently across video frames. The approach is optimized using a long-term reward, Reinforcement Learning function with deep feature extraction from Deep Convolutional Neural Network. Unlike previous age-progression methods that are only able to synthesize an aged likeness of a face from a single input image, the proposed approach is capable of age-progressing facial likenesses in videos with consistently synthesized facial features across frames. In addition, the deep reinforcement learning method guarantees preservation of the visual identity of input faces after age-progression. Results on videos of our new collected aging face AGFW-v2 database demonstrate the advantages of the proposed solution in terms of both quality of age-progressed faces, temporal smoothness, and cross-age face verification.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1811.11082,10.1109/CVPR.2019.01025,https://arxiv.org/pdf/1811.11082.pdf
7777cc8293d554509e6a9a645b3c97a8f97cda41,0,1,GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce,"In this paper, we present GrokNet, a deployed image recognition system for commerce applications. GrokNet leverages a multi-task learning approach to train a single computer vision trunk. We achieve a 2.1x improvement in exact product match accuracy when compared to the previous state-of-the-art Facebook product recognition system. We achieve this by training on 7 datasets across several commerce verticals, using 80 categorical loss functions and 3 embedding losses. We share our experience of combining diverse sources with wide-ranging label semantics and image statistics, including learning from human annotations, user-generated tags, and noisy search engine interaction data. GrokNet has demonstrated gains in production applications and operates at Facebook scale.",2020,KDD,,10.1145/3394486.3403311,
779465606b176b9083a8ef8a32b93959eb096cbe,0,1,TCN: Transferable Coupled Network for Cross-Resolution Face Recognition*,"Cross-resolution face recognition (CRFR) aims to learn the matching of a low-resolution (LR) probe image with a database of high-resolution (HR) gallery images. Existing methods including super resolution and projection-based algorithms are not recognition-oriented and computationally expensive, or ignore the inter-class associations across resolutions. To address the issues, we propose a novel end-to-end Transferable Coupled Network (TCN) for CRFR. Specifically, the TCN consists of two networks for the HR and LR domains, respectively. To reduce the resolution mismatch, a transferrable triple loss (TTL) is introduced to pull together cross-resolution positive pairs (intra-class) and also enforce margins towards negative ones (inter-class) from both domains. Besides, to keep stability and faster convergence, a novel online triplet selection method is proposed. Empirically, the proposed TCN model consistently outperforms the state-of-the-art methods among various low resolutions and architectures on public LFW and SCFace benchmarks.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8682384,
779904cb019c9d622b90948c99aded91cab9524a,1,0,Facial-based Intrusion Detection System with Deep Learning in Embedded Devices,"With the advent of deep learning based methods, facial recognition algorithms have become more effective and efficient. However, these algorithms have usually the disadvantage of requiring the use of dedicated hardware devices, such as graphical processing units (GPUs), which pose restrictions on their usage on embedded devices with limited computational power.  In this paper, we present an approach that allows building an intrusion detection system, based on face recognition, running on embedded devices. It relies on deep learning techniques and does not exploit the GPUs. Face recognition is performed using a knn classifier on features extracted from a 50-layers Residual Network (ResNet-50) trained on the VGGFace2 dataset. In our experiment, we determined the optimal confidence threshold that allows distinguishing legitimate users from intruders.  In order to validate the proposed system, we created a ground truth composed of 15,393 images of faces and 44 identities, captured by two smart cameras placed in two different offices, in a test period of six months. We show that the obtained results are good both from the efficiency and effectiveness perspective.",2018,SSIP 2018,,10.1145/3290589.3290598,
77d2963441c35514fcc10465b46025cc94797a76,0,1,Asymmetric metric learning for knowledge transfer,"Knowledge transfer from large teacher models to smaller student models has recently been studied for metric learning, focusing on fine-grained classification. In this work, focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we introduce asymmetric metric learning, a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task.  We systematically evaluate different teacher and student models, metric learning and knowledge transfer loss functions on the new asymmetric testing as well as the standard symmetric testing task, where database and queries are represented by the same model. We find that plain regression is surprisingly effective compared to more complex knowledge transfer mechanisms, working best in asymmetric testing. Interestingly, our asymmetric metric learning approach works best in symmetric testing, allowing the student to even outperform the teacher.",2020,ArXiv,2006.16331,,https://arxiv.org/pdf/2006.16331.pdf
77db92aeee3062161cc7e3b142f19563caef6756,1,0,IJB–S: IARPA Janus Surveillance Video Benchmark,"We present IJB–S dataset, an open-source IARPA Janus Surveillance Video Benchmark and associated protocols. The dataset consists of images and surveillance video collected from 202 subjects at a Department of Defense (DoD) training facility. Surveillance video was captured across multiple vignettes representative of a variety of real-world surveillance use cases that are particularly of interest to law enforcement and national security communities. Each video was annotated by human subject matter experts in order to generate ground truth identity and bounding box face labels. In total, over 10 million annotations were collected for the dataset. We present benchmark results utilizing state of the art deep learning approaches such as FaceNet. Our results illustrate and characterize the difficulty of the dataset.",2018,"2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)",,10.1109/BTAS.2018.8698584,http://biometrics.cse.msu.edu/Publications/Face/Kalkaetal_IJBSIARPPAJanusSurveillanceVideoBenchmark_BTAS2018.pdf
77e90d9fc582abc02f2f63e2f20f2ec035cf65e8,1,1,Cartoon Face Recognition: A Benchmark Dataset,"Recent years have witnessed increasing attention in cartoon media, powered by the strong demands of industrial applications. As the first step to understand this media, cartoon face recognition is a crucial but less-explored task with few datasets proposed. In this work, we first present a new challenging benchmark dataset, consisting of 389,678 images of 5,013 cartoon characters annotated with identity, bounding box, pose, and other auxiliary attributes. The dataset, named iCartoonFace, is currently the largest-scale, high-quality, rich-annotated, and spanning multiple occurrences in the field of image recognition, including near-duplications, occlusions, and appearance changes. In addition, we provide two types of annotations for cartoon media, i.e., face recognition, and face detection, with the help of a semi-automatic labeling algorithm. To further investigate this challenging dataset, we propose a multi-task domain adaptation approach that jointly utilizes the human and cartoon domain knowledge with three discriminative regularizations. We hence perform a benchmark analysis of the proposed dataset and verify the superiority of the proposed approach in the cartoon face recognition task. The dataset is available at https://iqiyi.cn/icartoonface.",2020,ACM Multimedia,1907.13394,10.1145/3394171.3413726,https://arxiv.org/pdf/1907.13394.pdf
77ee71a1c62b1464c5b9bda4333e172c0626e635,1,0,A Scalable Approach for Facial Action Unit Classifier Training Using Noisy Data for Pre-Training,"Machine learning systems are being used to automate many types of laborious labeling tasks. Facial actioncoding is an example of such a labeling task that requires copious amounts of time and a beyond average level of human domain expertise. In recent years, the use of end-to-end deep neural networks has led to significant improvements in action unit recognition performance and many network architectures have been proposed. Do the more complex deep neural network(DNN) architectures perform sufficiently well to justify the additional complexity? We show that pre-training on a large diverse set of noisy data can result in even a simple CNN model improving over the current state-of-the-art DNN architectures.The average F1-score achieved with our proposed method on the DISFA dataset is 0.60, compared to a previous state-of-the-art of 0.57. Additionally, we show how the number of subjects and number of images used for pre-training impacts the model performance. The approach that we have outlined is open-source, highly scalable, and not dependent on the model architecture. We release the code and data: this https URL.",2019,ArXiv,1911.05946,,https://arxiv.org/pdf/1911.05946.pdf
7834636155c6df9e674317ab2d151aace11ecc21,1,0,Unsupervised Domain-Specific Deblurring via Disentangled Representations,"Image deblurring aims to restore the latent sharp images from the corresponding blurred ones. In this paper, we present an unsupervised method for domain-specific, single-image deblurring based on disentangled representations. The disentanglement is achieved by splitting the content and blur features in a blurred image using content encoders and blur encoders. We enforce a KL divergence loss to regularize the distribution range of extracted blur attributes such that little content information is contained. Meanwhile, to handle the unpaired training data, a blurring branch and the cycle-consistency loss are added to guarantee that the content structures of the deblurred results match the original images. We also add an adversarial loss on deblurred results to generate visually realistic images and a perceptual loss to further mitigate the artifacts. We perform extensive experiments on the tasks of face and text deblurring using both synthetic datasets and real images, and achieve improved results compared to recent state-of-the-art deblurring methods.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.01594,10.1109/CVPR.2019.01047,https://arxiv.org/pdf/1903.01594.pdf
788ed02e387c36f1a389bdc8ba4489b1f22912c2,0,1,Facial UV map completion for pose-invariant face recognition: a novel adversarial approach based on coupled attention residual UNets,"Pose-invariant face recognition refers to the problem of identifying or verifying a person by analyzing face images captured from different poses. This problem is challenging due to the large variation of pose, illumination and facial expression. A promising approach to deal with pose variation is to fulfill incomplete UV maps extracted from in-the-wild faces, then attach the completed UV map to a fitted 3D mesh and finally generate different 2D faces of arbitrary poses. The synthesized faces increase the pose variation for training deep face recognition models and reduce the pose discrepancy during the testing phase. In this paper, we propose a novel generative model called Attention ResCUNet-GAN to improve the UV map completion. We enhance the original UV-GAN by using a couple of U-Nets. Particularly, the skip connections within each U-Net are boosted by attention gates. Meanwhile, the features from two U-Nets are fused with trainable scalar weights. The experiments on the popular benchmarks, including Multi-PIE, LFW, CPLWF and CFP datasets, show that the proposed method yields superior performance compared to other existing methods.",2020,Hum. centric Comput. Inf. Sci.,2011.00912,10.1186/s13673-020-00250-w,https://arxiv.org/pdf/2011.00912.pdf
78b6a569f579a4dfe8853bdc623bdc20fc95016e,1,1,RBF-Softmax: Learning Deep Representative Prototypes with Radial Basis Function Softmax,"Deep neural networks have achieved remarkable successes in learning feature representations for visual classification. However, deep features learned by the softmax cross-entropy loss generally show excessive intra-class variations. We argue that, because the traditional softmax losses aim to optimize only the relative differences between intra-class and inter-class distances (logits), it cannot obtain representative class prototypes (class weights/centers) to regularize intra-class distances, even when the training is converged. Previous efforts mitigate this problem by introducing auxiliary regularization losses. But these modified losses mainly focus on optimizing intra-class compactness, while ignoring keeping reasonable relations between different class prototypes. These lead to weak models and eventually limit their performance. To address this problem, this paper introduces a novel Radial Basis Function (RBF) distances to replace the commonly used inner products in the softmax loss function, such that it can adaptively assign losses to regularize the intra-class and inter-class distances by reshaping the relative differences, and thus creating more representative prototypes of classes to improve optimization. The proposed RBF-Softmax loss function not only effectively reduces intra-class distances, stabilizes the training behavior, and reserves ideal relations between prototypes, but also significantly improves the testing performance. Experiments on visual recognition benchmarks including MNIST, CIFAR-10/100, and ImageNet demonstrate that the proposed RBF-Softmax achieves better results than cross-entropy and other state-of-the-art classification losses. The code is at https://github.com/2han9x1a0release/RBF-Softmax.",2020,ECCV,,10.1007/978-3-030-58574-7_18,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710290.pdf
78bf2dfce251eaf76f69b062e2f688a0fbafe279,1,0,Facial Biometric Template Post-processing by Factorization,"Facial recognition systems have reached a relatively mature stage nowadays. In a typical facial recognition system, an input facial image is converted into a biometric template that is matched against stored templates in the database to have a final decision. This paper addresses a post-processing step applied to the biometric template in order to improve the overall system's recognition accuracy. The biometric template is built upon the well known Local Binary Pattern (LBP) method that is considered one of the state-of-the-art approaches for feature extraction in the field. As novel step, we have included a postprocessing module the template that decomposes the template into factors via Non-negative Matrix Factorization (NMF). By so doing we keep only the discriminative information while disregarding irrelevant information. By projecting the initial template into the NMF factors, we showed that the accuracy is increased compared to the case where this step is skipped. The method is applied to a challenging facial database, namely, the extended Yale Face Database B, faces acquired under difficult illumination conditions.",2019,"2019 International Symposium on Signals, Circuits and Systems (ISSCS)",,10.1109/ISSCS.2019.8801730,
78cc58c954c5e03c8ff910a06eeead6694b9854f,0,1,FusiformNet: Extracting Discriminative Facial Features on Different Levels,"Over the last several years, research on facial recognition based on Deep Neural Network has evolved with approaches like task-specific loss functions, image normalization and augmentation, network architectures, etc. However, there have been few approaches with attention to how human faces differ from person to person. Premising that inter-personal differences are found both generally and locally on the human face, I propose FusiformNet, a novel framework for feature extraction that leverages the nature of discriminative facial features. Tested on Image-Unrestricted setting of Labeled Face in the Wild benchmark, this method achieved a state-of-the-art accuracy of 96.67% without labeled outside data, image augmentation, normalization, or special loss functions. Likewise, the method also performed on par with previous state-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its ability to extract both general and local facial features, the utility of FusiformNet may not be limited to facial recognition but also extend to other DNN-based tasks.",2020,ArXiv,2011.00577,,https://arxiv.org/pdf/2011.00577.pdf
796fedd4b6dcdd709e6f721be3c4c5c35e4fdb63,1,0,Dynamic Deep Multi-task Learning for Caricature-Visual Face Recognition,"Rather than the visual images, the face recognition of the caricatures is far from the performance of the visual images. The challenge is the extreme non-rigid distortions of the caricatures introduced by exaggerating the facial features to strengthen the characters. In this paper, we propose dynamic multi-task learning based on deep CNNs for cross-modal caricature-visual face recognition. Instead of the conventional multi-task learning with fixed weights of the tasks, the proposed dynamic multi-task learning dynamically updates the weights of tasks according to the importance of the tasks, which enables the training of the networks focus on the hard task instead of being stuck in the overtraining of the easy task. The experimental results demonstrate the effectiveness of the dynamic multi-task learning for caricature-visual face recognition. The performance evaluated on the datasets CaVI and WebCaricature show the superiority over the state-of-art methods. The implementation code is available here.",2019,2019 International Conference on Document Analysis and Recognition Workshops (ICDARW),1911.03341,10.1109/ICDARW.2019.00021,https://arxiv.org/pdf/1911.03341.pdf
79c6b9f8e13463297e40c676227fc2478df0ea86,0,1,Privacy-Preserving Visual Feature Descriptors through Adversarial Affine Subspace Embedding,"Many computer vision systems require users to upload image features to the cloud for processing and storage. Such features can be exploited to recover sensitive information about the scene or subjects, e.g., by reconstructing the appearance of the original image. To address this privacy concern, we propose a new privacy-preserving feature representation. The core idea of our work is to drop constraints from each feature descriptor by embedding it within an affine subspace containing the original feature as well as one or more adversarial feature samples. Feature matching on the privacy-preserving representation is enabled based on the notion of subspace-to-subspace distance. We experimentally demonstrate the effectiveness of our method and its high practical relevance for applications such as crowd-sourced 3D scene reconstruction and face authentication. Compared to the original features, our approach has only marginal impact on performance but makes it significantly more difficult for an adversary to recover private information.",2020,ArXiv,2006.06634,,https://arxiv.org/pdf/2006.06634.pdf
79ff7b92d608301ac4ed43d13ff018c55ceba4f1,0,1,Combination of Deep Speaker Embeddings for Diarisation,"Recently, significant progress has been made in speaker diarisation after the introduction of d-vectors as speaker embeddings extracted from the neural network (NN) speaker classifiers for clustering speech segments. To extract better-performing and more robust speaker embeddings, this paper proposes a c-vector method by combining multiple sets of complementary d-vectors derived from systems with different NN components. Three structures are used to implement the c-vectors, namely 2D self-attentive, gated additive, and bilinear pooling structures, relying on attention mechanisms, a gating mechanism, and a low-rank bilinear pooling mechanism respectively. Furthermore, a neural-based single-pass speaker diarisation pipeline is also proposed in this paper, which uses NNs to achieve voice activity detection, speaker change point detection, and speaker embedding extraction. Experiments and detailed analyses are conducted on the challenging AMI and NIST RT05 datasets which consist of real meetings with 4--10 speakers and a wide range of acoustic conditions. Consistent improvements are obtained by using c-vectors instead of d-vectors, and similar relative improvements in diarisation error rates are observed on both AMI and RT05, which shows the robustness of the proposed methods.",2020,ArXiv,2010.12025,,https://arxiv.org/pdf/2010.12025.pdf
7a210274841c22ed24a404c28c5c5b4fecc4ddaf,0,1,Deep Speaker Embedding with Long Short Term Centroid Learning for Text-Independent Speaker Verification,"Recently, speaker verification systems using deep neural networks have shown their effectiveness on large scale datasets. The widely used pairwise loss functions only consider the discrimination within a mini-batch data (short-term), while either the speaker identity information or the whole training dataset is not fully exploited. Thus, these pairwise comparisons may suffer from the interferences and variances brought by speakerunrelated factors. To tackle this problem, we introduce the speaker identity information to form long-term speaker embedding centroids, which are determined by all the speakers in the training set. During the training process, each centroid dynamically accumulates the statistics of all samples belonging to a specific speaker. Since the long-term speaker embedding centroids are associated with a wide range of training samples, these centroids have the potential to be more robust and discriminative. Finally, these centroids are employed to construct a loss function, named long short term speaker loss (LSTSL). The proposed LSTSL constrains that the distances between samples and centroid from the same speaker are compact while those from different speakers are dispersed. Experiments are conducted on VoxCeleb1 and VoxCeleb2. Results on the VoxCeleb1 dataset demonstrate the effectiveness of our proposed LSTSL.",2020,INTERSPEECH,,10.21437/interspeech.2020-2470,https://isca-speech.org/archive/Interspeech_2020/pdfs/2470.pdf
7a49bed6ce511a892737970f361c4990ace75d2a,1,0,Cross-modal Multi-task Learning for Graphic Recognition of Caricature Face,"Face recognition of realistic visual images has been well studied and made a significant progress in the recent decade. Unlike the realistic visual images, the face recognition of the caricatures is far from the performance of the visual images. This is largely due to the extreme non-rigid distortions of the caricatures introduced by exaggerating the facial features to strengthen the characters. The heterogeneous modalities of the caricatures and the visual images result the caricature-visual face recognition is a cross-modal problem. In this paper, we propose a method to conduct caricature-visual face recognition via multi-task learning. Rather than the conventional multi-task learning with fixed weights of tasks, this work proposes an approach to learn the weights of tasks according to the importance of tasks. The proposed multi-task learning with dynamic tasks weights enables to appropriately train the hard task and easy task instead of being stuck in the over-training easy task as conventional methods. The experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning for cross-modal caricature-visual face recognition. The performances on the datasets CaVI and WebCaricature show the superiority over the state-of-art methods.",2020,ArXiv,2003.05787,,https://arxiv.org/pdf/2003.05787.pdf
7a7954f4989ebfbaf1f311e988c1ef05ba424738,0,1,DSFD: Dual Shot Face Detector,"Recently, Convolutional Neural Network (CNN) has achieved great success in face detection. However, it remains a challenging problem for the current face detection methods owing to high degree of variability in scale, pose, occlusion, expression, appearance and illumination. In this Paper, we propose a novel detection network named Dual Shot face Detector(DSFD). which inherits the architecture of SSD and introduces a Feature Enhance Module (FEM) for transferring the original feature maps to extend the single shot detector to dual shot detector. Specially, progressive anchor loss (PAL) computed by using two set of anchors is adopted to effectively facilitate the features. Additionally, we propose an improved anchor matching (IAM) method by integrating novel data augmentation techniques and anchor design strategy in our DSFD to provide better initialization for the regressor. Extensive experiments on popular benchmarks: WIDER FACE (easy: 0.966, medium: 0.957, hard: 0.904) and FDDB ( discontinuous: 0.991, continuous: 0.862 ) demonstrate the superiority of DSFD over the state-of-the-art face detection methods (e.g., PyramidBox and SRN). Code will be made available upon publication.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1810.1022,10.1109/CVPR.2019.00520,https://arxiv.org/pdf/1810.10220.pdf
7afb44b9382097a55edea8bd2e65a9475202199a,1,0,Improving Face Recognition Accuracy for Brazilian Faces in a Criminal Investigation Department,"This work addresses a critical problem in the use of the Face Recognition (FR) task by a police department state of Brazil. FR is a valuable crime-fighting tool that can help the police service prevent and detect crime, preserve public safety, and bring offenders to justice. Although significant advances have been shown in the last years, the works are based on large labeled datasets and supervised training. But with this approach, the lack of representative data distribution is an issue, known as data bias, mainly according to some aspects that makes FR harders: gender and race. Recent works have suggested that these two aspects may cause a significant accuracy drop. Thus, the paper is concerned over the FR data bias problem for Brazilian faces. Using pre-trained models learned from public datasets, we demonstrate that even in the small training dataset, it is possible to improve the accuracy of Brazilian faces with simple yet effective implementation tricks in fine-tuning. Two important conclusions wast obtained from this study using a non-public police dataset. First, there is a strong suggestion of data bias concerning ethnicity when evaluating models trained with public datasets on Brazilian faces, and second, the fine-tuning task implemented over non-public police dataset showed a relevant improvement to minimize the dataset bias problem.",2020,BRACIS,,10.1007/978-3-030-61377-8_20,
7b0ca99b4109f715aab8203cbed7283805e5851d,1,1,Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition,"Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15% F1-score increase respect to non-adaptive state-of-theart methods.",2020,,2012.09571,,https://arxiv.org/pdf/2012.09571.pdf
7b0ed3d67375a4542133c992f4e55fd4ade0cd90,0,1,Knowledge Distillation via Route Constrained Optimization,"Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a higher lower bound of congruence loss. In this work, we consider the knowledge distillation from the perspective of curriculum learning by teacher's routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace. RCO achieves 84.3% accuracy on one-to-million task with only 0.8 M parameters, which push the SOTA by a large margin.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.09149,10.1109/ICCV.2019.00143,https://arxiv.org/pdf/1904.09149.pdf
7b26823c1acbc10fadd53c3b71da55a95c8ae576,1,0,Improving Robustness to Model Inversion Attacks via Mutual Information Regularization,"This paper studies defense mechanisms against model inversion (MI) attacks -- a type of privacy attacks aimed at inferring information about the training data distribution given the access to a target machine learning model. Existing defense mechanisms rely on model-specific heuristics or noise injection. While being able to mitigate attacks, existing methods significantly hinder model performance. There remains a question of how to design a defense mechanism that is applicable to a variety of models and achieves better utility-privacy tradeoff. In this paper, we propose the Mutual Information Regularization based Defense (MID) against MI attacks. The key idea is to limit the information about the model input contained in the prediction, thereby limiting the ability of an adversary to infer the private training attributes from the model prediction. Our defense principle is model-agnostic and we present tractable approximations to the regularizer for linear regression, decision trees, and neural networks, which have been successfully attacked by prior work if not attached with any defenses. We present a formal study of MI attacks by devising a rigorous game-based definition and quantifying the associated information leakage. Our theoretical analysis sheds light on the inefficacy of DP in defending against MI attacks, which has been empirically observed in several prior works. Our experiments demonstrate that MID leads to state-of-the-art performance for a variety of MI attacks, target models and datasets.",2020,ArXiv,2009.05241,,https://arxiv.org/pdf/2009.05241.pdf
7b9302212440e9be73808d0d3b802c523b4ca1d1,0,1,HeadGAN: Video-and-Audio-Driven Talking Head Synthesis,"Recent attempts to solve the problem of talking head synthesis using a single reference image have shown promising results. However, most of them fail to meet the identity preservation problem, or perform poorly in terms of photo-realism, especially in extreme head poses. We propose HeadGAN, a novel reenactment approach that conditions synthesis on 3D face representations, which can be extracted from any driving video and adapted to the facial geometry of any source. We improve the plausibility of mouth movements, by utilising audio features as a complementary input to the Generator. Quantitative and qualitative experiments demonstrate the merits of our approach.",2020,,2012.08261,,https://arxiv.org/pdf/2012.08261.pdf
7bd3b85ae8e242037ffeda7d529ec7d1e3baabd0,0,1,Why face recognition accuracy varies due to race,"The varying accuracy of face recognition across race and gender has attracted a good deal of media attention. Publications ranging from the New York Times to Wired have carried headlines like ‘Facial Recognition Is Accurate, if You're a White Guy’ and ‘The best algorithms still struggle to recognize black faces equally’ 1 , 2 , 3 . Press releases by organisations like the American Civil Liberties Union (ACLU) and Georgetown Law Center on Privacy & Technology 4 , 5 have spurred some of this media coverage – which at times has been more sensationalised than well-informed about the technology.",2019,,,10.1016/s0969-4765(19)30114-6,
7bf29aae080ef83a38cd58baee623d2e54849e8b,0,1,Robust Text-Dependent Speaker Verification via Character-Level Information Preservation for the SdSV Challenge 2020,"This paper describes our submission to Task 1 of the Short-duration Speaker Verification (SdSV) challenge 2020. Task 1 is a text-dependent speaker verification task, where both the speaker and phrase are required to be verified. The submitted systems were composed of TDNN-based and ResNet-based front-end architectures, in which the frame-level features were aggregated with various pooling methods (e.g., statistical, self-attentive, ghostVLAD pooling). Although the conventional pooling methods provide embeddings with a sufficient amount of speaker-dependent information, our experiments show that these embeddings often lack phrase-dependent information. To mitigate this problem, we propose a new pooling and score compensation methods that leverage a CTC-based automatic speech recognition (ASR) model for taking the lexical content into account. Both methods showed improvement over the conventional techniques, and the best performance was achieved by fusing all the experimented systems, which showed 0.0785% MinDCF and 2.23% EER on the challenge's evaluation subset.",2020,INTERSPEECH,2010.11408,10.21437/interspeech.2020-2183,https://arxiv.org/pdf/2010.11408.pdf
7bf73e89c120c0152bf1273ccfd3f5f2c7c4114e,0,1,Face Recognition System for Complex Surveillance Scenarios,"In recent years, with the continuous development of the Internet and artificial intelligence, face recognition technology has also been widely used in many application scenarios. Facing complex surveillance scenarios, face recognition technology still faces great challenges. This paper focuses on implementing real-time and efficient face recognition systems in complex surveillance scenarios, such as insufficient lighting, small faces, dense crowds, and sides at 45 ° environment. The system is mainly based on RetinaFace for face detection and face alignment, and uses lightweight mobilenet (0.25) as the backbone network of RetinaFace. Facial feature extraction is based on deep residual neural network combined with ArcFace loss, and feature matching is performed by Euclidean distance. The experimental results show that the face recognition system has good real-time performance, accuracy and robustness.",2020,,,10.1088/1742-6596/1544/1/012146,
7c1e9eac39dfc98a54e4d3a80e5734cbfc33178b,1,0,Disentangled Variational Representation for Heterogeneous Face Recognition,"Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.",2019,AAAI,1809.01936,10.1609/aaai.v33i01.33019005,https://arxiv.org/pdf/1809.01936.pdf
7c9ef04c66bfc5c0ae4a6ae175a3f13414db8ecb,1,1,SISTEMA PARA EL RECONOCIMIENTO DE PERSONAS POR UN ROBOT MÓVIL ASISTENTE PEOPLE RECOGNITION SYSTEM FOR AN ASSISTIVE MOBILE ROBOT,"Human-Robot interaction is increasingly relevant to the field of robotics, as mobile robots are used in education, health care, or assisting humans in everyday tasks. In these applications, providing a personalized service is paramount to reach a satisfactory end-user experience. A required feature to yield such a service is to be able to recognize which person the robot has to interact with. To address that issue, this Bachelor’s thesis explores face recognition: a nonintrusive, autonomous approach of identification using biometric identifiers from an user’s face. Face recognition has gained relevancy in the recent years and can be used reliably in certain applications, as the advances in machine learning and the creation of huge public datasets have improved state-of-the-art performance considerably. In this way, the purpose of this work is to adapt and integrate a modern face recognition pipeline in the Robot Operating System (ROS), the most popular robotic software architecture, for its use in home environments by service robots. To accomplish this task, a number of open-source implementations for face detection and deep feature extraction have been considered, two of the main components of a face recognition pipeline. Additionally, a pose estimation method named OpenPose has been employed for the task of face detection, given that it has other useful features that can be applied to human-robot interaction, for example, approaching humans safely, or recognizing non-verbal behavior. These methods have been compared in terms of accuracy and performance in common benchmark datasets to aid the choice of the final implementation, which has been integrated in the ROS ecosystem.",2019,,,,https://pdfs.semanticscholar.org/7c9e/f04c66bfc5c0ae4a6ae175a3f13414db8ecb.pdf
7de2e5249531ba572673a231f876c6338bc6a6c6,1,0,Face Reconstruction from Profile to Frontal Evaluation of Face Recognition,"One of the main challenges in face recognition is handling extreme variation of poses which may be faced for images collected in labs and in the wild. Recognizing faces in profile view has been shown to perform poorly compared to using frontal view of faces. Indeed, previous approaches failed to capture distinct features of a profile face compared to a frontal one. Approaches to enhance face recognition on profile faces have been recently proposed following two different trends. One trend depends on training a neural network model with big multi-view face datasets to learn features of faces by handling all poses. The second trend generates a frontal face image (face reconstruction) from any given face pose and applies feature extraction and face recognition on the generated face instead of profile faces. Recent methods for face reconstruction use generative adversarial networks (GAN) learning model to train two competing neural networks to generate authentic frontal view of any pose preserving person’s identity. For the work described in this paper, we trained a feature extraction neural network model to learn representation of any face pose which is then compared with each other using Euclidean distance. We also used two recent face reconstruction techniques to generate frontal faces. We evaluated the performance of using the generated frontal faces against the posed counterparts. In the conducted experiments, we used three face datasets that contain several challenges for face recognition having faces in a variety of poses and in the wild.",2020,,,10.1007/978-3-030-32587-9_8,
7e2cfbfd43045fbd6aabd9a45090a5716fc4e179,0,1,Global Norm-Aware Pooling for Pose-Robust Face Recognition at Low False Positive Rate,"In this paper, we propose a novel Global Norm-Aware Pooling (GNAP) block, which reweights local features in a convolutional neural network (CNN) adaptively according to their L2 norms and outputs a global feature vector with a global average pooling layer. Our GNAP block is designed to give dynamic weights to local features in different spatial positions without losing spatial symmetry. We use a GNAP block in a face feature embedding CNN to produce discriminative face feature vectors for pose-robust face recognition. The GNAP block is of very cheap computational cost, but it is very powerful for frontal-profile face recognition. Under the CFP frontal-profile protocol, the GNAP block can not only reduce EER dramatically but also boost TPR@FPR=0.1% (TPR i.e. True Positive Rate, FPR i.e. False Positive Rate) substantially. Our experiments show that the GNAP block greatly promotes pose-robust face recognition over the base model especially at low false positive rate.",2018,ArXiv,1808.00435,,https://arxiv.org/pdf/1808.00435.pdf
7e87fb8fe78694d42bdc3b4a3b04ffb218148233,0,1,Survey on Deep Neural Networks in Speech and Vision Systems,"This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems.",2020,Neurocomputing,1908.07656,10.1016/j.neucom.2020.07.053,https://arxiv.org/pdf/1908.07656.pdf
7eadd5af7cff9872aeb72ad85498990450854a72,0,1,DMA Regularization: Enhancing Discriminability of Neural Networks by Decreasing the Minimal Angle,"Most of the discriminative feature learning methods are specifically developed for metric learning, however, the effectiveness may be not obvious for other tasks. In this letter, we propose a novel discrimination regularization method for image classification, which enhances the intra-class compactness and inter-class discrepancy simultaneously, through decreasing the minimal angle (DMA) between the feature vector and any one of the weight vectors in classification layer. This method can robustly improve the discriminability and generalizability of neural networks and easily exert its effect by plugging the DMA regularization term into the loss function with negligible computational overhead. The DMA regularization is simple, efficient, and effective. Therefore, it can be used as a basic regularization method for models based on neural networks. We evaluate DMA by applying it to various modern models on CIFAR10, CIFAR100, and TinyImageNet datasets, decreasing the test error rate by 0.2–0.4%, 0.2–1.5%, and 0.3-0.4% respectively. Code is available at: https://github.com/wznpub/DMA_Regularization.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.3037512,
7ed4428e587e7ee4e3e0b5971084e5889a3bc01c,0,1,Face illumination recovery for the deep learning feature under severe illumination variations,"Abstract The deep learning feature is the best for face recognition nowadays, but its performance exhibits unsatisfactorily under severe illumination variations. The main reason is that the deep learning feature was trained by the internet face images with variations of large pose/expression and slight/moderate illumination, which cannot well tackle severe illumination variations. Inspired by the fact that the deep learning feature can cope well with slight/moderate varying illumination, this paper proposes an illumination recovery model to transform severe varying illumination to slight/moderate varying illumination. The illumination recovery model enables the illumination of the severe illumination variation image close to that of the reference image with slight/moderate varying illumination. The reference image generated from the severe illumination variation image is termed as the generated reference image (GRI), which is obtained by normalizing singular values of the logarithm version of the severe illumination variation image to have unit L2-norm. The gradient descent algorithm is employed to address the proposed illumination recovery model, to obtain the generated reference image based illumination recovery image (GRIR). GRIR preserves better face inherent information than GRI such as the face color. Experimental results indicate that the proposed GRIR can efficiently improve the performance of the deep learning feature under severe illumination variations.",2021,,,10.1016/j.patcog.2020.107724,
7f2fca65dea8365d9013eac84f285db1581e869e,0,1,"The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices","Over the last decades, the world has been witnessing growing threats to the security in urban spaces, which has augmented the relevance given to visual surveillance solutions able to detect, track and identify persons of interest in crowds. In particular, unmanned aerial vehicles (UAVs) are a potential tool for this kind of analysis, as they provide a cheap way for data collection, cover large and difficult-to-reach areas, while reducing human staff demands. In this context, all the available datasets are exclusively suitable for the pedestrian re-identification problem, in which the multi-camera views per ID are taken on a single day, and allows the use of clothing appearance features for identification purposes. Accordingly, the main contributions of this paper are two-fold: 1) we announce the UAV-based P-DESTRE dataset, which is the first of its kind to provide consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions; and 2) we compare the results attained by state-of-the-art pedestrian detection, tracking, reidentification and search techniques in well-known surveillance datasets, to the effectiveness obtained by the same techniques in the P-DESTRE data. Such comparison enables to identify the most problematic data degradation factors of UAV-based data for each task, and can be used as baselines for subsequent advances in this kind of technology. The dataset and the full details of the empirical evaluation carried out are freely available at this http URL.",2020,ArXiv,2004.02782,,https://arxiv.org/pdf/2004.02782.pdf
7f4fb68852626bff6e3b2f1687f356ae072444bd,0,1,Unifying Deep Local and Global Features for Efficient Image Search,"A key challenge in large-scale image retrieval problems is the trade-off between scalability and accuracy. Recent research has made great strides to improve scalability with compact global image features, and accuracy with local image features. In this work, our main contribution is to unify global and local image features into a single deep model, enabling scalable retrieval with high accuracy. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads – requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Experiments on the Revisited Oxford and Paris datasets demonstrate that our jointly learned ResNet-50 based features outperform all previous results using deep global features (most with heavier backbones) and those that further re-rank with local features. Code and models will be released.",2020,ArXiv,,,
7f67307008a943a323ae23643e3af35be9779fc9,1,1,Unknown Identity Rejection Loss: Utilizing Unlabeled Data for Face Recognition,"Face recognition has advanced considerably with the availability of large-scale labeled datasets. However, how to further improve the performance with the easily accessible unlabeled dataset remains a challenge. In this paper, we propose the novel Unknown Identity Rejection (UIR) loss to utilize the unlabeled data. We categorize identities in unconstrained environment into the known set and the unknown set. The former corresponds to the identities that appear in the labeled training dataset while the latter is its complementary set. Besides training the model to accurately classify the known identities, we also force the model to reject unknown identities provided by the unlabeled dataset via our proposed UIR loss. In order to 'reject' faces of unknown identities, centers of the known identities are forced to keep enough margin from centers of unknown identities which are assumed to be approximated by the features of their samples. By this means, the discriminativeness of the face representations can be enhanced. Experimental results demonstrate that our approach can provide obvious performance improvement by utilizing the unlabeled data.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),1910.10896,10.1109/ICCVW.2019.00325,https://arxiv.org/pdf/1910.10896.pdf
7fa4e972da46735971aad52413d17c4014c49e6e,1,0,How to Train Triplet Networks with 100K Identities?,"Training triplet networks with large-scale data is challenging in face recognition. Due to the number of possible triplets explodes with the number of samples, previous studies adopt the online hard negative mining(OHNM) to handle it. However, as the number of identities becomes extremely large, the training will suffer from bad local minima because effective hard triplets are difficult to be found. To solve the problem, in this paper, we propose training triplet networks with subspace learning, which splits the space of all identities into subspaces consisting of only similar identities. Combined with the batch OHNM, hard triplets can be found much easier. Experiments on the large-scale MS-Celeb-1M challenge with 100 K identities demonstrate that the proposed method can largely improve the performance. In addition, to deal with heavy noise and large-scale retrieval, we also make some efforts on robust noise removing and efficient image retrieval, which are used jointly with the subspace learning to obtain the state-of-the-art performance on the MS-Celeb-1M competition (without external data in Challenge1).",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),1709.0294,10.1109/ICCVW.2017.225,https://arxiv.org/pdf/1709.02940.pdf
7fd1ec0ae6ff1a894f0359c6106271e2a041d7d1,0,1,Combined Center Dispersion Loss Function for Deep Facial Expression Recognition,"Abstract We propose a combined center dispersion loss function to reduce the intra-class variations and inter-class similarities of facial expression datasets and achieve high accuracy in facial expression recognition. Because of the lack of data, we strategically combine four publicly available facial expression datasets for training. Moreover, we propose an incremental cosine annealing method for deploying multiple models trained with incremental learning rates and ensemble predictions for achieving better accuracy. This method also reduces the computational cost and yields ensemble predictions of varied models, instead of similar models, that are trained with the same learning rates. We train our methods using the VGGFace network and achieve an accuracy of 74.71% on the FER2013 test set.",2020,,,10.1016/j.patrec.2020.11.002,
801639c024a46149bddb74398f1c3d57661a95b9,1,0,Faithful Face Image Completion for HMD Occlusion Removal,"Head-mounted-displays (HMDs) provide immersive experiences of virtual content. While being flexible, HMDs could be a hindrance for Virtual Reality (VR) applications such as VR teleconference where facial components and expressions of the user are partially occluded thus cannot be seen by others. We present an automatic face image completion solution that treats the occluded region as a hole and completes the hole with the help of an occlusion-free reference image of the same person. Given the occluded input image and an occlusion-free reference image, our method first computes head pose features from estimated facial landmarks. The head pose features, as well as images, are then fed into a generative adversarial network (GAN) to synthesize the output image. Our method can generate faithful results from various input cases and outperforms other face completion methods. It provides a light-weighted solution to HMD occlusion removal and has the potential to benefit VR applications.",2019,2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),,10.1109/ISMAR-Adjunct.2019.00-36,
8016c973136e75129a6fe78503aed6d8608084f3,1,1,Imbalance Robust Softmax for Deep Embeeding Learning,"Deep embedding learning is expected to learn a metric space in which features have smaller maximal intra-class distance than minimal inter-class distance. In recent years, one research focus is to solve the open-set problem by discriminative deep embedding learning in the field of face recognition (FR) and person re-identification (re-ID). Apart from open-set problem, we find that imbalanced training data is another main factor causing the performance degradation of FR and re-ID, and data imbalance widely exists in the real applications. However, very little research explores why and how data imbalance influences the performance of FR and re-ID with softmax or its variants. In this work, we deeply investigate data imbalance in the perspective of neural network optimisation and feature distribution about softmax. We find one main reason of performance degradation caused by data imbalance is that the weights (from the penultimate fully-connected layer) are far from their class centers in feature space. Based on this investigation, we propose a unified framework, Imbalance-Robust Softmax (IR-Softmax), which can simultaneously solve the open-set problem and reduce the influence of data imbalance. IR-Softmax can generalise to any softmax and its variants (which are discriminative for open-set problem) by directly setting the weights as their class centers, naturally solving the data imbalance problem. In this work, we explicitly re-formulate two discriminative softmax (A-Softmax and AM-Softmax) under the framework of IR-Softmax. We conduct extensive experiments on FR databases (LFW, MegaFace) and re-ID database (Market-1501, Duke), and IR-Softmax outperforms many state-of-the-art methods.",2020,ArXiv,2011.11155,,https://arxiv.org/pdf/2011.11155.pdf
806b45616b604d86fd40499e883d0a2aeb0d5ab0,1,1,Fewer-Shots and Lower-Resolutions: Towards Ultrafast Face Recognition in the Wild,"Is it possible to train an effective face recognition model with fewer shots that works efficiently on low-resolution faces in the wild? To answer this question, this paper proposes a few-shot knowledge distillation approach to learn an ultrafast face recognizer via two steps. In the first step, we initialize a simple yet effective face recognition model on synthetic low-resolution faces by distilling knowledge from an existing complex model. By removing the redundancies in both face images and the model structure, the initial model can provide an ultrafast speed with impressive recognition accuracy. To further adapt this model into the wild scenarios with fewer faces per person, the second step refines the model via few-shot learning by incorporating a relation module that compares low-resolution query faces with faces in the support set. In this manner, the performance of the model can be further enhanced with only fewer low-resolution faces in the wild. Experimental results show that the proposed approach performs favorably against state-of-the-arts in recognizing low-resolution faces with an extremely low memory of 30KB and runs at an ultrafast speed of 1,460 faces per second on CPU or 21,598 faces per second on GPU.",2019,MM '19,,10.1145/3343031.3351082,
80abd9084d70ed6abe86797370273dfe2992ebe5,1,0,Cross-Domain Knowledge Transfer for Incremental Deep Learning in Facial Expression Recognition,"For robotics and AI applications, automatic facial expression recognition can be used to measure user’s satisfaction on products and services that are provided through the human-computer interactions. Large-scale datasets are essentially required to construct a robust deep learning model, which leads to increased training computation cost and duration. This requirement is of particular issue when the training is supposed to be performed on an ongoing basis in devices with limited computation capacity, such as humanoid robots. Knowledge transfer has become a commonly used technique to adapt existing models and speed-up training process by supporting refinements on the existing parameters and weights for the target task. However, most state-of-the-art facial expression recognition models are still based on a single stage training (train at once), which would not be enough for achieving a satisfactory performance in real world scenarios. This paper proposes a knowledge transfer method to support learning using cross-domain datasets, from generic to specific domain. The experimental results demonstrate that shorter and incremental training using smaller-gap-domain from cross-domain datasets can achieve a comparable performance to training using a single large dataset from the target domain.",2019,2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA),,10.1109/RITAPP.2019.8932731,https://research-repository.griffith.edu.au/bitstream/10072/389873/3/Sugianto305286-Accepted.pdf
8172ca86b80e66c1b4ad9db8bbc34afe8c72d3fa,0,1,Boosting Few-Shot Learning With Adaptive Margin Loss,"Few-shot learning (FSL) has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in learning to generalize from a few examples. This paper proposes an adaptive margin principle to improve the generalization ability of metric-based meta-learning approaches for few-shot learning problems. Specifically, we first develop a class-relevant additive margin loss, where semantic similarity between each pair of classes is considered to separate samples in the feature embedding space from similar classes. Further, we incorporate the semantic context among all classes in a sampled training task and develop a task-relevant additive margin loss to better distinguish samples from different classes. Our adaptive margin method can be easily extended to a more realistic generalized FSL setting. Extensive experiments demonstrate that the proposed method can boost the performance of current metric-based meta-learning approaches, under both the standard FSL and generalized FSL settings.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2005.13826,10.1109/CVPR42600.2020.01259,https://arxiv.org/pdf/2005.13826.pdf
81e028b9ef5e6e6a1c6661fdd9120a2d955bd239,1,1,DebFace: De-biasing Face Recognition,"We address the problem of bias in automated face recognition algorithms, where errors are consistently lower on certain cohorts belonging to specific demographic groups. We present a novel de-biasing adversarial network that learns to extract disentangled feature representations for both unbiased face recognition and demographics estimation. The proposed network consists of one identity classifier and three demographic classifiers (for gender, age, and race) that are trained to distinguish identity and demographic attributes, respectively. Adversarial learning is adopted to minimize correlation among feature factors so as to abate bias influence from other factors. We also design a new scheme to combine demographics with identity features to strengthen robustness of face representation in different demographic groups. The experimental results show that our approach is able to reduce bias in face recognition as well as demographics estimation while achieving state-of-the-art performance.",2019,ArXiv,1911.0808,,http://cvlab.cse.msu.edu/pdfs/Gong_Liu_Jain_arxiv2019.pdf
8239255ebe5a346c8fc7732f32c1fa7e88b62050,0,1,Outlier-Robust Neural Aggregation Network for Video Face Identification,"Current approaches for video face recognition rely on image sets containing faces of exclusively one identity. However, as image sets are created by unsupervised methods, it is necessary to consider outlier-afflicted sets for real-life applications. In this paper, we propose an Outlier-Robust Neural Aggregation Network (ORNAN). First, we embed each image into a feature space using a Convolutional Neural Network (CNN). With the help of two cascaded attention blocks, we predict outliers within the image set. By integrating this knowledge into our aggregation network, we adaptively aggregate all feature vectors to form a single feature, mitigating the influence of outliers and noisy features. We show that our network is robust against outliers using outlier-afflicted IJB-B and IJB-C benchmarks while maintaining similar performance without outliers.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803028,
825eeae79a97825b69a4516a05e5a8975422609a,0,1,Domain-Aware Visual Bias Eliminating for Generalized Zero-Shot Learning,"Generalized zero-shot learning aims to recognize images from seen and unseen domains. Recent methods focus on learning a unified semantic-aligned visual representation to transfer knowledge between two domains, while ignoring the effect of semantic-free visual representation in alleviating the biased recognition problem. In this paper, we propose a novel Domain-aware Visual Bias Eliminating (DVBE) network that constructs two complementary visual representations, i.e., semantic-free and semantic-aligned, to treat seen and unseen domains separately. Specifically, we explore cross-attentive second-order visual statistics to compact the semantic-free representation, and design an adaptive margin Softmax to maximize inter-class divergences. Thus, the semantic-free representation becomes discriminative enough to not only predict seen class accurately but also filter out unseen images, i.e., domain detection, based on the predicted class entropy. For unseen images, we automatically search an optimal semantic-visual alignment architecture, rather than manual designs, to predict unseen classes. With accurate domain detection, the biased recognition problem towards the seen domain is significantly reduced. Experiments on five benchmarks for classification and segmentation show that DVBE outperforms existing methods by averaged 5.7% improvement.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.13261,10.1109/cvpr42600.2020.01268,https://arxiv.org/pdf/2003.13261.pdf
82b07b79e77089439526eda8d3202282a3504b9f,1,0,SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning,"We propose a new discrimination-aware learning method to improve both accuracy and fairness of face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments include two popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by gender and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present a strong algorithmic discrimination. We finally propose a discrimination-aware learning method, SensitiveLoss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory effects by automatic systems.",2020,ArXiv,2004.11246,,https://arxiv.org/pdf/2004.11246.pdf
82c8dedbd58a8a8b7031dc49ce57553231fb3ca4,1,0,Learning Deep Representations with Probabilistic Knowledge Transfer,"Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper we propose a novel probabilistic knowledge transfer method that works by matching the probability distribution of the data in the feature space instead of their actual representation. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several of their limitations providing new insight into KT as well as novel KT applications, ranging from KT from handcrafted feature extractors to cross-modal KT from the textual modality into the representation extracted from the visual modality of the data.",2018,ECCV,1803.10837,10.1007/978-3-030-01252-6_17,https://arxiv.org/pdf/1803.10837.pdf
82d87fcd81c118d00f38fe179c26e99b07721128,0,1,Exploring Voice Conversion based Data Augmentation in Text-Dependent Speaker Verification,"In this paper, we focus on improving the performance of the text-dependent speaker verification system in the scenario of limited training data. The speaker verification system deep learning based text-dependent generally needs a large scale text-dependent training data set which could be labor and cost expensive, especially for customized new wake-up words. In recent studies, voice conversion systems that can generate high quality synthesized speech of seen and unseen speakers have been proposed. Inspired by those works, we adopt two different voice conversion methods as well as the very simple re-sampling approach to generate new text-dependent speech samples for data augmentation purposes. Experimental results show that the proposed method significantly improves the Equal Error Rare performance from 6.51% to 4.51% in the scenario of limited training data.",2020,ArXiv,2011.1071,,https://arxiv.org/pdf/2011.10710.pdf
82e9d730c05a902a06e2447fd30d1ec75d2aef2f,0,1,A cloud-based face video retrieval system with deep learning,"Face video retrieval is an attractive research topic in computer vision. However, it remains challenges to overcome because of the significant variation in pose changes, illumination conditions, occlusions, and facial expressions. In video content analysis, face recognition has been playing a vital role. Besides, deep neural networks are being actively studied, and deep learning models have been widely used for object detection, especially for face recognition. Therefore, this study proposes a cloud-based face video retrieval system with deep learning. First, a dataset is collected and pre-processed. To produce a useful dataset for the CNN models, blurry images are removed, and face alignment is implemented on the remaining images. Then the final dataset is constructed and used to pre-train the CNN models (VGGFace, ArcFace, and FaceNet) for face recognition. We compare the results of these three models and choose the most efficient one to develop the system. To implement a query, users can type in the name of a person. If the system detects a new person, it performs enrolling that person. Finally, the result is a list of images and time associated with those images. In addition, a system prototype is implemented to verify the feasibility of the proposed system. Experimental results demonstrate that this system outperforms in terms of recognition accuracy and computational time.",2020,The Journal of Supercomputing,,10.1007/s11227-019-03123-x,
8326441dc5641d8175f8033fde64f4b44840a728,1,0,Recognizing Profile Faces by Imagining Frontal View,"Extreme pose variation is one of the key obstacles to accurate face recognition in practice. Compared with current techniques for pose-invariant face recognition, which either expect pose invariance from hand-crafted features or data-driven deep learning solutions, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose-Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network which simultaneously perceives global structures and local details, incorporating an unsupervised cross-domain adversarial training and a meta-learning (“learning to learn”) strategy using siamese discriminator with dynamic convolution for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representations with large intra-class affinity and inter-class separability. Qualitative and quantitative experiments on both controlled and in-the-wild benchmark datasets demonstrate the superiority of the proposed model over the state-of-the-arts.",2019,International Journal of Computer Vision,,10.1007/s11263-019-01252-7,
83447d47bb2837b831b982ebf9e63616742bfdec,1,1,An Automatic System for Unconstrained Video-Based Face Recognition,"Although deep learning approaches have achieved performance surpassing humans for still image-based face recognition, unconstrained video-based face recognition is still a challenging task due to large volume of data to be processed and intra/inter-video variations on pose, illumination, occlusion, scene, blur, video quality, etc. In this work, we consider challenging scenarios for unconstrained video-based face recognition from multiple-shot videos and surveillance videos with low-quality frames. To handle these problems, we propose a robust and efficient system for unconstrained video-based face recognition, which is composed of modules for face/fiducial detection, face association, and face recognition. First, we use multi-scale single-shot face detectors to efficiently localize faces in videos. The detected faces are then grouped through carefully designed face association methods, especially for multi-shot videos. Finally, the faces are recognized by the proposed face matcher based on an unsupervised subspace learning approach and a subspace-to-subspace similarity metric. Extensive experiments on challenging video datasets, such as Multiple Biometric Grand Challenge (MBGC), Face and Ocular Challenge Series (FOCS), IARPA Janus Surveillance Video Benchmark (IJB-S) for low-quality surveillance videos and IARPA JANUS Benchmark B (IJB-B) for multiple-shot videos, demonstrate that the proposed system can accurately detect and associate faces from unconstrained videos and effectively learn robust and discriminative features for recognition.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",1812.04058,10.1109/TBIOM.2020.2973504,https://arxiv.org/pdf/1812.04058.pdf
83479dcbf68bd988f2ad98bc791fa9eeccaa6620,0,1,DCASE 2019 TASK 2 : SEMI-SUPERVISED NETWORKS WITH HEAVY DATA AUGMENTATIONS TO BATTLE AGAINST LABEL NOISE IN AUDIO TAGGING TASK,"This technical report describes a system used for DCASE 2019 Task 2: Audio tagging with noisy labels and minimal supervision. Building a large-scale multi-label dataset normally requires extensive amount of manual effort, especially for general-purpose audio tagging system. To tackle the problem, we use a semi-supervised teacher-student convolutional neural network (CNN) to leverage substantial noisy labels and small curated labels in dataset. To further regularize the system, we exploit multiple data augmentation methods, including SpecAugment [1], mixup [2], and an innovative time reversal augmentation approach. Moreover, a combination of binary Focal [3] and ArcFace [4] losses are used to increase the accuracy of pseudo labels produced by the semi-supervised network, and accelerate the training process. Aadaptive test time augmentation (TTA) based on the lengths of audio samples is used as a final approach to improve the system. We choose a single system that generates the submission file Zhang BIsmart task2 3.output.csv to be the candidate model considered for the Judges’ Award. Other two systems use ensemble approach to furthur improve the performance.",2019,,,,http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Zhang_87_t2.pdf
838e7b46185e72c43dda1ceb2d5f1a0154542c16,1,0,An Ethical Highlighter for People-Centric Dataset Creation,"Important ethical concerns arising from computer vision datasets of people have been receiving significant attention, and a number of datasets have been withdrawn as a result. To meet the academic need for people-centric datasets, we propose an analytical framework to guide ethical evaluation of existing datasets and to serve future dataset creators in avoiding missteps. Our work is informed by a review and analysis of prior works and highlights where such ethical challenges arise.",2020,ArXiv,2011.13583,,https://arxiv.org/pdf/2011.13583.pdf
841c1d3a150a1f20eb932598e2e0a396094667ba,1,1,The Improved Siamese Network in Face Recognition,"This paper is an application of Siamese Network in face recognition; the learning process is minimizing the logistic regression loss function that drives the metric to be small for the same persons, and significant for different persons. In practice, we hope that the distinction between the same persons and different persons is as significant as possible. Nevertheless, the standard sigmoid function does not characterize this feature very well. This paper improves the standard sigmoid function for this defect. The hypothesis verified by experiments and has achieved a useful application in engineering. This paper introduces the complete experimental process, analyzes the experimental results, and provides a theoretical explanation for the results.",2019,"2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)",,10.1109/ICICAS48597.2019.00099,
8435ca0de617d4f9fe3e4bce36dc34c4d9ba579a,1,0,Does Generative Face Completion Help Face Recognition?,"Face occlusions, covering either the majority or discriminative parts of the face, can break facial perception and produce a drastic loss of information. Biometric systems such as recent deep face recognition models are not immune to obstructions or other objects covering parts of the face. While most of the current face recognition methods are not optimized to handle occlusions, there have been a few attempts to improve robustness directly in the training stage. Unlike those, we propose to study the effect of generative face completion on the recognition. We offer a face completion encoder-decoder, based on a convolutional operator with a gating mechanism, trained with an ample set of face occlusions. To systematically evaluate the impact of realistic occlusions on recognition, we propose to play the occlusion game: we render 3D objects onto different face parts, providing precious knowledge of what the impact is of effectively removing those occlusions. Extensive experiments on the Labeled Faces in the Wild (LFW), and its more difficult variant LFW-BLUFR, testify that face completion is able to partially restore face perception in machine vision systems for improved recognition.",2019,2019 International Conference on Biometrics (ICB),1906.02858,10.1109/ICB45273.2019.8987388,https://arxiv.org/pdf/1906.02858.pdf
84459dd9e5bdb6f98c1fd1cdde7837087709f82e,1,1,Rotation Consistent Margin Loss for Efficient Low-Bit Face Recognition,"In this paper, we consider the low-bit quantization problem of face recognition (FR) under the open-set protocol. Different from well explored low-bit quantization on closed-set image classification task, the open-set task is more sensitive to quantization errors (QEs). We redefine the QEs in angular space and disentangle it into class error and individual error. These two parts correspond to inter-class separability and intra-class compactness, respectively. Instead of eliminating the entire QEs, we propose the rotation consistent margin (RCM) loss to minimize the individual error, which is more essential to feature discriminative power. Extensive experiments on popular benchmark datasets such as MegaFace Challenge, Youtube Faces (YTF), Labeled Face in the Wild (LFW) and IJB-C show the superiority of proposed loss in low-bit FR quantization tasks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00690,http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Rotation_Consistent_Margin_Loss_for_Efficient_Low-Bit_Face_Recognition_CVPR_2020_paper.pdf
8449af7f2950e1beacdb5d759ca743815bb59748,0,1,Face Recognition via Deep Learning Using Data Augmentation Based on Orthogonal Experiments,"Class attendance is an important means in the management of university students. Using face recognition is one of the most effective techniques for taking daily class attendance. Recently, many face recognition algorithms via deep learning have achieved promising results with large-scale labeled samples. However, due to the difficulties of collecting samples, face recognition using convolutional neural networks (CNNs) for daily attendance taking remains a challenging problem. Data augmentation can enlarge the samples and has been applied to the small sample learning. In this paper, we address this problem using data augmentation through geometric transformation, image brightness changes, and the application of different filter operations. In addition, we determine the best data augmentation method based on orthogonal experiments. Finally, the performance of our attendance method is demonstrated in a real class. Compared with PCA and LBPH methods with data augmentation and VGG-16 network, the accuracy of our proposed method can achieve 86.3%. Additionally, after a period of collecting more data, the accuracy improves to 98.1%.",2019,,,10.3390/electronics8101088,https://pdfs.semanticscholar.org/ce68/1ac24ee648279f9923af7de516584ea84a46.pdf
8462491241ec48fa14c25398a6a3ad2f6fe6c6bc,1,1,More trainable inception-ResNet for face recognition,"Abstract In recent years, applications of face recognition have increased significantly. Despite the successful application of deep convolutional neural network (DCNN), training such networks is still a challenging task that needs a lot of experience and carefully tuning. Based on the Inception-ResNet network, we propose a novel method to mitigate the difficulty of training such deep convolutional neural network and improve its performance simultaneously. The residual scaling factor used in the Inception-ResNet module is a manually set fixed value. We believe that changing the value to a trainable parameter and initializing it to a small value can improve the stability of the model training. We further adopted a small trick of alternating the ReLU activation function with the Leaky ReLU and PReLU. The proposed model slightly increased the number of training parameters but improved training stability and performance significantly. Extensive experiments are conducted on VGGFace2, MS1MV2, IJBB and LFW datasets. The results show that the proposed trainable residual scaling factor (TRSF) and PReLU can promote the accuracy notably while stabilizing training process.",2020,Neurocomputing,,10.1016/j.neucom.2020.05.022,
846cc70e137dd6e090f58d1fedac8677a6ec609b,1,1,Discrimination-aware Network Pruning for Deep Model Compression,"We study network pruning which aims to remove redundant channels/kernels and hence speed up the inference of deep networks. Existing pruning methods either train from scratch with sparsity constraints or minimize the reconstruction error between the feature maps of the pre-trained models and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, while the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. In this paper, we propose a simple-yet-effective method called discrimination-aware channel pruning (DCP) to choose the channels that actually contribute to the discriminative power. Note that a channel often consists of a set of kernels. Besides the redundancy in channels, some kernels in a channel may also be redundant and fail to contribute to the discriminative power of the network, resulting in kernel level redundancy. To solve this, we propose a discrimination-aware kernel pruning (DKP) method to further compress deep networks by removing redundant kernels. To prevent DCP/DKP from selecting redundant channels/kernels, we propose a new adaptive stopping condition, which helps to automatically determine the number of selected channels/kernels and often results in more compact models with better performance. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of our methods. For example, on ILSVRC-12, the resultant ResNet-50 model with 30% reduction of channels even outperforms the baseline model by 0.36% in terms of Top-1 accuracy. The pruned MobileNetV1 and MobileNetV2 achieve 1.93x and 1.42x inference acceleration on a mobile device, respectively, with negligible performance degradation. The source code and the pre-trained models are available at this https URL.",2020,ArXiv,2001.0105,,https://arxiv.org/pdf/2001.01050.pdf
84ac31dc2370bb9c57fb08bfab58aab0df5b8335,1,0,Generative One-Shot Face Recognition,"One-shot face recognition measures the ability to identify persons with only seeing them at one glance, and is a hallmark of human visual intelligence. It is challenging for conventional machine learning approaches to mimic this way, since limited data are hard to effectively represent the data variance. The goal of one-shot face recognition is to learn a large-scale face recognizer, which is capable to fight off the data imbalance challenge. In this paper, we propose a novel generative adversarial one-shot face recognizer, attempting to synthesize meaningful data for one-shot classes by adapting the data variances from other normal classes. Specifically, we target at building a more effective general face classifier for both normal persons and one-shot persons. Technically, we design a new loss function by formulating knowledge transfer generator and a general classifier into a unified framework. Such a two-player minimax optimization can guide the generation of more effective data, which effectively promote the underrepresented classes in the learned model and lead to a remarkable improvement in face recognition performance. We evaluate our proposed model on the MS-Celeb-1M one-shot learning benchmark task, where we could recognize 94.98% of the test images at the precision of 99% for the one-shot classes, keeping an overall Top1 accuracy at $99.80\%$ for the normal classes. To the best of our knowledge, this is the best performance among all the published methods using this benchmark task with the same setup, including all the participants in the recent MS-Celeb-1M challenge at ICCV 2017\footnote{this http URL}.",2019,ArXiv,1910.0486,,https://arxiv.org/pdf/1910.04860.pdf
84ae2790a50d30081b8d84327fa95e4105af0b17,1,0,Visual Kinship Recognition: A Decade in the Making,"Kinship recognition is a challenging problem with many practical applications. With much progress and milestones having been reached after ten years since pioneered - it is now that today we are able to survey their research and create new milestones. We list and review the public resources and data challenges that enabled and inspired many to hone-in on one or more views of automatic kinship recognition in the visual domain. The different tasks are described in technical terms and syntax consistent across the problem domain and the practical value of each discussed and measured. State-of-the-art methods for visual kinship recognition problems, whether to discriminate between or generate from, are examined. As part of such, we review systems proposed as part of a recent data challenge held in conjunction with the 2020 IEEE Conference on Automatic Face and Gesture Recognition. We establish a stronghold for the state of progress for the different problems in a consistent manner. We intend for this survey will serve as the central resource for work of the next decade to build upon. For the tenth anniversary, demo code is provided for the various kin-based tasks. Detecting relatives with visual recognition and classifying the relationship is an area with high potential for impact in research and practice.",2020,ArXiv,,,https://arxiv.org/pdf/2006.16033.pdf
84c7d3b1d407e0d435a08574a3f82ecacf7841b6,0,1,Max-margin Class Imbalanced Learning with Gaussian Affinity,"Real-world object classes appear in imbalanced ratios. This poses a significant challenge for classifiers which get biased towards frequent classes. We hypothesize that improving the generalization capability of a classifier should improve learning on imbalanced datasets. Here, we introduce the first hybrid loss function that jointly performs classification and clustering in a single formulation. Our approach is based on an `affinity measure' in Euclidean space that leads to the following benefits: (1) direct enforcement of maximum margin constraints on classification boundaries, (2) a tractable way to ensure uniformly spaced and equidistant cluster centers, (3) flexibility to learn multiple class prototypes to support diversity and discriminability in feature space. Our extensive experiments demonstrate the significant performance improvements on visual classification and verification tasks on multiple imbalanced datasets. The proposed loss can easily be plugged in any deep architecture as a differentiable block and demonstrates robustness against different levels of data imbalance and corrupted labels.",2019,ArXiv,1901.07711,,https://arxiv.org/pdf/1901.07711.pdf
8509abbde2f4b42dc26a45cafddcccb2d370712f,1,0,A way to improve precision of face recognition in SIPP without retrain of the deep neural network model,"Although face recognition has been improved much as the development of Deep Neural Networks, SIPP(Single Image Per Person) problem in face recognition has not been better solved. In this paper, multiple methods will be introduced to improve the precision of SIPP face recognition without retrain of the DNN model. First, a modified SVD based method will be introduced to get more face images of one person in order to get more intra-class variations. Second, some more tricks will be introduced to help get the most similar person ID in a complex dataset, and some theoretical explain included to prove of why our tricks effective. Third, we would like to emphasize, no need to retrain of the DNN model and this would be easy to be extended in many applications without much efforts. We do some practical testing in competition of Msceleb challenge-2 2017 which was hold by Microsoft Research and finally we rank top-10. Great improvement of coverage from 13.39\% to 19.25\%, 29.94\%, 42.11\%, 47.52\% at precision P99(99\%) would be shown in this paper.",2017,ArXiv,1709.03872,,https://arxiv.org/pdf/1709.03872.pdf
850f51cbfaef254769004750eac5c4f293df0773,1,1,Loss Function Search for Face Recognition,"In face recognition, designing margin-based (e.g., angular, additive, additive angular margins) softmax loss functions plays an important role in learning discriminative features. However, these hand-crafted heuristic methods are sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.",2020,ICML 2020,2007.06542,,https://arxiv.org/pdf/2007.06542.pdf
8550d18f7148d0aa1fec89b4871a40b2ff28be84,0,1,Expression-Aware Face Reconstruction Via A Dual-Stream Network,"Recently, 3D face reconstruction from a single image has achieved promising results by adopting the 3D Morphable Model (3DMM). However, as face images in-the-wild have various expressions, it is difficult for 3DMM to handle diverse facial expressions with a large range of variations, due to the limited expressive ability of its linear model, thereby resulting in distortion and ambiguity on facial local regions. To tackle this issue, we present a novel dual-stream network to deal with expression variations. Specifically, in the geometry stream, we propose novel Attribute Spatial Maps to record the spatial information of facial identity and expression attributes in the 2D image space separately. This avoids the interaction between the two attributes, thus keeping the identity information and further improving the ability to cope with expression changes. In the texture stream, we utilize the 3DMM albedo map to a style transfer based method for synthesizing facial appearance, which results in expression-irrelevant as well as realistic face textures. Both quantitative and qualitative evaluations on public datasets demonstrate the ability of our approach to achieve comparable results in face reconstructions under expression variations.",2020,2020 IEEE International Conference on Multimedia and Expo (ICME),,10.1109/icme46284.2020.9102811,
85860d38c66a5cf2e6ffd6475a3a2ba096ea2920,1,0,Celeb-500K: A Large Training Dataset for Face Recognition,"In this paper, we propose a large training dataset named Celeb-500K for face recognition, which contains 50M images from 500K persons. To better facilitate academic research, we clean Celeb-500K to obtain Celeb-500K-2R, which contains 25M aligned face images from 365K persons. Based on the developed dataset, we achieve state-of-the-art face recognition performance and reveal two important observations on face recognition study. First, metric learning methods have limited performance gain when the training dataset contains a large number of identities. Second, in order to develop an efficient training dataset, the number of identities is more important than the average image number of each identity from the perspective of face recognition performance. Extensive experimental results show the superiority of Celeb-500K and provide a strong support to the two observations.",2018,2018 25th IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2018.8451704,
8598526318f685ea29dd151c09e72cf4929b7838,0,1,Triple ANet: Adaptive Abnormal-aware Attention Network for WCE Image Classification,"Accurate detection of abnormal regions in Wireless Capsule Endoscopy (WCE) images is crucial for early intestine cancer diagnosis and treatment, while it still remains challenging due to the relatively low contrasts and ambiguous boundaries between abnormalities and normal regions. Additionally, the huge intra-class variances, alone with the high degree of visual similarities shared by inter-class abnormalities prevent the network from robust classification. To tackle these dilemmas, we propose an Adaptive Abnormal-aware Attention Network (Triple ANet) with Adaptive Dense Block (ADB) and Abnormal-aware Attention Module (AAM) for automatic WCE image analysis. ADB is designed to assign one attention score for each dense connection in dense blocks and to enhance useful features, while AAM aims to adaptively adjust the respective field according to the abnormal regions and help pay attention to abnormalities. Moreover, we propose a novel Angular Contrastive loss (AC Loss) to reduce the intra-class variances and enlarge the inter-class differences effectively. Our methods achieved 89.41% overall accuracy and showed better performance compared with state-of-the-art WCE image classification methods. The source code is available at https://github.com/Guo-Xiaoqing/Triple-ANet.",2019,MICCAI,,10.1007/978-3-030-32239-7_33,
859ab66622889e943ae790529607d5b1ff953f25,1,0,3D Face Recognition Based on Hybrid Data,"Unlike 2D face recognition (FR), the problem of insufficient training data is a major difficulty in 3D face recognition. Traditional Convolutional neural networks (CNNs) can not comprehensively learn all proper filters for FR applications. We embed a handcrafted feature map into our CNN framework—A hybrid data representation is proposed for 3D face. Furthermore, we use a Squeeze-Excitation block to learn the weights of data channels from training face datasets. To overcome the bias of training model based on a small 3D dataset, transfer learning is applied by fine-turning pre-training models, which is trained based on a large 2D face datasets. Tests show that, under challenge conditions such as expression and occlusion, our method outperforms other state-of-the-art methods and can run in real-time.",2019,IJCRS,,10.1007/978-3-030-22815-6_35,
85b73afafb6fc1527ba46d7bfc1210d5112c1ca2,0,1,Neural Head Reenactment with Latent Pose Descriptors,"We propose a neural head reenactment system, which is driven by a latent pose representation and is capable of predicting the foreground segmentation alongside the RGB image. The latent pose representation is learned as a part of the entire reenactment system, and the learning process is based solely on image reconstruction losses. We show that despite its simplicity, with a large and diverse enough training dataset, such learning successfully decomposes pose from identity. The resulting system can then reproduce mimics of the driving person and, furthermore, can perform cross-person reenactment. Additionally, we show that the learned descriptors are useful for other pose-related tasks, such as keypoint prediction and pose-based retrieval.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.12,10.1109/CVPR42600.2020.01380,https://arxiv.org/pdf/2004.12000.pdf
85bac5655cd03f14da0e6787b12a721cf237f9ac,0,1,The IDLAB VoxSRC-20 Submission: Large Margin Fine-Tuning and Quality-Aware Score Calibration in DNN Based Speaker Verification,"In this paper we propose and analyse a large margin fine-tuning strategy and a quality-aware score calibration in text-independent speaker verification. Large margin fine-tuning is a secondary training stage for DNN based speaker verification systems trained with margin-based loss functions. It enables the network to create more robust speaker embeddings by enabling the use of longer training utterances in combination with a more aggressive margin penalty. Score calibration is a common practice in speaker verification systems to map output scores to well-calibrated log-likelihood-ratios, which can be converted to interpretable probabilities. By including quality features in the calibration system, the decision thresholds of the evaluation metrics become quality-dependent and more consistent across varying trial conditions. Applying both enhancements on the ECAPA-TDNN architecture leads to state-of-the-art results on all publicly available VoxCeleb1 test sets and contributed to our winning submissions in the supervised verification tracks of the VoxCeleb Speaker Recognition Challenge 2020.",2020,ArXiv,2010.11255,,https://arxiv.org/pdf/2010.11255.pdf
85dcb674edef911d733cf93f3a4807926e1244cc,0,1,MSP-Face Corpus: A Natural Audiovisual Emotional Database,"Expressive behaviors conveyed during daily interactions are difficult to determine, because they often consist of a blend of different emotions. The complexity in expressive human communication is an important challenge to build and evaluate automatic systems that can reliably predict emotions. Emotion recognition systems are often trained with limited databases, where the emotions are either elicited or recorded by actors. These approaches do not necessarily reflect real emotions, creating a mismatch when the same emotion recognition systems are applied to practical applications. Developing rich emotional databases that reflect the complexity in the externalization of emotion is an important step to build better models to recognize emotions. This study presents the MSP-Face database, a natural audiovisual database obtained from video-sharing websites, where multiple individuals discuss various topics expressing their opinions and experiences. The natural recordings convey a broad range of emotions that are difficult to obtain with other alternative data collection protocols. A feature of the corpus is the addition of two sets. The first set includes videos that have been annotated with emotional labels using a crowd-sourcing protocol (9,370 recordings -- 24 hrs, 41 m). The second set includes similar videos without emotional labels (17,955 recordings -- 45 hrs, 57 m), offering the perfect infrastructure to explore semi-supervised and unsupervised machine-learning algorithms on natural emotional videos. This study describes the process of collecting and annotating the corpus. It also provides baselines over this new database using unimodal (audio, video) and multimodal emotional recognition systems.",2020,ICMI,,10.1145/3382507.3418872,https://ecs.utdallas.edu/research/researchlabs/msp-lab/publications/Vidal_2020.pdf
85eed639f7367c794a6d8b38619697af3efaacfe,1,0,Learning Deep Features via Congenerous Cosine Loss for Person Recognition,"Person recognition aims at recognizing the same identity across time and space with complicated scenes and similar appearance. In this paper, we propose a novel method to address this task by training a network to obtain robust and representative features. The intuition is that we directly compare and optimize the cosine distance between two features - enlarging inter-class distinction as well as alleviating inner-class variance. We propose a congenerous cosine loss by minimizing the cosine distance between samples and their cluster centroid in a cooperative way. Such a design reduces the complexity and could be implemented via softmax with normalized inputs. Our method also differs from previous work in person recognition that we do not conduct a second training on the test subset. The identity of a person is determined by measuring the similarity from several body regions in the reference set. Experimental results show that the proposed approach achieves better classification accuracy against previous state-of-the-arts.",2017,ArXiv,1702.0689,,https://arxiv.org/pdf/1702.06890.pdf
865406a63fc0e9ae6ea4b7015ffca358f40b5646,0,1,Temporal Consistency Based Deep Face Forgery Detection Network,,2020,ML4CS,,10.1007/978-3-030-62463-7_6,
865cab74e0c9b32698a4972266a5261f7a144b1c,1,1,Global-Local GCN: Large-Scale Label Noise Cleansing for Face Recognition,"In the field of face recognition, large-scale web-collected datasets are essential for learning discriminative representations, but they suffer from noisy identity labels, such as outliers and label flips. It is beneficial to automatically cleanse their label noise for improving recognition accuracy. Unfortunately, existing cleansing methods cannot accurately identify noise in the wild. To solve this problem, we propose an effective automatic label noise cleansing framework for face recognition datasets, FaceGraph. Using two cascaded graph convolutional networks, FaceGraph performs global-to-local discrimination to select useful data in a noisy environment. Extensive experiments show that cleansing widely used datasets, such as CASIA-WebFace, VGGFace2, MegaFace2, and MS-Celeb-1M, using the proposed method can improve the recognition performance of state-of-the-art representation learning methods like Arcface. Further, we cleanse massive self-collected celebrity data, namely MillionCelebs, to provide 18.8M images of 636K identities. Training with the new data, Arcface surpasses state-of-the-art performance by a notable margin to reach 95.62% TPR at 1e-5 FPR on the IJB-C benchmark.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.00775,https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.pdf
86a2301e7e4779e7ad3d1abda9d188f9ace4f1f3,1,1,LinCos-Softmax: Learning Angle-Discriminative Face Representations With Linearity-Enhanced Cosine Logits,"In recent years, the angle-based softmax losses have significantly improved the performance of face recognition whereas these loss functions are all based on cosine logit. A potential weakness is that the nonlinearity of the cosine function may undesirably saturate the angular optimization between the features and the corresponding weight vectors, thereby preventing the network from fully learning to maximize the angular discriminability of features. As a result, the generalization of learned features may be compromised. To tackle this issue, we propose a Linear-Cosine Softmax Loss (LinCos-Softmax) to more effectively learn angle-discriminative facial features. The main characteristic of the loss function we propose is the use of an approximated linear logit. Compared with the conventional cosine logit, it has a stronger linear relationship with the angle on enhancing angular discrimination through Taylor expansion. We also propose an automatic scale parameter selection scheme, which can conveniently provide an appropriate scale for different logits without the need for exhaustive parameter search to improve performance. In addition, we propose a margin-enhanced Linear-Cosine Softmax Loss (m-LinCos-Softmax) to further enlarge inter-class distances and reduce intra-class variations. Experimental results on several face recognition benchmarks (LFW, AgeDB-30, CFP-FP, MegaFace Challenge 1) demonstrate the effectiveness of the proposed method and its superiority to existing angular softmax loss variants.",2020,IEEE Access,,10.1109/ACCESS.2020.3002270,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09116942.pdf
86b7e26018eb1dac51d1067ecf236b35b0a41cda,0,1,Deep Metric Learning With Tuplet Margin Loss,"Deep metric learning, in which the loss function plays a key role, has proven to be extremely useful in visual recognition tasks. However, existing deep metric learning loss functions such as contrastive loss and triplet loss usually rely on delicately selected samples (pairs or triplets) for fast convergence. In this paper, we propose a new deep metric learning loss function, tuplet margin loss, using randomly selected samples from each mini-batch. Specifically, the proposed tuplet margin loss implicitly up-weights hard samples and down-weights easy samples, while a slack margin in angular space is introduced to mitigate the problem of overfitting on the hardest sample. Furthermore, we address the problem of intra-pair variation by disentangling class-specific information to improve the generalizability of tuplet margin loss. Experimental results on three widely used deep metric learning datasets, CARS196, CUB200-2011, and Stanford Online Products, demonstrate significant improvements over existing deep metric learning methods.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00659,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf
86f9c76611562aa7f6ab4057b0879f69937a0a76,0,1,Supervised attention for speaker recognition,"The recently proposed self-attentive pooling (SAP) has shown good performance in several speaker recognition systems. In SAP systems, the context vector is trained end-to-end together with the feature extractor, where the role of context vector is to select the most discriminative frames for speaker recognition. However, the SAP underperforms compared to the temporal average pooling (TAP) baseline in some settings, which implies that the attention is not learnt effectively in end-to-end training. To tackle this problem, we introduce strategies for training the attention mechanism in a supervised manner, which learns the context vector using classified samples. With our proposed methods, context vector can be boosted to select the most informative frames. We show that our method outperforms existing methods in various experimental settings including short utterance speaker recognition, and achieves competitive performance over the existing baselines on the VoxCeleb datasets.",2020,ArXiv,2011.05189,,https://arxiv.org/pdf/2011.05189.pdf
8722ab858c45c9d0400cdf3ccffe7be7ab0b87d7,0,1,Introduction to deep learning,Deep Learning (DL) has made a major impact on data science in the last decade. This chapter introduces the basic concepts of this field. It includes both the basic structures used to design deep neural networks and a brief survey of some of its popular use cases.,2020,ArXiv,2003.03253,,https://arxiv.org/pdf/2003.03253.pdf
876e25d37151a825cd42a10b79f7c41e83a4d3dc,1,1,Asymmetric Rejection Loss for Fairer Face Recognition,"Face recognition performance has seen a tremendous gain in recent years, mostly due to the availability of large-scale face images dataset that can be exploited by deep neural networks to learn powerful face representations. However, recent research has shown differences in face recognition performance across different ethnic groups mostly due to the racial imbalance in the training datasets where Caucasian identities largely dominate other ethnicities. This is actually symptomatic of the under-representation of non-Caucasian ethnic groups in the celebdom from which face datasets are usually gathered, rendering the acquisition of labeled data of the under-represented groups challenging. In this paper, we propose an Asymmetric Rejection Loss, which aims at making full use of unlabeled images of those under-represented groups, to reduce the racial bias of face recognition models. We view each unlabeled image as a unique class, however as we cannot guarantee that two unlabeled samples are from a distinct class we exploit both labeled and unlabeled data in an asymmetric manner in our loss formalism. Extensive experiments show our method's strength in mitigating racial bias, outperforming state-of-the-art semi-supervision methods. Performance on the under-represented ethnicity groups increases while that on the well-represented group is nearly unchanged.",2020,ArXiv,2002.03276,,https://arxiv.org/pdf/2002.03276.pdf
8788b88e28e469f5a5d0d9ca3a9ecde0870abc17,0,1,Convolutional Neural Networks for Mobile Face Recognition with Hierarchical Feature integration,"Significant advances in face recognition tasks have been made with the development of convolutional neural networks. With large scale convolutional neural networks we are now able to achieve 99% accuracy on many public face test sets. However, the face recognition networks commonly used for mobile are limited by their narrow network width which makes it difficult to learn better distributed features in face recognition feature embedding learning. In this work, based on MobileFaceNets we propose an attention-guided hierarchical feature fusion model. Compared with the traditional cascading convolutional neural network classification model, our approach can explicitly consider both global and local features. And our approach requires only a small increase in computation to obtain better results than directly increasing the feature dimension.",2020,"2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)",,10.1109/ICIBA50161.2020.9276966,
87bfe8fbc17e16a95bb86a5eaddaefad4af510e4,1,1,Git Loss for Deep Face Recognition,"Convolutional Neural Networks (CNNs) have been widely used in computer vision tasks, such as face recognition and verification, and have achieved state-of-the-art results due to their ability to capture discriminative deep features. Conventionally, CNNs have been trained with softmax as supervision signal to penalize the classification loss. In order to further enhance the discriminative capability of deep features, we introduce a joint supervision signal, Git loss, which leverages on softmax and center loss functions. The aim of our loss function is to minimize the intra-class variations as well as maximize the inter-class distances. Such minimization and maximization of deep features are considered ideal for face recognition task. We perform experiments on two popular face recognition benchmarks datasets and show that our proposed loss function achieves maximum separability between deep face features of different identities and achieves state-of-the-art accuracy on two major face recognition benchmark datasets: Labeled Faces in the Wild (LFW) and YouTube Faces (YTF). However, it should be noted that the major objective of Git loss is to achieve maximum separability between deep features of divergent identities.",2018,BMVC,1807.08512,,https://arxiv.org/pdf/1807.08512.pdf
87e37a94e463ff15e7586282bf4d1e633ecd4939,1,1,Multi-Face: Self-supervised Multiview Adaptation for Robust Face Clustering in Videos,"Robust face clustering is a key step towards computational understanding of visual character portrayals in media. Face clustering for long-form content such as movies is challenging because of variations in appearance and lack of large-scale labeled video resources. However, local face tracking in videos can be used to mine samples belonging to same/different persons by examining the faces co-occurring in a video frame. In this work, we use this idea of self-supervision to harvest large amounts of weakly labeled face tracks in movies. We propose a nearest-neighbor search in the embedding space to mine hard examples from the face tracks followed by domain adaptation using multiview shared subspace learning. Our benchmarking on movie datasets demonstrate the robustness of multiview adaptation for face verification and clustering. We hope that the large-scale data resources developed in this work can further advance automatic character labeling in videos.",2020,ArXiv,2008.11289,,https://arxiv.org/pdf/2008.11289.pdf
87e78bcc187eabc8bc1689602f510b18ada4ce9e,1,0,Deep Residual Equivariant Mapping for Multi-angle Face Recognition,"Face recognition has caught a lot of attention and plenty of valuable methods have been proposed during the past decades. However, because it is hard to learn geometrically invariant representations, existing face recognition methods still perform relatively poorly in conducting multi-angle face recognition. In this paper, we hypothesize that there is an inherent mapping between the frontal and non-frontal faces, and the non-frontal face representations can be converted into the frontal face representations by an equivariant mapping. To carry out the mapping, we propose a Multi-Angle Deep Residual Equivariant Mapping (MADREM) block which adaptively maps the non-frontal face representation to the frontal face representation. It can be considered the MADREM block carry out face alignment and face normalization in the feature space. The residual equivariant mapping block can enhance the discriminative power of the face representations. Finally, we achieve an accuracy of 99.78% on the LFW dataset and 94.25% on CFP-FP dataset based on proposed multiscale-convolution and residual equivariant mapping block.",2019,CCBR,,10.1007/978-3-030-31456-9_16,
8806be8191e871282dc5dd93f8a1561fc3621dc3,0,1,Illumination-Invariant Face Recognition With Deep Relit Face Images,"Uncontrolled illumination is one of the most significant challenges in face recognition. The performance of state-of-the-art face recognition algorithms drops drastically when measured on datasets with large illumination variations. In this paper, we propose a deep face relighting algorithm and employ it as a data augmentation method to enrich training data with illumination variations. For an input image, the proposed face relighting as data augmentation (FRADA) approach first estimates its 3D morphable model coefficients and spherical harmonic lighting coefficients. Then, it extracts the face normals, face mask, face shading, and face albedo, and renders new face images under random lighting conditions following physically-based image formation theory. Qualitative results demonstrate that FRADA produces more realistic images than the state-of-the-art face relighting algorithm. Quantitative experiments confirm the effectiveness of our relighting approach for face recognition. We successfully enhance the robustness of face templates to illumination variations simply by training face recognition algorithms with our relit images.",2019,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV.2019.00232,
8835d031a74fb8429c82f9dfb1272e33a71730ba,0,1,AvatarMe: Realistically Renderable 3D Facial Reconstruction “In-the-Wild”,"Over the last years, with the advent of Generative Adversarial Networks (GANs), many face analysis tasks have accomplished astounding performance, with applications including, but not limited to, face generation and 3D face reconstruction from a single ""in-the-wild"" image. Nevertheless, to the best of our knowledge, there is no method which can produce high-resolution photorealistic 3D faces from ""in-the-wild"" images and this can be attributed to the: (a) scarcity of available data for training, and (b) lack of robust methodologies that can successfully be applied on very high-resolution data. In this paper, we introduce AvatarMe, the first method that is able to reconstruct photorealistic 3D faces from a single ""in-the-wild"" image with an increasing level of detail. To achieve this, we capture a large dataset of facial shape and reflectance and build on a state-of-the-art 3D texture and shape reconstruction method and successively refine its results, while generating the per-pixel diffuse and specular components that are required for realistic rendering. As we demonstrate in a series of qualitative and quantitative experiments, AvatarMe outperforms the existing arts by a significant margin and reconstructs authentic, 4K by 6K-resolution 3D faces from a single low-resolution image that, for the first time, bridges the uncanny valley.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.13845,10.1109/cvpr42600.2020.00084,https://arxiv.org/pdf/2003.13845.pdf
887109fb1cf30322911c315308d868d7aefc9572,0,1,NaMemo: Enhancing Lecturers' Interpersonal Competence of Remembering Students' Names,"Addressing students by their names helps a teacher to start building rapport with students and thus facilitates their classroom participation. However, this basic yet effective skill has become rather challenging for university lecturers, who have to handle large-sized (sometimes exceeding 100) groups in their daily teaching. To enhance lecturers' competence in delivering interpersonal interaction, we developed NaMemo, a real-time name-indicating system based on a dedicated face-recognition pipeline. This paper presents the system design, the pilot feasibility test, and our plan for the following study, which aims to evaluate NaMemo's impacts on learning and teaching, as well as to probe design implications including privacy considerations.",2020,Conference on Designing Interactive Systems,1911.09279,10.1145/3393914.3395860,https://arxiv.org/pdf/1911.09279.pdf
8875ae233bc074f5cd6c4ebba447b536a7e847a5,1,0,VoxCeleb2: Deep Speaker Recognition,"The objective of this paper is speaker recognition under noisy and unconstrained conditions.  We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset.  Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.",2018,INTERSPEECH,1806.05622,10.21437/Interspeech.2018-1929,https://arxiv.org/pdf/1806.05622.pdf
8878428c0edb28fadf45cd2d97d1718c3a0ebbce,1,1,Towards Universal Representation Learning for Deep Face Recognition,"Recognizing wild faces is extremely hard as they appear with all kinds of variations. Traditional methods either train with specifically annotated variation data from target domains, or by introducing unlabeled target variation data to adapt from the training data. Instead, we propose a universal representation learning framework that can deal with larger variation unseen in the given training data without leveraging target domain knowledge. We firstly synthesize training data alongside some semantically meaningful variations, such as low resolution, occlusion and head pose. However, directly feeding the augmented data for training will not converge well as the newly introduced samples are mostly hard examples. We propose to split the feature embedding into multiple sub-embeddings, and associate different confidence values for each sub-embedding to smooth the training procedure. The sub-embeddings are further decorrelated by regularizing variation classification loss and variation adversarial loss on different partitions of them. Experiments show that our method achieves top performance on general face recognition datasets such as LFW and MegaFace, while significantly better on extreme benchmarks such as TinyFace and IJB-S.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.11841,10.1109/cvpr42600.2020.00685,https://arxiv.org/pdf/2002.11841.pdf
888018da941ba3b4f38c1a28138f693b665fdeec,0,1,SpeakerNet: 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification.,"We propose SpeakerNet - a new neural architecture for speaker recognition and speaker verification tasks. It is composed of residual blocks with 1D depth-wise separable convolutions, batch-normalization, and ReLU layers. This architecture uses x-vector based statistics pooling layer to map variable-length utterances to a fixed-length embedding (q-vector). SpeakerNet-M is a simple lightweight model with just 5M parameters. It doesn't use voice activity detection (VAD) and achieves close to state-of-the-art performance scoring an Equal Error Rate (EER) of 2.10% on the VoxCeleb1 cleaned and 2.29% on the VoxCeleb1 trial files.",2020,,2010.12653,,https://arxiv.org/pdf/2010.12653.pdf
88e03dacc1934d638f39153bad479c4333145ab3,0,1,Subjective Versus Objective Face Image Quality Evaluation For Face Recognition,"The performance of any face recognition system gets affected by the quality of the probe and the reference images. Rejecting or recapturing images with low-quality can improve the overall performance of the biometric system. There are many statistical as well as learning-based methods that provide quality scores given an image for the task of face recognition. In this study, we take a different approach by asking 26 participants to provide subjective quality scores that represent the ease of recognizing the face on the images from a smartphone based face image dataset. These scores are then compared to measures implemented from ISO/IEC TR 29794-5. We observe that the subjective scores outperform the implemented objective scores while having a low correlation with them. Furthermore, we analyze the effect of pose, illumination, and distance on face recognition similarity scores as well as the generated mean opinion scores.",2019,ICBEA,,10.1145/3345336.3345338,
89197832db92247a4ebcb6b558e9c557d25fb092,1,0,A Meta-Analysis of the Impact of Skin Type and Gender on Non-contact Photoplethysmography Measurements,"It is well established that many datasets used for computer vision tasks are not representative and may be biased. The result of this is that evaluation metrics may not reflect real-world performance and might expose some groups (often minorities) to greater risks than others. Imaging photoplethysmography is a set of techniques that enables noncontact measurement of vital signs using imaging devices. While these methods hold great promise for low-cost and scalable physiological monitoring, it is important that performance is characterized accurately over diverse populations. We perform a meta-analysis across three datasets, including 73 people and over 400 videos featuring a broad range of skin types to study how skin types and gender affect the measurements. While heart rate measurement can be performed on all skin types under certain conditions, we find that average performance drops significantly for the darkest skin type. We also observe a slight drop in the performance for females. We compare supervised and unsupervised learning algorithms and find that skin type does not impact all methods equally. The imaging photoplethysmography community should devote greater efforts to addressing these disparities and collecting representative datasets.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00150,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w19/Nowara_A_Meta-Analysis_of_the_Impact_of_Skin_Tone_and_Gender_CVPRW_2020_paper.pdf
891dfc8c124680a5b03c8dbf84e6136286015861,1,1,Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition,"Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the “committee” and the “mediator”, which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.",2018,ECCV,1809.01407,10.1007/978-3-030-01240-3_35,https://arxiv.org/pdf/1809.01407.pdf
894a7488fa96ef6fb6374b676ee4819c509cb4f6,1,0,Evaluation of Face Detectors and Feature Association Metrics for Real-Time Multi-Face Tracking,"Video annotation, control of camera direction, labelling and other tasks can benefit from online visual multi-face tracking. Given the availability of high quality general purpose detectors and tracking-by-detection frameworks, we develop a multi-face tracker and comparatively evaluate its components. In this thesis, we train common object detectors on large databases of faces to understand how well these detectors can perform, specifically on faces. We evaluate different face association methods and appearance metrics with different classifier loss functions to track detected faces across frames. We find that while online tracking based on combining state-of-the-art methods can lead to high-quality tracking results, there is still a large gap between offline and online methods. We develop a multi-tracking system in order to achieve an online and real-time standard, one that can track most of the faces in unconstrained settings.",2020,,,10.20381/RUOR-24982,https://ruor.uottawa.ca/bitstream/10393/40755/1/Wang_Jianzhou_2020_thesis.pdf
89535c99fc07ec6478b9d5429e85019e1544c0cb,1,1,Deep Diamond Re-ID,"Re-identification neural networks are widely used in numerous applications such as crowd control, crime investigations, safety systems and even in most smartphones to unlock the phone with a picture of the owner. These techniques are mostly used to re-identify faces or persons but in this paper we investigate the possibility to adapt these to also re-identify similar looking objects such as diamonds. Since polished diamonds are very similar to the naked eye, it is difficult to distinguish one diamond from another. We have indications that diamonds are sometimes switched by trained switchers with fake or less expensive stones, while they pretend to inspect the stone. A solution to this is diamond fingerprinting. We therefore propose a technique to generate a unique ID for each stone, which allows to re-identify the diamond solely based on an image of the gem. Since each diamond is assigned a unique ID it is even possible to keep track of the diamonds over time. This allows the seller to verify his stones before and after trading while switchers don't stand a chance. For this task we trained and adapted a classification network optimized for both speed and accuracy.",2019,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),,10.1109/ICMLA.2019.00323,
89604e46edefeead4317392c0bff37dff453f4d8,0,1,THE PURDUE UNIVERSITY GRADUATE SCHOOL STATEMENT OF THESIS APPROVAL,"Mas Montserrat, Daniel Ph.D., Purdue University, June 2020. Machine Learning-Based Multimedia Analytics. Major Professor: Edward J. Delp. Machine learning is widely used to extract meaningful information from video, images, audio, text, and other multimedia data. Through a hierarchical structure, modern neural networks coupled with backpropagation learn to extract information from large amounts of data and to perform specific tasks such as classification or regression. In this thesis, we explore various approaches to multimedia analytics with neural networks. We present several image synthesis and rendering techniques to generate new images for training neural networks. Furthermore, we present multiple neural network architectures and systems for commercial logo detection, 3D pose estimation and tracking, deepfakes detection, and manipulation detection in satellite images.",2017,,,,https://engineering.purdue.edu/~ace/thesis/danni/daniel-thesis-final.pdf
8969ae0630b093f623124c5f4d1012e1ddaee404,1,0,Heterogeneous Facial Analysis and Synthesis,"This chapter gives an overview of the heterogeneous problem in facial analysis as well as its synthesis solution, followed by a brief outline of the rest chapters. We start from the background and challenges in research of heterogeneous facial analysis. Then the heterogeneous facial synthesis is emphasized as one of the promising and effective solutions. The instantiated tasks to be elaborated in the following chapters are finally introduced in a compact manner.",2020,Springer Briefs in Computer Science,,10.1007/978-981-13-9148-4,
896a8c90bcd86de2e0440f1ee2352708310bac2e,1,0,People Groups Analysis for AR Applications,"Automatically characterizing groups and crowds of people plays an important role in different domains such as psychology, architecture or entertainment. In the engineering field, people grouping is important in social signal processing but also videosurveilance. Our challenge is to introduce such solutions into the field of augmented reality, where people get added content on real groups of people.As a preliminary work, this paper presents a system which provides a live visual feedback in virtual reality augmenting real groups of people with added information such as their ID, group ID or group coherence. The scene was analyzed with mainly one classical RGB camera and enhanced with a smartphone that a holder points towards the other persons.This paper goes through the proposed system, which is capable of tracking people, performing people grouping, analyzing groups and augmenting those groups in a virtual world.The first qualitative results show the feasibility of an augmented crowd environment and provide a set of interesting practical insights on the different modules of this system in the context of real-life scenes.",2018,2018 International Conference on 3D Immersion (IC3D),,10.1109/IC3D.2018.8657892,
897a416b12eb7f624fd841f1e846936184dffb37,1,0,Deep Facial Diagnosis: Deep Transfer Learning From Face Recognition to Facial Diagnosis,"The relationship between face and disease has been discussed from thousands years ago, which leads to the occurrence of facial diagnosis. The objective here is to explore the possibility of identifying diseases from uncontrolled 2D face images by deep learning techniques. In this paper, we propose using deep transfer learning from face recognition to perform the computer-aided facial diagnosis on various diseases. In the experiments, we perform the computer-aided facial diagnosis on single (beta-thalassemia) and multiple diseases (beta-thalassemia, hyperthyroidism, Down syndrome, and leprosy) with a relatively small dataset. The overall top-1 accuracy by deep transfer learning from face recognition can reach over 90% which outperforms the performance of both traditional machine learning methods and clinicians in the experiments. In practical, collecting disease-specific face images is complex, expensive and time consuming, and imposes ethical limitations due to personal data treatment. Therefore, the datasets of facial diagnosis related researches are private and generally small comparing with the ones of other machine learning application areas. The success of deep transfer learning applications in the facial diagnosis with a small dataset could provide a low-cost and noninvasive way for disease screening and detection.",2020,IEEE Access,,10.1109/ACCESS.2020.3005687,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09127907.pdf
89998030721e58ade6349b9426cc8c8d81103028,0,1,A Comprehensive Survey of Loss Functions in Machine Learning,"As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.",2020,,,10.1007/s40745-020-00253-5,
89cb2854c65894345eb161941be3d0e7f06ae84e,0,1,An efficient face recognition approach combining likelihood-based sufficient dimension reduction and LDA,"In this paper, an efficient approach is proposed for face recognition (FR) under pose and illumination variations. It is based on combining likelihood-based sufficient dimension reduction (LSDR) and linear discriminant analysis (LDA) using different facial features. LDA is a well–established technique for dimensionality reduction, while LSDR is a relatively new supervised subspace learning method based on the key concept of sufficiency, and consists of estimating a central subspace for low-dimensional representation of facial images. We show, empirically, that this combination can obviously increase the separability between face classes. The facial features that were considered here are either engineered (hand-crafted) features (e.g., discrete shearlet transform coefficients) or learned features which are obtained by retraining a deep learning-based system called FaceNet. To assess the performance of the proposed approach, different classification methods were used during the evaluation phase, namely collaborative representation based classifier (CRC), KNN, linear SVM and three regression-based classifiers (LSRC, LRC and LDRC). The extensive experiments performed on four publicly available face databases – the extended Yale B, FERET, LFW and CMU Multi-PIE– have demonstrated that, on the one hand, LSDR is an effective and efficiency dimensionality reduction technique for face recognition. Particularly, it can be used to significantly increase the performance of a deep learning-based system such as FaceNet, mainly, when training samples are insufficient. On the other hand, its combination with LDA can outperform best individual face recognition algorithm based on LSDR or LDA. Furthermore, the proposed approach compares favorably to current state-of-the-art methods on face recognition, in terms of classification accuracy, under illumination and pose variations.",2020,,,10.1007/s11042-020-09527-9,
8a09a0cb36fd3b70d81566d5423f8b136f76edef,0,1,Facial Attribute Capsules for Noise Face Super Resolution,"Existing face super-resolution (SR) methods mainly assume the input image to be noise-free. Their performance degrades drastically when applied to real-world scenarios where the input image is always contaminated by noise. In this paper, we propose a Facial Attribute Capsules Network (FACN) to deal with the problem of high-scale super-resolution of noisy face image. Capsule is a group of neurons whose activity vector models different properties of the same entity. Inspired by the concept of capsule, we propose an integrated representation model of facial information, which named Facial Attribute Capsule (FAC). In the SR processing, we first generated a group of FACs from the input LR face, and then reconstructed the HR face from this group of FACs. Aiming to effectively improve the robustness of FAC to noise, we generate FAC in semantic, probabilistic and facial attributes manners by means of integrated learning strategy. Each FAC can be divided into two sub-capsules: Semantic Capsule (SC) and Probabilistic Capsule (PC). Them describe an explicit facial attribute in detail from two aspects of semantic representation and probability distribution. The group of FACs model an image as a combination of facial attribute information in the semantic space and probabilistic space by an attribute-disentangling way. The diverse FACs could better combine the face prior information to generate the face images with fine-grained semantic attributes. Extensive benchmark experiments show that our method achieves superior hallucination results and outperforms state-of-the-art for very low resolution (LR) noise face image super resolution.",2020,AAAI,2002.06518,10.1609/AAAI.V34I07.6935,https://arxiv.org/pdf/2002.06518.pdf
8a2deb2b4216f6c065c5e955706ce157d96625a1,1,1,"A Fast and Accurate System for Face Detection, Identification, and Verification","The availability of large annotated datasets and affordable computation power have led to impressive improvements in the performance of convolutional neural networks (CNNs) on various face analysis tasks. In this paper, we describe a deep learning pipeline for unconstrained face identification and verification which achieves state-of-the-art performance on several benchmark datasets. We provide the design details of the various modules involved in automatic face recognition: face detection, landmark localization and alignment, and face identification/verification. We propose a novel face detector, deep pyramid single shot face detector (DPSSD), which is fast and detects faces with large scale variations (especially tiny faces). Additionally, we propose a new loss function, called crystal loss, for the tasks of face verification and identification. Crystal loss restricts the feature descriptors to lie on a hypersphere of a fixed radius, thus minimizing the angular distance between positive subject pairs and maximizing the angular distance between negative subject pairs. We provide evaluation results of the proposed face detector on challenging unconstrained face detection datasets. Then, we present experimental results for end-to-end face verification and identification on IARPA Janus Benchmarks A, B, and C (IJB-A, IJB-B, IJB-C), and the Janus Challenge Set 5 (CS5).",2019,"IEEE Transactions on Biometrics, Behavior, and Identity Science",1809.07586,10.1109/TBIOM.2019.2908436,https://arxiv.org/pdf/1809.07586.pdf
8accd7a0f4fbb86ed1ebd6eae20b344b42727410,1,0,Audio-visual Speech Separation with Adversarially Disentangled Visual Representation,"Speech separation aims to separate individual voice from an audio mixture of multiple simultaneous talkers. Although audio-only approaches achieve satisfactory performance, they build on a strategy to handle the predefined conditions, limiting their application in the complex auditory scene. Towards the cocktail party problem, we propose a novel audio-visual speech separation model. In our model, we use the face detector to detect the number of speakers in the scene and use visual information to avoid the permutation problem. To improve our model’s generalization ability to unknown speakers, we extract speech-related visual features from visual inputs explicitly by the adversarially disentangled method, and use this feature to assist speech separation. Besides, the time-domain approach is adopted, which could avoid the phase reconstruction problem existing in the time-frequency domain models. To compare our model’s performance with other models, we create two benchmark datasets of 2-speaker mixture from GRID and TCDTIMIT audio-visual datasets. Through a series of experiments, our proposed model is shown to outperform the state-of-the-art audio-only model and three audio-visual models.",2020,ArXiv,2011.14334,,https://arxiv.org/pdf/2011.14334.pdf
8b069be51c20ac10e8ca055a111c981fbbac44d1,1,0,Universal Approximation Capability of Broad Learning System and Its Structural Variations,"After a very fast and efficient discriminative broad learning system (BLS) that takes advantage of flatted structure and incremental learning has been developed, here, a mathematical proof of the universal approximation property of BLS is provided. In addition, the framework of several BLS variants with their mathematical modeling is given. The variations include cascade, recurrent, and broad–deep combination structures. From the experimental results, the BLS and its variations outperform several exist learning algorithms on regression performance over function approximation, time series prediction, and face recognition databases. In addition, experiments on the extremely challenging data set, such as MS-Celeb-1M, are given. Compared with other convolutional networks, the effectiveness and efficiency of the variants of BLS are demonstrated.",2019,IEEE Transactions on Neural Networks and Learning Systems,,10.1109/TNNLS.2018.2866622,
8b2132b20a824d4ce779142e1e7805fda9794792,0,1,Ts-Fen: Probing Feature Selection Strategy for Face Anti-Spoofing,"Deep features extracted from different domains have shown great advantages in the face anti-spoofing task. Previous extraction strategies consider less of the extent variation in distinction among feature properties. Many of them straightly make classification using the extracted information but generalize weakly. In this paper, we propose a novel Two-Stream Feature Extraction Network (TS-FEN) based on depth and chrominance cues, guiding both sparsity and density of the feature distribution. We specifically design a Feature Enhancement (FE) Structure in the depth stream to strengthen the discrimination capacity, as well as a Feature Selection (FS) Module in the chroma stream to keep feature diversity and distinction. Besides, the specially designed bias arcface loss aims to enlarge the central distribution dispersion of opposite categories. Extensive experiments on three benchmark datasets validate that our proposed approach achieves explicit improvement on both intra-testing and cross-testing.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9054115,
8b2c695865ddcd63f0fe83cd0c5abca26bacb295,0,1,Harnessing GAN with Metric Learning for One-Shot Generation on a Fine-Grained Category,"We propose a GAN-based one-shot generation method on a fine-grained category, which represents a subclass of a category, typically with diverse examples. One-shot generation refers to a task of taking an image which belongs to a class not used in the training phase and then generating a set of new images belonging to the same class. Generative Adversarial Network (GAN), which represents a type of deep neural networks with competing generator and discriminator, has proven to be useful in generating realistic images. Especially DAGAN, which maps the input image to a low-dimensional space via an encoder and then back to the example space via a decoder, has been quite effective with datasets such as handwritten character datasets. However, when the class corresponds to a fine-grained category, DAGAN occasionally generates images which are regarded as belonging to other classes due to the rich variety of the examples in the class and the low dissimilarities of the examples among the classes. For example, it accidentally generates facial images of different persons when the class corresponds to a specific person. To circumvent this problem, we introduce a metric learning with a triplet loss to the bottleneck layer of DAGAN to penalize such a generation. We also extend the optimization algorithm of DAGAN to an alternating procedure for two types of loss functions. Our proposed method outperforms DAGAN in the GAN-test task for VGG-Face dataset and CompCars dataset by 5.6% and 4.8% in accuracy, respectively. We also conducted experiments for the data augmentation task and observed 4.5% higher accuracy for our proposed method over DAGAN for VGG-Face dataset.",2019,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),,10.1109/ICTAI.2019.00130,
8b3cb5fc6c270b704fecc4755042dab79a954a84,1,0,Coupled Deep Learning for Heterogeneous Face Recognition,"Heterogeneous face matching is a challenge issue in face recognition due to large domain difference as well as insufficient pairwise images in different modalities during training. This paper proposes a coupled deep learning (CDL) approach for the heterogeneous face matching. CDL seeks a shared feature space in which the heterogeneous face matching problem can be approximately treated as a homogeneous face matching problem. The objective function of CDL mainly includes two parts. The first part contains a trace norm and a block-diagonal prior as relevance constraints, which not only make unpaired images from multiple modalities be clustered and correlated, but also regularize the parameters to alleviate overfitting. An approximate variational formulation is introduced to deal with the difficulties of optimizing low-rank constraint directly. The second part contains a cross modal ranking among triplet domain specific images to maximize the margin for different identities and increase data for a small amount of training samples. Besides, an alternating minimization method is employed to iteratively update the parameters of CDL. Experimental results show that CDL achieves better performance on the challenging CASIA NIR-VIS 2.0 face recognition database, the IIIT-D Sketch database, the CUHK Face Sketch (CUFS), and the CUHK Face Sketch FERET (CUFSF), which significantly outperforms state-of-the-art heterogeneous face recognition methods.",2018,AAAI,1704.0245,,https://arxiv.org/pdf/1704.02450.pdf
8b621677a6e8329542ce383891cc03f350bdd0eb,1,1,Search to Distill: Pearls Are Everywhere but Not the Eyes,"Standard Knowledge Distillation (KD) approaches distill the knowledge of a cumbersome teacher model into the parameters of a student model with a pre-defined architecture. However, the knowledge of a neural network, which is represented by the network's output distribution conditioned on its input, depends not only on its parameters but also on its architecture. Hence, a more generalized approach for KD is to distill the teacher's knowledge into both the parameters and architecture of the student. To achieve this, we present a new \textit{Architecture-aware Knowledge Distillation (AKD)} approach that finds student models (pearls for the teacher) that are best for distilling the given teacher model. In particular, we leverage Neural Architecture Search (NAS), equipped with our KD-guided reward, to search for the best student architectures for a given teacher. Experimental results show our proposed AKD consistently outperforms the conventional NAS plus KD approach, and achieves state-of-the-art results on the ImageNet classification task under various latency settings. Furthermore, the best AKD student architecture for the ImageNet classification task also transfers well to other tasks such as million level face recognition and ensemble learning.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1911.09074,10.1109/cvpr42600.2020.00756,https://arxiv.org/pdf/1911.09074.pdf
8b79bfc93c7001dd7bb1eea7715af47d76202827,0,1,Decorrelated Adversarial Learning for Age-Invariant Face Recognition,"There has been an increasing research interest in age-invariant face recognition. However, matching faces with big age gaps remains a challenging problem, primarily due to the significant discrepancy of face appearance caused by aging. To reduce such discrepancy, in this paper we present a novel algorithm to remove age-related components from features mixed with both identity and age information. Specifically, we factorize a mixed face feature into two uncorrelated components: identity-dependent component and age-dependent component, where the identity-dependent component contains information that is useful for face recognition. To implement this idea, we propose the Decorrelated Adversarial Learning (DAL) algorithm, where a Canonical Mapping Module (CMM) is introduced to find maximum correlation of the paired features generated by the backbone network, while the backbone network and the factorization module are trained to generate features reducing the correlation. Thus, the proposed model learns the decomposed features of age and identity whose correlation is significantly reduced. Simultaneously, the identity-dependent feature and the age-dependent feature are supervised by ID and age preserving signals respectively to ensure they contain the correct information. Extensive experiments have been conducted on the popular public-domain face aging datasets (FG-NET, MORPH Album 2, and CACD-VS) to demonstrate the effectiveness of the proposed approach.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1904.04972,10.1109/CVPR.2019.00364,
8bb8e427ec84f3e75383c1e3866c8467ac7cb0e3,0,1,Think about boundary: Fusing multi-level boundary information for landmark heatmap regression,"Although current face alignment algorithms have obtained pretty good performances at predicting the location of facial landmarks, huge challenges remain for faces with severe occlusion and large pose variations, etc. On the contrary, semantic location of facial boundary is more likely to be reserved and estimated on these scenes. Therefore, we study a two-stage but end-to-end approach for exploring the relationship between the facial boundary and landmarks to get boundary-aware landmark predictions, which consists of two modules: the self-calibrated boundary estimation (SCBE) module and the boundary-aware landmark transform (BALT) module. In the SCBE module, we modify the stem layers and employ intermediate supervision to help generate high-quality facial boundary heatmaps. Boundary-aware features inherited from the SCBE module are integrated into the BALT module in a multi-scale fusion framework to better model the transformation from boundary to landmark heatmap. Experimental results conducted on the challenging benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in the literature.",2020,ArXiv,2008.10924,,https://arxiv.org/pdf/2008.10924.pdf
8bca75107f6bf1e329feb6643c0fe3abf557732d,0,1,FastReID: A Pytorch Toolbox for General Instance Re-identification,"General Instance Re-identification is a very important task in the computer vision, which can be widely used in many practical applications, such as person/vehicle re-identification, face recognition, wildlife protection, commodity tracing, and snapshop, etc.. To meet the increasing application demand for general instance re-identification, we present FastReID as a widely used software system in JD AI Research. In FastReID, highly modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art projects, including person re-id, partial re-id, cross-domain re-id and vehicle re-id, and plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most general and high-performance toolbox that supports single and multiple GPU servers, you can reproduce our project results very easily and are very welcome to use it, the code and models are available at this https URL.",2020,ArXiv,2006.02631,,https://arxiv.org/pdf/2006.02631.pdf
8c52b095a682182d481779761f58691773a1175e,0,1,Collaborative Learning for Faster StyleGAN Embedding,"The latent code of the recent popular model StyleGAN has learned disentangled representations thanks to the multi-layer style-based generator. Embedding a given image back to the latent space of StyleGAN enables wide interesting semantic image editing applications. Although previous works are able to yield impressive inversion results based on an optimization framework, which however suffers from the efficiency issue. In this work, we propose a novel collaborative learning framework that consists of an efficient embedding network and an optimization-based iterator. On one hand, with the progress of training, the embedding network gives a reasonable latent code initialization for the iterator. On the other hand, the updated latent code from the iterator in turn supervises the embedding network. In the end, high-quality latent code can be obtained efficiently with a single forward pass through our embedding network. Extensive experiments demonstrate the effectiveness and efficiency of our work.",2020,ArXiv,2007.01758,,https://arxiv.org/pdf/2007.01758.pdf
8cdbcca38600696905f70bab7b315ceeb68e4dab,0,1,DeepMEF: A Deep Model Ensemble Framework for Video Based Multi-modal Person Identification,"The goal of video based multi-modal person identification is to identify a person of interest using multi-modal video features, such as person's face, body, audio or head features. This task is challenging due to many factors, for example, variant body or face poses, poor face image quality, low frame resolution, etc. To address these problems, we propose a deep model ensemble framework, namely DeepMEF. Specifically, the proposed framework includes three novel modules, i.e., the video feature fusion module, the multi-modal feature fusion module and the model ensemble module. The first and second module form the basic deep model for ensemble, with the video feature fusion module fuses facial features from different frames as one. Then the multi-modal feature fusion module further fuses the face feature and features of other modalities for identification. In this work, we adopt the scene feature extracted by ourselves as the additional input of the multi-modal module. At last, the model ensemble module promotes the overall performance by combining the predictions of multiple multi-modal learners. The proposed method achieves a competitive result of 89.86% in mAP on the iQIYI-VID-2019 dataset, which helps us win the third place in the 2019 iQIYI Celebrity Video Identification Challenge.",2019,ACM Multimedia,,10.1145/3343031.3356057,
8d5aa448fefeed5646143b56978c6f4337887f0e,1,1,Deep Feature Augmentation for Occluded Image Classification,"Abstract Due to the difficulty in acquiring massive task-specific occluded images, the classification of occluded images with deep convolutional neural networks (CNNs) remains highly challenging. To alleviate the dependency on large-scale occluded image datasets, we propose a novel approach to improve the classification accuracy of occluded images by fine-tuning the pre-trained models with a set of augmented deep feature vectors (DFVs). The set of augmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs are generated by randomly adding difference vectors (DVs), extracted from a small set of clean and occluded image pairs, to the real DFVs. In the fine-tuning, the back-propagation is conducted on the DFV data flow to update the network parameters. The experiments on various datasets and network structures show that the deep feature augmentation significantly improves the classification accuracy of occluded images without a noticeable influence on the performance of clean images. Specifically, on the ILSVRC2012 dataset with synthetic occluded images, the proposed approach achieves 11.21% and 9.14% average increases in classification accuracy for the ResNet50 networks fine-tuned on the occlusion-exclusive and occlusion-inclusive training sets, respectively.",2020,ArXiv,2011.00768,10.1016/j.patcog.2020.107737,https://arxiv.org/pdf/2011.00768.pdf
8d6e70947bfb865c6d151691ef1e2a31a93891d0,1,0,Extended Labeled Faces in-the-Wild (ELFW): Augmenting Classes for Face Segmentation,"Existing face datasets often lack sufficient representation of occluding objects, which can hinder recognition, but also supply meaningful information to understand the visual context. In this work, we introduce Extended Labeled Faces in-the-Wild (ELFW), a dataset supplementing with additional face-related categories -- and also additional faces -- the originally released semantic labels in the vastly used Labeled Faces in-the-Wild (LFW) dataset. Additionally, two object-based data augmentation techniques are deployed to synthetically enrich under-represented categories which, in benchmarking experiments, reveal that not only segmenting the augmented categories improves, but also the remaining ones benefit.",2020,ArXiv,2006.1398,,https://arxiv.org/pdf/2006.13980.pdf
8d965fbe99b5bdd71d764f502b239aae5dde89ec,0,1,Research on Inception Module Incorporated Siamese Convolutional Neural Networks to Realize Face Recognition,"Face recognition is an active research subject of biometrics due to its significant research and application prospects. The performance of face recognition can be affected by a series of uncontrollable factors, such as illumination, expression, posture and occlusion, which restricts its real-world applications. Therefore, improving the robustness of face recognition to environmental changes became an urgent problem. In this paper, a simplified deep convolutional neural network structure having high robustness under unlimited conditions is designed for face recognition. This structure can improve training speed and face recognition accuracy, and be suitable for small-scale data sets. Inception Module Incorporated Siamese Convolutional Neural Networks (IMISCNN) is developed based on effective reduction of external interference and better features extraction by adopting the Siamese network structure. A cyclical learning rate strategy is also introduced in IMISCNN for better model convergence. Compared to classical face recognition algorithms, such as PCA, PCA and SVM, CNN, PCANet, and the original SNN et al. The accuracy of IMISCNN in CASIA-webface and Extended Yale B standard face database is 99.36% and 99.21%, respectively. Its feasibility and effectiveness have been verified in our experiments.",2020,IEEE Access,,10.1109/ACCESS.2019.2963211,
8de1c724a42d204c0050fe4c4b4e81a675d7f57c,1,0,Deep Face Recognition: A Survey,"Face recognition made tremendous leaps in the last five years with a myriad of systems proposing novel techniques substantially backed by deep convolutional neural networks (DCNN). Although face recognition performance sky-rocketed using deep-learning in classic datasets like LFW, leading to the belief that this technique reached human performance, it still remains an open problem in unconstrained environments as demonstrated by the newly released IJB datasets. This survey aims to summarize the main advances in deep face recognition and, more in general, in learning face representations for verification and identification. The survey provides a clear, structured presentation of the principal, state-of-the-art (SOTA) face recognition techniques appearing within the past five years in top computer vision venues. The survey is broken down into multiple parts that follow a standard face recognition pipeline: (a) how SOTA systems are trained and which public data sets have they used; (b) face preprocessing part (detection, alignment, etc.); (c) architecture and loss functions used for transfer learning (d) face recognition for verification and identification. The survey concludes with an overview of the SOTA results at a glance along with some open issues currently overlooked by the community.",2018,"2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)",,10.1109/SIBGRAPI.2018.00067,https://talhassner.github.io/home/projects/DeepFaceSurvey/Masietal2018deepfacesurvey.pdf
8e000216feb96cf335bcff744ccfc2bd428080d4,0,1,Data-Free Point Cloud Network for 3D Face Recognition,"Point clouds-based Networks have achieved great attention in 3D object classification, segmentation and indoor scene semantic parsing. In terms of face recognition, 3D face recognition method which directly consume point clouds as input is still under study. Two main factors account for this: One is how to get discriminative face representations from 3D point clouds using deep network; the other is the lack of large 3D training dataset. To address these problems, a data-free 3D face recognition method is proposed only using synthesized unreal data from statistical 3D Morphable Model to train a deep point cloud network. To ease the inconsistent distribution between model data and real faces, different point sampling methods are used in train and test phase. In this paper, we propose a curvature-aware point sampling(CPS) strategy replacing the original furthest point sampling(FPS) to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate features deeply. A PointNet++ like Network is used to extract face features directly from point clouds. The experimental results show that the network trained on generated data generalizes well for real 3D faces. Fine tuning on a small part of FRGCv2.0 and Bosphorus, which include real faces in different poses and expressions, further improves recognition accuracy.",2019,ArXiv,1911.04731,,https://arxiv.org/pdf/1911.04731.pdf
8e092639b59b5807a72f483a26dbdee250633846,1,0,Maxout Networks for Visual Recognition,,2019,Int. J. Multim. Data Eng. Manag.,,10.4018/ijmdem.2019100101,
8e523b951db4350eecde8e121516998eb42b167e,1,0,A Dataset for Comparing Mirrored and Non-Mirrored Male Bust Images for Facial Recognition,"Facial recognition, as well as other types of human recognition, have found uses in identification, security, and learning about behavior, among other uses. Because of the high cost of data collection for training purposes, logistical challenges and other impediments, mirroring images has frequently been used to increase the size of data sets. However, while these larger data sets have shown to be beneficial, their comparative level of benefit to the data collection of similar data has not been assessed. This paper presented a data set collected and prepared for this and related research purposes. The data set included both non-occluded and occluded data for mirroring assessment.",2019,Data,,10.3390/DATA4010026,https://pdfs.semanticscholar.org/8e52/3b951db4350eecde8e121516998eb42b167e.pdf
8e5e1ecd7f5c27e891573d40137e696dda98837a,1,0,Longitudinal Study of Child Face Recognition,"We present a longitudinal study of face recognition performance on Children Longitudinal Face (CLF) dataset containing 3,682 face images of 919 subjects, in the age group [2,18] years. Each subject has at least four face images acquired over a time span of up to six years. Face comparison scores are obtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source matcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTSA and FaceNet matchers. To improve the performance of the open-source FaceNet matcher for child face recognition, we were able to fine-tune it on an independent training set of 3,294 face images of 1,119 children in the age group [3,18] years. Multilevel statistical models are fit to genuine comparison scores from the CLF dataset to determine the decrease in face recognition accuracy over time. Additionally, we analyze both the verification and open-set identification accuracies in order to evaluate state-of-the-art face recognition technology for tracing and identifying children lost at a young age as victims of child trafficking or abduction.",2018,2018 International Conference on Biometrics (ICB),1711.0399,10.1109/ICB2018.2018.00042,https://arxiv.org/pdf/1711.03990.pdf
8eabed69bbebd83d90c7c27b731ff76edcd6b0a9,0,1,SSN: Learning Sparse Switchable Normalization via SparsestMax,"Normalization method deals with parameters training of convolution neural networks (CNNs) in which there are often multiple convolution layers. Despite the fact that layers in CNN are not homogeneous in the role they play at representing a prediction function, existing works often employ identical normalizer in different layers, making performance away from idealism. To tackle this problem and further boost performance, a recently-proposed switchable normalization (SN) provides a new perspective for deep learning: it learns to select different normalizers for different convolution layers of a ConvNet. However, SN uses softmax function to learn importance ratios to combine normalizers, not only leading to redundant computations compared to a single normalizer but also making model less interpretable. This work addresses this issue by presenting sparse switchable normalization (SSN) where the importance ratios are constrained to be sparse. Unlike ℓ1\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\ell _1$$\end{document} and ℓ0\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\ell _0$$\end{document} regularizations that impose difficulties in tuning layer-wise regularization coefficients, we turn this sparse-constrained optimization problem into feed-forward computation by proposing SparsestMax, which is a sparse version of softmax. SSN has several appealing properties. (1) It inherits all benefits from SN such as applicability in various tasks and robustness to a wide range of batch sizes. (2) It is guaranteed to select only one normalizer for each normalization layer, avoiding redundant computations and improving interpretability of normalizer selection. (3) SSN can be transferred to various tasks in an end-to-end manner. Extensive experiments show that SSN outperforms its counterparts on various challenging benchmarks such as ImageNet, COCO, Cityscapes, ADE20K, Kinetics and MegaFace. Models and code are available at https://github.com/switchablenorms/Sparse_SwitchNorm.",2019,International Journal of Computer Vision,1903.03793,10.1007/s11263-019-01269-y,https://arxiv.org/pdf/1903.03793.pdf
8ed7d8a07ac21328d3c03ff3880e6f6f939257f6,0,1,Adversarial Learning for Age-Invariant Face Recognition,"There has been an increasing research interest in ageinvariant face recognition. However, matching faces with big age gaps remains a challenging problem, primarily due to the significant discrepancy of face appearance caused by aging. To reduce such discrepancy, in this paper we present a novel algorithm to remove age-related components from features mixed with both identity and age information. Specifically, we factorize a mixed face feature into two uncorrelated components: identity-dependent component and age-dependent component, where the identitydependent component contains information that is useful for face recognition. To implement this idea, we propose the Decorrelated Adversarial Learning (DAL) algorithm, where a Canonical Mapping Module (CMM) is introduced to find maximum correlation of the paired features generated by the backbone network, while the backbone network and the factorization module are trained to generate features reducing the correlation. Thus, the proposed model learns the decomposed features of age and identity whose correlation is significantly reduced. Simultaneously, the identity-dependent feature and the age-dependent feature are supervised by ID and age preserving signals respectively to ensure they contain the correct information. Extensive experiments have been conducted on the popular public-domain face aging datasets (FG-NET, MORPH Album 2, and CACD-VS) to demonstrate the effectiveness of the proposed approach.",,,,,https://pdfs.semanticscholar.org/8ed7/d8a07ac21328d3c03ff3880e6f6f939257f6.pdf
8f06950d93a257363f0f85e5be4fc238a4c9c0e4,0,1,Joint Face Super-Resolution and Deblurring Using Generative Adversarial Network,"Facial image super-resolution (SR) is an important aspect of facial analysis, and it can contribute significantly to tasks such as face alignment, face recognition, and image-based 3D reconstruction. Recent convolutional neural network (CNN) based models have exhibited significant advancements by learning mapping relations using pairs of low-resolution (LR) and high-resolution (HR) facial images. However, because these methods are conventionally aimed at increasing the PSNR and SSIM metrics, the reconstructed HR images might be blurry and have an overall unsatisfactory perceptual quality even when state-of-the-art quantitative results are achieved. In this study, we address this limitation by proposing an adversarial framework intended to reconstruct perceptually high-quality HR facial images while simultaneously removing blur. To this end, a simple five-layer CNN is employed to extract feature maps from LR facial images, and this feature information is provided to two-branch encoder-decoder networks that generate HR facial images with and without blur. In addition, local and global discriminators are combined to focus on the reconstruction of HR facial structures. Both qualitative and quantitative results demonstrate the effectiveness of the proposed method for generating photorealistic HR facial images from a variety of LR inputs. Moreover, it was also verified, through a use case scenario that the proposed method can contribute more to the field of face recognition than existing approaches.",2020,IEEE Access,1912.10427,10.1109/ACCESS.2020.3020729,https://arxiv.org/pdf/1912.10427.pdf
8f1abae983acc7123257bece2afd334549dfe94d,1,0,An Experimental Evaluation of Covariates Effects on Unconstrained Face Verification,"Covariates are factors that have a debilitating influence on face verification performance. In this paper, we comprehensively study two covariate related problems for unconstrained face verification: first, how covariates affect the performance of deep neural networks on the large-scale unconstrained face verification problem; second, how to utilize covariates to improve verification performance. To study the first problem, we implement five state-of-the-art deep convolutional networks and evaluate them on three challenging covariates datasets. In total, seven covariates are considered: pose (yaw and roll), age, facial hair, gender, indoor/outdoor, occlusion (nose and mouth visibility, and forehead visibility), and skin tone. We first report the performance of each individual network on the overall protocol and use the score-level fusion method to analyze each covariate. Some of the results confirm and extend the findings of previous studies, and others are new findings that were rarely mentioned previously or did not show consistent trends. For the second problem, we demonstrate that with the assistance of gender information, the quality of a precurated noisy large-scale face dataset for face recognition can be further improved. After retraining the face recognition model using the curated data, performance improvement is observed at low false acceptance rates.",2019,"IEEE Transactions on Biometrics, Behavior, and Identity Science",1808.05508,10.1109/TBIOM.2018.2890577,https://arxiv.org/pdf/1808.05508.pdf
8f61cccd0dffc2ba00bc85dde4a8594f769a29de,1,0,Pose-Weighted Gan for Photorealistic Face Frontalization,"Face recognition methods have achieved high accuracy when faces are captured in frontal pose and constrained scenes. However, severe drop in accuracy is observed when large pose variations exist. The main reason is that the large yaw angle leads to ID information loss. In this paper, we intend to solve the large pose variations in a generation manner. Specifically, we propose a Pose-Weighted Generative Adversarial Network (PW-GAN) for photorealistic frontal view synthesis. We find frontalizing the faces in large poses (yaw angle larger than 60◦) is so difficult that the results are not photorealistic and the ID information is lost. To simplify the problem, we first frontalize the face image through 3D face model, which is then used to guide the network predicting. Second, we refine the pose code in the loss function to make the network pay more attention to large poses. Quantitative and qualitative experimental results on the Multi-PIE and LFW demonstrate our method achieves state of the art.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803362,
8f84e162a4f26a97b1dc42a40546ffc5b5769f3e,0,1,Robust Line Segments Matching via Graph Convolution Networks,"Line matching plays an essential role in structure from motion (SFM) and simultaneous localization and mapping (SLAM), especially in low-textured and repetitive scenes. In this paper, we present a new method of using a graph convolution network to match line segments in a pair of images, and we design a graph-based strategy of matching line segments with relaxing to an optimal transport problem. In contrast to hand-crafted line matching algorithms, our approach learns local line segment descriptor and the matching simultaneously through end-to-end training. The results show our method outperforms the state-of-the-art techniques, and especially, the recall is improved from 45.28% to 70.47% under a similar presicion. The code of our work is available at this https URL.",2020,ArXiv,2004.04993,,https://arxiv.org/pdf/2004.04993.pdf
8fc0465900ee5305e506d9b8df99815c9e1dac0c,0,1,Privacy-sensitive Objects Pixelation for Live Video Streaming,"With the prevailing of live video streaming, establishing an online pixelation method for privacy-sensitive objects is an urgency. Caused by the inaccurate detection of privacy-sensitive objects, simply migrating the tracking-by-detection structure applied in offline pixelation into the online form will incur problems in target initialization, drifting, and over-pixelation. To cope with the inevitable but impacting detection issue, we propose a novel Privacy-sensitive Objects Pixelation (PsOP) framework for automatic personal privacy filtering during live video streaming. Leveraging pre-trained detection networks, our PsOP is extendable to any potential privacy-sensitive objects pixelation. Employing the embedding networks and the proposed Positioned Incremental Affinity Propagation (PIAP) clustering algorithm as the backbone, our PsOP unifies the pixelation of discriminating and indiscriminating pixelation objects through trajectories generation. In addition to the pixelation accuracy boosting, experiment results on the streaming video data we built show that the proposed PsOP can significantly reduce the over-pixelation ratio in privacy-sensitive object pixelation.",2020,ACM Multimedia,,10.1145/3394171.3413972,
8fc6702796837ee227cc29e9f7e07d0c4fd4a026,0,1,Can Generative Colourisation Help Face Recognition?,"Generative colourisation methods can be applied to automatically convert greyscale images to realistically looking colour images. In a face recognition system, such techniques might be employed as a pre-processing step in scenarios where either one or both face images to be compared are only available in greyscale format. In an experimental setup which reflects said scenarios, we investigate if generative colourisation can improve face sample utility and overall biometric performance of face recognition. To this end, subsets of the FERET and FRGCv2 face image databases are converted to greyscale and colourised applying two versions of the DeOldify colourisation algorithm. Face sample quality assessment is done using the FaceQnet quality estimator. Biometric performance measurements are conducted for the widely used ArcFace system and reported according to standardised metrics. Obtained results indicate that, for the tested systems, the application of generative colourisation does neither improve face image quality nor recognition performance. However, generative colourisation was found to aid face detection and subsequent feature extraction of the used face recognition system which results in a decrease of the overall false reject rate.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
8fdda881582d515315fed22d26a9793ce9862224,0,1,"A Survey of Label-noise Representation Learning: Past, Present and Future","Classical machine learning implicitly assumes that labels of the training data are sampled from a clean distribution, which can be too restrictive for real-world scenarios. However, statistical learning-based methods may not train deep learning models robustly with these noisy labels. Therefore, it is urgent to design Label-Noise Representation Learning (LNRL) methods for robustly training deep models with noisy labels. To fully understand LNRL, we conduct a survey study. We first clarify a formal definition for LNRL from the perspective of machine learning. Then, via the lens of learning theory and empirical study, we figure out why noisy labels affect deep models' performance. Based on the theoretical guidance, we categorize different LNRL methods into three directions. Under this unified taxonomy, we provide a thorough discussion of the pros and cons of different categories. More importantly, we summarize the essential components of robust LNRL, which can spark new directions. Lastly, we propose possible research directions within LNRL, such as new datasets, instance-dependent LNRL, and adversarial LNRL. Finally, we envision potential directions beyond LNRL, such as learning with feature-noise, preference-noise, domain-noise, similarity-noise, graph-noise, and demonstration-noise.",2020,ArXiv,2011.04406,,https://arxiv.org/pdf/2011.04406.pdf
8fee9b8c44626c4ac6b96ef183394bc4f36dc95f,1,0,Quantifying Facial Age by Posterior of Age Comparisons,"We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person's actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed `MegaAge', which consists of 41,941 images. Data can be downloaded from our project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and github.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-the-art results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.",2017,BMVC,1708.09687,10.5244/C.31.108,https://arxiv.org/pdf/1708.09687.pdf
8ff6200de38e0448451312e9545f25a1310a4f8f,1,0,Real or Fake? Spoofing State-Of-The-Art Face Synthesis Detection Systems,"The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, which raises obvious concerns about the potential for misuse. These concerns have fostered the research of manipulation detection methods that, contrary to humans, have already achieved astonishing results in some scenarios. In this study, we focus on the entire face synthesis, which is one specific type of facial manipulation. The main contributions of this study are: i) a novel strategy to remove GAN ""fingerprints"" from synthetic fake images in order to spoof facial manipulation detection systems, while keeping the visual quality of the resulting images, ii) an in-depth analysis of state-of-the-art detection approaches for the entire face synthesis manipulation, iii) a complete experimental assessment of this type of facial manipulation considering state-of-the-art detection systems, remarking how challenging is this task in unconstrained scenarios, and finally iv) a novel public database named FSRemovalDB produced after applying our proposed GAN-fingerprint removal approach to original synthetic fake images.  The observed results led us to conclude that more efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques such as the one proposed in this study.",2019,ArXiv,,,http://atvs.ii.uam.es/atvs/files/GANRemoval_Arxiv.pdf
8ff988530e3329bd6ab00dc5eef635a1bc5812ca,1,1,SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness,"Face image quality is an important factor to enable high-performance face recognition systems. Face quality assessment aims at estimating the suitability of a face image for the purpose of recognition. Previous work proposed supervised solutions that require artificially or human labelled quality values. However, both labelling mechanisms are error prone as they do not rely on a clear definition of quality and may not know the best characteristics for the utilized face recognition system. Avoiding the use of inaccurate quality labels, we proposed a novel concept to measure face quality based on an arbitrary face recognition model. By determining the embedding variations generated from random subnetworks of a face model, the robustness of a sample representation and thus, its quality is estimated. The experiments are conducted in a cross-database evaluation setting on three publicly available databases. We compare our proposed solution on two face embeddings against six state-of-the-art approaches from academia and industry. The results show that our unsupervised solution outperforms all other approaches in the majority of the investigated scenarios. In contrast to previous works, the proposed solution shows a stable performance over all scenarios. Utilizing the deployed face recognition model for our face quality assessment methodology avoids the training phase completely and further outperforms all baseline approaches by a large margin. Our solution can be easily integrated into current face recognition systems, and can be modified to other tasks beyond face recognition.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.09373,10.1109/cvpr42600.2020.00569,https://arxiv.org/pdf/2003.09373.pdf
8ffd43898ec4460119449eaad6e7c1e76e78f78a,0,1,SeLENet: A Semi-Supervised Low Light Face Enhancement Method for Mobile Face Unlock,"Facial recognition is becoming a standard feature on new smartphones. However, the face unlocking feature of devices using regular 2D camera sensors exhibits poor performance in low light environments. In this paper, we propose a semi-supervised low light face enhancement method to improve face verification performance on low light face images. The proposed method is a network with two components: decomposition and reconstruction. The decomposition component splits an input low light face image into face normals and face albedo, while the reconstruction component enhances and reconstructs the lighting condition of the input image using the spherical harmonic lighting coefficients of a direct ambient white light. The network is trained in a semi-supervised manner using both labeled synthetic data and unlabeled real data. Qualitative results demonstrate that the proposed method produces more realistic images than the state-of-the-art low light enhancement algorithms. Quantitative experiments confirm the effectiveness of our low light face enhancement method for face verification. By applying the proposed method, the gap of verification accuracy between extreme low light and neutral light face images is reduced from approximately 3% to 0.5%.",2019,2019 International Conference on Biometrics (ICB),,10.1109/ICB45273.2019.8987344,
901dc54e02700b8626d0bc8621c6de1216930c2b,0,1,Gaussian Soft Margin Angular Loss for Face Recognition,"Advances in deep learning has lead to drastic improvements in face recognition. A key part of these deep models is their loss function. Consequently developing an efficient and suitable loss function has been an important topic in face recognition in the recent years. Angular-margin-based losses achieve an acceptable performance and inter-class separability. However they are held back by their enforcement of hard margins on all the samples of the training dataset, regardless of whether these samples actually differ from all the other classes enough to enforce a margin. It can be argued that in a large enough dataset with many different settings and age gaps, some faces will look similar to the faces of other classes. In an intuitive and expressive embedding, we expect some faces to be embedded near similar classes with a small margin. Thus we propose a loss function that while maximizing the inter-class distance and intra-class compactness, allows for the samples which naturally reside further from class center to have a smaller margin. We implement an extremely light and fast to train model using MobileNets and achieve accuracy comparable to state of the art method.",2020,2020 International Conference on Machine Vision and Image Processing (MVIP),,10.1109/MVIP49855.2020.9116917,
9022cfe07451593ea39b8e58fea3c2d4c529cbef,1,0,Synthetic Data for Deep Learning,"Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.",2019,ArXiv,1909.11512,,https://arxiv.org/pdf/1909.11512.pdf
902e14d3ec8862bd70077c2ec9b318955a13ce91,0,1,DSAM: A Distance Shrinking with Angular Marginalizing Loss for High Performance Vehicle Re-identificatio,"Vehicle Re-identification (ReID) is an important yet challenging problem in computer vision. Compared to other visual objects like faces and persons, vehicles simultaneously exhibit much larger intraclass viewpoint variations and interclass visual similarities, making most exiting loss functions designed for face recognition and person ReID unsuitable for vehicle ReID. To obtain a high-performance vehicle ReID model, we present a novel Distance Shrinking with Angular Marginalizing (DSAM) loss function to perform hybrid learning in both the Original Feature Space (OFS) and the Feature Angular Space (FAS) using the local verification and the global identification information. Specifically, it shrinks the distance between samples of the same class locally in the Original Feature Space while keeps samples of different classes far away in the Feature Angular Space. The shrinking and marginalizing operations are performed during each iteration of the training process and are suitable for different SoftMax based loss functions. We evaluate the DSAM loss function on three large vehicle ReID datasets with detailed analyses and extensive comparisons with many competing vehicle ReID methods. Experimental results show that our DSAM loss enhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset: 10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is increased by 9.34% on the PKU-VehicleID dataset and 8.73% on the VeRi-776 dataset. Source code will be released to facilitate further studies in this research direction.",2020,ArXiv,2011.06228,,https://arxiv.org/pdf/2011.06228.pdf
90307ae0012d5388c6786a2a0ca4685f76aabfc5,0,1,Speaker Representation Learning using Global Context Guided Channel and Time-Frequency Transformations,"In this study, we propose the global context guided channel and time-frequency transformations to model the long-range, non-local time-frequency dependencies and channel variances in speaker representations. We use the global context information to enhance important channels and recalibrate salient time-frequency locations by computing the similarity between the global context and local features. The proposed modules, together with a popular ResNet based model, are evaluated on the VoxCeleb1 dataset, which is a large scale speaker verification corpus collected in the wild. This lightweight block can be easily incorporated into a CNN model with little additional computational costs and effectively improves the speaker verification performance compared to the baseline ResNet-LDE model and the Squeeze&Excitation block by a large margin. Detailed ablation studies are also performed to analyze various factors that may impact the performance of the proposed modules. We find that by employing the proposed L2-tf-GTFC transformation block, the Equal Error Rate decreases from 4.56% to 3.07%, a relative 32.68% reduction, and a relative 27.28% improvement in terms of the DCF score. The results indicate that our proposed global context guided transformation modules can efficiently improve the learned speaker representations by achieving time-frequency and channel-wise feature recalibration.",2020,,,,https://isca-speech.org/archive/Interspeech_2020/pdfs/1845.pdf
9033c90915e30bf5de2d1c495caa9044781e628a,1,0,A Comprehensive Experimental and Reproducible Study on Selfie Biometrics in Multistream and Heterogeneous Settings,"This contribution presents a new database to address current challenges in face recognition. It contains face video sequences of 75 individuals acquired either through a laptop webcam or when mimicking the front-facing camera of a smartphone. Sequences have been acquired with a device allowing to record visual, near-infrared, and depth data at the same time. Recordings have been made across three sessions with different, challenging illumination conditions and variations in pose. Together with the database, several experimental protocols are provided and correspond to real world scenarios, when a mismatch in conditions between enrollment and probe images occurs. A comprehensive set of baseline experiments using publicly available baseline algorithms show that extreme illumination conditions and pose variations are remaining issues. However, the usage of different data domains—and their fusion—allows to mitigate such variation. Finally, experiments on heterogeneous face recognition are also presented using a state-of-the-art model based on deep neural networks, and showed better performance. When applied to other tasks, this model proved to surpass all existing baselines as well. The data, as well as the code to reproduce all experiments are made publicly available to help foster research in selfie biometrics using latest imaging devices.",2019,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2019.2927692,http://publications.idiap.ch/downloads/reports/2019/Heusch_Idiap-RR-09-2019.pdf
90407769c86de1fa4562542144cbd37bbb8eb03c,1,1,Learning deep face representation with long-tail data: An aggregate-and-disperse approach,"Abstract In this work, we study the problem of deep representation learning on a large face dataset with long-tail distribution. Training convolutional neural networks on such dataset with conventional strategy suffers from imbalance problem which results in biased classification boundary, and the few-shot classes lying in tail parts further make the model prone to overfitting. Aiming to learn more discriminative CNN model from long-tail data, we propose a novel aggregate-and-disperse training schema. Firstly, our proposed method aggregates similar classes in tail part to avoid imbalance problem. Based on the aggregated super classes and those original head classes, a model is pre-trained to capture accurate discrimination in head classes as well as coarse discrinimation in tail classes. Secondly, we selectively disperses those aggregated super classes to learn precise inter-class variations and refine the representation for better generalization. We perform extensive experiments on MS-Celeb-1M, BLUFR and MegaFace. Compared with baselines and existing methods, our method achieves better performance of face recognition, demonstrating its effectiveness of handling long-tail distribution.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.02.007,
904c48e6bff55d952c9e9274c50c6fd4117782d6,0,1,A local multiple patterns feature descriptor for face recognition,"Abstract Human perception of a signal depends on the ratio of the change of stimulus to the stimulus itself while the change of stimulus to the stimulus itself is usually ignored in hand-crafted feature descriptors. However, it is important for extracting discriminant feature. To address this problem, we firstly develop a local multiple patterns (LMP) feature descriptor based on the Weber's law for feature extraction and face recognition. In LMP, (1) the Weber's ratio is modified to contain the change direction, and thus the modified Weber's ratio is quantized into several intervals to generate multiple feature maps for describing different changes; (2) LMP concatenates the histograms of the non-overlapping regions of the feature maps for image representation. Secondly, since LMP could only capture small-scale structures, a multi-scale block LMP (MB-LMP) is presented to generate more discriminative and robust visual feature. Thus, MB-LMP could capture both the small-scale and large-scale structures. In implementation, MB-LMP is computationally efficient using integral image. In experiments, LMP and MB-LMP are evaluated on four public datasets for face recognition. The experimental results demonstrate the promise of the proposed LMP and MB-LMP descriptors with desirable efficiency.",2020,Neurocomputing,,10.1016/j.neucom.2019.09.102,
90c77cf5f1eff50196404d94fe6ad12c7a94667f,0,1,SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape Parameters,"Image-to-image (i2i) translation is the dense regression problem of learning how to transform an input image into an output using aligned image pairs. Remarkable progress has been made in i2i translation with the advent of deep convolutional neural networks and particular using the learning paradigm of generative adversarial networks (GANs). In the absence of paired images, i2i translation is tackled with one or multiple domain transformations (i.e., CycleGAN, StarGAN etc.). In this paper, we study the problem of image-to-image translation, under a set of continuous parameters that correspond to a model describing a physical process. In particular, we propose the SliderGAN which transforms an input face image into a new one according to the continuous values of a statistical blendshape model of facial motion. We show that it is possible to edit a facial image according to expression and speech blendshapes, using sliders that control the continuous values of the blendshape model. This provides much more flexibility in various tasks, including but not limited to face editing, expression transfer and face neutralisation, comparing to models based on discrete expressions or action units.",2020,International Journal of Computer Vision,1908.09638,10.1007/s11263-020-01338-7,https://link.springer.com/content/pdf/10.1007/s11263-020-01338-7.pdf
90e9702ca01ebda8d131e1ec7c46506fc4959f2e,0,1,Whose hand is this? Person Identification from Egocentric Hand Gestures,"Recognizing people by faces and other biometrics has been extensively studied in computer vision. But these techniques do not work for identifying the wearer of an egocentric (first-person) camera because that person rarely (if ever) appears in their own first-person view. But while one's own face is not frequently visible, their hands are: in fact, hands are among the most common objects in one's own field of view. It is thus natural to ask whether the appearance and motion patterns of people's hands are distinctive enough to recognize them. In this paper, we systematically study the possibility of Egocentric Hand Identification (EHI) with unconstrained egocentric hand gestures. We explore several different visual cues, including color, shape, skin texture, and depth maps to identify users' hands. Extensive ablation experiments are conducted to analyze the properties of hands that are most distinctive. Finally, we show that EHI can improve generalization of other tasks, such as gesture recognition, by training adversarially to encourage these models to ignore differences between users.",2020,ArXiv,2011.089,,https://arxiv.org/pdf/2011.08900.pdf
9146ee0cd87a00992bdb49c7a7e23679120b160b,0,1,Large Margin Meta-Learning for Few-Shot Classification,"This paper proposes a large margin principle to overcome the problem of data scarcity in training meta-learning models for few-shot classification. To realize it, we develop an efficient framework that can be easily incorporated in existing metric-based meta-learning models. We demonstrate that the large margin principle can improve the generalization capacity of state-of-the-art meta-learning methods and lead to consistent and considerable improvements on few-shot classification.",2018,,,,http://metalearning.ml/2018/papers/metalearn2018_paper11.pdf
91755d69c7d45df131a4721a02676e572272b3f6,0,1,Improving Embedding-based Neural-Network Speaker Recognition,"In this paper, we integrate multiple ideas and techniques into an embedding-based neural-network speaker recognition (NSR) system. Such an NSR system essentially consists of a front-end speaker-embedding extractor and a back-end speakermatching component. The frontend is a neural network trained with millions of utterances from thousands of speakers. Currently, the backend is based on simple similarity measures such as angle, Euclidean distance, or probabilistic score. We begin with the well-known x-vector baseline, and then incrementally modify the system modules. Regarding front-end extractor, we investigate modification on network architecture, network function, training criteria, and hyper-parameter setting. Regarding back-end matcher, we evaluate PLDA training/adaptation data and system fusion. On the public SRE 2018 Evaluation Dataset, the performance of system as measured by equal-error rate (EER) is improved from 7.01% to 5.16%, which marks a significant relative improvement of 26.5%.",2020,,,10.21437/odyssey.2020-8,https://www.isca-speech.org/archive/Odyssey_2020/pdfs/33.pdf
918996bac4632a1673e69e2907b145dabc20dda4,0,1,Stacked Multi-Target Network for Robust Facial Landmark Localisation,"We thoroughly analyse regression-based face alignment methods and introduce a novel stacked multi-target network for robust facial landmark localisation. The primary heatmap regression-based network concentrates on locating the coarse position of pre-defined landmarks while the secondary coordinate regression-based network is responsible for modelling fine sub-pixel features. Specifically, we elaborate the differences among widely-used Cross Entropy related loss functions and propose a new Bilateral Inhibition Cross Entropy loss function, which enlarges the margin between elements in the output heatmaps. Besides, in order to deal with the discrepancy between optimization and evaluation, we propose to dynamically adjust the radius of kernel function during the training process. We demonstrate that training with decreasing radius in temporal order performs much better than assigning it spatially, i.e. decreasing radius along the stages of stacked hourglass networks. Finally, we innovatively limit the output of the secondary coordinate regression network to a reasonable range by importing the hinge loss to refine the coarse coordinate locations for sub-pixel accuracy. Extensive experiments on public datasets such as 300-W, COFW, and AFLW demonstrate that our proposed method performs superiorly to the state-of-the-art approaches.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00028,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Yang_Stacked_Multi-Target_Network_for_Robust_Facial_Landmark_Localisation_CVPRW_2019_paper.pdf
91b8b926c87b3db6d03bf22b2c3c031e8ff16edb,0,1,Classification System with Capability to Reject Unknowns,"In this paper, we propose a novel method for object classification with capability to reject unknown inputs. In the real world application such as an image-recognition-based checkout system, it is crucial to reject unknown inputs while correctly classifying registered objects. Conventional deep-learning-based classification systems with softmax output suffer from overconfident score on unknown objects. We tackled the problem by the following two approaches. First, we incorporated a metric-learning-based method proposed for face verification into object classification. Second, we utilize available unregistered objects (known unknowns) in the training phase by proposing a novel “Margined Unknown Loss”. In the experiment, we showed the effectiveness of the proposed method by confirming that it outperformed conventional softmax-based approaches which also use the known unknowns, on two datasets, MNIST dataset and a retail product dataset, in terms of Recall at a low false positive rate.",2019,2019 IEEE International Conference on Imaging Systems and Techniques (IST),,10.1109/IST48021.2019.9010169,
91bcb46689c9379b71e3a606c30f9cea11e84c58,1,1,Attention-based convolutional neural network for deep face recognition,"Discriminative feature embedding is of essential importance in the field of large scale face recognition. In this paper, we propose an attention-based convolutional neural network (ACNN) for discriminative face feature embedding, which aims to decrease the information redundancy among channels and focus on the most informative components of spatial feature maps. More specifically, the proposed attention module consists of a channel attention block and a spatial attention block which adaptively aggregate the feature maps in both channel and spatial domains to learn the inter-channel relationship matrix and the inter-spatial relationship matrix, then matrix multiplications are conducted for a refined and robust face feature. With the attention module we proposed, we can make standard convolutional neural networks (CNNs), such as ResNet-50, ResNet-101 have more discriminative power for deep face recognition. The experiments on Labelled Faces in the Wild (LFW), Age Database (AgeDB), Celebrities in Frontal Profile (CFP) and MegaFace Challenge 1 (MF1) show that our proposed ACNN architecture consistently outperforms naive CNNs and achieves the state-of-the-art performance.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-08422-2,
91dfc942039eaf6cbb714a599e339c899f341346,0,1,COVID-MobileXpert: On-Device COVID-19 Screening using Snapshots of Chest X-Ray,"With the increasing demand for millions of COVID-19 screenings, Computed Tomography (CT) based test has emerged as a promising alternative to the gold standard RT-PCR test. However, it is primarily provided in hospital setting due to the need for expensive equipment and experienced radiologists. An accurate, rapid yet inexpensive test that is suitable for COVID-19 population screenings at mobile, urgent and primary care clinics is urgently needed. We present COVID-MobileXpert: a lightweight deep neural network (DNN) based mobile app that can use noisy snapshots of chest X-ray (CXR) for point-of-care COVID-19 screening. We design and implement a novel three-player knowledge transfer and distillation (KTD) framework including a pre-trained attending physician (AP) network that extracts CXR imaging features from large scale of lung disease CXR images, a fine-tuned resident fellow (RF) network that learns the essential CXR imaging features to discriminate COVID-19 from pneumonia and/or normal cases using a small amount of COVID-19 cases, and a trained lightweight medical student (MS) network that performs on-device COVID-19 screening. To accommodate the need for screening using noisy snapshots of CXR images, we employ novel loss functions and training schemes for the MS network to learn the robust imaging features for accurate on-device COVID-19 screening. We demonstrate the strong potential of COVID-MobileXpert for rapid deployment via extensive experiments with diverse MS network architecture, CXR imaging quality, and tuning parameter settings. The source code of cloud and mobile based models are available from this https URL.",2020,ArXiv,,,
91e2349d5609e2ee0e2ddce84fc44302fdf2bab0,0,1,HERS: Homomorphically Encrypted Representation Search,"We present a method to search for a probe (or query) image representation against a large gallery in the encrypted domain. We require that the probe and gallery images be represented in terms of a fixed-length representation, which is typical for representations obtained from learned networks. Our encryption scheme is agnostic to how the fixed-length representation is obtained and can, therefore, be applied to any fixed-length representation in any application domain. Our method, dubbed HERS (Homomorphically Encrypted Representation Search), operates by (i) compressing the representation towards its estimated intrinsic dimensionality, (ii) encrypting the compressed representation using the proposed fully homomorphic encryption scheme, and (iii) searching against a gallery of encrypted representations directly in the encrypted domain, without decrypting them, and with minimal loss of accuracy. Numerical results on large galleries of face, fingerprint, and object datasets such as ImageNet show that, for the first time, accurate and fast image search within the encrypted domain is feasible at scale (296 seconds; 46x speedup over state-of-the-art for face search against a background of 1 million).",2020,ArXiv,2003.12197,,https://arxiv.org/pdf/2003.12197.pdf
91f7892c8376cc2c1fbe839f43d59ba863d714a3,0,1,Measuring Long Context Dependency in Birdsong Using an Artificial Neural Network with a Long-Lasting Working Memory,"The production of grammatically and semantically appropriate human language requires reference to non-trivially long history of past utterance, which is referred to as the context dependency of human language. Similarly, it is of particular interest to biologists how much effect past behavioral records of individual animals have on their future behavioral decisions. In particular, birdsong serves a representative case to study context dependency in sequential signals produced by animals. Previous studies have suggested that the songs of Bengalese finches (Lonchura striata var. domestica) exhibited a long dependency on previous outputs, while their estimates were upper-bounded by methodological limitations at that time. This study newly estimated the context dependency in Bengalese finch9s song in a more scalable manner using a neural network-based language model, Transformer, whose accessible context length reaches 900 tokens and is thus nearly free from model limitations, unlike the methods adopted in previous studies. A quantitative comparison with a parallel analysis of English sentences revealed that context dependency in Bengalese finch song is much shorter than that in human language but is comparable to human language syntax that excludes semantic factors of dependency. Our findings are in accordance with the previous generalization reported in related studies that birdsong is more homologous to human language syntax than the entire human language, including semantics. Thus, this study supports the hypothesis that human language modules, such as syntax and semantics, evolved from different precursors that are shared with other animals.",2020,,,10.1101/2020.05.09.083907,https://www.biorxiv.org/content/biorxiv/early/2020/05/12/2020.05.09.083907.full.pdf
92006a84036f9e37e11c773c19141ad8a6675e39,1,1,Suppressing Uncertainties for Large-Scale Facial Expression Recognition,"Annotating a qualitative large-scale facial expression dataset is extremely difficult due to the uncertainties caused by ambiguous facial expressions, low-quality facial images, and the subjectiveness of annotators. These uncertainties suspend the progress of large-scale Facial Expression Recognition (FER) in data-driven deep learning era. To address this problelm, this paper proposes to suppress the uncertainties by a simple yet efficient Self-Cure Network (SCN). Specifically, SCN suppresses the uncertainty from two different aspects: 1) a self-attention mechanism over FER dataset to weight each sample in training with a ranking regularization, and 2) a careful relabeling mechanism to modify the labels of these samples in the lowest-ranked group. Experiments on synthetic FER datasets and our collected WebEmotion dataset validate the effectiveness of our method. Results on public benchmarks demonstrate that our SCN outperforms current state-of-the-art methods with \textbf{88.14}\% on RAF-DB, \textbf{60.23}\% on AffectNet, and \textbf{89.35}\% on FERPlus.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.10392,10.1109/cvpr42600.2020.00693,https://arxiv.org/pdf/2002.10392.pdf
921fb85ce4c8c63126db10e814c319173e687ab7,0,1,Multi-Features Fusion and Decomposition for Age-Invariant Face Recognition,"Although the General Face Recognition (GFR) research achieves great success, Age-Invariant Face Recognition (AIFR) is still a challenging problem since facial appearance changing over time brings significant intra-class variations. The existing discriminative methods for the AIFR task mostly focus on decomposing the facial feature from a sigle image into age-related feature and age-independent feature for recognition, which suffer from the loss of facial identity information. To address this issue, in this work we propose a novel Multi-Features Fusion and Decomposition (MFFD) framework to learn more discriminative feature representations and alleviate the intra-class variations for AIFR. Specifically, we first sample multiple face images of different ages with the same identity as a face time series. Next, we combine feature decomposition with fusion based on the face time series to ensure that the final age-independent features effectively represent the identity information of the face and have stronger robustness against aging. Moreover, we also present two feature fusion methods and several different training strategies to explore the impact on the model. Extensive experiments on several cross-age datasets (CACD, CACD-VS) demonstrate the effectiveness of our proposed method. Besides, our method also shows comparable generalization performance on the well-known LFW dataset.",2020,ACM Multimedia,,10.1145/3394171.3413499,
92459ffb2f5ec02190da7f76744a88a6277fad1a,1,0,When Person Re-identification Meets Changing Clothes,"Person re-identification (ReID) is now an active research topic for AI-based video surveillance applications such as specific person search, but the practical issue that the target person(s) may change clothes (clothes inconsistency problem) has been overlooked for long. For the first time, this paper systematically studies this problem. We first overcome the difficulty of lack of suitable dataset, by collecting a small yet representative real dataset for testing whilst building a large realistic synthetic dataset for training and deeper studies. Facilitated by our new datasets, we are able to conduct various interesting new experiments for studying the influence of clothes inconsistency. We find that changing clothes makes ReID a much harder problem in the sense of bringing difficulties to learning effective representations and also challenges the generalization ability of previous ReID models to identify persons with unseen (new) clothes. Representative existing ReID models are adopted to show informative results on such a challenging setting, and we also provide some preliminary efforts on improving the robustness of existing models on handling the clothes inconsistency issue in the data. We believe that this study can be inspiring and helpful for encouraging more researches in this direction. The dataset is available on the project website: https://wanfb.github.io/dataset.html.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2003.0407,10.1109/CVPRW50498.2020.00423,https://arxiv.org/pdf/2003.04070.pdf
92678a034cc37d8e0fb7211af41917b25591a973,0,1,Robust Deep Feature Extraction Method for Acoustic Scene Classification,"In recent years, increasing number of acoustic scene classification (ASC) methods are based on deep learning models. In these models, the extraction of robust deep feature plays an important role on the classification accuracy. However the complex combination of acoustic phenomena in an acoustic scene results in overlapping of the analysis features, which degrades the performance of ASC. To enhance the compactness of feature and fit the multi-classification task, we explored the data label learning for deep feature extraction. And we combined the method of label smoothing(LS) and the additive margin softmax loss (AM-softmax) to extract deep feature based on VGG-style deep neural network. The comparison experiments show that the best classification results are obtained by the proposed method, which accuracy on ESC-50 dataset is 81.9%, which is beyond human performance.",2019,2019 IEEE 19th International Conference on Communication Technology (ICCT),,10.1109/ICCT46805.2019.8947252,
926f5674a8143733241a544bcaa0dcfc3ab42646,1,0,A Comparative Study of Face Re-identification Systems under Real-World Conditions,"Face re-identification is largely thought of as a solved problem in the research community, with State-ofthe-art systems attaining human-level performance on unconstrained image datasets. However, these results do not seem to translate to the real world. In systems matching people in surveillance-like footage to highquality images, reported performance is much lower than what the literature would suggest. In this work, we contribute a multi-modal dataset for evaluating real-world performance of facial re-identification systems. We then perform a verification and re-identification evaluation for state-of-art systems on both this dataset and the popular benchmarking dataset Labelled Faces in the Wild (LFW).",2018,,,,https://pure.qub.ac.uk/portal/files/154793203/IMVIP_2018_paper_18.pdf
9286eab328444401a848cd2e13186840be8f0409,1,0,Multicolumn Networks for Face Recognition,"The objective of this work is set-based face recognition, i.e. to decide if two sets of images of a face are of the same person or not. Conventionally, the set-wise feature descriptor is computed as an average of the descriptors from individual face images within the set. In this paper, we design a neural network architecture that learns to aggregate based on both ""visual"" quality (resolution, illumination), and ""content"" quality (relative importance for discriminative classification). To this end, we propose a Multicolumn Network (MN) that takes a set of images (the number in the set can vary) as input, and learns to compute a fix-sized feature descriptor for the entire set. To encourage high-quality representations, each individual input image is first weighted by its ""visual"" quality, determined by a self-quality assessment module, and followed by a dynamic recalibration based on ""content"" qualities relative to the other images within the set. Both of these qualities are learnt implicitly during training for set-wise classification. Comparing with the previous state-of-the-art architectures trained with the same dataset (VGGFace2), our Multicolumn Networks show an improvement of between 2-6% on the IARPA IJB face recognition benchmarks, and exceed the state of the art for all methods on these benchmarks.",2018,BMVC,1807.09192,,https://arxiv.org/pdf/1807.09192.pdf
92bf99c385c47755130cc844e6be6b556f7f5667,0,1,Face Recognition Speed Optimization Method for Embedded Environment,"To solve the problem that it is difficult to combine performance and speed of face recognition in embedded environment, we design and implement a fast face recognition method based on deep learning. Firstly, face detection and quality selection are performed based on MTCNN algorithm to reduce the amount of feature extraction data needed for recognition. Secondly, the RKNN model is used to quantify the acceleration of feature extraction. Finally, fast face recognition is realized by feature matching. The experiment shows that the method is beneficial for face recognition processing on the RK3399Pro platform, and the average time is shortened by 80% compared with that before acceleration. In the case of ensuring the feature extraction accuracy and recognition accuracy, it basically meets the real-time requirements.",2020,2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI),,10.1109/IWECAI50956.2020.00037,
9355c9d9ffd4d6dea960f2dfd9f5d3000fde60ca,0,1,Birdsong sequence exhibits long context dependency comparable to human language syntax,"Context dependency is a key feature in sequential structures of human language, which requires reference between words far apart in produced sequence. Assessing how long the past context has effect on the current status provides crucial information to understand the mechanism for complex sequential behaviors. Birdsongs serve as a representative model for studying the context dependency in sequential signals produced by non-human animals, while previous reports were upper bounded by methodological limitations. Here we show that birdsongs have a long context dependency comparable to grammatical structure in human language. We newly estimated the context dependency in birdsongs in a scalable way using a neural-network-based language model whose accessible context length is sufficiently long. Quantitative comparison with the parallel analysis of English sentences revealed that the context dependency in the birdsong was much shorter than that in the sentence, but was comparable to the grammatical structure when semantic factors were removed. Our findings are in accordance with the previous generalization in comparative studies that birdsong is more homologous to human language syntax than the entirety of human language including semantics.",2020,,,10.1101/2020.05.09.083907,
93afe4898ce4c996ee5edd13348392137ea5bc0a,1,0,Differentiable Unrolled Alternating Direction Method of Multipliers for OneNet,"Deep neural networks achieve state-of-the-art results on numerous image processing tasks, but this typically requires training problem-specific networks. Towards multi-task learning, the One Network to Solve Them All (OneNet) method was recently proposed that first pretrains an adversarial denoising autoencoder and subsequently uses it as the proximal operator in Alternating Direction Method of Multipliers (ADMM) solvers of multiple imaging problems. In this work, we highlight training and ADMM convergence issues of OneNet, and resolve them by proposing an end-to-end learned architecture for training the two steps jointly using Unrolled Optimization with backpropagation. In our experiments, our solution achieves superior or on par results compared to the original OneNet and Wavelet sparsity on four imaging problems (pixelwise inpainting-denoising, blockwise inpainting, scattered inpainting and super resolution) on the MS-Celeb-1M and ImageNet data sets, even with a much smaller ADMM iteration count.",2019,BMVC,,,https://bmvc2019.org/wp-content/uploads/papers/0717-paper.pdf
93c2d77809867ac4b8444106bde69970f45f9e8a,1,1,Recognizing Families In the Wild: White Paper for the 4th Edition Data Challenge.,"Recognizing Families In the Wild (RFIW): an annual large-scale, multi-track automatic kinship recognition evaluation that supports various visual kin-based problems on scales much higher than ever before. Organized in conjunction with the 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG) as a Challenge, RFIW provides a platform for publishing original work and the gathering of experts for a discussion of the next steps. This paper summarizes the supported tasks (i.e., kinship verification, tri-subject verification, and search & retrieval of missing children) in the evaluation protocols, which include the practical motivation, technical background, data splits, metrics, and benchmark results. Furthermore, top submissions (i.e., leader-board stats) are listed and reviewed as a high-level analysis on the state of the problem. In the end, the purpose of this paper is to describe the 2020 RFIW challenge, end-to-end, along with forecasts in promising future directions.",2020,,2002.06303,10.1109/FG47880.2020.00138,https://arxiv.org/pdf/2002.06303.pdf
94088f73fa5824e00dac0442827a549bc0033938,1,0,DocFace+: ID Document to Selfie Matching,"Numerous activities in our daily life require us to verify who we are by showing our ID documents containing face images, such as passports and driver licenses, to human operators. However, this process is slow, labor intensive and unreliable. As such, an automated system for matching ID document photographs to live face images (selfies<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref>) in real time and with high accuracy is required. In this paper, we propose DocFace+ to meet this objective. We first show that gradient-based optimization methods converge slowly (due to the underfitting of classifier weights) when many classes have very few samples, a characteristic of existing ID-selfie datasets. To overcome this shortcoming, we propose a method, called dynamic weight imprinting, to update the classifier weights, which allows faster convergence and more generalizable representations. Next, a pair of sibling networks with partially shared parameters are trained to learn a unified face representation with domain-specific parameters. Cross-validation on an ID-selfie dataset shows that while a publicly available general face matcher (InsightFace) only achieves a true accept rate (TAR) of 88.78 ± 1.30% at a false accept rate of 0.01% on the problem, DocFace+ improves the TAR to 95.95 ± 0.54%.<fn id=""fn1""><label><sup>1</sup></label><p>Technically, the word “selfies” refers to self-captured photos from mobile phones. But here, we define “selfies” as any self-captured live face photos, including those from mobile phones and kiosks.</p></fn>",2019,"IEEE Transactions on Biometrics, Behavior, and Identity Science",1809.0562,10.1109/TBIOM.2019.2897807,https://arxiv.org/pdf/1809.05620.pdf
945482270482e3fd5e05b65e9c0c1de5065dcdb0,1,1,SaFace: Towards Scenario-aware Face Recognition via Edge Computing System,"Deep Convolutional Neural Networks (CNNs) have achieved remarkable progress in the field of face recognition (FR). However, developing a robust FR system in the real-world is still challenging due to vast variance of illumination, visual quality, and camera angles in different scenarios. These factors may result in significant accuracy drop, if the pretrained model doesn’t have perfect generalization ability. To mitigate this issue, we present a solution named SAFACE, which helps to improve FR accuracy through unsupervised online-learning in an edge computing system. Specifically, we propose a novel scenario-aware FR flow, then decouple the flow into different phases and map each of them to different levels of a three-layer edge computing system. For evaluation, we implement a prototype and demonstrate its advantages in both improving recognition accuracy and reducing processing latency.",2020,HotEdge,,,https://www.usenix.org/system/files/hotedge20_paper_zhou-zhe_0.pdf
94655c1a6b94438439658a3b95b7bfbf7b6aebd9,1,1,Mitigating Face Recognition Bias via Group Adaptive Classifier,"Face recognition is known to exhibit bias - subjects in certain demographic group can be better recognized than other groups. This work aims to learn a fair face representation, where faces of every group could be equally well-represented. Our proposed group adaptive classifier, GAC, learns to mitigate bias by using adaptive convolution kernels and attention mechanisms on faces based on their demographic attributes. The adaptive module comprises kernel masks and channel-wise attention maps for each demographic group so as to activate different facial regions for identification, leading to more discriminative features pertinent to their demographics. We also introduce an automated adaptation strategy which determines whether to apply adaptation to a certain layer by iteratively computing the dissimilarity among demographic-adaptive parameters, thereby increasing the efficiency of the adaptation learning. Experiments on benchmark face datasets (RFW, LFW, IJB-A, and IJB-C) show that our framework is able to mitigate face recognition bias on various demographic groups as well as maintain the competitive performance.",2020,ArXiv,2006.07576,,https://arxiv.org/pdf/2006.07576.pdf
9465d341f04c255ba268140c61431c7d52104f48,1,1,Face Representation Learning using Composite Mini-Batches,"Mini-batch construction strategy is an important part of the deep representation learning. Different strategies have their advantages and limitations. Usually only one of them is selected to create mini-batches for training. However, in many cases their combination can be more efficient than using only one of them. In this paper, we propose Composite Mini-Batches - a technique to combine several mini-batch sampling strategies in one training process. The main idea is to compose mini-batches from several parts, and use different sampling strategy for each part. With this kind of mini-batch construction, we combine the advantages and reduce the limitations of the individual mini-batch sampling strategies. We also propose Interpolated Embeddings and Priority Class Sampling as complementary methods to improve the training of face representations. Our experiments on a challenging task of disguised face recognition confirm the advantages of the proposed methods.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00068,http://openaccess.thecvf.com/content_ICCVW_2019/papers/DFW/Smirnov_Face_Representation_Learning_using_Composite_Mini-Batches_ICCVW_2019_paper.pdf
948e616800b453f4f555197ed22ff6c608a3c893,1,1,Masked Face Recognition with Generative Data Augmentation and Domain Constrained Ranking,"Masked faces recognition (MFR) aims to match a masked face with its corresponding full face, which is an important task especially during the global outbreak of COVID-19. However, most existing face recognition models generalize poorly in this case, and it is hard to train a robust MFR model due to two main reasons: 1) the absence of large scale training data as well as ground truth testing data, and 2) the presence of large intra-class variation between masked faces and full faces. To address the first challenge, this paper firstly contributes a new dataset denoted as MFSR, which consists of two parts. The first part contains 9,742 masked face images with mask region segmentation annotation. The second part contains 11,615 images of 1,004 identities, and each identity has masked and full face images with various orientations, lighting conditions and mask types. However, it is still not enough for training MFR models with deep learning. To obtain sufficient training data, based on the MFSR, we introduce a novel Identity Aware Mask GAN (IAMGAN) with segmentation guided multi-level identity preserve module to generate the synthetic masked face images from the full face images. In addition, to tackle the second challenge, a Domain Constrained Ranking (DCR) loss is proposed by adopting a center-based cross-domain ranking strategy. For each identity, two centers are designed which correspond to the full face images and the masked face images respectively. The DCR forces the feature of masked faces getting closer to its corresponding full face center and vice-versa. Experimental results on the MFSR dataset demonstrate the effectiveness of the proposed approaches.",2020,ACM Multimedia,,10.1145/3394171.3413723,
94a3b65b358d0fd8d161110004ecd137a6ec557f,1,0,A Plug-in Method for Representation Factorization,"In this work, we focus on decomposing the latent representations in GANs or learned feature representations in deep auto-encoders into semantically controllable factors in a semi-supervised manner, without modifying the original trained models. Specifically, we propose a Factors Decomposer-Entangler Network (FDEN) that learns to decompose a latent representation into mutually independent factors. Given a latent representation, the proposed framework draws a set of interpretable factors, each aligned to independent factors of variations by maximizing their total correlation in an information-theoretic means. As a plug-in method, we have applied our proposed FDEN to the existing networks of Adversarially Learned Inference and Pioneer Network and conducted computer vision tasks of image-to-image translation in semantic ways, e.g., changing styles while keeping an identify of a subject, and object classification in a few-shot learning scheme. We have also validated the effectiveness of our method with various ablation studies in qualitative, quantitative, and statistical examination.",2019,,1905.11088,,https://arxiv.org/pdf/1905.11088.pdf
94df1d75715b83f9306f8837844336f6a3851042,1,1,Contraction Mapping of Feature Norms for Classifier Learning on the Data with Different Quality,"The popular softmax loss and its recent extensions have achieved great success in the deep learning-based image classification. However, the data for training image classifiers usually has different quality. Ignoring such problem, the correct classification of low quality data is hard to be solved. In this paper, we discover the positive correlation between the feature norm of an image and its quality through careful experiments on various applications and various deep neural networks. Based on this finding, we propose a contraction mapping function to compress the range of feature norms of training images according to their quality and embed this contraction mapping function into softmax loss or its extensions to produce novel learning objectives. The experiments on various classification applications, including handwritten digit recognition, lung nodule classification, face verification and face recognition, demonstrate that the proposed approach is promising to effectively deal with the problem of learning on the data with different quality and leads to the significant and stable improvements in the classification accuracy.",2020,ArXiv,2007.13406,,https://arxiv.org/pdf/2007.13406.pdf
94fadc98dcc340ca14642412aa328a9c9af2d2c4,1,1,Qamface: Quadratic Additive Angular Margin Loss For Face Recognition,"The angular-based softmax losses and their variants achieve great success in face recognition based on deep learning. ArcFace [1] which directly maximize decision boundary in angular space is one of the most popular and effective loss function. In this paper, we analyze the inherent limitations of ArcFace, including the non-monotonic logit and gradient curve, and inappropriate trend of loss value. To address these problems, we propose a novel loss function named the Quadratic Additive Angular Margin Loss (QAMFace). It takes the value of the angle through a quadratic function rather than cosine function as the target logit. Our QAMFace is easy to implement and only adds negligible computational overhead. Experiments on several relevant benchmarks show that QAMFace performs better in convergence on feature embedding, and consistently outperforms the state-of-the-art face recognition methods. Our codes will be released soon.1",2020,2020 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP40778.2020.9191004,
9507ea9cdeda30115e9bbbfa8090b24f9e1a26a3,0,1,Supporting large-scale image recognition with out-of-domain samples,"This article presents an efficient end-to-end method to perform instance-level recognition employed to the task of labeling and ranking landmark images. In a first step, we embed images in a high dimensional feature space using convolutional neural networks trained with an additive angular margin loss and classify images using visual similarity. We then efficiently re-rank predictions and filter noise utilizing similarity to out-of-domain images. Using this approach we achieved the 1st place in the 2020 edition of the Google Landmark Recognition challenge.",2020,ArXiv,2010.0165,,https://arxiv.org/pdf/2010.01650.pdf
95a309ad99b3a94ee38bfebf40c7af0635687e3d,1,1,xCos: An Explainable Cosine Metric for Face Verification Task,"We study the XAI (explainable AI) on the face recognition task, particularly the face verification here. Face verification is a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this paper, we propose a novel similarity metric, called explainable cosine ($xCos$), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of $xCos$, we can see which parts of the 2 input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output $xCos$ score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, resulting in not only providing novel and desiring model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.",2020,ArXiv,2003.05383,,https://arxiv.org/pdf/2003.05383.pdf
95df57cf3e15d75a8526b4fd3212d39538d31100,1,0,Towards Pose Invariant Face Recognition in the Wild,"Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a ""learning to learn"" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00235,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf
96a99d4a310f03bd4d10dbd77d31f7a0ba17db22,1,0,Weighted Softmax Loss for Face Recognition via Cosine Distance,"Softmax loss is commonly used to train convolutional neural networks (CNNs), but it treats all samples equally. Focal loss focus on training hard samples and takes the probability as the measurement of whether the sample is easy or hard one. In this paper, we use cosine distance of features and the corresponding centers as weight and propose weighted softmax loss (called C-Softmax). Unlike focal loss, we give greater weight to easy samples. Experiment results show that the proposed C-Softmax loss can train many well known models like ResNet, ResNeXt, DenseNet and Inception V3, and the performance of the proposed loss is better than softmax loss and focal loss.",2018,CCBR,,10.1007/978-3-319-97909-0_37,
96bdf413fba41dbdd86ccae762805192d1ca7948,1,0,Unsupervised Enhancement of Soft-biometric Privacy with Negative Face Recognition,"Current research on soft-biometrics showed that privacy-sensitive information can be deduced from biometric templates of an individual. Since for many applications, these templates are expected to be used for recognition purposes only, this raises major privacy issues. Previous works focused on supervised privacy-enhancing solutions that require privacy-sensitive information about individuals and limit their application to the suppression of single and pre-defined attributes. Consequently, they do not take into account attributes that are not considered in the training. In this work, we present Negative Face Recognition (NFR), a novel face recognition approach that enhances the soft-biometric privacy on the template-level by representing face templates in a complementary (negative) domain. While ordinary templates characterize facial properties of an individual, negative templates describe facial properties that does not exist for this individual. This suppresses privacy-sensitive information from stored templates. Experiments are conducted on two publicly available datasets captured under controlled and uncontrolled scenarios on three privacy-sensitive attributes. The experiments demonstrate that our proposed approach reaches higher suppression rates than previous work, while maintaining higher recognition performances as well. Unlike previous works, our approach does not require privacy-sensitive labels and offers a more comprehensive privacy-protection not limited to pre-defined attributes.",2020,ArXiv,2002.09181,,https://arxiv.org/pdf/2002.09181.pdf
96e318f8ff91ba0b10348d4de4cb7c2142eb8ba9,1,0,State-of-the-art face recognition performance using publicly available software and datasets,"We are interested in the reproducibility of face recognition systems. By reproducibility we mean: is the scientific community, and are the researchers from different sides, capable of reproducing the last published results by a big company, that has at its disposal huge computational power and huge proprietary databases? With the constant advancements in GPU computation power and availability of open-source software, the reproducibility of published results should not be a problem. But, if architectures of the systems are private and databases are proprietary, the reproducibility of published results can not be easily attained. To tackle this problem, we focus on training and evaluation of face recognition systems on publicly available data and software. We are also interested in comparing the best Deep Neural Net (DNN) based results with a baseline “classical” system. This paper exploits the OpenFace open-source system to generate a deep convolutional neural network model using publicly available datasets. We study the impact of the size of the datasets, their quality and compare the performance to a classical face recognition approach. Our focus is to have a fully reproducible model. To this end, we used publicly available datasets (FRGC, MS-celeb-lM, MOBIO, LFW), as well publicly available software (OpenFace) to train our model in order to do face recognition. Our best trained model achieves 97.52% accuracy on the Labelled in the Wild dataset (LFW) dataset which is lower than Google's best reported results of 99.96% but slightly better than FaceBook's reported result of 97.35%. We also evaluated our best model on the challenging video dataset MOBIO and report competitive results with the best reported results on this database.",2018,2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP),,10.1109/ATSIP.2018.8364450,https://zenodo.org/record/1323670/files/hmani-atsip-2018.pdf
972d5fd56d8fc554a9a1c01077dd2eb68c51532d,0,1,Analysis of the Human Protein Atlas Image Classification competition,"Pinpointing subcellular protein localizations from microscopy images is easy to the trained eye, but challenging to automate. Based on the Human Protein Atlas image collection, we held a competition to identify deep learning solutions to solve this task. Challenges included training on highly imbalanced classes and predicting multiple labels per image. Over 3 months, 2,172 teams participated. Despite convergence on popular networks and training techniques, there was considerable variety among the solutions. Participants applied strategies for modifying neural networks and loss functions, augmenting data and using pretrained networks. The winning models far outperformed our previous effort at multi-label classification of protein localization patterns by ~20%. These models can be used as classifiers to annotate new images, feature extractors to measure pattern similarity or pretrained networks for a wide range of biological applications. The 2018 Human Protein Atlas Image Classification competition sought to improve automated classification of protein subcellular localizations from fluorescence images. The winning strategies involved innovative deep learning approaches for multi-label classification.",2019,Nature Methods,,10.1038/s41592-019-0658-6,https://www.nature.com/articles/s41592-019-0658-6.pdf
976ca54593290fea2b3a76b70a00d1bd48587fe8,0,1,PSNet: Parametric Sigmoid Norm Based CNN for Face Recognition,"The Convolutional Neural Networks (CNN) have become very popular recently due to its outstanding performance in various computer vision applications. It is also used over widely studied face recognition problem. However, the existing layers of CNN are unable to cope with the problem of hard examples which generally produce lower class scores. Thus, the existing methods become biased towards the easy examples. In this paper, we resolve this problem by incorporating a Parametric Sigmoid Norm (PSN) layer just before the final fully-connected layer. We propose a PSNet CNN model by using the PSN layer. The PSN layer facilitates high gradient flow for harder examples as compared to easy examples. Thus, it forces the network to learn the visual features of difficult examples. We conduct the face recognition experiments to test the performance of PSN layer. The suitability of the PSN layer with different loss functions is also experimented. The widely used YouTube Faces (YTF) and Labeled Faces in the Wild (LFW) datasets are used in the experiments. The experimental results confirm the relevance of the proposed PSN laver.",2019,2019 IEEE Conference on Information and Communication Technology,1912.10946,10.1109/CICT48419.2019.9066169,https://arxiv.org/pdf/1912.10946.pdf
98518fc368d7e1478cef40f5f8fd4468763645ad,1,0,A Community Detection Approach to Cleaning Extremely Large Face Database,"Though it has been easier to build large face datasets by collecting images from the Internet in this Big Data era, the time-consuming manual annotation process prevents researchers from constructing larger ones, which makes the automatic cleaning of noisy labels highly desirable. However, identifying mislabeled faces by machine is quite challenging because the diversity of a person's face images that are captured wildly at all ages is extraordinarily rich. In view of this, we propose a graph-based cleaning method that mainly employs the community detection algorithm and deep CNN models to delete mislabeled images. As the diversity of faces is preserved in multiple large communities, our cleaning results have both high cleanness and rich data diversity. With our method, we clean the extremely large MS-Celeb-1M face dataset (approximately 10 million images with noisy labels) and obtain a clean version of it called C-MS-Celeb (6,464,018 images of 94,682 celebrities). By training a single-net model using our C-MS-Celeb dataset, without fine-tuning, we achieve 99.67% at Equal Error Rate on the LFW face recognition benchmark, which is comparable to other state-of-the-art results. This demonstrates the data cleaning positive effects on the model training. To the best of our knowledge, our C-MS-Celeb is the largest clean face dataset that is publicly available so far, which will benefit face recognition researchers.",2018,Comput. Intell. Neurosci.,,10.1155/2018/4512473,https://pdfs.semanticscholar.org/9851/8fc368d7e1478cef40f5f8fd4468763645ad.pdf
9855893e09ed45ee201f31052286b283704df35a,1,0,Distributed Facial Feature Clustering Algorithm Based on Spatiotemporal Locality,"Data clustering, as one of the major algorithms in facial feature data mining, its efficiency and quality plays a key role. DBSCAN is a classic density-based clustering algorithm. It is applicable to any shape of subset or cluster and is comparatively anti-noise, but because of its slow running speed, it has long convergence time when the data set is large. For this reason, the paper presents an improved DBSCAN algorithm based on distributed computing system, by making full use of the characteristic of high time partial similarity of facial feature under the monitoring scene, first to merge the features which belong to the same target, then to segment the merged result using DBSCAN algorithm. The test results show that although this new algorithm has no modification in the aspect of time complexity, but greatly improved clustering speed, which enables the system to process million-level datasets.",2020,IMIS,,10.1007/978-3-030-50399-4_38,
98837489a947b34d28aed6667cadae79950ee90a,0,1,"Exploring Deep Learning-Based Architecture, Strategies, Applications and Current Trends in Generic Object Detection: A Comprehensive Review","Object detection is a fundamental but challenging issue in the field of generic image analysis; it plays an important role in a wide range of applications and has been receiving special attention in recent years. Although there are enomerous methods exist, an in-depth review of the literature concerning generic detection remains. This paper provides a comprehensive survey of recent advances in visual object detection with deep learning. Covering about 300 publications that we survey 1) region proposal-based object detection methods such as R-CNN, SPPnet, Fast R-CNN, Faster R-CNN, Mask RCN, RFCN, FPN, 2) classification/regression base object detection methods such as YOLO(v2 to v5), SSD, DSSD, RetinaNet, RefineDet, CornerNet, EfficientDet, M2Det 3) Some latest detectors such as, relation network for object detection, DCN v2, NAS FPN. Moreover, five publicly available benchmark datasets and their standard evaluation metrics are also discussed. We mainly focus on the application of deep learning architectures to five major applications, namely Object Detection in Surveillance, Military, Transportation, Medical, and Daily Life. In the survey, we cover a variety of factors affecting the detection performance in detail, such as i) a wide range of object categories and intra-class variations, ii) limited storage capacity and computational power. Finally, we finish the survey by identifying fifteen current trends and promising direction for future research.",2020,IEEE Access,,10.1109/ACCESS.2020.3021508,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09186021.pdf
98881f674e520f0f352f0577a2e2a38e67ba4228,1,1,Fast large scale deep face search,"Abstract Towards the large scale face search problem, this paper proposes a fast deep face search method which is realized by combination of deep convolution neural network (CNN), semantic hashing, and hash-based similarity search. First of all, to boost the performance in accuracy of face search, the residual network (Resnet) is exploited to construct a deep face feature model and then train it over the cleaned MS-Celeb-1M, which is used to extract real-valued face feature. Next, by imposing PCA and binarization operations, we convert the real-valued feature into compact hash code used for speeding up the face search. Based on the extracted dual features, the face search can be efficiently performed by adopting two-stage matching (i.e., coarse matching and fine matching) strategy. The coarse matching is implemented under the support of efficient hash indexing technique for yielding a small number of candidates while the fine stage is to filter out the unrelated images by cosine distance comparison of real-valued features. It is worth noting that we offer two coarse matching methods, such as GPU-hash and M-index-hash based matching, which are suitable for tens-of million and billion scale scenarios respectively. The experimental results demonstrate that the proposed method is very effective for large scale face search in both aspects of accuracy and real time property.",2020,Pattern Recognit. Lett.,,10.1016/J.PATREC.2019.01.012,
98a0a153c4d2cd67a829bcce226d7d97c4d6483c,1,1,Multi-View Face Recognition Via Well-Advised Pose Normalization Network,"Numerous face frontalization methods based on 3D Morphable Model (3DMM) and Generative Adversarial Networks (GAN) have made great progress in multi-view face recognition. However, facial feature analysis and identity discrimination often suffer from failure frontalization results because of monotonous single-domain training and unpredictable input profile faces. To overcome the drawback, we present a novel approach named Well-advised Pose Normalization Network (WAPNN), which leverages multiple domains and extracts features considering their frontalization qualities wisely, to achieve a high accuracy on multi-view face recognition. Through multi-domain datasets, we design an end-to-end facial pose normalization network with adaptive weights on different objectives to exploit potentialities of various profile-front relationships. Meanwhile, the proposed method encourages intra-class compactness and inter-class separability between facial features by introducing quality-aware feature fusion. Experimental analyses show that our method effectively recovers frontal faces with good-quality textures and high identity-preserving, and significantly reduces the impact of various poses on face recognition under both constrained and wild environments.",2020,IEEE Access,,10.1109/ACCESS.2020.2983459,https://ieeexplore.ieee.org/ielx7/6287639/8948470/09047862.pdf
98b2f21db344b8b9f7747feaf86f92558595990c,1,0,PACES OF G ENERATIVE A DVERSARIAL N ETWORKS,"We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm’s ability to generate convincing, identity-matched photographs.",2018,,,,https://pdfs.semanticscholar.org/98b2/f21db344b8b9f7747feaf86f92558595990c.pdf
98d11cf63dc36a8c69f65a334b7734f55a87bcee,0,1,"Morphing Attack Detection - Database, Evaluation Platform and Benchmarking","Morphing attacks have posed a severe threat to Face Recognition System (FRS). Despite the number of advancements reported in recent works, we note serious open issues that are not addressed. Morphing Attack Detection (MAD) algorithms often are prone to generalization challenges as they are database dependent. The existing databases, mostly of semi-public nature, lack in diversity in terms of ethnicity, various morphing process and post-processing pipelines. Further, they do not reflect a realistic operational scenario for Automated Border Control (ABC) and do not provide a basis to test MAD on unseen data, in order to benchmark the robustness of algorithms. In this work, we present a new sequestered dataset for facilitating the advancements of MAD where the algorithms can be tested on unseen data in an effort to better generalize. The newly constructed dataset consists of facial images from 150 subjects from various ethnicities, age-groups and both genders. In order to challenge the existing MAD algorithms, the morphed images are with careful subject pre-selection created from the subjects, and further post-processed to remove the morphing artifacts. The images are also printed and scanned to remove all digital cues and to simulate a realistic challenge for MAD algorithms. Further, we present a new online evaluation platform to test algorithms on sequestered data. With the platform we can benchmark the morph detection performance and study the generalization ability. This work also presents a detailed analysis on various subsets of sequestered data and outlines open challenges for future directions in MAD research.",2020,ArXiv,2006.06458,10.1109/tifs.2020.3035252,https://arxiv.org/pdf/2006.06458.pdf
9921683cb4ce6d49bc85974b6897cdb6edcb614c,0,1,Attention-Based Query Expansion Learning,"Query expansion is a technique widely used in image search consisting in combining highly ranked images from an original query into an expanded query that is then reissued, generally leading to increased recall and precision. An important aspect of query expansion is choosing an appropriate way to combine the images into a new query. Interestingly, despite the undeniable empirical success of query expansion, ad-hoc methods with different caveats have dominated the landscape, and not a lot of research has been done on learning how to do query expansion. In this paper we propose a more principled framework to query expansion, where one trains, in a discriminative manner, a model that learns how images should be aggregated to form the expanded query. Within this framework, we propose a model that leverages a self-attention mechanism to effectively learn how to transfer information between the different images before aggregating them. Our approach obtains higher accuracy than existing approaches on standard benchmarks. More importantly, our approach is the only one that consistently shows high accuracy under different regimes, overcoming caveats of existing methods.",2020,ECCV,2007.08019,10.1007/978-3-030-58604-1_11,https://arxiv.org/pdf/2007.08019.pdf
993a0e8248a3bfbfb1e9eb978c2d4deea2fdd8b6,0,1,Cycle Age-Adversarial Model Based on Identity Preserving Network and Transfer Learning for Cross-Age Face Recognition,"Age variations bring a large challenge for face recognition tasks. Existing Cross-Age Face Recognition (CAFR) methods have two limitations. Firstly, many CAFR approaches require both age labels and identity labels for training. However, it is difficult to collect images under a large age span from each individual. Secondly, many works are based on the assumption that age and identity information are independent of each other, which may not satisfy various conditions. In this paper, a Cycle Age-Adversarial Model (CAAM) is proposed for CAFR, which only uses the age labels for training without considering independence hypothesis. CAAM includes two different branch networks. Firstly, the branch of Age-robust Feature Extracting Model (AFEM) is designed to adaptively learn age-invariant features by adversarial learning scheme, which includes an age discriminator network and a feature generator network. The age discriminator network is trained to discriminate the age information, and the generator extracts age-invariant features through adversarial learning with discriminator. Secondly, a branch of the Identity Preserving Network (IPN) is proposed to keep identity information, which introduces Unsupervised Identity Loss (UIL) to enlarge the inter-class distance, and decrease the loss of identity information in the learning process. Finally, the features of the two branches are cyclically optimized through minmizing Feature Consistency Loss (FCL), which integrates age invariance learning and identity discrimination learning into final feature representation. Different from existing CAFR networks, our adversarial learning strategy for age-robust feature learning can be generalized to other attributes including pose and expression. Moreover, we introduce cycle optimization strategy to merge the advantages of two branch networks, which is a novel strategy to fuse multi-task features. Extensive CAFR experiments performed on the benchmark MORPH Album2, CACD-VS and Cross Age LFW databases demonstrate the effectiveness and superiority of CAAM.",2020,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2019.2960585,
9967c717188a92bd4cf9aab3d78dca93bb012e83,0,1,Single-Stage Joint Face Detection and Alignment,"In practice, there are huge demands to localize faces in images and videos under unconstrained pose variation, illumination change, severe occlusion and low resolution, which pose a great challenge to existing face detectors. This challenge report presents a single-stage joint face detection and alignment method. In detail, we employ feature pyramid network, single-stage detection, context modelling, multi-task learning and cascade regression to construct a practical face detector. On the Wider Face Hard validation subset, our single model achieves state-of-the-art performance (92.0% AP) compared with both academic and commercial face detectors for detecting unconstrained faces in cluttered scenes. In the Wider Face AND PERSON CHALLENGE 2019, our ensemble model achieves 56.66% average AP (runner-up) in the face detection track. To facilitate further research on the topic, the training code and models have been provided publicly available.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00228,http://openaccess.thecvf.com/content_ICCVW_2019/papers/WIDER/Deng_Single-Stage_Joint_Face_Detection_and_Alignment_ICCVW_2019_paper.pdf
99d86a078e6aacffa39489af93051c27f8432f10,1,1,Extended T: Learning with Mixed Closed-set and Open-set Noisy Labels,"The label noise transition matrix $T$, reflecting the probabilities that true labels flip into noisy ones, is of vital importance to model label noise and design statistically consistent classifiers. The traditional transition matrix is limited to model closed-set label noise, where noisy training data has true class labels within the noisy label set. It is unfitted to employ such a transition matrix to model open-set label noise, where some true class labels are outside the noisy label set. Thus when considering a more realistic situation, i.e., both closed-set and open-set label noise occurs, existing methods will undesirably give biased solutions. Besides, the traditional transition matrix is limited to model instance-independent label noise, which may not perform well in practice. In this paper, we focus on learning under the mixed closed-set and open-set label noise. We address the aforementioned issues by extending the traditional transition matrix to be able to model mixed label noise, and further to the cluster-dependent transition matrix to better approximate the instance-dependent label noise in real-world applications. We term the proposed transition matrix as the cluster-dependent extended transition matrix. An unbiased estimator (i.e., extended $T$-estimator) has been designed to estimate the cluster-dependent extended transition matrix by only exploiting the noisy data. Comprehensive synthetic and real experiments validate that our method can better model the mixed label noise, following its more robust performance than the prior state-of-the-art label-noise learning methods.",2020,ArXiv,2012.00932,,https://arxiv.org/pdf/2012.00932.pdf
99f347901fd9d41b2995557780480bc8a994efa0,0,1,Survey on 3D face reconstruction from uncalibrated images,"Recently, a lot of attention has been focused on the incorporation of 3D data into face analysis and its applications. Despite providing a more accurate representation of the face, 3D face images are more complex to acquire than 2D pictures. As a consequence, great effort has been invested in developing systems that reconstruct 3D faces from an uncalibrated 2D image. However, the 3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is needed to restrict the solutions space. In this work, we review 3D face reconstruction methods in the last decade, focusing on those that only use 2D pictures captured under uncontrolled conditions. We present a classification of the proposed methods based on the technique used to add prior knowledge, considering three main strategies, namely, statistical model fitting, photometry, and deep learning, and reviewing each of them separately. In addition, given the relevance of statistical 3D facial models as prior knowledge, we explain the construction procedure and provide a comprehensive list of the publicly available 3D facial models. After the exhaustive study of 3D-from-2D face reconstruction approaches, we observe that the deep learning strategy is rapidly growing since the last few years, matching its extension to that of the widespread statistical model fitting. Unlike the other two strategies, photometry-based methods have decreased in number since the required strong assumptions cause the reconstructions to be of more limited quality than those resulting from model fitting and deep learning methods. The review also identifies current gaps and suggests avenues for future research.",2020,ArXiv,2011.0574,,https://arxiv.org/pdf/2011.05740.pdf
9a04f4e4e6c2b756638091b2c4ce69719bd10a37,0,1,Deep Transferring Quantization,"Network quantization is an effective method for network compression. Existing methods train a low-precision network by fine-tuning from a pre-trained model. However, training a low-precision network often requires large-scale labeled data to achieve superior performance. In many real-world scenarios, only limited labeled data are available due to expensive labeling costs or privacy protection. With limited training data, fine-tuning methods may suffer from the overfitting issue and substantial accuracy loss. To alleviate these issues, we introduce transfer learning into network quantization to obtain an accurate low-precision model. Specifically, we propose a method named deep transferring quantization (DTQ) to effectively exploit the knowledge in a pre-trained fullprecision model. To this end, we propose a learnable attentive transfer module to identify the informative channels for alignment. In addition, we introduce the Kullback–Leibler (KL) divergence to further help train a low-precision model. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of DTQ.",2020,ECCV,,10.1007/978-3-030-58598-3_37,https://pdfs.semanticscholar.org/9a04/f4e4e6c2b756638091b2c4ce69719bd10a37.pdf
9a2d83ed7d7cc647421e976d8669b023974fff67,0,1,MaskGAN: Towards Diverse and Interactive Facial Image Manipulation,"Facial image manipulation has achieved great progress in recent years. However, previous methods either operate on a predefined set of face attributes or leave users little freedom to interactively manipulate images. To overcome these drawbacks, we propose a novel framework termed MaskGAN, enabling diverse and interactive face manipulation. Our key insight is that semantic masks serve as a suitable intermediate representation for flexible face manipulation with fidelity preservation. MaskGAN has two main components: 1) Dense Mapping Network (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically, DMN learns style mapping between a free-form user modified mask and a target image, enabling diverse generation results. EBST models the user editing behavior on the source mask, making the overall framework more robust to various manipulated inputs. Specifically, it introduces dual-editing consistency as the auxiliary supervision signal. To facilitate extensive studies, we construct a large-scale high-resolution face dataset with fine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively evaluated on two challenging tasks: attribute transfer and style copy, demonstrating superior performance over other state-of-the-art methods. The code, models, and dataset are available at https://github.com/switchablenorms/CelebAMask-HQ.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1907.11922,10.1109/cvpr42600.2020.00559,https://arxiv.org/pdf/1907.11922.pdf
9a53f3939ddd89f9f061bcee96c3a9f11f29c512,0,1,"Machine Learning for Cyber Security: Third International Conference, ML4CS 2020, Guangzhou, China, October 8–10, 2020, Proceedings, Part III","Scientifically, the identification of salt ore has definite practical significance for the exploitation of oil and gas. Traditionally, this is achieved by picking the salt boundaries with manual vision, which may introduce serious systematic bias. Nowadays, with the technological progress of machine vision used in image analysis, human effort has been replaced by machine capacity in salt mine recognition. Especially, with the in-depth application of deep learning technology in the field of machine vision, salt mine recognition using image analysis is revolutionizing with more acceptable efficiency and accuracy. To this end, with exploratory data analysis to mine the characteristics and data processing to increase the size of the image data for further enhancing the generalization capability of the designed model, a deep convolutional neural network based image segmentation model is investigated to achieve salt mine recognition in this paper. Concretely, a UNet model integrated modified ResNet34 is first designed as a basic recognition model, and many attempts then are conducted to further optimizing the model according to the data characteristics, including adding auxiliary function, hypercolumn, scSE and depth supervision scheme. In addition, multiple loss functions are also attempted to be adapted to further improving the model generalization capacity. The numerical analysis and evaluation finally show the efficiency of the investigations on loss value and recognition accuracy.",2020,ML4CS,,10.1007/978-3-030-62463-7,
9a709ce59c2e95319b2bcb7314db3b220e75e77b,1,0,Improving Face Clustering in Videos,IMPROVING FACE CLUSTERING IN VIDEOS,2020,,,10.7275/R09E-VH85,http://vis-www.cs.umass.edu/papers/SouYoung_PhD_Thesis.pdf
9a840a0aef4a28fa503d62be7dc1b881d6236221,0,1,New Advances in Speaker Diarization,"Recently, speaker diarization based on speaker embeddings has shown excellent results in many works. In this paper we propose several enhancements throughout the diarization pipeline. This work addresses two clustering frameworks: agglomerative hierarchical clustering (AHC) and spectral clustering (SC). First, we use multiple speaker embeddings. We show that fusion of x-vectors and d-vectors boosts accuracy significantly. Second, we train neural networks to leverage both acoustic and duration information for scoring similarity of segments or clusters. Third, we introduce a novel method to guide the AHC clustering mechanism using a neural network. Fourth, we handle short duration segments in SC by deemphasizing their effect on setting the number of speakers. Finally, we propose a novel method for estimating the number of clusters in the SC framework. The method takes each eigenvalue and analyzes the projections of the SC similarity matrix on the corresponding eigenvector. We evaluated our system on NIST SRE 2000 CALLHOME and, using cross-validation, we achieved an error rate of 5.1%, going beyond state-of-the-art speaker diarization.",2020,INTERSPEECH,,10.21437/interspeech.2020-1879,https://isca-speech.org/archive/Interspeech_2020/pdfs/1879.pdf
9a9fc31939e3b637d7adc91753490b7345dcdcfe,0,1,Lifting 2D StyleGAN for 3D-Aware Face Generation,"We propose a framework, called LiftedGAN, that disentangles and lifts a pre-trained StyleGAN2 for 3D-aware face generation. Our model is “3D-aware” in the sense that it is able to (1) disentangle the latent space of StyleGAN2 into texture, shape, viewpoint, lighting and (2) generate 3D components for rendering synthetic images. Unlike most previous methods, our method is completely self-supervised, i.e. it neither requires any manual annotation nor 3DMM model for training. Instead, it learns to generate images as well as their 3D components by distilling the prior knowledge in StyleGAN2 with a differentiable renderer. The proposed model is able to output both the 3D shape and texture, allowing explicit pose and lighting control over generated images. Qualitative and quantitative results show the superiority of our approach over existing methods on 3D-controllable GANs in content controllability while generating realistic high quality images.",2020,ArXiv,2011.13126,,https://arxiv.org/pdf/2011.13126.pdf
9aa9379a390f547552943b4f6fb02470183493d5,1,0,GhostVLAD for set-based face recognition,"The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes {\em ghost clusters}, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network's ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.",2018,ACCV,1810.09951,10.1007/978-3-030-20890-5_3,https://arxiv.org/pdf/1810.09951.pdf
9aea9ec4b4f83c35840df0ffa27cc257580d5703,1,0,Face Verification and Recognition for Digital Forensics and Information Security,"In this paper, we present an extensive evaluation of face recognition and verification approaches performed by the European COST Action MULTI-modal Imaging of FOREnsic SciEnce Evidence (MULTI-FORESEE). The aim of the study is to evaluate various face recognition and verification methods, ranging from methods based on facial landmarks to state-of-the-art off-the-shelf pre-trained Convolutional Neural Networks (CNN), as well as CNN models directly trained for the task at hand. To fulfill this objective, we carefully designed and implemented a realistic data acquisition process, that corresponds to a typical face verification setup, and collected a challenging dataset to evaluate the real world performance of the aforementioned methods. Apart from verifying the effectiveness of deep learning approaches in a specific scenario, several important limitations are identified and discussed through the paper, providing valuable insight for future research directions in the field.",2019,2019 7th International Symposium on Digital Forensics and Security (ISDFS),,10.1109/ISDFS.2019.8757511,
9af0c4bd5255974c2842179de4df93a85b150314,0,1,Redesigning the classification layer by randomizing the class representation vectors,"Neural image classification models typically consist of two components. The first is an image encoder, which is responsible for encoding a given raw image into a representative vector. The second is the classification component, which is often implemented by projecting the representative vector onto target class vectors. The target class vectors, along with the rest of the model parameters, are estimated so as to minimize the loss function. In this paper, we analyze how simple design choices for the classification layer affect the learning dynamics. We show that the standard cross-entropy training implicitly captures visual similarities between different classes, which might deteriorate accuracy or even prevents some models from converging. We propose to draw the class vectors randomly and set them as fixed during training, thus invalidating the visual similarities encoded in these vectors. We analyze the effects of keeping the class vectors fixed and show that it can increase the inter-class separability, intra-class compactness, and the overall model accuracy, while maintaining the robustness to image corruptions and the generalization of the learned concepts.",2020,ArXiv,2011.08704,,https://arxiv.org/pdf/2011.08704.pdf
9b73de079e63ca32b990e724704226a64e1d55a0,1,1,Refined CNNs for Face Recognition Applications on Embedded Devices,"Deployment of deep learning models to embedded devices like smart phones or other smart end-user devices has been the hotspot in computer vision and artificial intelligence. However, the performance of recent state-of-the-art CNNs is dissatisfactory in real-world face applications. To solve this issue, we refined an efficient CNN architecture for face verification with extreme efficiency for real-time face applications in embedded environment, namely R-MobileFaceNet. We also proposed Dynamically Fuzzy Image Dataset, namely DFID, to evaluate the capacity of the models to deploy on embedded platforms. Our experiments proved our refined model was capable of embedded deployment, achieving higher accuracy improvement on DFID and neglectable accuracy loss on LFW. This paper also serves as an effective and efficient solution for deploying deep learning models to real-world face applications.",2020,ICMLC,,10.1145/3383972.3384025,
9ba3332d7c2ea5766ac5d72f1392d5acf58dde60,0,1,Zoom in Lesions for Better Diagnosis: Attention Guided Deformation Network for WCE Image Classification,"Wireless capsule endoscopy (WCE) is a novel imaging tool that allows noninvasive visualization of the entire gastrointestinal (GI) tract without causing discomfort to patients. Convolutional neural networks (CNNs), though perform favorably against traditional machine learning methods, show limited capacity in WCE image classification due to the small lesions and background interference. To overcome these limits, we propose a two-branch Attention Guided Deformation Network (AGDN) for WCE image classification. Specifically, the attention maps of branch1 are utilized to guide the amplification of lesion regions on the input images of branch2, thus leading to better representation and inspection of the small lesions. What’s more, we devise and insert Third-order Long-range Feature Aggregation (TLFA) modules into the network. By capturing long-range dependencies and aggregating contextual features, TLFAs endow the network with a global contextual view and stronger feature representation and discrimination capability. Furthermore, we propose a novel Deformation based Attention Consistency (DAC) loss to refine the attention maps and achieve the mutual promotion of the two branches. Finally, the global feature embeddings from the two branches are fused to make image label predictions. Extensive experiments show that the proposed AGDN outperforms state-of-the-art methods with an overall classification accuracy of 91.29% on two public WCE datasets. The source code is available at https://github.com/hathawayxxh/WCE-AGDN.",2020,IEEE Transactions on Medical Imaging,,10.1109/TMI.2020.3010102,
9bc2df9c07d939442844b4b71c6e20565cbd5038,1,0,Face familiarity detection with complex synapses,"Synaptic plasticity is a complex phenomenon involving multiple biochemical processes that operate on different timescales. We recently showed that this complexity can greatly increase the memory capacity of neural networks when the variables that characterize the synaptic dynamics have limited precision, as in biological systems. These types of complex synapses have been tested mostly on simple memory retrieval problems involving random and uncorrelated patterns. Here we turn to a real-world problem, face familiarity detection, and we show that also in this case it is possible to take advantage of synaptic complexity to store in memory a large number of faces that can be recognized at a later time. In particular, we show that the memory capacity of a system with complex synapses grows almost linearly with the number of the synapses and quadratically with the number of neurons. Complex synapses are superior to simple ones, which are characterized by a single variable, even when the total number of dynamical variables is matched. Our results indicate that a memory system with complex synapses can be used in real-world applications such as familiarity detection.",2019,,,10.1101/854059,https://www.biorxiv.org/content/biorxiv/early/2019/11/25/854059.full.pdf
9bcaa98e7d13dff011feb54cca2bd5ea2895bbfb,0,1,Caffe Barista: Brewing Caffe with FPGAs in the Training Loop,"As the complexity of deep learning (DL) modelsincreases, their compute requirements increase accordingly. De-ploying a Convolutional Neural Network (CNN) involves twophases: training and inference. With the inference task typicallytaking place on resource-constrained devices, a lot of research hasexplored the field of low-power inference on custom hardwareaccelerators. On the other hand, training is both more compute-and memory-intensive and is primarily performed on power-hungry GPUs in large-scale data centres. CNN training onFPGAs is a nascent field of research. This is primarily due tothe lack of tools to easily prototype and deploy various hardwareand/or algorithmic techniques for power-efficient CNN training. This work presentsBarista, an automated toolflow that providesseamless integration of FPGAs into the training of CNNs withinthe popular deep learning framework Caffe. To the best of ourknowledge, this is the only tool that allows for such versatile andrapid deployment of hardware and algorithms for the FPGA-based training of CNNs, providing the necessary infrastructurefor further research and development.",2020,2020 30th International Conference on Field-Programmable Logic and Applications (FPL),2006.13829,10.1109/FPL50879.2020.00059,https://arxiv.org/pdf/2006.13829.pdf
9c79a853f7beb5726506c358b5fd1a1b7ce12aac,1,0,Word ID Person ID pid information audio wid information visual wid information wid information wid adversarial against pid pid adversarial against wid wid,"Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.",2019,,,,
9cd69a48273dfc18dea77246eca984654cae9e6a,1,0,Investigating Bias in Facial Analysis Systems: A Systematic Review,"Recent studies have demonstrated that most commercial facial analysis systems are biased against certain categories of race, ethnicity, culture, age and gender. The bias can be traced in some cases to the algorithms used and in other cases to insufficient training of algorithms, while in still other cases bias can be traced to insufficient databases. To date, no comprehensive literature review exists which systematically investigates bias and discrimination in the currently available facial analysis software. To address the gap, this study conducts a systematic literature review (SLR) in which the context of facial analysis system bias is investigated in detail. The review, involving 24 studies, additionally aims to identify (a) facial analysis databases that were created to alleviate bias, (b) the full range of bias in facial analysis software and (c) algorithms and techniques implemented to mitigate bias in facial analysis.",2020,IEEE Access,,10.1109/ACCESS.2020.3006051,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09130131.pdf
9d14cdbf0af4af7a7bc0a62084fe9c0ab43d6502,1,1,Towards a Fast and Accurate Face Recognition System from Deep Representations,"Title of Dissertation: TOWARDS A FAST AND ACCURATE FACE RECOGNITION SYSTEM FROM DEEP REPRESENTATIONS Rajeev Ranjan Doctor of Philosophy, 2019 Dissertation directed by: Professor Rama Chellappa Department of Electrical and Computer Engineering The key components of a machine perception algorithm are feature extraction followed by classification or regression. The features representing the input data should have the following desirable properties: 1) they should contain the discriminative information required for accurate classification, 2) they should be robust and adaptive to several variations in the input data due to illumination, translation/rotation, resolution, and input noise, 3) they should lie on a simple manifold for easy classification or regression. Over the years, researchers have come up with various hand crafted techniques to extract meaningful features. However, these features do not perform well for data collected in unconstrained settings due to large variations in appearance and other nuisance factors. Recent developments in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on various machine perception tasks such as object detection and recognition. DCNNs are highly non-linear regressors because of the presence of hierarchical convolutional layers with non-linear activation. Unlike the hand crafted features, DCNNs learn the feature extraction and feature classification/regression modules from the data itself in an end-to-end fashion. This enables the DCNNs to be robust to variations present in the data and at the same time improve their discriminative ability. Ever-increasing computation power and availability of large datasets have led to significant performance gains from DCNNs. However, these developments in deep learning are not directly applicable to the face analysis tasks due to large variations in illumination, resolution, viewpoint, and attributes of faces acquired in unconstrained settings. In this dissertation, we address this issue by developing efficient DCNN architectures and loss functions for multiple face analysis tasks such as face detection, pose estimation, landmarks localization, and face recognition from unconstrained images and videos. In the first part of this dissertation, we present two face detection algorithms based on deep pyramidal features. The first face detector, called DP2MFD, utilizes the concepts of deformable parts model (DPM) in the context of deep learning. It is able to detect faces of various sizes and poses in unconstrained conditions. It reduces the gap in training and testing of DPM on deep features by adding a normalization layer to the DCNN. The second face detector, called Deep Pyramid Single Shot Face Detector (DPSSD), is fast and capable of detecting faces with large scale variations (especially tiny faces). It makes use of the inbuilt pyramidal hierarchy present in a DCNN, instead of creating an image pyramid. Extensive experiments on publicly available unconstrained face detection datasets show that both these face detectors are able to capture the meaningful structure of faces and perform significantly better than many traditional face detection algorithms. In the second part of this dissertation, we present two algorithms for simultaneous face detection, landmarks localization, pose estimation and gender recognition using DCNNs. The first method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. The second approach extends HyperFace to incorporate additional tasks of face verification, age estimation and smile detection, in All-In-One Face. HyperFace and All-In-One Face exploit the synergy among the tasks which improves individual performances. In the third part of this dissertation, we focus on improving the task of face verification by designing a novel loss function that maximizes the inter-class distance and minimizes the intra-class distance in the feature space. We propose a new loss function, called Crystal Loss, that adds an L2-constraint to the feature descriptors which restricts them to lie on a hypersphere of a fixed radius. This module can be easily implemented using existing deep learning frameworks. We show that integrating this simple step in the training pipeline significantly boosts the performance of face verification. We additionally describe a deep learning pipeline for unconstrained face identification and verification which achieves state-of-the-art performance on several benchmark datasets. We provide the design details of the various modules involved in automatic face recognition: face detection, landmark localization and alignment, and face identification/verification. We present experimental results for end-to-end face verification and identification on IARPA Janus Benchmarks A, B and C (IJB-A, IJB-B, IJB-C), and the Janus Challenge Set 5 (CS5). Though DCNNs have surpassed human-level performance on tasks such as object classification and face verification, they can easily be fooled by adversarial attacks. These attacks add a small perturbation to the input image that causes the network to mis-classify the sample. In the final part of this dissertation, we focus on safeguarding the DCNNs and neutralizing adversarial attacks by compact feature learning. In particular, we show that learning features in a closed and bounded space improves the robustness of the network. We explore the effect of Crystal Loss, that enforces compactness in the learned features, thus resulting in enhanced robustness to adversarial perturbations. Additionally, we propose compact convolution, a novel method of convolution that when incorporated in conventional CNNs improves their robustness. Compact convolution ensures feature compactness at every layer such that they are bounded and close to each other. Extensive experiments show that Compact Convolutional Networks (CCNs) neutralize multiple types of attacks, and perform better than existing methods in defending adversarial attacks, without incurring any additional training overhead compared to CNNs. Towards a Fast and Accurate Face Recognition System from Deep Representations",2019,,,10.13016/hdxh-ign6,
9d3ab64f84b267572035a0f32a818c773b1845d9,0,1,Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds,"Local and global patterns of an object are closely related. Although each part of an object is incomplete, the underlying attributes about the object are shared among all parts, which makes reasoning the whole object from a single part possible. We hypothesize that a powerful representation of a 3D object should model the attributes that are shared between parts and the whole object, and distinguishable from other objects. Based on this hypothesis, we propose to learn point cloud representation by bidirectional reasoning between the local structures at different abstraction hierarchies and the global shape without human supervision. Experimental results on various benchmark datasets demonstrate the unsupervisedly learned representation is even better than supervised representation in discriminative power, generalization ability, and robustness. We show that unsupervisedly trained point cloud models can outperform their supervised counterparts on downstream classification tasks. Most notably, by simply increasing the channel width of an SSG PointNet++, our unsupervised model surpasses the state-of-the-art supervised methods on both synthetic and real-world 3D object classification datasets. We expect our observations to offer a new perspective on learning better representation from data structures instead of human annotations for point cloud understanding.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.12971,10.1109/cvpr42600.2020.00542,https://arxiv.org/pdf/2003.12971.pdf
9d5d0ddb665009c0c3d53193722e14768b11d464,1,0,IPU-Net: Multi Scale Identity-Preserved U-Net for Low Resolution Face Recognition,"State-of-the-art deep neural network models have reached near perfect face recognition accuracy rates on controlled high resolution face images. However, their performance is drastically degraded when they are tested with very low resolution face images. This is particularly critical in surveillance systems, where a low resolution probe image is to be matched with high resolution gallery images. Super resolution techniques aim at producing high resolution face images from low resolution counterparts. While they are capable of reconstructing images that are visually appealing, the identity-related information is not preserved. Here, we propose an identity-preserved U-Net which is capable of super-resolving very low resolution faces to their high resolution counterparts while preserving identity-related information. We achieve this by training a U-Net with a combination of a reconstruction and an identity-preserving loss, on multi-scale low resolution conditions. Extensive quantitative evaluations of our proposed model demonstrated that it outperforms competing super resolution and low resolution face recognition methods on natural and artificial low resolution face data sets and even unseen identities.",2020,ArXiv,2010.12249,,https://arxiv.org/pdf/2010.12249.pdf
9d60bc023f5a1a36ed0b509c0f445fcad02d0abf,1,0,FaceLiveNet+: A Holistic Networks For Face Authentication Based On Dynamic Multi-task Convolutional Neural Networks,"This paper proposes a holistic multi-task Convolutional Neural Networks (CNNs) with the dynamic weights of the tasks,namely FaceLiveNet+, for face authentication. FaceLiveNet+ can employ face verification and facial expression recognition as a solution of liveness control simultaneously. Comparing to the single-task learning, the proposed multi-task learning can better capture the feature representation for all of the tasks. The experimental results show the superiority of the multi-task learning to the single-task learning for both the face verification task and facial expression recognition task. Rather using a conventional multi-task learning with fixed weights for the tasks, this work proposes a so called dynamic-weight-unit to automatically learn the weights of the tasks. The experiments have shown the effectiveness of the dynamic weights for training the networks. Finally, the holistic evaluation for face authentication based on the proposed protocol has shown the feasibility to apply the FaceLiveNet+ for face authentication.",2019,ArXiv,1902.11179,,https://arxiv.org/pdf/1902.11179.pdf
9de109452c02ebe3d588365435fd5c0822fbecb7,1,0,Transformation on Computer-Generated Facial Image to Avoid Detection by Spoofing Detector,"Making computer-generated (CG) images more difficult to detect is an interesting problem in computer graphics and security. While most approaches focus on the image rendering phase, this paper presents a method based on increasing the naturalness of CG facial images from the perspective of spoofing detectors. The proposed method is implemented using a convolutional neural network (CNN) comprising two autoencoders and a transformer and is trained using a black-box discriminator without gradient information. Over 50% of the transformed CG images were not detected by three state-of-the-art spoofing detectors. This capability raises an alarm regarding the reliability of facial authentication systems, which are becoming widely used in daily life.",2018,2018 IEEE International Conference on Multimedia and Expo (ICME),1804.04418,10.1109/ICME.2018.8486579,https://www.pure.ed.ac.uk/ws/files/65084440/TRANSFORMATION_ON_COMPUTER_GENERATED_FACIAL_IMAGE_TO_AVOID_DETECTION.pdf
9e0fa1c5ed091355917fe271efa269ad03c82d55,1,1,Heterogeneous Face Recognition Based on Multiple Deep Networks With Scatter Loss and Diversity Combination,"Due to the gap between sensing patterns of different domains and a lack of sufficient training sample, heterogeneous face recognition (HFR) is still a challenging issue in the computer vision community. In this paper, we propose a novel method called multiple deep networks with scatter loss and diversity combination (MDNDC) for solving the HFR problem. As we know, the performance of deep models is affected by data, network structure, and loss function, so we devote much effort to improve the HFR performance from all these three aspects. First, to reduce the intra-class variations and increase the inter-class variations, the scatter loss (SL) is used as an objective function that can bridge the modality gap while preserving the identity information. Second, we design a multiple deep networks (MDN) structure for feature extraction and propose a joint decision strategy called diversity combination (DC) to adaptively adjust the weights of each deep network and make a joint classification decision. Finally, instead of using only one publicly available dataset, we make full use of multiple datasets to train the networks, which can further improve the HFR performance. The extensive experiments are carried out on two challenging NIR-VIS HFR datasets, CASIA NIR-VIS 2.0 and Oulu-CASIA NIR-VIS, demonstrating the superiority of the proposed method.",2019,IEEE Access,,10.1109/ACCESS.2019.2920855,
9e1b0f50417867317a8cb8fe35c6b2617ad9641e,1,1,Diversity in Faces,"Face recognition is a long standing challenge in the field of Artificial Intelligence (AI). The goal is to create systems that accurately detect, recognize, verify, and understand human faces. There are significant technical hurdles in making these systems accurate, particularly in unconstrained settings due to confounding factors related to pose, resolution, illumination, occlusion, and viewpoint. However, with recent advances in neural networks, face recognition has achieved unprecedented accuracy, largely built on data-driven deep learning methods. While this is encouraging, a critical aspect that is limiting facial recognition accuracy and fairness is inherent facial diversity. Every face is different. Every face reflects something unique about us. Aspects of our heritage - including race, ethnicity, culture, geography - and our individual identify - age, gender, and other visible manifestations of self-expression, are reflected in our faces. We expect face recognition to work equally accurately for every face. Face recognition needs to be fair. As we rely on data-driven methods to create face recognition technology, we need to ensure necessary balance and coverage in training data. However, there are still scientific questions about how to represent and extract pertinent facial features and quantitatively measure facial diversity. Towards this goal, Diversity in Faces (DiF) provides a data set of one million annotated human face images for advancing the study of facial diversity. The annotations are generated using ten well-established facial coding schemes from the scientific literature. The facial coding schemes provide human-interpretable quantitative measures of facial features. We believe that by making the extracted coding schemes available on a large set of faces, we can accelerate research and development towards creating more fair and accurate facial recognition systems.",2019,ArXiv,1901.10436,,https://arxiv.org/pdf/1901.10436.pdf
9e22ef04a1794d1c53213d5b713feee26589d5cd,0,1,Importance of Data Loading Pipeline in Training Deep Neural Networks,"Training large-scale deep neural networks is a long, time-consuming operation, often requiring many GPUs to accelerate. In large models, the time spent loading data takes a significant portion of model training time. As GPU servers are typically expensive, tricks that can save training time are valuable.Slow training is observed especially on real-world applications where exhaustive data augmentation operations are required. Data augmentation techniques include: padding, rotation, adding noise, down sampling, up sampling, etc. These additional operations increase the need to build an efficient data loading pipeline, and to explore existing tools to speed up training time. We focus on the comparison of two main tools designed for this task, namely binary data format to accelerate data reading, and NVIDIA DALI to accelerate data augmentation. Our study shows improvement on the order of 20% to 40% if such dedicated tools are used.",2020,ArXiv,2005.0213,,https://arxiv.org/pdf/2005.02130.pdf
9e3a9bddd773cd34b186cbd3489a112598583294,0,1,Masked Face Recognition Dataset and Application,"In order to effectively prevent the spread of COVID-19 virus, almost everyone wears a mask during coronavirus epidemic. This almost makes conventional facial recognition technology ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc. Therefore, it is very urgent to improve the recognition performance of the existing face recognition technology on the masked faces. Most current advanced face recognition approaches are designed based on deep learning, which depend on a large number of face samples. However, at present, there are no publicly available masked face recognition datasets. To this end, this work proposes three types of masked face datasets, including Masked Face Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Simulated Masked Face Recognition Dataset (SMFRD). Among them, to the best of our knowledge, RMFRD is currently theworld's largest real-world masked face dataset. These datasets are freely available to industry and academia, based on which various applications on masked faces can be developed. The multi-granularity masked face recognition model we developed achieves 95% accuracy, exceeding the results reported by the industry. Our datasets are available at: this https URL.",2020,ArXiv,2003.09093,,https://arxiv.org/pdf/2003.09093.pdf
9e45de6fcf14e6c600dde8e8743e8b4c246ff432,0,1,Feature extraction based on deep‐convolutional neural network for face recognition,"Feature extraction is a critical technology that affects the accuracy of face recognition. However, certain features are highly related to changes in face are difficult to extract because of the influences of individual differences and illumination. Therefore, features can accurately describe the changes in face are urgently required. For this reason, this article proposes a feature extraction method based on deep learning. This method combines the features extracted by Local Binary Patterns and by Convolutional Neural Network convolutional layer in the network connection layer, thus obtaining classification features with high representation ability and solving the problem of single feature extraction. The VGG‐16 network proposed in this article has been improved by changing the framework structure. Some experiments based on the Labeled Faces in the Wild dataset are performed, and results show that, in terms of accuracy and the sensitivity to light, the proposed method reaches 99.56% and 80.35% respectively. The recognition results obtained from fused features are superior to which of single feature recognition. Simulation results show that the method is more robust to changes in the illumination condition and more efficient than the existing methods.",2020,Concurr. Comput. Pract. Exp.,,10.1002/cpe.5851,
9e4fb830ac2b074275c9665b0f58675d30ebe415,1,0,A Network for Makeup Face Verification Based upon Deep Learning,"Makeup, derived from the human pursuit of beauty, it changes the image of people appearance, brings more beautiful enjoyment and spiritual pleasure. However, recent studies have shown that facial makeup have a negative effect on face verification. To solve this problem, we formulate an end-to-end deep learning network which is composed of a stem CNN and a novel mapping module. Specifically, we pre-train our framework on a comprehensive dataset and fine-tune our mapping module on makeup datasets. Then we experimentally validate the proposal on these datasets. Experimental results demonstrate that the proposal achieves promising performance compared to the existing state-of-the-art methods.",2020,"2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC)",,10.1109/ICIVC50857.2020.9177431,
9e7464ba9bdaa0e3de8e02547fbd8267abfa4191,0,1,Mesh Guided One-shot Face Reenactment Using Graph Convolutional Networks,"Face reenactment aims to animate a source face image to a different pose and expression provided by a driving image. Existing approaches are either designed for a specific identity, or suffer from the identity preservation problem in the one-shot or few-shot scenarios. In this paper, we introduce a method for one-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the source mesh and driving mesh) as guidance to learn the optical flow needed for the reenacted face synthesis. Technically, we explicitly exclude the driving face's identity information in the reconstructed driving mesh. In this way, our network can focus on the motion estimation for the source face without the interference of driving face shape. We propose a motion net to learn the face motion, which is an asymmetric autoencoder. The encoder is a graph convolutional network (GCN) that learns a latent motion vector from the meshes, and the decoder serves to produce an optical flow image from the latent vector with CNNs. Compared to previous methods using sparse keypoints to guide the optical flow learning, our motion net learns the optical flow directly from 3D dense meshes, which provide the detailed shape and pose information for the optical flow, so it can achieve more accurate expression and pose on the reenacted face. Extensive experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",2020,ACM Multimedia,2008.07783,10.1145/3394171.3413865,https://arxiv.org/pdf/2008.07783.pdf
9e9d6b1763b660485cf2117389a1c27bc98cc1ec,1,1,A Progressive Learning Framework for Unconstrained Face Recognition,"The carefully designed backbone network, the increase of training data and the improved training skills have boosted the performance of modern face recognition systems. However, in some deployment cases which aim at model compactness and energy efficiency, some of the existing systems may fail due to the high complexity. Lightweight Face Recognition Challenge is proposed in order to make some progress in this direction and establishes a new comprehensive benchmark. In this challenge, we have designed a light weight backbone architecture and all the parameters are trained in a progressive way. Finally we achieve the 5th in track 1 and the 4th in track 3.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00331,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Chai_A_Progressive_Learning_Framework_for_Unconstrained_Face_Recognition_ICCVW_2019_paper.pdf
9ea37d031a8f112292c0d0f8d731b837462714e9,1,1,Face Recognition: From Traditional to Deep Learning Methods,"Starting in the seventies, face recognition has become one of the most researched topics in computer vision and biometrics. Traditional methods based on hand-crafted features and traditional machine learning techniques have recently been superseded by deep neural networks trained with very large datasets. In this paper we provide a comprehensive and up-to-date literature review of popular face recognition methods including both traditional (geometry-based, holistic, feature-based and hybrid methods) and deep learning methods.",2018,ArXiv,1811.00116,,https://arxiv.org/pdf/1811.00116.pdf
9eedf157222527670b565fd025067ab2062c24ac,0,1,Delving into Inter-Image Invariance for Unsupervised Visual Representations,"Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remain much less explored. One major obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since there are no pair annotations available. In this work, we present a rigorous and comprehensive study on inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. Through carefully-designed comparisons and analysis, we propose a unified framework that supports the integration of unsupervised intra- and inter-image invariance learning. With all the obtained recipes, our final model, namely InterCLR, achieves state-of-the-art performance on standard benchmarks. Code and models will be available at this https URL.",2020,ArXiv,2008.11702,,https://arxiv.org/pdf/2008.11702.pdf
9f65319b8a33c8ec11da2f034731d928bf92e29d,1,0,TAKING ROLL : A PIPELINE FOR FACE RECOGNITION,"We propose a generic pipeline for a face recognition system capable of creating or cleaning datasets when videos or images come from a finite set of identities. Face recognition has wide practical applicability for organizations and can be solved using an approach based on Convolutional Neural Networks, such as FaceNet. Differently from FaceNet, we proposed a solution based on a Convolutional Neural Network model with center loss, that speeds-up the labeling of faces in a video. With this pipeline, we show that cleaning a dataset of faces is a fully automatable process and improves the performance of the face recognition system. Together these two elements of the pipeline significantly improve face recognition results.",2018,,,,https://pdfs.semanticscholar.org/9f65/319b8a33c8ec11da2f034731d928bf92e29d.pdf
9f73735295bb0c9a569307023cf4f887084d9a4a,0,1,Multilevel Feature Fusion With 3D Convolutional Neural Network for EEG-Based Workload Estimation,"Mental workload is defined as the proportion of the information processing capability used to perform a task. High cognitive load requires additional resources to process information; this demand for additional resources may reduce the processing efficiency and performance. Therefore, the technique of workload estimation can ensure a proper working environment to promote the working efficiency of each person. In this paper, we propose a three-dimensional convolutional neural network (3D CNN) employing a multilevel feature fusion algorithm for mental workload estimation using electroencephalogram (EEG) signals. The 1D EEG signals are converted to 3D EEG images to enable the 3D CNN to learn the spectral and spatial information over the scalp. The multilevel feature fusion framework integrates local and global neuronal activities by workload tasks in the 3D CNN algorithm. Multilevel features are extracted in each layer of the 3D convolution operation and each multilevel feature is multiplied by a weighting factor, which determines the importance of the feature. The weighting factor is adaptively estimated for each EEG image by a backpropagation process. Furthermore, we generate subframes from each EEG image and propose a temporal attention technique based on the long short-term memory model (LSTM) to extract a significant subframe at each multilevel feature that is strongly correlated with task difficulty. To verify the performance of our network, we performed the Sternberg task to measure the mental workload of the participant, which was classified according to its difficulty as low or high workload condition. We showed that the difficulty of the workload was well designed, which was reflected in the behavior of the participant. Our network is trained on this dataset and the accuracy of our network is 90.8 %, which is better than that of conventional algorithms. We also evaluated our method using the public EEG dataset and achieved 93.9 % accuracy.",2020,IEEE Access,,10.1109/ACCESS.2020.2966834,https://ieeexplore.ieee.org/ielx7/6287639/8948470/08960298.pdf
9f753f67da834e59f9a5c8cdf9a88ee84c496b2d,1,1,Minimizing FLOPs to Learn Efficient Sparse Representations,"Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.",2020,ICLR,2004.05665,,https://arxiv.org/pdf/2004.05665.pdf
9f99b7fcf947723248e435ef0a7d553752d30965,1,1,A Face Recognition System for Assistive Robots,"Assistive robots collaborating with people demand strong Human-Robot interaction capabilities. In this way, recognizing the person the robot has to interact with is paramount to provide a personalized service and reach a satisfactory end-user experience. To this end, face recognition: a non-intrusive, automatic mechanism of identification using biometric identifiers from an user's face, has gained relevance in the recent years, as the advances in machine learning and the creation of huge public datasets have considerably improved the state-of-the-art performance. In this work we study different open-source implementations of the typical components of state-of-the-art face recognition pipelines, including face detection, feature extraction and classification, and propose a recognition system integrating the most suitable methods for their utilization in assistant robots. Concretely, for face detection we have considered MTCNN, OpenCV's DNN, and OpenPose, while for feature extraction we have analyzed InsightFace and Facenet. We have made public an implementation of the proposed recognition framework, ready to be used by any robot running the Robot Operating System (ROS). The methods in the spotlight have been compared in terms of accuracy and performance in common benchmark datasets, namely FDDB and LFW, to aid the choice of the final system implementation, which has been tested in a real robotic platform.",2020,APPIS,,10.1145/3378184.3378225,https://riuma.uma.es/xmlui/bitstream/10630/19402/3/APPIS_Face_Resumen.pdf
9fc17fa5708584fa848164461f82a69e97f6ed69,0,1,Additive Margin Softmax for Face Verification,"In this letter, we propose a conceptually simple and intuitive learning objective function, i.e., additive margin softmax, for face verification. In general, face verification tasks can be viewed as metric learning problems, even though lots of face verification models are trained in classification schemes. It is possible when a large-margin strategy is introduced into the classification model to encourage intraclass variance minimization. As one alternative, angular softmax has been proposed to incorporate the margin. In this letter, we introduce another kind of margin to the softmax loss function, which is more intuitive and interpretable. Experiments on LFW and MegaFace show that our algorithm performs better when the evaluation criteria are designed for very low false alarm rate.",2018,IEEE Signal Processing Letters,1801.05599,10.1109/LSP.2018.2822810,https://arxiv.org/pdf/1801.05599.pdf
a01b25dee8ccccae1cfc7a354fb4eab5428cae55,1,1,The Effect of Wearing a Mask on Face Recognition Performance: an Exploratory Study,"Face recognition has become essential in our daily lives as a convenient and contactless method of accurate identity verification. Process such as identity verification at automatic border control gates or the secure login to electronic devices are increasingly dependant on such technologies. The recent COVID-19 pandemic have increased the value of hygienic and contactless identity verification. However, the pandemic led to the wide use of face masks, essential to keep the pandemic under control. The effect of wearing a mask on face recognition in a collaborative environment is currently sensitive yet understudied issue. We address that by presenting a specifically collected database containing three session, each with three different capture instructions, to simulate realistic use cases. We further study the effect of masked face probes on the behaviour of three top-performing face recognition systems, two academic solutions and one commercial off-the-shelf (COTS) system.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),2007.13521,,https://arxiv.org/pdf/2007.13521.pdf
a05cb60388e1d7cbf399422b2f9bc954219021c9,0,1,Exploring Racial Bias within Face Recognition via per-subject Adversarially-Enabled Data Augmentation,"Whilst face recognition applications are becoming increasingly prevalent within our daily lives, leading approaches in the field still suffer from performance bias to the detriment of some racial profiles within society. In this study, we propose a novel adversarial derived data augmentation methodology that aims to enable dataset balance at a per-subject level via the use of image-to-image transformation for the transfer of sensitive racial characteristic facial features. Our aim is to automatically construct a synthesised dataset by transforming facial images across varying racial domains, while still preserving identity-related features, such that racially dependant features subsequently become irrelevant within the determination of subject identity. We construct our experiments on three significant face recognition variants: Softmax, CosFace and ArcFace loss over a common convolutional neural network backbone. In a side-by-side comparison, we show the positive impact our proposed technique can have on the recognition performance for (racial) minority groups within an originally imbalanced training dataset by reducing the per-race variance in performance.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2004.08945,10.1109/CVPRW50498.2020.00017,https://arxiv.org/pdf/2004.08945.pdf
a069bce9492641698515498d920c22f79c6d2079,0,1,RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild,"Though tremendous strides have been made in uncontrolled face detection, accurate and efficient 2D face alignment and 3D face reconstruction in-the-wild remain an open challenge. In this paper, we present a novel single-shot, multi-level face localisation method, named RetinaFace, which unifies face box prediction, 2D facial landmark localisation and 3D vertices regression under one common target: point regression on the image plane. To fill the data gap, we manually annotated five facial landmarks on the WIDER FACE dataset and employed a semi-automatic annotation pipeline to generate 3D vertices for face images from the WIDER FACE, AFLW and FDDB datasets. Based on extra annotations, we propose a mutually beneficial regression target for 3D face reconstruction, that is predicting 3D vertices projected on the image plane constrained by a common 3D topology. The proposed 3D face reconstruction branch can be easily incorporated, without any optimisation difficulty, in parallel with the existing box and 2D landmark regression branches during joint training. Extensive experimental results show that RetinaFace can simultaneously achieve stable face detection, accurate 2D face alignment and robust 3D face reconstruction while being efficient through single-shot inference.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR42600.2020.00525,https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.pdf
a075602bb73acd5af73c85070fa99b5eae06c6df,1,0,Adversarial Cross-Spectral Face Completion for NIR-VIS Face Recognition,"Near infrared-visible (NIR-VIS) heterogeneous face recognition refers to the process of matching NIR to VIS face images. Current heterogeneous methods try to extend VIS face recognition methods to the NIR spectrum by synthesizing VIS images from NIR images. However, due to the self-occlusion and sensing gap, NIR face images lose some visible lighting contents so that they are always incomplete compared to VIS face images. This paper models high-resolution heterogeneous face synthesis as a complementary combination of two components: a texture inpainting component and a pose correction component. The inpainting component synthesizes and inpaints VIS image textures from NIR image textures. The correction component maps any pose in NIR images to a frontal pose in VIS images, resulting in paired NIR and VIS textures. A warping procedure is developed to integrate the two components into an end-to-end deep network. A fine-grained discriminator and a wavelet-based discriminator are designed to improve visual quality. A novel 3D-based pose correction loss, two adversarial losses, and a pixel loss are imposed to ensure synthesis results. We demonstrate that by attaching the correction component, we can simplify heterogeneous face synthesis from one-to-many unpaired image translation to one-to-one paired image translation, and minimize the spectral and pose discrepancy during heterogeneous recognition. Extensive experimental results show that our network not only generates high-resolution VIS face images but also facilitates the accuracy improvement of heterogeneous face recognition.",2020,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2019.2961900,
a1027f7c27273c8c0aac30bbd31da87a0fa342db,1,1,Discriminability Distillation in Group Representation Learning,"Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin.",2020,ECCV,2008.1085,10.1007/978-3-030-58607-2_1,https://arxiv.org/pdf/2008.10850.pdf
a16a493813792f15f72fcd743af386cdc47b428a,0,1,Constructions of High-Performance Face Recognition Pipeline and Embedded Deep Learning Framework,,2018,,,,http://summit.sfu.ca/system/files/iritems1/19088/etd10772.pdf
a17452c8096f1ca9f48d69e655ec406af42221ac,0,1,Cancelable multi-biometric recognition system based on deep learning,"In this paper, we propose a cancelable multi-biometric face recognition method that uses multiple convolutional neural networks (CNNs) to extract deep features from different facial regions. We also propose a new CNN architecture that exploits batch normalization, depth concatenation and a residual learning framework. The proposed method adopts a region-based technique in which face, eyes, nose and mouth regions are detected from the original face images. Multiple CNNs are used to extract deep features from each region, and then, a fusion network combines these features. Moreover, to provide user’s privacy and increase the system resistance against spoof attacks, a cancelable biometric technique using bio-convolving encryption is performed on the final facial descriptor. Our experiments on the FERET, LFW and PaSC datasets show excellent and competitive results compared to state-of-the-art methods in terms of recognition accuracy, specificity, precision, recall and f score .",2019,The Visual Computer,,10.1007/s00371-019-01715-5,
a1c6e6774e3dfcd8c1cffd5493a359ad373e6ffa,0,1,Cross attentive pooling for speaker verification,"The goal of this paper is text-independent speaker verification where utterances come from 'in the wild' videos and may contain irrelevant signal. While speaker verification is naturally a pair-wise problem, existing methods to produce the speaker embeddings are instance-wise. In this paper, we propose Cross Attentive Pooling (CAP) that utilizes the context information across the reference-query pair to generate utterance-level embeddings that contain the most discriminative information for the pair-wise matching problem. Experiments are performed on the VoxCeleb dataset in which our method outperforms comparable pooling strategies.",2020,ArXiv,2008.05983,,https://arxiv.org/pdf/2008.05983.pdf
a1d206ec41d0a3667643a9e3d358e6a5a75040a0,1,1,Biometric Recognition Using Deep Learning: A Survey,"Deep learning-based models have been very successful in achieving state-of-the-art results in many of the computer vision, speech recognition, and natural language processing tasks in the last few years. These models seem a natural fit for handling the ever-increasing scale of biometric recognition problems, from cellphone authentication to airport security systems. Deep learning-based models have increasingly been leveraged to improve the accuracy of different biometric recognition systems in recent years. In this work, we provide a comprehensive survey of more than 120 promising works on biometric recognition (including face, fingerprint, iris, palmprint, ear, voice, signature, and gait recognition), which deploy deep learning models, and show their strengths and potentials in different applications. For each biometric, we first introduce the available datasets that are widely used in the literature and their characteristics. We will then talk about several promising deep learning works developed for that biometric, and show their performance on popular public benchmarks. We will also discuss some of the main challenges while using these models for biometric recognition, and possible future directions to which research in this area is headed.",2019,ArXiv,1912.00271,,https://arxiv.org/pdf/1912.00271.pdf
a2014a3a0658d85edbd78686401d7abfa00b2a08,0,1,Improved Active Speaker Detection based on Optical Flow,"Active speaker detection refers to the task of inferring which (if any) of the visible people in a video is/are speaking. Existing methods based on audiovisual fusion are often confused by factors such as non-speaking facial motions, varied illumination, and low-resolution recording. To address these problems, we propose a robust active speaker detection model by incorporating the dense optical flow to strengthen the visual representation of the facial motion. These audio and visual features are processed by a two-stream embedding network, and the embeddings are fed into a prediction network for the binary speaking/non-speaking classification. To improve the learning efficiency of the entire network, we design a multi-task learning strategy to train the network. The proposed method is evaluated on the most challenging audiovisual speaker detection benchmark, the AVA-ActiveSpeaker dataset. The results demonstrate that optical flow can improve the performance of neural networks when combined with raw pixels and audio signal. It is also shown that our method consistently outperforms the state-of-the-art method [22] in terms of both the area under the receiver operating characteristic curve (+4.4%) and the balanced accuracy (+5.28%).",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00483,https://openaccess.thecvf.com/content_CVPRW_2020/papers/w56/Huang_Improved_Active_Speaker_Detection_Based_on_Optical_Flow_CVPRW_2020_paper.pdf
a22441947b93b99b761a336d47278fa9a532b808,1,1,Neighborhood-Aware Attention Network for Semi-supervised Face Recognition,"Although face recognition has achieved fairly remarkable results in recent years, it heavily relies on large- scale labeled data to train the high-capacity deep convolutional neural networks. It is unrealistic to collect larger labeled datasets to further boost the performance, which requires burdensome and expensive annotation efforts. Meanwhile, there exist numerous unlabeled face images. It is challenging but promising to jointly utilize limited labeled and abundant unlabeled data to obtain higher performance gain, which is the target of semi-supervised learning. In this paper, we propose a bottom- up method, Neighborhood-Aware Attention Network (NAAN), for semi-supervised face recognition. It clusters unlabeled face images by collaboratively predicting pairwise relations based on their neighborhood information, where the neighborhood is defined as a k-hop ego network centered in the given sample called ""ego"". Considering the different importance of neighbors, we employ the graph attention network to learn the ego's representation. We evaluate our model on two face recognition datasets MegaFace and IJB-A, and it yields favorably comparable performance to the fully-supervised results.",2020,2020 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN48605.2020.9207042,http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20121.pdf
a249f4e25a26069687493511c4e6dc2b429f7253,1,0,Content-based Image Understanding with Applications to Affective Computing and Person Recognition in Natural Settings,"Chen Ming PhD, Purdue University, December 2016. Content-based Image Understanding with Applications to A↵ective Computing and Person Recognition in Natural Settings. Major Professor: Jan P. Allebach. Understanding the visual content of images is one of the most important topics in computer vision. Many researchers have tried to teach the machine to see and perceive like human. In this dissertation, we develop several new approaches for image understanding with applications to a↵ective computing, and person detection and recognition. Our proposed method applied to fashion photo analysis can understand the aesthetic quality of photos. Further, a bilinear model that takes into account the relative confidence of region proposals and the mutual relationship between multiple labels is developed to boost multi-label classification. It is evaluated both on object recognition and aesthetic attributes learning. We also develop a person detection and recognition system in natural settings that can robustly handle various pose, viewpoints, and lighting conditions. The system is then put into several real scenarios that has di↵erent amount of labelled data. Our algorithm that utilizes unlabelled data reduces the e↵ort needed for data annotation while achieving similar results as with labelled data.",2016,,,,
a286e00f1927979e457eeeda4eabaef061a2a81b,0,1,Deep Position-Sensitive Tracking,"Classification-based tracking strategies often face more challenges from intra-class discrimination than from inter-class separability. Even for deep convolutional neural networks that have been widely proven to be effective in various vision tasks, their intra-class discriminative capability is still limited by the weakness of softmax loss, especially for targets not seen in the training dataset. By taking intrinsic attributes of training samples into account, in this paper, we propose a position-sensitive loss coupled with softmax loss to achieve intra-class compactness and inter-class explicitness. Particularly, two additive margins are introduced to encode the position attribute for decision boundary maximization, which is also utilized with the proposed loss to supervise the fine-tuned features on the pre-trained model. With the nearest neighbor ranking measurement in the feature embedding domain, the whole scheme is able to reach an optimized balance between the feature-level inter-class semantic separability and instance-level intra-class relative distance ranking. We evaluate the proposed work on different popular benchmarks, and experimental results demonstrate that our tracking strategy performs favorably against most of the state-of-the-art trackers in the comparison of accuracy and robustness.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2019.2922125,
a288ea08fed1e00b223a561951d24bce1352fdba,1,0,Integrated Face Analytics Networks through Cross-Dataset Hybrid Training,"Face analytics benefits many multimedia applications. It consists of a number of tasks, such as facial emotion recognition and face parsing, and most existing approaches generally treat these tasks independently, which limits their deployment in real scenarios. In this paper we propose an integrated Face Analytics Network (iFAN), which is able to perform multiple tasks jointly for face analytics with a novel carefully designed network architecture to fully facilitate the informative interaction among different tasks. The proposed integrated network explicitly models the interactions between tasks so that the correlations between tasks can be fully exploited for performance boost. In addition, to solve the bottleneck of the absence of datasets with comprehensive training data for various tasks, we propose a novel cross-dataset hybrid training strategy. It allows ""plug-in and play'' of multiple datasets annotated for different tasks without the requirement of a fully labeled common dataset for all the tasks. We experimentally show that the proposed iFAN achieves state-of-the-art performance on multiple face analytics tasks using a single integrated model. Specifically, iFAN achieves an overall F-score of 91.15% on the Helen dataset for face parsing, a normalized mean error of 5.81% on the MTFL dataset for facial landmark localization and an accuracy of 45.73% on the BNU dataset for emotion recognition with a single model.",2017,ACM Multimedia,1711.06055,10.1145/3123266.3123438,https://arxiv.org/pdf/1711.06055.pdf
a28e5985afd80abaa95b2ba1fe08667a6cfb1227,0,1,SAR-Net: A End-to-End Deep Speech Accent Recognition Network,"This paper proposes a end-to-end deep network to recognize kinds of accents under the same language, where we develop and transfer the deep architecture in speaker-recognition area to accent classification task for learning utterance-level accent representation. Compared with the individual-level feature in speaker-recognition, accent recognition throws a more challenging issue in acquiring compact group-level features for the speakers with the same accent, hence a good discriminative accent feature space is desired. Our deep framework adopts multitask-learning mechanism and mainly consists of three modules: a shared CNNs and RNNs based front-end encoder, a core accent recognition branch, and an auxiliary speech recognition branch, where we take speech spectrogram as input. More specifically, with the sequential descriptors learned from a shared encoder, the accent recognition branch first condenses all descriptors into an embedding vector, and then explores different discriminative loss functions which are popular in face recognition domain to enhance embedding discrimination. Additionally, due to the accent is a speaking-related timbre, adding speech recognition branch effectively curbs the over-fitting phenomenon in accent recognition during training. We show that our network without any data-augment preproccessings is significantly ahead of the baseline system on the accent classification track in the Accented English Speech Recognition Challenge 2020 (AESRC2020), where the state-of-the-art loss function Circle-Loss achieves the best discriminative optimization for accent representation.",2020,ArXiv,2011.12461,,https://arxiv.org/pdf/2011.12461.pdf
a2f7e16738dac272e6a4ff5f213395ef12c3a989,0,1,Partial FC: Training 10 Million Identities on a Single Machine,"Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available this https URL.",2020,ArXiv,2010.05222,,https://arxiv.org/pdf/2010.05222.pdf
a2fe090231fa8e2de361f06b0ed20435f178a2e3,1,0,InsightGAN: Semi-Supervised Feature Learning with Generative Adversarial Network for Drug Abuse Detection,"We present a novel generative adversarial network (GAN) model, called InsightGAN, for drug abuse detection. Our model is inspired by two closely related works on machine learning for healthcare applications: (1) drug abuse detection has been solved by machine learning with plentiful data from social media (where face pictures can be easily obtained); (2) facial characteristics have been explored in mental disorder diagnosis (drug addiction is also a mental disorder). In this paper, we adopt deep learning to extract discriminative facial features for drug abuse detection. However, in this application, the face pictures with ground-truth labels are far from sufficient for training a deep learning model. To alleviate the scarcity of labelled data, we thus propose a semi-supervised facial feature learning model based on GAN. Moreover, we also develop a robust algorithm for training our InsightGAN. Experimental results show the promising performance of our InsightGAN.",2018,ICONIP,,10.1007/978-3-030-04182-3_36,
a327c57ef2f32ed8e55580a7d1b6b52c310c2e04,0,1,Max Margin Cosine Loss for Speaker Identification on Short Utterances,"Speaker identification has made extraordinary progress owing to the advancement of deep neural networks. Speaker feature discrimination is a vital term in speaker recognition. However, the traditional softmax loss usually lacks the power of discrimination. To address this problem, this paper explores a novel loss function, namely max margin cosine loss (MMCL). To be specific, we realize the function by L2 normalizing both features and weight vectors in the softmax loss, together with a cosine margin term to maximize the decision margin in the angular space. In addition, max margin constraint, as one regularization term, is incorporated into the proposed loss function. Experimental results demonstrate the effectiveness of our proposed max margin cosine loss and superiority over pervious losses. For example, on 2s condition, MMCL reduces the equal error rate by 10.63% relatively compared to additive angular margin cosine loss (AMCL), while AMCL has already obtained 6.37% relative reduction than softmax loss.1",2018,2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP),,10.1109/ISCSLP.2018.8706654,
a32811ad17ff692e7f5826057dccd2c767b5b458,0,1,Full Face-and-Head 3D Model With Photorealistic Texture,"In the recent period, significant progress has been achieved towards reconstructing the 3D face model from face image. With the support of the render engines and sufficient data, the reconstruction results are fine in detail. Nevertheless, the research on the 3D face reconstruction with texture from a single unrestricted face image is imperfect. The rebuild process lacks essential structure and texture information in the profile and the craniofacial region. To address this problem, we present a method of creating a 3D full face-and-head model with photorealistic texture from a single “in-the-wild” face image in this paper. To this end, we introduce a pipeline to integrate the highly-detailed face model into the basic model. Specifically, the basic model was built by multilinear optimization, and the highly-detailed face model which represents the facial features generated by constrained illumination distribution. Additionally, to infer the invisible region texture information corresponding to the input face image, we design an effective architecture with the generative adversarial network (GAN) for panoramic UV texture generation. The final results after UV texture mapping were visualized in the experiment, which demonstrates that the model faithfully recovers the photorealistic details in arbitrary perspective. Furthermore, compared to the state-of-the-art facial modeling techniques and existing commercial solutions, our method takes less input and performs better in surface detail.",2020,IEEE Access,,10.1109/ACCESS.2020.3031886,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09241395.pdf
a32a143147d449bf771902075f96d5e08eefabc4,1,1,Deep Face Recognition Model Compression via Knowledge Transfer and Distillation,"Fully convolutional networks (FCNs) have become de facto tool to achieve very high-level performance for many vision and non-vision tasks in general and face recognition in particular. Such high-level accuracies are normally obtained by very deep networks or their ensemble. However, deploying such high performing models to resource constraint devices or real-time applications is challenging. In this paper, we present a novel model compression approach based on student-teacher paradigm for face recognition applications. The proposed approach consists of training teacher FCN at bigger image resolution while student FCNs are trained at lower image resolutions than that of teacher FCN. We explored three different approaches to train student FCNs: knowledge transfer (KT), knowledge distillation (KD) and their combination. Experimental evaluation on LFW and IJB-C datasets demonstrate comparable improvements in accuracies with these approaches. Training low-resolution student FCNs from higher resolution teacher offer fourfold advantage of accelerated training, accelerated inference, reduced memory requirements and improved accuracies. We evaluated all models on IJB-C dataset and achieved state-of-the-art results on this benchmark. The teacher network and some student networks even achieved Top-1 performance on IJB-C dataset. The proposed approach is simple and hardware friendly, thus enables the deployment of high performing face recognition deep models to resource constraint devices.",2019,ArXiv,1906.00619,,https://arxiv.org/pdf/1906.00619.pdf
a35483c9becc95faa16bf70a8c6355566a205091,1,0,FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis,"Face synthesis has achieved advanced development by using generative adversarial networks (GANs). Existing methods typically formulate GAN as a two-player game, where a discriminator distinguishes face images from the real and synthesized domains, while a generator reduces its discriminativeness by synthesizing a face of photorealistic quality. Their competition converges when the discriminator is unable to differentiate these two domains. Unlike two-player GANs, this work generates identity-preserving faces by proposing FaceID-GAN, which treats a classifier of face identity as the third player, competing with the generator by distinguishing the identities of the real and synthesized faces (see Fig.1). A stationary point is reached when the generator produces faces that have high quality as well as preserve identity. Instead of simply modeling the identity classifier as an additional discriminator, FaceID-GAN is formulated by satisfying information symmetry, which ensures that the real and synthesized images are projected into the same feature space. In other words, the identity classifier is used to extract identity features from both input (real) and output (synthesized) face images of the generator, substantially alleviating training difficulty of GAN. Extensive experiments show that FaceID-GAN is able to generate faces of arbitrary viewpoint while preserve identity, outperforming recent advanced approaches.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00092,http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2021.pdf
a370e9d9ddd76a368815459d361beda58eb65e30,1,1,SeesawFaceNets: sparse and robust face verification model for mobile platform,"Deep Convolutional Neural Network (DCNNs) come to be the most widely used solution for most computer vision related tasks, and one of the most important application scenes is face verification. Due to its high-accuracy performance, deep face verification models of which the inference stage occurs on cloud platform through internet plays the key role on most prectical scenes. However, two critical issues exist: First, individual privacy may not be well protected since they have to upload their personal photo and other private information to the online cloud backend. Secondly, either training or inference stage is time-comsuming and the latency may affect customer experience, especially when the internet link speed is not so stable or in remote areas where mobile reception is not so good, but also in cities where building and other construction may block mobile signals. Therefore, designing lightweight networks with low memory requirement and computational cost is one of the most practical solutions for face verification on mobile platform. In this paper, a novel mobile network named SeesawFaceNets, a simple but effective model, is proposed for productively deploying face recognition for mobile devices. Dense experimental results have shown that our proposed model SeesawFaceNets outperforms the baseline MobilefaceNets, with only {\bf66\%}(146M VS 221M MAdds) computational cost, smaller batch size and less training steps, and SeesawFaceNets achieve comparable performance with other SOTA model e.g. mobiface with only {\bf54.2\%}(1.3M VS 2.4M) parameters and {\bf31.6\%}(146M VS 462M MAdds) computational cost, It is also eventually competitive against large-scale deep-networks face recognition on all 5 listed public validation datasets, with {\bf6.5\%}(4.2M VS 65M) parameters and {\bf4.35\%}(526M VS 12G MAdds) computational cost.",2019,ArXiv,1908.09124,,https://arxiv.org/pdf/1908.09124.pdf
a38f2f320b8a359521c31a3c00ecc111234a746e,1,0,Towards Learning Structure via Consensus for Face Segmentation and Parsing,"Face segmentation is the task of densely labeling pixels on the face according to their semantics. While current methods place an emphasis on developing sophisticated architectures, use conditional random fields for smoothness, or rather employ adversarial training, we follow an alternative path towards robust face segmentation and parsing. Occlusions, along with other parts of the face, have a proper structure that needs to be propagated in the model during training. Unlike state-of-the-art methods that treat face segmentation as an independent pixel prediction problem, we argue instead that it should hold highly correlated outputs within the same object pixels. We thereby offer a novel learning mechanism to enforce structure in the prediction via consensus, guided by a robust loss function that forces pixel objects to be consistent with each other. Our face parser is trained by transferring knowledge from another model, yet it encourages spatial consistency while fitting the labels. Different than current practice, our method enjoys pixel-wise predictions, yet paves the way for fewer artifacts, less sparse masks, and spatially coherent outputs.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1911.00957,10.1109/cvpr42600.2020.00555,https://arxiv.org/pdf/1911.00957.pdf
a3a5ba95591fe73e1dbcdaa51a4eba90280e14c9,1,1,Geometry Guided Feature Aggregation in Video Face Recognition,"Video-based face recognition has attracted a significant amount of research interest in both academia and industry due to its wide applications such as surveillance and security. Different from image-based face recognition, abundant information, extracted from a series of frames in a video, would contribute a lot to successful recognition. In other words, the key to improving video face recognition capability is aggregating and integrating profuse information within a video. Existing methods of feature aggregation across frames narrowly focus on the importance of a single frame, while ignoring the geometric relationship among frames in feature space. In this work, we present a geometry-based feature aggregation method rather than a better recognition model. It considers not only the importance of each frame but also the geometric relationship among frames in feature space, which yields more distinguishing video-level representation. Extensive evaluations on IJB-A and YTF datasets indicate that the proposed aggregation method considerably outperforms other feature aggregation methods.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00326,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Peng_Geometry_Guided_Feature_Aggregation_in_Video_Face_Recognition_ICCVW_2019_paper.pdf
a3c8b858bf8922477685e2c6ca52ec268d0d934b,0,1,Robust Image Retrieval-based Visual Localization using Kapture,"In this paper, we present a versatile method for visual localization. It is based on robust image retrieval for coarse camera pose estimation and robust local features for accurate pose refinement. Our method is top ranked on various public datasets showing its ability of generalization and its great variety of applications. To facilitate experiments, we introduce kapture, a flexible data format and processing pipeline for structure from motion and visual localization that is released open source. We furthermore provide all datasets used in this paper in the kapture format to facilitate research and data processing. Code and datasets can be found at this https URL, more information, updates, and news can be found at this https URL.",2020,ArXiv,2007.13867,,https://arxiv.org/pdf/2007.13867.pdf
a3db0796e08a3752a1e3aea444cc886d5f6b9fa4,0,1,Deep Speaker Embeddings for Far-Field Speaker Recognition on Short Utterances,"Speaker recognition systems based on deep speaker embeddings have achieved significant performance in controlled conditions according to the results obtained for early NIST SRE (Speaker Recognition Evaluation) datasets. From the practical point of view, taking into account the increased interest in virtual assistants (such as Amazon Alexa, Google Home, AppleSiri, etc.), speaker verification on short utterances in uncontrolled noisy environment conditions is one of the most challenging and highly demanded tasks. This paper presents approaches aimed to achieve two goals: a) improve the quality of far-field speaker verification systems in the presence of environmental noise, reverberation and b) reduce the system qualitydegradation for short utterances. For these purposes, we considered deep neural network architectures based on TDNN (TimeDelay Neural Network) and ResNet (Residual Neural Network) blocks. We experimented with state-of-the-art embedding extractors and their training procedures. Obtained results confirm that ResNet architectures outperform the standard x-vector approach in terms of speaker verification quality for both long-duration and short-duration utterances. We also investigate the impact of speech activity detector, different scoring models, adaptation and score normalization techniques. The experimental results are presented for publicly available data and verification protocols for the VoxCeleb1, VoxCeleb2, and VOiCES datasets.",2020,ArXiv,2002.06033,10.21437/odyssey.2020-26,https://arxiv.org/pdf/2002.06033.pdf
a4bea43910441bfdb2e3f4223c6f30269128fccf,0,1,Deep Relationship Analysis in Video with Multimodal Feature Fusion,"In this paper, we propose a novel multimodal feature fusion method based on scene segmentation to detect the relationships between entities in a long duration video. Specifically, a long video is split into some scenes and entities in the scenes are tracked. Text, audio and visual features in a scene are extracted to predict relationships between different entities in the scene. The relationships between entities construct a knowledge graph of the video and can be used to answer some queries about the video. The experimental results show that our method performs well for deep video understanding on the HLVU dataset.",2020,ACM Multimedia,,10.1145/3394171.3416303,
a4c9808de4626df6d94053df5497fafecc44c7a2,1,0,Collaborations on YouTube,"YouTube is the most popular platform for streaming of user-generated videos. Nowadays, professional YouTubers are organized in so-called multichannel networks (MCNs). These networks offer services such as brand deals, equipment, and strategic advice in exchange for a share of the YouTubers’ revenues. A dominant strategy to gain more subscribers and, hence, revenue is collaborating with other YouTubers. Yet, collaborations on YouTube have not been studied in a detailed quantitative manner. To close this gap, first, we collect a YouTube dataset covering video statistics over 3 months for 7,942 channels. Second, we design a framework for collaboration detection given a previously unknown number of persons featured in YouTube videos. We denote this framework, for the detection and analysis of collaborations in YouTube videos using a Deep Neural Network (DNN)-based approach, as CATANA. Third, we analyze about 2.4 years of video content and use CATANA to answer research questions guiding YouTubers and MCNs for efficient collaboration strategies. Thereby, we focus on (1) collaboration frequency and partner selectivity, (2) the influence of MCNs on channel collaborations, (3) collaborating channel types, and (4) the impact of collaborations on video and channel popularity. Our results show that collaborations are in many cases significantly beneficial regarding viewers and newly attracted subscribers for both collaborating channels, often showing more than 100% popularity growth compared with noncollaboration videos.",2018,ACM Trans. Multim. Comput. Commun. Appl.,1805.01887,10.1145/3241054,https://arxiv.org/pdf/1805.01887.pdf
a4e709bbf0797963461fd5b2b1695f6313f65e3f,0,1,Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing,"Face anti-spoofing aims to detect presentation attack to face recognition--based authentication systems. It has drawn growing attention due to the high security demand. The widely adopted CNN-based methods usually well recognize the spoofing faces when training and testing spoofing samples display similar patterns, but their performance would drop drastically on testing spoofing faces of novel patterns or unseen scenes, leading to poor generalization performance. Furthermore, almost all current methods treat face anti-spoofing as a prior step to face recognition, which prolongs the response time and makes face authentication inefficient. In this article, we try to boost the generalizability and applicability of face anti-spoofing methods by designing a new generalizable face authentication CNN (GFA-CNN) model with three novelties. First, GFA-CNN introduces a simple yet effective total pairwise confusion loss for CNN training that properly balances contributions of all spoofing patterns for recognizing the spoofing faces. Second, it incorporate a fast domain adaptation component to alleviate negative effects brought by domain variation. Third, it deploys filter diversification learning to make the learned representations more adaptable to new scenes. In addition, the proposed GFA-CNN works in a multi-task manner—it performs face anti-spoofing and face recognition simultaneously. Experimental results on five popular face anti-spoofing and face recognition benchmarks show that GFA-CNN outperforms previous face anti-spoofing methods on cross-test protocols significantly and also well preserves the identity information of input face images.",2020,ACM Trans. Intell. Syst. Technol.,1901.05602,10.1145/3402446,https://arxiv.org/pdf/1901.05602.pdf
a4fb2f9f43a08f434d79847e67c70c97630afc2c,1,0,Scale-Varying Triplet Ranking with Classification Loss for Facial Age Estimation,"In recent years, considerable efforts based on convolutional neural networks have been devoted to age estimation from face images. Among them, classification-based approaches have shown promising results, but there has been little investigation of age differences and ordinal age information. In this paper, we propose a ranking objective with two novel schemes jointly performed with an age classification objective to take ordinal age labels into account. We first introduce relative triplet sampling in which a set of triplets is constructed considering the relative differences in ages. This also addresses the problem of having limited triplet candidates, that occurs in conventional triplet sampling. We then propose the scale-varying ranking constraint, which decides the importance of a relative triplet and adjusts a scale of gradients accordingly. Our adaptive ranking loss with relative sampling not only lowers the generalization error but ultimately has a meaningful performance improvement over the state-of-the-art methods on two well-known benchmarks.",2018,ACCV,,10.1007/978-3-030-20873-8_16,https://sgvr.kaist.ac.kr/~wbim/paper/accv2018-age-estimation/0639.pdf
a50fa5048c61209149de0711b5f1b1806b43da00,1,0,Deep Features for Recognizing Disguised Faces in the Wild,"Unconstrained face verification is a challenging problem owing to variations in pose, illumination, resolution of image, age, etc. This problem becomes even more complex when the subjects are actively trying to deceive face verification systems by wearing a disguise. The problem under consideration here is to identify a subject under disguises and reject impostors trying to look like the subject of interest. In this paper we present a DCNN-based approach for recognizing people under disguises and picking out impostors. We train two different networks on a large dataset comprising of still images and video frames with L2-softmax loss. We fuse features obtained from the two networks and show that the resulting features are effective for discriminating between disguised faces and impostors in the wild. We present results on the recently introduced Disguised Faces in the Wild challenge dataset.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00009,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w1/Bansal_Deep_Features_for_CVPR_2018_paper.pdf
a54c51439f2907a09f471230513320cbcac538dd,0,1,A deep facial recognition system using computational intelligent algorithms,"The development of biometric applications, such as facial recognition (FR), has recently become important in smart cities. Many scientists and engineers around the world have focused on establishing increasingly robust and accurate algorithms and methods for these types of systems and their applications in everyday life. FR is developing technology with multiple real-time applications. The goal of this paper is to develop a complete FR system using transfer learning in fog computing and cloud computing. The developed system uses deep convolutional neural networks (DCNN) because of the dominant representation; there are some conditions including occlusions, expressions, illuminations, and pose, which can affect the deep FR performance. DCNN is used to extract relevant facial features. These features allow us to compare faces between them in an efficient way. The system can be trained to recognize a set of people and to learn via an online method, by integrating the new people it processes and improving its predictions on the ones it already has. The proposed recognition method was tested with different three standard machine learning algorithms (Decision Tree (DT), K Nearest Neighbor(KNN), Support Vector Machine (SVM)). The proposed system has been evaluated using three datasets of face images (SDUMLA-HMT, 113, and CASIA) via performance metrics of accuracy, precision, sensitivity, specificity, and time. The experimental results show that the proposed method achieves superiority over other algorithms according to all parameters. The suggested algorithm results in higher accuracy (99.06%), higher precision (99.12%), higher recall (99.07%), and higher specificity (99.10%) than the comparison algorithms.",2020,PloS one,,10.1371/journal.pone.0242269,https://pdfs.semanticscholar.org/2b1a/ed55be0167e9bacbdb122035ffc592e382eb.pdf
a56065201159c33ab5bda585fd2286764eab15a5,1,0,Led3D: A Lightweight and Efficient Deep Approach to Recognizing Low-Quality 3D Faces,"Due to the intrinsic invariance to pose and illumination changes, 3D Face Recognition (FR) has a promising potential in the real world. 3D FR using high-quality faces, which are of high resolutions and with smooth surfaces, have been widely studied. However, research on that with low-quality input is limited, although it involves more applications. In this paper, we focus on 3D FR using low-quality data, targeting an efficient and accurate deep learning solution. To achieve this, we work on two aspects: (1) designing a lightweight yet powerful CNN; (2) generating finer and bigger training data. For (1), we propose a Multi-Scale Feature Fusion (MSFF) module and a Spatial Attention Vectorization (SAV) module to build a compact and discriminative CNN. For (2), we propose a data processing system including point-cloud recovery, surface refinement, and data augmentation (with newly proposed shape jittering and shape scaling). We conduct extensive experiments on Lock3DFace and achieve state-of-the-art results, outperforming many heavy CNNs such as VGG-16 and ResNet-34. In addition, our model can operate at a very high speed (136 fps) on Jetson TX2, and the promising accuracy and efficiency reached show its great applicability on edge/mobile devices.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00592,
a5749f1685de56fc279c46c24bb71c208220e25a,1,1,Relational Deep Feature Learning for Heterogeneous Face Recognition,"Heterogeneous Face Recognition (HFR) is a task that matches faces across two different domains such as visible light (VIS), near-infrared (NIR), or the sketch domain. Due to the lack of databases, HFR methods usually exploit the pre-trained features on a large-scale visual database that contain general facial information. However, these pre-trained features cause performance degradation due to the texture discrepancy with the visual domain. With this motivation, we propose a graph-structured module called Relational Graph Module (RGM) that extracts global relational information in addition to general facial features. Because each identity’s relational information between intra-facial parts is similar in any modality, the modeling relationship between features can help cross-domain matching. Through the RGM, relation propagation diminishes texture dependency without losing its advantages from the pre-trained features. Furthermore, the RGM captures global facial geometrics from locally correlated convolutional features to identify long-range relationships. In addition, we propose a Node Attention Unit (NAU) that performs node-wise recalibration to concentrate on the more informative nodes arising from relation-based propagation. Furthermore, we suggest a novel conditional-margin loss function ( $C$ -softmax) for the efficient projection learning of the embedding vector in HFR. The proposed method outperforms other state-of-the-art methods on five HFR databases. Furthermore, we demonstrate performance improvement on three backbones because our module can be plugged into any pre-trained face recognition backbone to overcome the limitations of a small HFR database.",2021,IEEE Transactions on Information Forensics and Security,2003.00697,10.1109/TIFS.2020.3013186,https://arxiv.org/pdf/2003.00697.pdf
a57db31a302385a35ebfdf9e6eb9ad3c3d209669,0,1,Real-time face attributes recognition via HPGC: horizontal pyramid global convolution,"Recognizing face attributes in the wild is a challenging problem. With the development of embedded devices, smart phone and deep learning, face attributes recognition based on deep learning has raised increasing attention. Accuracy and efficiency are the two key-points in any application which uses these face attributes as an aid system. In most of the previous papers, multi-independency classifiers are proposed; and most of them just focus on accurate rate while neglecting efficiency. This paper proposes a horizontal pyramid global convolution (HPGC) module as feature mapping operator to extract more local information; designs a light-weight attribute convolution neural network (LACNN) combining with HPGC; and utilizes sigmoid cross entropy loss function for improving the accuracy and efficiency of the face attributes recognition model. Replacing full connection or global average pooling with the proposed HPGC module, we balance the accuracy performance and computation cost. As a result, we not only get high accuracy but also reduce the computational cost. Extensive experiments results on two widely used face attribute datasets, LFW and CelebA, demonstrate that our LACNN-HPGC framework achieves significantly improved efficiency compared with state-of-the-art lightweight models for face attributes recognition.",2019,Journal of Real-Time Image Processing,,10.1007/s11554-019-00932-4,
a5c10cd70e515eb763d6c92e764bf89ba0aad984,1,0,Low-Resolution Face Recognition,"Whilst recent face-recognition (FR) techniques have made significant progress on recognising constrained high-resolution web images, the same cannot be said on natively unconstrained low-resolution images at large scales. In this work, we examine systematically this under-studied FR problem, and introduce a novel Complement Super-Resolution and Identity (CSRI) joint deep learning method with a unified end-to-end network architecture. We further construct a new large-scale dataset TinyFace of native unconstrained low-resolution face images from selected public datasets, because none benchmark of this nature exists in the literature. With extensive experiments we show there is a significant gap between the reported FR performances on popular benchmarks and the results on TinyFace, and the advantages of the proposed CSRI over a variety of state-of-the-art FR and super-resolution deep models on solving this largely ignored FR scenario. The TinyFace dataset is released publicly at: this https URL.",2018,ACCV,1811.08965,10.1007/978-3-030-20893-6_38,https://arxiv.org/pdf/1811.08965.pdf
a5f80b45ba1339f68798d7d1591d690ee249a60e,1,0,On Measuring the Iconicity of a Face,"For a given identity in a face dataset, there are certain iconic images which are more representative of the subject than others. In this paper, we explore the problem of computing the iconicity of a face. The premise of the proposed approach is as follows: For an identity containing a mixture of iconic and non iconic images, if a given face cannot be successfully matched with any other face of the same identity, then the iconicity of the face image is low. Using this information, we train a Siamese Multi-Layer Perceptron network, such that each of its twins predict iconicity scores of the image feature pair, fed in as input. We observe the variation of the obtained scores with respect to covariates such as blur, yaw, pitch, roll and occlusion to demonstrate that they effectively predict the quality of the image and compare it with other existing metrics. Furthermore, we use these scores to weight features for template-based face verification and compare it with media averaging of features.",2019,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),1903.01581,10.1109/WACV.2019.00231,https://arxiv.org/pdf/1903.01581.pdf
a5fcc57b60b154ce28044ffb6124a4021b4116be,1,0,Two-branch Recurrent Network for Isolating Deepfakes in Videos,,2020,,2008.03412,,https://arxiv.org/pdf/2008.03412.pdf
a5fd689c0890fa15fcdc3d97e805d38d342ed524,1,0,Towards Transferable Adversarial Attack against Deep Face Recognition,"Face recognition has achieved great success in the last five years due to the development of deep learning methods. However, deep convolutional neural networks (DCNNs) have been found to be vulnerable to adversarial examples. In particular, the existence of transferable adversarial examples could severely hinder the robustness of DCNNs since this type of attacks could be applied in a fully black-box manner without queries on the target system. In this work, we first investigate the characteristics of transferable adversarial attacks in face recognition by showing the superiority of feature-level methods over label-level methods. Then, to further improve transferability of feature-level adversarial examples, we propose DFANet, a dropout-based method used in convolutional layers, which could increase the diversity of surrogate models and obtain ensemble-like effects. Extensive experiments on state-of-the-art face models with various training databases, loss functions and network architectures show that the proposed method can significantly enhance the transferability of existing attack methods. Finally, by applying DFANet to the LFW database, we generate a new set of adversarial face pairs that can successfully attack four commercial APIs without any queries. This TALFW database is available to facilitate research on the robustness and defense of deep face recognition.",2020,ArXiv,2004.0579,,https://arxiv.org/pdf/2004.05790.pdf
a5fea75ba9003a8da2d0dde2d14ed7bae78e3a12,0,1,The DKU-DukeECE Systems for VoxCeleb Speaker Recognition Challenge 2020.,"In this paper, we present the system submission for the VoxCeleb Speaker Recognition Challenge 2020 (VoxSRC-20) by the DKU-DukeECE team. For track 1, we explore various kinds of state-of-the-art front-end extractors with different pooling layers and objective loss functions. For track 3, we employ an iterative framework for self-supervised speaker representation learning based on a deep neural network (DNN). For track 4, we investigate the whole system pipeline for speaker diarization, including voice activity detection (VAD), uniform segmentation, speaker embedding extraction, and clustering.",2020,,2010.12731,,https://arxiv.org/pdf/2010.12731.pdf
a63bf09cd7dc8520c0444792242a1757d31551dd,0,1,"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification","Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.",2020,INTERSPEECH,2005.07143,10.21437/Interspeech.2020-2650,https://arxiv.org/pdf/2005.07143.pdf
a65a37078a7afe35a70c2f9a4b8c228f6ca1f198,0,1,Regularizing Neural Networks via Minimizing Hyperspherical Energy (with Appendix),"Inspired by the Thomson problem in physics where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy, hyperspherical energy minimization has demonstrated its potential in regularizing neural networks and improving their generalization power. In this paper, we first study the important role that hyperspherical energy plays in neural network training by analyzing its training dynamics. Then we show that naively minimizing hyperspherical energy suffers from some difficulties due to highly non-linear and non-convex optimization as the space dimensionality becomes higher, therefore limiting the potential to further improve the generalization. To address these problems, we propose the compressive minimum hyperspherical energy (CoMHE) as a more effective regularization for neural networks. Specifically, CoMHE utilizes projection mappings to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different designs for the projection mapping, we propose several distinct yet well-performing variants and provide some theoretical guarantees to justify their effectiveness. Our experiments show that CoMHE consistently outperforms existing regularization methods, and can be easily applied to different neural networks.",2020,,,,https://pdfs.semanticscholar.org/a65a/37078a7afe35a70c2f9a4b8c228f6ca1f198.pdf
a716d1d9f67ff6ad08761fa94dead23524679370,1,0,Multi-views Embedding for Cattle Re-identification,"People re-identification task has seen enormous improvements in the latest years, mainly due to the development of better image features extraction from deep Convolutional Neural Networks (CNN) and the availability of large datasets. However, little research has been conducted on animal identification and re-identification, even if this knowledge may be useful in a rich variety of different scenarios. Here, we tackle cattle re-identification exploiting deep CNN and show how this task is poorly related with the human one, presenting unique challenges that makes it far from being solved. We present various baselines, both based on deep architectures or on standard machine learning algorithms, and compared them with our solution. Finally, a rich ablation study has been conducted to further investigate the unique peculiarities of this task.",2018,2018 14th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),1902.04886,10.1109/SITIS.2018.00036,https://arxiv.org/pdf/1902.04886.pdf
a76b49268fc3c711b70983b0646db60c786cd8c0,0,1,Image Animation with Perturbed Masks,"We present a novel approach for image-animation of a source image by a driving video, both depicting the same type of object. We do not assume the existence of pose models and our method is able to animate arbitrary objects without knowledge of the object's structure. Furthermore, both the driving video and the course image are only seen during test-time. Our method is based on a shared mask generator, which separates the foreground object from its background, and captures the object's general pose and shape. A mask-refinement module then replaces, in the mask extracted from the driver image, the identity of the driver with the identity of the source. Conditioned on the source image, the transformed mask is then decoded by a multi-scale generator that renders a realistic image, in which the content of the source frame is animated by the pose in the driving video. Due to lack of fully supervised data, we train on the task of reconstructing frames from the same video the source image is taken from. In order to control source of the identity of the output frame, we employ during training perturbations that remove the unwanted identity information. Our method is shown to greatly outperform the state of the art methods on multiple benchmarks. Our code and samples are available at this https URL.",2020,ArXiv,2011.06922,,https://arxiv.org/pdf/2011.06922.pdf
a77f13dc813c592dd86f90543b379989bf4c04e3,0,1,A Metric Learning Reality Check,"Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best.",2020,ECCV,2003.08505,10.1007/978-3-030-58595-2_41,https://arxiv.org/pdf/2003.08505.pdf
a7bb20f6d5d07c2043712f9b17aa1150aedec362,0,1,Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder,"The problem of fair classification can be mollified if we develop a method to remove the embedded sensitive information from the classification features. This line of separating the sensitive information is developed through the causal inference, and the causal inference enables the counterfactual generations to contrast the what-if case of the opposite sensitive attribute. Along with this separation with the causality, a frequent assumption in the deep latent causal model defines a single latent variable to absorb the entire exogenous uncertainty of the causal graph. However, we claim that such structure cannot distinguish the 1) information caused by the intervention (i.e., sensitive variable) and 2) information correlated with the intervention from the data. Therefore, this paper proposes Disentangled Causal Effect Variational Autoencoder (DCEVAE) to resolve this limitation by disentangling the exogenous uncertainty into two latent variables: either 1) independent to interventions or 2) correlated to interventions without causality. Particularly, our disentangling approach preserves the latent variable correlated to interventions in generating counterfactual examples. We show that our method estimates the total effect and the counterfactual effect without a complete causal graph. By adding a fairness regularization, DCEVAE generates a counterfactual fair dataset while losing less original information. Also, DCEVAE generates natural counterfactual images by only flipping sensitive information. Additionally, we theoretically show the differences in the covariance structures of DCEVAE and prior works from the perspective of the latent disentanglement.",2020,ArXiv,2011.11878,,https://arxiv.org/pdf/2011.11878.pdf
a7cd674f36411f1b38a5b7e15f0b25e3420b873a,0,1,A Black-box Attack on Neural Networks Based on Swarm Evolutionary Algorithm,"Neural networks play an increasingly important role in the field of machine learning and are included in many applications in society. Unfortunately, neural networks suffer from adversarial samples generated to attack them. However, most of the generation approaches either assume that the attacker has full knowledge of the neural network model or are limited by the type of attacked model. In this paper, we propose a new approach that generates a black-box attack to neural networks based on the swarm evolutionary algorithm. Benefiting from the improvements in the technology and theoretical characteristics of evolutionary algorithms, our approach has the advantages of effectiveness, black-box attack, generality, and randomness. Our experimental results show that both the MNIST images and the CIFAR-10 images can be perturbed to successful generate a black-box attack with 100\% probability on average. In addition, the proposed attack, which is successful on distilled neural networks with almost 100\% probability, is resistant to defensive distillation. The experimental results also indicate that the robustness of the artificial intelligence algorithm is related to the complexity of the model and the data set. In addition, we find that the adversarial samples to some extent reproduce the characteristics of the sample data learned by the neural network model.",2020,ACISP,1901.09892,10.1007/978-3-030-55304-3_14,https://arxiv.org/pdf/1901.09892.pdf
a7e80cce0cebfed6d521befd42825035b340d342,1,0,Protecting Multimedia Privacy from Both Humans and AI,"With the development of artificial intelligence (AI), multimedia privacy issues have become more challenging than ever. AI-assisted malicious entities can steal private information from multimedia data more easily than humans. Traditional multimedia privacy protection only considers the situation when humans are the adversaries, therefore they are ineffective against AI-assisted attackers. In this paper, we develop a new framework and new algorithms that can protect image privacy from both humans and AI. We combine the idea of adversarial image perturbation which is effective against AI and the obfuscation technique for human adversaries. Experiments show that our proposed methods work well for all types of attackers.",2019,2019 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),,10.1109/BMSB47279.2019.8971914,
a7fbe71d638fd654defbf7ff869e299a783e9309,0,1,MBFN: A Multi-branch Face Network for Facial Analysis,"We present a multi-task algorithm for simultaneous face identification, gender recognition, age estimation, several facial attributes prediction and facial expression classification by using a single deep multi-branch convolutional neural network (CNN) based on MobileFaceNet. The proposed method, called MBFN (multi-branch face network), uses the backbone network for face identification and pulls branches from the backbone for other facial tasks. It exploits the synergy among the facial tasks which enhances their individual performances. Extensive experiments show that the proposed network is able to capture global and local information in faces and achieve state-of-the-art result for most of these tasks.",2019,ACAI 2019,,10.1145/3377713.3377719,
a83f52b6fac2a72ec1c7e25d658ba8917b2c2c7d,1,1,Multi-Margin based Decorrelation Learning for Heterogeneous Face Recognition,"Heterogeneous face recognition (HFR) refers to matching face images acquired from different domains with wide applications in security scenarios. This paper presents a deep neural network approach namely Multi-Margin based Decorrelation Learning (MMDL) to extract decorrelation representations in a hyperspherical space for cross-domain face images. The proposed framework can be divided into two components: heterogeneous representation network and decorrelation representation learning. First, we employ a large scale of accessible visual face images to train heterogeneous representation network. The decorrelation layer projects the output of the first component into decorrelation latent subspace and obtains decorrelation representation. In addition, we design a multi-margin loss (MML), which consists of quadruplet margin loss (QML) and heterogeneous angular margin loss (HAML), to constrain the proposed framework. Experimental results on two challenging heterogeneous face databases show that our approach achieves superior performance on both verification and recognition tasks, comparing with state-of-the-art methods.",2019,IJCAI,2005.11945,10.24963/ijcai.2019/96,https://arxiv.org/pdf/2005.11945.pdf
a89cbc90bbb4477a48aec185f2a112ea7ebe9b4d,1,0,High Performance Large Scale Face Recognition with Multi-cognition Softmax and Feature Retrieval,"In this paper, we introduce our solution to the Challenge-1 of the MS-Celeb-lM challenges which aims to recognize one million celebrities. To solve this large scale face recognition problem, a Multi-Cognition Softmax Model (MCSM) is proposed to distribute training data to several cognition units by a data shuffling strategy. Here we introduce one cognition unit as a group of independent softmax models, which is designed to increase the diversity of the one softmax model to boost the performance for models ensemble. Meanwhile, a template-based Feature Retrieval (FR) module is adopted to improve the performance of MCSM by a specific voting scheme. Moreover, a one-shot learning method is applied on collected extra 600K identities due to each identity has one image only. Finally, testing images with lower score from MCSM and FR are assigned new labels with higher score by merging one-shot learning results. Extensive experiments on the MS-Celeb-1M testing set demonstrate the superiority of the proposed method. Our solution ranks the first place in both two settings of the final evaluation and outperforms other teams by a large margin.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.224,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Xu_High_Performance_Large_ICCV_2017_paper.pdf
a8c256e5b8e0496fee6625f1d1f1d1add99b5ca1,0,1,Optimization and scaling of patient-derived brain organoids uncovers deep phenotypes of disease,"Cerebral organoids provide unparalleled access to human brain development in vitro. However, variability induced by current culture methodologies precludes using organoids as robust disease models. To address this, we developed an automated Organoid Culture and Assay (ORCA) system to support longitudinal unbiased phenotyping of organoids at scale across multiple patient lines. We then characterized organoid variability using novel machine learning methods and found that the contribution of donor, clone, and batch is significant and remarkably consistent over gene expression, morphology, and cell-type composition. Next, we performed multi-factorial protocol optimization, producing a directed forebrain protocol compatible with 96-well culture that exhibits low variability while preserving tissue complexity. Finally, we used ORCA to study tuberous sclerosis, a disease with known genetics but poorly representative animal models. For the first time, we report highly reproducible early morphological and molecular signatures of disease in heterozygous TSC+/− forebrain organoids, demonstrating the benefit of a scaled organoid system for phenotype discovery in human disease models.",2020,,,10.1101/2020.08.26.251611,https://www.biorxiv.org/content/biorxiv/early/2020/08/28/2020.08.26.251611.full.pdf
a8d84a20321fcb11729370d44daec97bc4ad98ae,1,0,A feature learning approach for face recognition with robustness to noisy label based on top-N prediction,"Abstract Collecting a vast amount of face data with identity labels to train a convolutional neural network is an effective mean to learn a discriminative feature representation for face recognition. However, the datasets with larger scale often contain more noisy labels, that directly affects the ultimate performance of the learned model. This paper proposes an end-to-end feature learning method with robustness to noisy label. First, a data filtering method is proposed to automatically online filter out the data with false label, by checking the consistency between the annotated label and the results of top-N prediction. Then the loss functions of softmax and center loss are simply revised to only supervise the reserved feature. Finally, we use MS-Celeb-1M dataset, which contains massive noisy labels, to train a 128-D feature representation without any pre-train or data pre-clean. A single learned model gets an accuracy of 99.43% on LFW test set, that is very close to the model trained using the clean data.",2019,Neurocomputing,,10.1016/j.neucom.2018.10.075,
a8f70f4aaf6713b44c9f9b48aff5fb0e394e842e,0,1,Image-based Kinship Verification using Fusion Convolutional Neural Network,"In this paper, we investigate the performance of fusion convolutional neural network (CNN) classifier for image-based kinship verification problem. Two fusion configurations were used for the experiments, early fusion CNN classifier and late fusion CNN classifier. The early fusion configuration of the CNN classifier takes combined two face images as input for verification. The advantages of early fusion configuration are no heavy changes in the classifier architecture and only the first layer that have a different filter size. The late fusion configuration of the CNN classifier formed by creating dual CNN network for extracting the deep features of each face image and classify the kinship relationship using two fully-connected layers. The softmax and angular softmax (a-softmax) loss are used for evaluating the network in the training process with fine-tuning strategy. The classifier then evaluated using large-scale FIW (Family in the Wild) kinship verification dataset consists of 1,000 family and 11 different kinship relationship. Experiments using the 5-fold configuration on FIW dataset show that the ensemble of fusion CNN classifier produces comparable performance with several different state-of-the-art methods.",2019,2019 IEEE 11th International Workshop on Computational Intelligence and Applications (IWCIA),,10.1109/IWCIA47330.2019.8955092,
a913096d4c7fc435bbaf379a6866d7dff3991efb,0,1,Deep Learning and Open Set Malware Classification: A Survey,"As the Internet is growing rapidly these years, the variant of malicious software, which often referred to as malware, has become one of the major and serious threats to Internet users. The dramatic increase of malware has led to a research area of not only using cutting edge machine learning techniques classify malware into their known families, moreover, recognize the unknown ones, which can be related to Open Set Recognition (OSR) problem in machine learning. Recent machine learning works have shed light on Open Set Recognition (OSR) from different scenarios. Under the situation of missing unknown training samples, the OSR system should not only correctly classify the known classes, but also recognize the unknown class. This survey provides an overview of different deep learning techniques, a discussion of OSR and graph representation solutions and an introduction of malware classification systems.",2020,ArXiv,2004.04272,,https://arxiv.org/pdf/2004.04272.pdf
a9340e71bacff66d4cdeb5a8750052e19e6b811c,1,0,Accurate and Efficient Similarity Search for Large Scale Face Recognition,"Face verification is a relatively easy task with the help of discriminative features from deep neural networks. However, it is still a challenge to recognize faces on millions of identities while keeping high performance and efficiency. The challenge 2 of MS-Celeb-1M is a classification task. However, the number of identities is too large and it is not that elegant to treat the task as an image classification task. We treat the classification task as similarity search and do experiments on different similarity search strategies. Similarity search strategy accelerates the speed of searching and boosts the accuracy of final results. The model used for extracting features is a single deep neural network pretrained on CASIA-Webface, which is not trained on the base set or novel set offered by official. Finally, we rank \textbf{3rd}, while the speed of searching is 1ms/image.",2018,ArXiv,1806.00365,,https://arxiv.org/pdf/1806.00365.pdf
a97f957df37cd17dadc791ea7b85b694361e5d13,1,0,A survey on deep learning based face recognition,"Abstract Deep learning, in particular the deep convolutional neural networks, has received increasing interests in face recognition recently, and a number of deep learning methods have been proposed. This paper summarizes about 330 contributions in this area. It reviews major deep learning concepts pertinent to face image analysis and face recognition, and provides a concise overview of studies on specific face recognition problems, such as handling variations in pose, age, illumination, expression, and heterogeneous face matching. A summary of databases used for deep face recognition is given as well. Finally, some open challenges and directions are discussed for future research.",2019,Comput. Vis. Image Underst.,,10.1016/j.cviu.2019.102805,
a99193bfc31184f551a5f1b32cf7637417078b75,0,1,Towards a Reliable Face Recognition System,"Face Recognition (FR) is an important area in computer vision with many applications such as security and automated border controls. The recent advancements in this domain have pushed the performance of models to human-level accuracy. However, the varying conditions in the real-world expose more challenges for their adoption. In this paper, we investigate the performance of these models. We analyze the performance of a cross-section of face detection and recognition models. Experiments were carried out without any preprocessing on three state-of-the-art face detection methods namely HOG, YOLO and MTCNN, and three recognition models namely, VGGface2, FaceNet and Arcface. Our results indicated that there is a significant reliance by these methods on preprocessing for optimum performance.",2020,EANN,,10.1007/978-3-030-48791-1_23,
a9b9b3a6720622e109d55596f3d442b8888384bd,0,1,Dual Embedding Expansion for Vehicle Re-identification,"Vehicle re-identification plays a crucial role in the management of transportation infrastructure and traffic flow. However, this is a challenging task due to the large view-point variations in appearance, environmental and instance-related factors. Modern systems deploy CNNs to produce unique representations from the images of each vehicle instance. Most work focuses on leveraging new losses and network architectures to improve the descriptiveness of these representations. In contrast, our work concentrates on re-ranking and embedding expansion techniques. We propose an efficient approach for combining the outputs of multiple models at various scales while exploiting tracklet and neighbor information, called dual embedding expansion (DEx). Additionally, a comparative study of several common image retrieval techniques is presented in the context of vehicle re-ID. Our system yields competitive performance in the 2020 NVIDIA AI City Challenge with promising results. We demonstrate that DEx when combined with other re-ranking techniques, can produce an even larger gain without any additional attribute labels or manual supervision.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2004.08665,10.1109/CVPRW50498.2020.00298,https://arxiv.org/pdf/2004.08665.pdf
a9d06af95a81a0e800162965878fe9a973235021,1,0,One Network to Solve Them All — Solving Linear Inverse Problems Using Deep Projection Models,"While deep learning methods have achieved state-of-theart performance in many challenging inverse problems like image inpainting and super-resolution, they invariably involve problem-specific training of the networks. Under this approach, each inverse problem requires its own dedicated network. In scenarios where we need to solve a wide variety of problems, e.g., on a mobile camera, it is inefficient and expensive to use these problem-specific networks. On the other hand, traditional methods using analytic signal priors can be used to solve any linear inverse problem; this often comes with a performance that is worse than learning-based methods. In this work, we provide a middle ground between the two kinds of methods — we propose a general framework to train a single deep neural network that solves arbitrary linear inverse problems. We achieve this by training a network that acts as a quasi-projection operator for the set of natural images and show that any linear inverse problem involving natural images can be solved using iterative methods. We empirically show that the proposed framework demonstrates superior performance over traditional methods using wavelet sparsity prior while achieving performance comparable to specially-trained networks on tasks including compressive sensing and pixel-wise inpainting.",2017,2017 IEEE International Conference on Computer Vision (ICCV),1703.09912,10.1109/ICCV.2017.627,https://arxiv.org/pdf/1703.09912.pdf
aa0f8b324a98f8a5ae3037ccb9debd130894faf0,1,0,ATFaceGAN: Single Face Image Restoration and Recognition from Atmospheric Turbulence,"Image degradation due to atmospheric turbulence is common while capturing images at long ranges. To mitigate the degradation due to turbulence which includes deformation and blur, we propose a generative single frame restoration algorithm which disentangles the blur and deformation due to turbulence and reconstructs a restored image. The disentanglement is achieved by decomposing the distortion due to turbulence into blur and deformation components using deblur generator and deformation correction generator. Two paths of restoration are implemented to regularize the disentanglement and generate two restored images from one degraded image. A fusion function combines the features of the restored images to reconstruct a sharp image with rich details. Adversarial and perceptual losses are added to reconstruct a sharp image and suppress the artifacts respectively. Extensive experiments demonstrate the effectiveness of the proposed restoration algorithm, which achieves satisfactory performance in face restoration and face recognition.",2019,ArXiv,1910.03119,10.1109/FG47880.2020.00012,https://arxiv.org/pdf/1910.03119.pdf
aa1439a1d6c2c2257f8bb3f2dd1d9c488a020447,0,1,SoftTriple Loss: Deep Metric Learning Without Triplet Sampling,"Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1909.05235,10.1109/ICCV.2019.00655,https://arxiv.org/pdf/1909.05235.pdf
aa45e43f5b8ab488cf521275753f2eeb42f4a02e,1,0,DemogPairs: Quantifying the Impact of Demographic Imbalance in Deep Face Recognition,"Although deep face recognition has achieved impressive results in recent years, controversy has arisen regarding racial and gender bias of the models, questioning their deployment into sensitive scenarios. This work quantifies for the first time the demographic imbalance of popular public face datasets in terms of identity, gender and ethnicity. We also publicly release DemogPairs, a new validation set with 10.8K facial images and 58.3M identity verification pairs, distributed in demographically-balanced folds of Asian, Black and White females and males. A benchmark of experiments is carried out using DemogPairs over state-of-the-art deep face recognition models (SphereFace, FaceNet and ResNet50), in order to analyze their cross-demographic behavior. Experimental results demonstrate that studied models suffer from a very structured and damaging demographic bias. Our experiments shine a light on novel testing protocols to appropriately validate the generalization capabilities of face recognition models.",2019,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),,10.1109/FG.2019.8756625,
aa8722b7eec297d008b3fc88fd7ab6451cf4df59,0,1,Vision based Indoor Localization Method via Convolution Neural Network,"Existing indoor localization methods have bottleneck constraints such as multipath effect for Wi-Fi based methods, high cost for ultra-wide-band based methods and poor anti-interference for Bluetooth-based methods and so on. In order to avoid these problems, a vision-based indoor localization method is proposed. Firstly, the whole deployment environment is departed into several regions and each region is assigned to a location center. Then, in offline mode, the VGG16NET is pre-trained by ImageNet dataset and it is fine-tuned by images on a custom dataset towards indoor localization. In online mode, the fully trained and converged VGG16NET takes as input a video stream captured by the front RGB camera of a mobile robot and outputs features specific to the current location. The features are then used as input to an ArcFace classifier which outputs the current location of the mobile robot. Experimental results show that our method can estimate the location of a mobile object with imaging capability accurately in cluttered unstructured scenes without any other additional device. The localization accuracy can reach to 94.7%.",2019,,,10.14569/ijacsa.2019.0100709,https://pdfs.semanticscholar.org/aa87/22b7eec297d008b3fc88fd7ab6451cf4df59.pdf
aa8a08735f3714a6d9ebf1151548ecb3e47c52f7,0,1,"Vehicle Attribute Recognition by Appearance: Computer Vision Methods for Vehicle Type, Make and Model Classification","This paper studies vehicle attribute recognition by appearance. In the literature, image-based target recognition has been extensively investigated in many use cases, such as facial recognition, but less so in the field of vehicle attribute recognition. We survey a number of algorithms that identify vehicle properties ranging from coarse-grained level (vehicle type) to fine-grained level (vehicle make and model). Moreover, we discuss two alternative approaches for these tasks, including straightforward classification and a more flexible metric learning method. Furthermore, we design a simulated real-world scenario for vehicle attribute recognition and present an experimental comparison of the two approaches.",2020,ArXiv,2006.164,10.1007/s11265-020-01567-6,https://link.springer.com/content/pdf/10.1007/s11265-020-01567-6.pdf
aa9436f24f21d716909f7e7b4724ce299f5d6804,1,1,Open-set face identification with index-of-max hashing by learning,"Abstract Large-scale face identification or 1-to-N matching where N is huge, plays a vital role in biometrics and surveillance. The system demands accurate and speedy matching where compact facial feature representation and a simple matcher are favored. On the other hand, most research considers closed-set identification that assumes that all identities of probe samples are enclosed in the gallery. On the contrary, open-set identification expects that some probe identities are not known to the system. This setup poses an additional challenge, where the system should be able to reject those probes that correspond to unknown identities. In this paper, we address the large-scale open-set face identification problem with a compact facial representation that is based on the index-of-maximum (IoM) hashing, which was designed for biometric template protection. To be specific, the existing random IoM hashing is advanced to a data-driven based hashing technique, where the hashed face code can be made compact and matching can be easily performed by the Hamming distance, which can offer highly efficient matching. Furthermore, since IoM hashing transforms the original facial features non-invertibly, the privacy of users can also be preserved. Along with IoM hashed face code, we explore several fusion strategies to address the open-set face identification problem. The comprehensive evaluations are carried out with three large-scale unconstrained face datasets, namely LFW, VGG2 and IJB-C.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107277,
aaa2b45153051e23d5a35ccf9af8ecabc0fe24cd,1,0,1 How Good can Human Predict Facial Age ?,"We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person's actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed `MegaAge', which consists of 41,941 images. Data can be downloaded from our project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and github.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-the-art results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.",2017,,,,
aaffcd199daa6055e524b72114ba3ee012c3528b,0,1,FaceTimeMap: Multi-Level Bitmap Index for Temporal Querying of Faces in Videos,"In﻿ this﻿ article,﻿ the﻿ authors﻿ study﻿ bitmap﻿ indexing﻿ for﻿ temporal﻿ querying﻿ of﻿ faces﻿ that﻿ appear﻿ in﻿ videos.﻿Since﻿the﻿bitmap﻿index﻿is﻿originally﻿designed﻿to﻿select﻿a﻿set﻿of﻿records﻿that﻿satisfy﻿a﻿value﻿ in﻿the﻿domain﻿of﻿the﻿attribute,﻿there﻿is﻿no﻿clear﻿strategy﻿for﻿how﻿to﻿apply﻿it﻿for﻿temporal﻿querying.﻿ Accordingly,﻿the﻿authors﻿introduce﻿a﻿multi-level﻿bitmap﻿index﻿that﻿the﻿authors﻿call﻿“FaceTimeMap”﻿ for﻿ temporal﻿ querying﻿ of﻿ faces﻿ in﻿ videos.﻿ The﻿ first﻿ level﻿ of﻿ the﻿ FaceTimeMap﻿ index﻿ is﻿ used﻿ for﻿ determining﻿whether﻿a﻿person﻿appears﻿in﻿a﻿video﻿or﻿not,﻿whereas﻿the﻿second﻿level﻿of﻿the﻿index﻿is﻿ used﻿for﻿determining﻿intervals﻿when﻿a﻿person﻿appears.﻿First,﻿the﻿authors﻿analyze﻿the﻿co-appearance﻿ query﻿where﻿two﻿or﻿more﻿people﻿appear﻿simultaneously﻿in﻿a﻿video,﻿and﻿then﻿examine﻿next-appearance﻿ query﻿where﻿a﻿person﻿appears﻿right﻿after﻿another﻿person.﻿In﻿addition,﻿to﻿consider﻿the﻿gap﻿between﻿the﻿ appearance﻿of﻿people,﻿the﻿authors﻿study﻿eventual-﻿and﻿prior-appearance﻿queries.﻿Queries﻿are﻿satisfied﻿ by﻿applying﻿bitwise﻿operations﻿on﻿the﻿FaceTimeMap﻿index.﻿The﻿authors﻿provide﻿some﻿performance﻿ studies﻿associated﻿with﻿this﻿index. KEywoRDS Allen’s Intervals, Co-Appearance, Eventual-Appearance, Face Search, Next-Appearance",2019,Int. J. Multim. Data Eng. Manag.,,10.4018/IJMDEM.2019040103,
ab1a2b98bd09f23ba577808a7d83969edf6fc7eb,1,0,Consistent and Flexible Selectivity Estimation for High-dimensional Data,"Selectivity estimation aims at estimating the number of database objects that satisfy a selection criterion. Answering this problem accurately and efficiently is essential to many applications, such as density estimation, outlier detection, query optimization, and data integration. The estimation problem is especially challenging for large-scale high-dimensional data due to the curse of dimensionality, the large variance of selectivity across different queries, and the need to make the estimator consistent (i.e., the selectivity is non-decreasing in the threshold). We propose a new deep learning-based model that learns a query-dependent piecewise linear function as selectivity estimator, which is flexible to fit the selectivity curve of any query object and threshold, while guaranteeing that the output is non-decreasing in the threshold. To improve the accuracy for large datasets, we propose to partition the dataset into multiple disjoint subsets and build a local model on each of them. We perform experiments on real datasets and show that the proposed model significantly outperforms state-of-the-art models in accuracy and is competitive in efficiency.",2020,ArXiv,2005.09908,,https://arxiv.org/pdf/2005.09908.pdf
ab5b5496bda3002c0072b5ef670b310029ad7b3d,0,1,Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype Mining and Language-Dependent Score Normalization,"In this paper we describe the top-scoring IDLab submission for the text-independent task of the Short-duration Speaker Verification (SdSV) Challenge 2020. The main difficulty of the challenge exists in the large degree of varying phonetic overlap between the potentially cross-lingual trials, along with the limited availability of in-domain DeepMine Farsi training data. We introduce domain-balanced hard prototype mining to fine-tune the state-of-the-art ECAPA-TDNN x-vector based speaker embedding extractor. The sample mining technique efficiently exploits speaker distances between the speaker prototypes of the popular AAM-softmax loss function to construct challenging training batches that are balanced on the domain-level. To enhance the scoring of cross-lingual trials, we propose a language-dependent s-norm score normalization. The imposter cohort only contains data from the Farsi target-domain which simulates the enrollment data always being Farsi. In case a Gaussian-Backend language model detects the test speaker embedding to contain English, a cross-language compensation offset determined on the AAM-softmax speaker prototypes is subtracted from the maximum expected imposter mean score. A fusion of five systems with minor topological tweaks resulted in a final MinDCF and EER of 0.065 and 1.45% respectively on the SdSVC evaluation set.",2020,INTERSPEECH,2007.07689,10.21437/Interspeech.2020-2662,https://arxiv.org/pdf/2007.07689.pdf
ac0500599c6d9d7367850e76cf176894dafaa974,0,1,Mitigating the Impact of Adversarial Attacks in Very Deep Networks,"Deep Neural Network (DNN) models have vulnerabilities related to security concerns, with attackers usually employing complex hacking techniques to expose their structures. Data poisoning-enabled perturbation attacks are complex adversarial ones that inject false data into models. They negatively impact the learning process, with no benefit to deeper networks, as they degrade a model’s accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method for mitigating their influence. In it, a Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture which assists in neutralizing the effects of illegitimate perturbation samples in the feature space. To boost the robustness and trustworthiness of this method for correctly classifying attacked input samples, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). It improves discrimination among samples in different classes and maintains the resemblance of those in the same class. Also, we integrate a DFL and PCL in a compact model for defending against data poisoning attacks. This method is trained and tested using the CIFAR∗Corresponding author: Mohammed Hassanin Preprint submitted to Elsevier December 10, 2020 ar X iv :2 01 2. 04 75 0v 1 [ cs .C V ] 8 D ec 2 02 0 10 and MNIST datasets with data poisoning-enabled perturbation attacks, with the experimental results revealing its excellent performance compared with those of recent peer techniques.",2020,,2012.0475,,https://arxiv.org/pdf/2012.04750.pdf
ac057602f513f5abae0ddffbd49acc21f8592559,0,1,Disentangling in Latent Space by Harnessing a Pretrained Generator,"Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality generative power, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through this extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.",2020,ArXiv,2005.07728,,https://arxiv.org/pdf/2005.07728.pdf
ac304ee2425cb923d2fecebef9d84dbac8429ac8,1,1,Learning to Cluster Faces via Confidence and Connectivity Estimation,"Face clustering is an essential tool for exploiting the unlabeled face data, and has a wide range of applications including face annotation and retrieval. Recent works show that supervised clustering can result in noticeable performance gain. However, they usually involve heuristic steps and require numerous overlapped subgraphs, severely restricting their accuracy and efficiency. In this paper, we propose a fully learnable clustering framework without requiring a large number of overlapped subgraphs. Instead, we transform the clustering problem into two sub-problems. Specifically, two graph convolutional networks, named GCN-V and GCN-E, are designed to estimate the confidence of vertices and the connectivity of edges, respectively. With the vertex confidence and edge connectivity, we can naturally organize more relevant vertices on the affinity graph and group them into clusters. Experiments on two large-scale benchmarks show that our method significantly improves clustering accuracy and thus performance of the recognition models trained on top, yet it is an order of magnitude more efficient than existing supervised methods.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.00445,10.1109/cvpr42600.2020.01338,https://arxiv.org/pdf/2004.00445.pdf
ac5d96111cf57a2062f627cbcefcd5303ba929d2,0,1,RegularFace: Deep Face Recognition via Exclusive Regularization,"We consider the face recognition task where facial images of the same identity (person) is expected to be closer in the representation space, while different identities be far apart. Several recent studies encourage the intra-class compactness by developing loss functions that penalize the variance of representations of the same identity. In this paper, we propose the `exclusive regularization' that focuses on the other aspect of discriminability -- the inter-class separability, which is neglected in many recent approaches. The proposed method, named RegularFace, explicitly distances identities by penalizing the angle between an identity and its nearest neighbor, resulting in discriminative face representations. Our method has intuitive geometric interpretation and presents unique benefits that are absent in previous works. Quantitative comparisons against prior methods on several open benchmarks demonstrate the superiority of our method. In addition, our method is easy to implement and requires only a few lines of python code on modern deep learning frameworks.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00123,http://mftp.mmcheng.net/Papers/19cvprRegularFace.pdf
acaa953781442cfe9d787dd6893f3d194c6ce877,0,1,Chinese Sentence Semantic Matching Based on Multi-Granularity Fusion Model,"Sentence semantic matching is the cornerstone of many natural language processing tasks, including Chinese language processing. It is well known that Chinese sentences with different polysemous words or word order may have totally different semantic meanings. Thus, to represent and match the sentence semantic meaning accurately, one challenge that must be solved is how to capture the semantic features from the multi-granularity perspective, e.g., characters and words. To address the above challenge, we propose a novel sentence semantic matching model which is based on the fusion of semantic features from character-granularity and word-granularity, respectively. Particularly, the multi-granularity fusion intends to extract more semantic features to better optimize the downstream sentence semantic matching. In addition, we propose the equilibrium cross-entropy, a novel loss function, by setting mean square error (MSE) as an equilibrium factor of cross-entropy. The experimental results conducted on Chinese open data set demonstrate that our proposed model combined with binary equilibrium cross-entropy loss function is superior to the existing state-of-the-art sentence semantic matching models.",2020,PAKDD,,10.1007/978-3-030-47436-2_19,https://link.springer.com/content/pdf/10.1007%2F978-3-030-47436-2_19.pdf
acb8c042fc0391a72cae9d71b4a287d9e22a33fe,0,1,Regularizing Neural Networks via Minimizing Hyperspherical Energy,"Inspired by the Thomson problem in physics where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy, hyperspherical energy minimization has demonstrated its potential in regularizing neural networks and improving their generalization power. In this paper, we first study the important role that hyperspherical energy plays in neural network training by analyzing its training dynamics. Then we show that naively minimizing hyperspherical energy suffers from some difficulties due to highly non-linear and non-convex optimization as the space dimensionality becomes higher, therefore limiting the potential to further improve the generalization. To address these problems, we propose the compressive minimum hyperspherical energy (CoMHE) as a more effective regularization for neural networks. Specifically, CoMHE utilizes projection mappings to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different designs for the projection mapping, we propose several distinct yet well-performing variants and provide some theoretical guarantees to justify their effectiveness. Our experiments show that CoMHE consistently outperforms existing regularization methods, and can be easily applied to different neural networks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1906.04892,10.1109/cvpr42600.2020.00695,https://arxiv.org/pdf/1906.04892.pdf
ad2e66fa0c49d118f65465cde796294d4663a682,1,0,Cross-Spectral Face Hallucination via Disentangling Independent Factors,"The cross-sensor gap is one of the challenges that have aroused much research interests in Heterogeneous Face Recognition (HFR). Although recent methods have attempted to fill the gap with deep generative networks, most of them suffer from the inevitable misalignment between different face modalities. Instead of imaging sensors, the misalignment primarily results from facial geometric variations that are independent of the spectrum. Rather than building a monolithic but complex structure, this paper proposes a Pose Aligned Cross-spectral Hallucination (PACH) approach to disentangle the independent factors and deal with them in individual stages. In the first stage, an Unsupervised Face Alignment (UFA) module is designed to align the facial shapes of the near-infrared (NIR) images with those of the visible (VIS) images in a generative way, where UV maps are effectively utilized as the shape guidance. Thus the task of the second stage becomes spectrum translation with aligned paired data. We develop a Texture Prior Synthesis (TPS) module to achieve complexion control and consequently generate more realistic VIS images than existing methods. Experiments on three challenging NIR-VIS datasets verify the effectiveness of our approach in producing visually appealing images and achieving state-of-the-art performance in HFR.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00795,
ad41947ed62a09da96cdc6ec75a5efe2c93a4dc1,1,0,Feature Space Singularity for Out-of-Distribution Detection,"Out-of-Distribution (OoD) detection is important for building safe artificial intelligence systems. However, current OoD detection methods still cannot meet the performance requirements for practical deployment. In this paper, we propose a simple yet effective algorithm based on a novel observation: in a trained neural network, OoD samples with bounded norms well concentrate in the feature space. We call the center of OoD features the Feature Space Singularity (FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD samples can be identified by taking a threshold on the FSSD. Our analysis of the phenomenon reveals why our algorithm works. We demonstrate that our algorithm achieves state-of-the-art performance on various OoD detection benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test data and can be further enhanced by ensembling. These make FSSD a promising algorithm to be employed in real world. We release our code at https://github.com/megvii-research/FSSD_OoD_Detection.",2020,ArXiv,2011.14654,,https://arxiv.org/pdf/2011.14654.pdf
ad45f860ee674ec2ae6d2972c3e45584424a15a9,1,1,An Equalized Margin Loss for Face Recognition,"In this paper, we propose a new loss function, termed the equalized margin (EqM) loss, which is designed to make both intra-class scopes and inter-class margins similar over all classes, such that all the classes can be evenly distributed on the hypersphere of the feature space. The EqM loss controls both the lower limit of intra-class similarity by exploiting hard-sample mining and the upper limit of inter-class similarity by assuring equalized margins. Therefore, using the EqM loss, we can not only obtain more discriminative features, but also overcome the negative impacts from the data imbalance on the inter-class margins. We also observe that the EqM loss is stable with the variation of the scale in normalized Softmax. Furthermore, by conducting extensive experiments on LFW, YTF, CFP, MegaFace and IJB-B, we are able to verify the effectiveness and superiority of the EqM loss, compared with other state-of-the-art loss functions for face recognition.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2020.2966863,https://pdfs.semanticscholar.org/d95e/d029bc827a162ccb75decc908032ec874d88.pdf
ad52a6d1f89995389a05d1e3dcca103952327296,1,1,Self-attention aggregation network for video face representation and recognition,"Models based on self-attention mechanisms have been successful in analyzing temporal data and have been widely used in the natural language domain. We propose a new model architecture for video face representation and recognition based on a self-attention mechanism. Our approach could be used for video with single and multiple identities. To the best of our knowledge, no one has explored the aggregation approaches that consider the video with multiple identities. The proposed approach utilizes existing models to get the face representation for each video frame, e.g., ArcFace and MobileFaceNet, and the aggregation module produces the aggregated face representation vector for video by taking into consideration the order of frames and their quality scores. We demonstrate empirical results on a public dataset for video face recognition called IJB-C to indicate that the self-attention aggregation network (SAAN) outperforms naive average pooling. Moreover, we introduce a new multi-identity video dataset based on the publicly available UMDFaces dataset and collected GIFs from Giphy. We show that SAAN is capable of producing a compact face representation for both single and multiple identities in a video. The dataset and source code will be publicly available.",2020,ArXiv,2010.0534,,https://arxiv.org/pdf/2010.05340.pdf
ad93c0628000879431eaed4da70aa0fe45b20813,0,1,Watchlist Adaptation: Protecting the Innocent,"One of the most important government applications of face recognition is the watchlist problem, where the goal is to identify a few people enlisted on a watchlist while ignoring the majority of innocent passersby. Since watchlists dynamically change and training times can be expensive, the deployed approaches use pre-trained deep networks only to provide deep features for face comparison. Since these networks never specifically trained on the operational setting or faces from the watchlist, the system will often confuse them with the faces of innocent non-watchlist subjects leading to difficult situations, e.g., being detained at the airport to resolve their identity. We develop a novel approach to take an existing pre-trained face network and use adaptation layers trained with our recently developed Objectosphere loss to provide an open-set recognition system that is rapidly adapted to the gallery while also ignoring non-watchlist faces as well as any background detections from the face detector. While our adapter network can be quickly trained without the need of retraining the entire representation network, it can also significantly improve the performance of any state-of-the-art face recognition network like VGG2. We experiment with the largest open-set face recognition dataset, the UnConstrained College Students (UCCS). It contains real surveillance camera stills including both known and unknown subjects, as well as many non-face regions from the face detector. We show that the Objectosphere approach is able to reduce the feature magnitude of unknown subjects as well as background detections, so that we can apply a specifically designed similarity function on the deep features of the Objectosphere network, which works much better than the direct prediction of the very same network. Additionally, our approach outperforms the VGG2 baseline by a large margin by rejecting the non-face data, and also outperforms prior state-of-the-art open-set recognition algorithms on the VGG2 baseline data.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
adbb663b140d3d14fc21d85f1b05403cad8e867a,1,1,Neural Architecture Search for Deep Face Recognition,"By the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature that does not need too much cooperation and can significantly improve the user's experience at the same time. Face recognition is one of the main functions of electronic equipment propaganda. Hence it's virtually worth researching in computer vision. Previous work in this field has focused on two directions: converting loss function to improve recognition accuracy in traditional deep convolution neural networks (Resnet); combining the latest loss function with the lightweight system (MobileNet) to reduce network size at the minimal expense of accuracy. But none of these has changed the network structure. With the development of AutoML, neural architecture search (NAS) has shown excellent performance in the benchmark of image classification. In this paper, we integrate NAS technology into face recognition to customize a more suitable network. We quote the framework of neural architecture search which trains child and controller network alternately. At the same time, we mutate NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize policy gradient algorithm to search the architecture automatically with the most classical cross-entropy loss. The network architectures we searched out have got state-of-the-art accuracy in the large-scale face dataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with relatively small network size. To the best of our knowledge, this proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.",2019,ArXiv,1904.09523,,https://arxiv.org/pdf/1904.09523.pdf
ae284bc18eb5e7ebe43d824b967287a5494ed17f,1,0,Human Identification Based on Deep Feature and Transfer Learning,"Biometric based identity authentication has attracted much attention due to its unique advantages. Among all the biometric which can be used for authentication, human face based methods have been the most popular research area in both identity authentication and recognition. However, traditional method may result in poor performance when conducting face recognition under uncontrolled environmental. Deep network provides a more proper way to extract distinctive features for face recognition, however the performance of most deep network is usually limited by the number of training samples. Accordingly, this paper proposes a deep convolutional neural network combining with the idea of transfer learning and sparse representation to combat the disadvantage of traditional CNN on small sample task while simplifying the computational complexity. Abundant experimental results in different database show that compared with traditional method, our proposed method achieves higher and promising recognition rate.",2018,BDIOT 2018,,10.1145/3289430.3289464,
ae77aa8f631d17e364593543f64675f8dfb2d033,1,1,Inducing Predictive Uncertainty Estimation for Face Recognition,"Knowing when an output can be trusted is critical for reliably using face recognition systems. While there has been enormous effort in recent research on improving face verification performance, understanding when a model's predictions should or should not be trusted has received far less attention. Our goal is to assign a confidence score for a face image that reflects its quality in terms of recognizable information. To this end, we propose a method for generating image quality training data automatically from 'mated-pairs' of face images, and use the generated data to train a lightweight Predictive Confidence Network, termed as PCNet, for estimating the confidence score of a face image. We systematically evaluate the usefulness of PCNet with its error versus reject performance, and demonstrate that it can be universally paired with and improve the robustness of any verification model. We describe three use cases on the public IJB-C face verification benchmark: (i) to improve 1:1 image-based verification error rates by rejecting low-quality face images; (ii) to improve quality score based fusion performance on the 1:1 set-based verification benchmark; and (iii) its use as a quality measure for selecting high quality (unblurred, good lighting, more frontal) faces from a collection, e.g. for automatic enrolment or display.",2020,ArXiv,2009.00603,,https://arxiv.org/pdf/2009.00603.pdf
aea1bc7f8c098763a3291bee4212ce766e60543e,0,1,Cosine-Distance Virtual Adversarial Training for Semi-Supervised Speaker-Discriminative Acoustic Embeddings,"In this paper, we propose a semi-supervised learning (SSL) technique for training deep neural networks (DNNs) to generate speaker-discriminative acoustic embeddings (speaker embeddings). Obtaining large amounts of speaker recognition train-ing data can be difficult for desired target domains, especially under privacy constraints. The proposed technique reduces requirements for labelled data by leveraging unlabelled data. The technique is a variant of virtual adversarial training (VAT) [1] in the form of a loss that is defined as the robustness of the speaker embedding against input perturbations, as measured by the cosine-distance. Thus, we term the technique cosine-distance virtual adversarial training (CD-VAT). In comparison to many existing SSL techniques, the unlabelled data does not have to come from the same set of classes (here speakers) as the labelled data. The effectiveness of CD-VAT is shown on the 2750+ hour VoxCeleb data set, where on a speaker verification task it achieves a reduction in equal error rate (EER) of 11.1% relative to a purely supervised baseline. This is 32.5% of the improvement that would be achieved from supervised training if the speaker labels for the unlabelled data were available.",2020,INTERSPEECH,,10.21437/interspeech.2020-2270,
af56cedf7b80626d4ef18a34ee8bc2eeab1718b0,0,1,Face recognition approach by subspace extended sparse representation and discriminative feature learning,"Abstract To address the problem of face recognition where the number of the labeled samples is insufficient and those samples involve pose, illumination and expression variations, etc., this paper proposes a face recognition approach by subspace extended sparse representation and discriminative feature learning, called SESRC & LDF. In SESRC&LDF, each test image is considered to be the image with small pose variation or the image with large pose variation according to its symmetry. For each test image, if it is considered to be the former, it will be recognized by the proposed subspace extended sparse representation classifier (SESRC), otherwise, it will be recognized by the face recognition method based on learning discriminative feature (LDF) proposed in this paper. On eight benchmark face databases, including Yale, AR, LFW, Extended Yale B, FEI, FERET, UMIST and Georgia Tech, empirical results show that SESRC & LDF achieves the highest recognition rates, outperforming many algorithms. Those algorithms include some state-of-the-art ones, such as PLR, MDFR and OPR.",2020,Neurocomputing,,10.1016/j.neucom.2019.09.025,
af8a72cee9150d52512c238fc5d5f8b400adaa40,0,1,Dual Encoder-Decoder Based Generative Adversarial Networks for Disentangled Facial Representation Learning,"To learn disentangled representations of facial images, we present a Dual Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed method, both the generator and discriminator are designed with deep encoder-decoder architectures as their backbones. To be more specific, the encoder-decoder structured generator is used to learn a pose disentangled face representation, and the encoder-decoder structured discriminator is tasked to perform real/fake classification, face reconstruction, determining identity and estimating face pose. We further improve the proposed network architecture by minimizing the additional pixel-wise loss defined by the Wasserstein distance at the output of the discriminator so that the adversarial framework can be better trained. Additionally, we consider face pose variation to be continuous, rather than discrete in existing literature, to inject richer pose information into our model. The pose estimation task is formulated as a regression problem, which helps to disentangle identity information from pose variations. The proposed network is evaluated on the tasks of pose-invariant face recognition (PIFR) and face synthesis across poses. An extensive quantitative and qualitative evaluation carried out on several controlled and in-the-wild benchmarking datasets demonstrates the superiority of the proposed DED-GAN method over the state-of-the-art approaches.",2020,IEEE Access,1909.08797,10.1109/ACCESS.2020.3009512,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09141259.pdf
afc7760940ace91ec55666030ca1238dbe8c7f4a,0,1,Large Scale Landmark Recognition via Deep Metric Learning,"This paper presents a novel approach for landmark recognition in images that we've successfully deployed at Mail.ru. This method enables us to recognize famous places, buildings, monuments, and other landmarks in user photos. The main challenge lies in the fact that it's very complicated to give a precise definition of what is and what is not a landmark. Some buildings, statues and natural objects are landmarks; others are not. There's also no database with a fairly large number of landmarks to train a recognition model. A key feature of using landmark recognition in a production environment is that the number of photos containing landmarks is extremely small. This is why the model should have a very low false positive rate as well as high recognition accuracy. We propose a metric learning-based approach that successfully deals with existing challenges and efficiently handles a large number of landmarks. Our method uses a deep neural network and requires a single pass inference that makes it fast to use in production. We also describe an algorithm for cleaning landmarks database which is essential for training a metric learning model. We provide an in-depth description of basic components of our method like neural network architecture, the learning strategy, and the features of our metric learning approach. We show the results of proposed solutions in tests that emulate the distribution of photos with and without landmarks from a user collection. We compare our method with others during these tests. The described system has been deployed as a part of a photo recognition solution at Cloud Mail.ru, which is the photo sharing and storage service at Mail.ru Group.",2019,CIKM,1908.10192,10.1145/3357384.3357956,https://arxiv.org/pdf/1908.10192.pdf
b00b11352c895f8e586d1f5230de705d241ea661,1,0,Are Adaptive Face Recognition Systems still Necessary? Experiments on the APE Dataset,"In the last five years, deep learning methods, in particular CNN, have attracted considerable attention in the field of face-based recognition, achieving impressive results. Despite this progress, it is not yet clear precisely to what extent deep features are able to follow all the intra-class variations that the face can present over time. In this paper we investigate the performance the performance improvement of face recognition systems by adopting self updating strategies of the face templates. For that purpose, we evaluate the performance of a well-known deep-learning face representation, namely, FaceNet, on a dataset that we generated explicitly conceived to embed intra-class variations of users on a large time span of captures: the APhotoEveryday (APE) dataset. Moreover, we compare these deep features with handcrafted features extracted using the BSIF algorithm. In both cases, we evaluate various template update strategies, in order to detect the most useful for such kind of features. Experimental results show the effectiveness of ""optimized"" self-update methods with respect to systems without update or random selection of templates.",2020,ArXiv,2010.04072,,https://arxiv.org/pdf/2010.04072.pdf
b02e43949de233d72b83ec4c1e2c7f5edb9eeab5,1,1,Towards Privacy Protection by Generating Adversarial Identity Masks,"As billions of personal data such as photos are shared through social media and network, the privacy and security of data have drawn an increasing attention. Several attempts have been made to alleviate the leakage of identity information with the aid of image obfuscation techniques. However, most of the present results are either perceptually unsatisfactory or ineffective against real-world recognition systems. In this paper, we argue that an algorithm for privacy protection must block the ability of automatic inference of the identity and at the same time, make the resultant image natural from the users' point of view. To achieve this, we propose a targeted identity-protection iterative method (TIP-IM), which can generate natural face images by adding adversarial identity masks to conceal ones' identity against a recognition system. Extensive experiments on various state-of-the-art face recognition models demonstrate the effectiveness of our proposed method on alleviating the identity leakage of face images, without sacrificing? the visual quality of the protected images.",2020,ArXiv,2003.06814,,https://arxiv.org/pdf/2003.06814.pdf
b030bf2b14c09f4980cbf5a8d77057413f23aee8,0,1,Interpretable and Generalizable Person Re-identification with Query-Adaptive Convolution and Temporal Lifting,"For person re-identification, existing deep networks often focus on representation learning. However, without domain adaptation or transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and result are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this image matching architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation without further transfer learning, the proposed Query-Adaptive Convolution (QAConv) method achieves better results than many transfer learning methods for person re-identification. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, resulting in state-of-the-art results in cross-dataset evaluations.",2020,ECCV,1904.10424,10.1007/978-3-030-58621-8_27,https://arxiv.org/pdf/1904.10424.pdf
b0b5eb4804144f73da168ea78fa762fc042b83c4,1,0,Weakly supervised multiscale-inception learning for web-scale face recognition,"Supervised deep learning models like convolutional neural network (CNN) have shown very promising results for the face recognition problem, which often require a huge number of labeled face images. Since manually labeling a large training set is a very difficult and time-consuming task, it is very beneficial if the deep model can be trained from face samples with only weak annotations. In this paper, we propose a general framework to train a deep CNN model with weakly labeled facial images that are available on the Internet. Specifically, we first design a deep Multiscale-Inception CNN (MICNN) architecture to exploit the multi-scale information for face recognition. Then, we train an initial MICNN model with only a limited number of labeled samples. After that, we propose a dual-level sample selection strategy to further fine-tune the MICNN model with the weakly labeled samples from both the sample level and class level, which aims to skip outliers and select more samples from confusing class pairs during training. Extensive experimental results on the LFW and YTF benchmarks demonstrate the effectiveness of the proposed method.",2017,2017 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2017.8296394,http://ir.ia.ac.cn/bitstream/173211/20078/1/ICIP17FaceRecognitionV4Xing.pdf
b0f15fa49c805800824dde18d07a84e2e65408a8,0,1,Semi-supervised WCE image classification with adaptive aggregated attention,"Accurate abnormality classification in Wireless Capsule Endoscopy (WCE) images is crucial for early gastrointestinal (GI) tract cancer diagnosis and treatment, while it remains challenging due to the limited annotated dataset, the huge intra-class variances and the high degree of inter-class similarities. To tackle these dilemmas, we propose a novel semi-supervised learning method with Adaptive Aggregated Attention (AAA) module for automatic WCE image classification. Firstly, a novel deformation field based image preprocessing strategy is proposed to remove the black background and circular boundaries in WCE images. Then we propose a synergic network to learn discriminative image features, consisting of two branches: an abnormal regions estimator (the first branch) and an abnormal information distiller (the second branch). The first branch utilizes the proposed AAA module to capture global dependencies and incorporate context information to highlight the most meaningful regions, while the second branch mainly focuses on these calculated attention regions for accurate and robust abnormality classification. Finally, these two branches are jointly optimized by minimizing the proposed discriminative angular (DA) loss and Jensen-Shannon divergence (JS) loss with labeled data as well as unlabeled data. Comprehensive experiments have been conducted on the public CAD-CAP WCE dataset. The proposed method achieves 93.17% overall accuracy in a fourfold cross-validation, verifying its effectiveness for WCE image classification. The source code is available at https://github.com/Guo-Xiaoqing/SSL_WCE.",2020,Medical Image Anal.,,10.1016/j.media.2020.101733,
b1254778c965601ae6b901be7c95834d7a87351b,1,0,Comparison-Level Mitigation of Ethnic Bias in Face Recognition,"Current face recognition systems achieve high performance on several benchmark tests. Despite this progress, recent works showed that these systems are strongly biased against demographic sub-groups. Previous works introduced approaches that aim at learning less biased representations. However, applying these approaches in real applications requires a complete replacement of the templates in the database. This replacement procedure further requires that a face image of each enrolled individual is stored as well. In this work, we propose the first bias-mitigating solution that works on the comparison-level of a biometric system. We propose a fairness- driven neural network classifier for the comparison of two biometric templates to replace the systems similarity function. This fair classifier is trained with a novel penalization term in the loss function to introduce the criteria of group and individual fairness to the decision process. This penalization term forces the score distributions of different ethnicities to be similar, leading to a reduction of the intra-ethnic performance differences. Experiments were conducted on two publicly available datasets and evaluated the performance of four different ethnicities. The results showed that for both fairness criteria, our proposed approach is able to significantly reduce the ethnic bias, while it preserves a high recognition ability. Our model, build on individual fairness, achieves bias reduction rate between 15.35% and 52.67%. In contrast to previous work, our solution is easy to integrate into existing systems by simply replacing the systems similarity functions with our fair template comparison approach.",2020,2020 8th International Workshop on Biometrics and Forensics (IWBF),,10.1109/IWBF49977.2020.9107956,
b129336e4ef00b39e251e72cc10796f9d199ebd1,1,1,3D-Aided Data Augmentation for Robust Face Understanding,"Data augmentation has been highly effective in narrowing the data gap and reducing the cost for human annotation, especially for tasks where ground truth labels are difficult and expensive to acquire. In face recognition, large pose and illumination variation of face images has been a key factor for performance degradation. However, human annotation for the various face understanding tasks including face landmark localization, face attributes classification and face recognition under these challenging scenarios are highly costly to acquire. Therefore, it would be desirable to perform data augmentation for these cases. But simple 2D data augmentation techniques on the image domain are not able to satisfy the requirement of these challenging cases. As such, 3D face modeling, in particular, single image 3D face modeling, stands a feasible solution for these challenging conditions beyond 2D based data augmentation. To this end, we propose a method that produces realistic 3D augmented images from multiple viewpoints with different illumination conditions through 3D face modeling, each associated with geometrically accurate face landmarks, attributes and identity information. Experiments demonstrate that the proposed 3D data augmentation method significantly improves the performance and robustness of various face understanding tasks while achieving state-of-arts on multiple benchmarks.",2020,ArXiv,2010.01246,,https://arxiv.org/pdf/2010.01246.pdf
b138fb51119d73743f36b174ccd0f1622b0fa704,0,1,Enhancing Image Representations for Occluded Face Recognition via Reference Conditioned Low-Rank projection,"Deep learning in face recognition is widely explored in recent times due to its ability to produce state-of-the-art results and availability of large public datasets. While recent deep learning approaches involving margin loss based image representations produce 99% accuracy across benchmarks, none of these studies focus explicitly on occluded face verification. Further, in real world scenarios, there is a need for efficient methods that cater to the cases of occlusion of faces with hats, scarves, goggle or sometimes exaggerated facial expression. Moreover, with face verification gathering traction in mainstream real-time embedded applications of surveillance, the proposed approaches need to be highly accurate. In this paper, we revisit the same through a large-scale study involving multiple synthetically created goggle-occluded face datasets using multiple state-of-the-art face representations. Through this study, we identify that occlusion in faces results in non-isotropic face representations in feature space which results in a drop in performance. Therefore, we propose an approach to enhance existing face representations by learning reference conditioned Low-Rank projections (RCLP), which can create isotropic representations thereby improving face recognition. We benchmark the developed approach over synthetically goggled versions of LFW, CFP-FP, ATT, FEI, Georgia Tech and Essex University face databases with representations from ResNet-ArcFace, VGGFace, MobilefaceNet-ArcFace LightCNN resulting in a total of 100 + experiments where we achieve improvements in the accuracy-rate across all with a maximum of 4% on FEI dataset. Finally, to validate the approach in a realistic scenario, we additionally present results over our internal face verification dataset of 1k images and confirm that the proposed approach only shows positive results without degrading existing baseline performance.",2019,2019 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),,10.1109/AIPR47015.2019.9174567,
b143cf896cdc00965a24637f4c5b36b08b88a923,1,0,Modeling and Mapping Location-Dependent Human Appearance,"OF DISSERTATION Modeling and Mapping Location-Dependent Human Appearance Human appearance is highly variable and depends on individual preferences, such as fashion, facial expression, and makeup. These preferences depend on many factors including a person’s sense of style, what they are doing, and the weather. These factors, in turn, are dependent upon geographic location and time. In our work, we build computational models to learn the relationship between human appearance, geographic location, and time. The primary contributions are a framework for collecting and processing geotagged imagery of people, a large dataset collected by our framework, and several generative and discriminative models that use our dataset to learn the relationship between human appearance, location, and time. Additionally, we build interactive maps that allow for inspection and demonstration of what our models have learned.",2018,,,10.13023/etd.2018.469,
b19b1b3d055b000bc8c8195c5fd6a808ab63c319,0,1,Diversified Mutual Learning for Deep Metric Learning,"Mutual learning is an ensemble training strategy to improve generalization by transferring individual knowledge to each other while simultaneously training multiple models. In this work, we propose an effective mutual learning method for deep metric learning, called Diversified Mutual Metric Learning, which enhances embedding models with diversified mutual learning. We transfer relational knowledge for deep metric learning by leveraging three kinds of diversities in mutual learning: (1) model diversity from different initializations of models, (2) temporal diversity from different frequencies of parameter update, and (3) view diversity from different augmentations of inputs. Our method is particularly adequate for inductive transfer learning at the lack of large-scale data, where the embedding model is initialized with a pretrained model and then fine-tuned on a target dataset. Extensive experiments show that our method significantly improves individual models as well as their ensemble. Finally, the proposed method with a conventional triplet loss achieves the state-of-the-art performance of Recall@1 on standard datasets: 69.9 on CUB-200-2011 and 89.1 on CARS-196.",2020,ArXiv,2009.0417,,https://arxiv.org/pdf/2009.04170.pdf
b23a06e93f6f5ad6cf4076f3c31cc788de4e301e,0,1,ARET: Aggregated Residual Extended Time-Delay Neural Networks for Speaker Verification,"The time-delay neural network (TDNN) is widely used in speaker verification to extract long-term temporal features of speakers. Although common TDNN approaches well capture time-sequential information, they lack the delicate transformations needed for deep representation. To solve this problem, we propose two TDNN architectures. RET integrates shortcut connections into conventional time-delay blocks, and ARET adopts a split-transform-merge strategy to extract more discriminative representation. Experiments on VoxCeleb datasets without augmentation indicate that ARET realizes satisfactory performance on the VoxCeleb1 test set, VoxCeleb1-E, and VoxCeleb1-H, with 1.389%, 1.520%, and 2.614% equal error rate (EER), respectively. Compared to state-of-the-art results on these test sets, RET achieves a 23% ∼ 43% relative reduction in EER, and ARET reaches 32% ∼ 45%.",2020,INTERSPEECH,,10.21437/interspeech.2020-1626,https://isca-speech.org/archive/Interspeech_2020/pdfs/1626.pdf
b23f4e6b1313c5a5b3a1e05424e4424d1b2cda1a,0,1,Fair Loss: Margin-Aware Reinforcement Learning for Deep Face Recognition,"Recently, large-margin softmax loss methods, such as angular softmax loss (SphereFace), large margin cosine loss (CosFace), and additive angular margin loss (ArcFace), have demonstrated impressive performance on deep face recognition. These methods incorporate a fixed additive margin to all the classes, ignoring the class imbalance problem. However, imbalanced problem widely exists in various real-world face datasets, in which samples from some classes are in a higher number than others. We argue that the number of a class would influence its demand for the additive margin. In this paper, we introduce a new margin-aware reinforcement learning based loss function, namely fair loss, in which each class will learn an appropriate adaptive margin by Deep Q-learning. Specifically, we train an agent to learn a margin adaptive strategy for each class, and make the additive margins for different classes more reasonable. Our method has better performance than present large-margin loss functions on three benchmarks, Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace, which demonstrates that our method could learn better face representation on imbalanced face datasets.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.01015,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Fair_Loss_Margin-Aware_Reinforcement_Learning_for_Deep_Face_Recognition_ICCV_2019_paper.pdf
b23ff46d8a80873448717813d65c7a7d929dc01e,1,0,A Privacy-Preserving Framework for Collecting Demographic Information,"Currently one of the biggest challenges regarding demographic detection in images and social media is the lack of labelled demographic data. A big part of the challenge is that no suitable mechanism exists to replace traditional intercept surveys in a way that ensures fairness and inclusion. The lack of labelled data has also impacted the training of AI algorithms. That is the lack of labels relevant to the target domains has made it hard to estimate the accuracy of the AI algorithms when they are applied to real world situations. In this paper, we propose a framework for collecting in-the-wild images and demographic labels from ordinary people (e.g., park visitors) that ensures that privacy is integrated at every stage of the data collection process from storage to processing and sharing.",2020,CHI Extended Abstracts,,10.1145/3334480.3382800,
b286bd248a25a21433ff1be716a9f26b86b19eed,1,0,Face Recognition Algorithm Bias: Performance Differences on Images of Children and Adults,"In this work, we examine if current state-of-the-art deep learning face recognition systems exhibit a negative bias (i.e., poorer performance) for children when compared to the performance obtained on adults. The systems selected for this work are five top performing commercial-off-the-shelf face recognition systems, two government-off-the-shelf face recognition systems and one open-source face recognition solution. The datasets used to evaluate the performance of the systems are both unconstrained in age, pose, illumination, and expression and are publicly available. These datasets are indicative of photo journalistic face datasets published and evaluated on over the last few years. Our findings show a negative bias for each algorithm on children. Genuine and imposter distributions highlight the performance bias between the datasets further supporting the need for a deeper investigation into algorithm bias as a function of age cohorts. To combat the performance decline on the child demographic, several score-level fusion strategies were evaluated. This work identifies the best score-level fusion technique for the child demographic.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00280,
b2e953416dff3f8097df5ae4d8ffc0c8e930bc55,0,1,Security analysis of cancellable biometrics using constrained-optimized similarity-based attack,"Cancellable biometrics (CB) intentionally distorts biometric template for security protection, and simultaneously preserving the distance/similarity for matching in the transformed domain. Despite its effectiveness, the security issues attributed to similarity preservation property of CB is underestimated. Dong et al. [BTAS'19], exploited the similarity preservation trait of CB and proposed a similarity-based attack with high successful attack rate. The similarity-based attack utilizes preimage that generated from the protected biometric template for impersonation and perform cross matching. In this paper, we propose a constrained optimization similarity-based attack (CSA), which is improved upon Dong's genetic algorithm enabled similarity-based attack (GASA). The CSA applies algorithm-specific equality or inequality relations as constraints, to optimize preimage generation. We justify the effectiveness of CSA from the supervised learning perspective. We conduct extensive experiments to demonstrate CSA against Index-of-Max (IoM) hashing with LFW face dataset. The results suggest that CSA is effective to breach IoM hashing security, and outperforms GASA remarkably. Furthermore, we reveal the correlation of IoM hash code size and the attack performance of CSA.",2020,ArXiv,2006.13051,,https://arxiv.org/pdf/2006.13051.pdf
b301fd2fc33f24d6f75224e7c0991f4f04b64a65,1,0,Faces as Lighting Probes via Unsupervised Deep Highlight Extraction,"We present a method for estimating detailed scene illumination using human faces in a single image. In contrast to previous works that estimate lighting in terms of low-order basis functions or distant point lights, our technique estimates illumination at a higher precision in the form of a non-parametric environment map. Based on the observation that faces can exhibit strong highlight reflections from a broad range of lighting directions, we propose a deep neural network for extracting highlights from faces, and then trace these reflections back to the scene to acquire the environment map. Since real training data for highlight extraction is very limited, we introduce an unsupervised scheme for finetuning the network on real images, based on the consistent diffuse chromaticity of a given face seen in multiple real images. In tracing the estimated highlights to the environment, we reduce the blurring effect of skin reflectance on reflected light through a deconvolution determined by prior knowledge on face material properties. Comparisons to previous techniques for highlight extraction and illumination estimation show the state-of-the-art performance of this approach on a variety of indoor and outdoor scenes.",2018,ECCV,1803.0634,10.1007/978-3-030-01240-3_20,https://arxiv.org/pdf/1803.06340.pdf
b325046c0bb112373f8e79f8b032fd5df0a5c98e,0,1,Self-supervised Text-independent Speaker Verification using Prototypical Momentum Contrastive Learning,"In this study, we investigate self-supervised representation learning for speaker verification (SV). First, we examine a simple contrastive learning approach (SimCLR) with a momentum contrastive (MoCo) learning framework, where the MoCo speaker embedding system utilizes a queue to maintain a large set of negative examples. We show that better speaker embeddings can be learned by momentum contrastive learning. Next, alternative augmentation strategies are explored to normalize extrinsic speaker variabilities of two random segments from the same speech utterance. Specifically, augmentation in the waveform largely improves the speaker representations for SV tasks. The proposed MoCo speaker embedding is further improved when a prototypical memory bank is introduced, which encourages the speaker embeddings to be closer to their assigned prototypes with an intermediate clustering step. In addition, we generalize the self-supervised framework to a semi-supervised scenario where only a small portion of the data is labeled. Comprehensive experiments on the Voxceleb dataset demonstrate that our proposed selfsupervised approach achieves competitive performance compared with existing techniques, and can approach fully supervised results with partially labeled data.",2020,,2012.07178,,https://arxiv.org/pdf/2012.07178.pdf
b346fa22262908c1d9dbe7ec469bfb19715e365a,1,0,AttGAN: Facial Attribute Editing by Only Changing What You Want,"Facial attribute editing aims to manipulate single or multiple attributes on a given face image, i.e., to generate a new face image with desired attributes while preserving other details. Recently, the generative adversarial net (GAN) and encoder–decoder architecture are usually incorporated to handle this task with promising results. Based on the encoder–decoder architecture, facial attribute editing is achieved by decoding the latent representation of a given face conditioned on the desired attributes. Some existing methods attempt to establish an attribute-independent latent representation for further attribute editing. However, such attribute-independent constraint on the latent representation is excessive because it restricts the capacity of the latent representation and may result in information loss, leading to over-smooth or distorted generation. Instead of imposing constraints on the latent representation, in this work, we propose to apply an attribute classification constraint to the generated image to just guarantee the correct change of desired attributes, i.e., to change what you want. Meanwhile, the reconstruction learning is introduced to preserve attribute-excluding details, in other words, to only change what you want. Besides, the adversarial learning is employed for visually realistic editing. These three components cooperate with each other forming an effective framework for high quality facial attribute editing, referred as AttGAN. Furthermore, the proposed method is extended for attribute style manipulation in an unsupervised manner. Experiments on two wild datasets, CelebA and LFW, show that the proposed method outperforms the state-of-the-art on realistic attribute editing with other facial details well preserved.",2019,IEEE Transactions on Image Processing,1711.10678,10.1109/TIP.2019.2916751,https://arxiv.org/pdf/1711.10678.pdf
b35ff9985aaee9371588330bcef0dfc88d1401d7,1,0,Deep Density Clustering of Unconstrained Faces,"In this paper, we consider the problem of grouping a collection of unconstrained face images in which the number of subjects is not known. We propose an unsupervised clustering algorithm called Deep Density Clustering (DDC) which is based on measuring density affinities between local neighborhoods in the feature space. By learning the minimal covering sphere for each neighborhood, information about the underlying structure is encapsulated. The encapsulation is also capable of locating high-density region of the neighborhood, which aids in measuring the neighborhood similarity. We theoretically show that the encapsulation asymptotically converges to a Parzen window density estimator. Our experiments show that DDC is a superior candidate for clustering unconstrained faces when the number of subjects is unknown. Unlike conventional linkage and density-based methods that are sensitive to the selection operating points, DDC attains more consistent and improved performance. Furthermore, the density-aware property reduces the difficulty in finding appropriate operating points.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,,10.1109/CVPR.2018.00848,http://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Deep_Density_Clustering_CVPR_2018_paper.pdf
b3650e931deb9e8b43882a3d4918e5702e332e94,1,0,Cross-spectral Face Completion for NIR-VIS Heterogeneous Face Recognition,"Near infrared-visible (NIR-VIS) heterogeneous face recognition refers to the process of matching NIR to VIS face images. Current heterogeneous methods try to extend VIS face recognition methods to the NIR spectrum by synthesizing VIS images from NIR images. However, due to self-occlusion and sensing gap, NIR face images lose some visible lighting contents so that they are always incomplete compared to VIS face images. This paper models high resolution heterogeneous face synthesis as a complementary combination of two components, a texture inpainting component and pose correction component. The inpainting component synthesizes and inpaints VIS image textures from NIR image textures. The correction component maps any pose in NIR images to a frontal pose in VIS images, resulting in paired NIR and VIS textures. A warping procedure is developed to integrate the two components into an end-to-end deep network. A fine-grained discriminator and a wavelet-based discriminator are designed to supervise intra-class variance and visual quality respectively. One UV loss, two adversarial losses and one pixel loss are imposed to ensure synthesis results. We demonstrate that by attaching the correction component, we can simplify heterogeneous face synthesis from one-to-many unpaired image translation to one-to-one paired image translation, and minimize spectral and pose discrepancy during heterogeneous recognition. Extensive experimental results show that our network not only generates high-resolution VIS face images and but also facilitates the accuracy improvement of heterogeneous face recognition.",2019,ArXiv,1902.03565,,https://arxiv.org/pdf/1902.03565.pdf
b37e3d9b6c811a5436a1fac4995e4245bf98416f,0,1,Angular Learning: Toward Discriminative Embedded Features,"The margin-based softmax loss functions greatly enhance intra-class compactness and perform well on the tasks of face recognition and object classification. Outperformance, however, depends on the careful hyperparameter selection. Moreover, the hard angle restriction also increases the risk of overfitting. In this paper, angular loss suggested by maximizing the angular gradient to promote intra-class compactness avoids overfitting. Besides, our method has only one adjustable constant for intra-class compactness control. We define three metrics to measure inter-class separability and intra-class compactness. In experiments, we test our method, as well as other methods, on many well-known datasets. Experimental results reveal that our method has the superiority of accuracy improvement, discriminative information, and time-consumption.",2019,ArXiv,1912.07819,,https://arxiv.org/pdf/1912.07819.pdf
b3dc4f8e7711262db37392c663504ba34d399b17,1,1,VarGFaceNet: An Efficient Variable Group Convolutional Neural Network for Lightweight Face Recognition,"To improve the discriminative and generalization ability of lightweight network for face recognition, we propose an efficient variable group convolutional network called VarGFaceNet. Variable group convolution is introduced by VarGNet to solve the conflict between small computational cost and the unbalance of computational intensity inside a block. We employ variable group convolution to design our network which can support large scale face identification while reduce computational cost and parameters. Specifically, we use a head setting to reserve essential information at the start of the network and propose a particular embedding setting to reduce parameters of fully-connected layer for embedding. To enhance interpretation ability, we employ an equivalence of angular distillation loss to guide our lightweight network and we apply recursive knowledge distillation to relieve the discrepancy between the teacher model and the student model. The champion of deepglint-light track of LFR (2019) challenge demonstrates the effectiveness of our model and approach. Implementation of VarGFaceNet will be released at https://github.com/zma-c-137/VarGFaceNet soon.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),1910.04985,10.1109/ICCVW.2019.00323,https://arxiv.org/pdf/1910.04985.pdf
b403017aa5063cad25ea680122226e10d63ca1ab,0,1,Face Analysis: State of the Art and Ethical Challenges,"In face analysis, the task is to identify a subject appearing in an image as a unique individual and to extract facial attributes like age, gender, and expressions from the face image. Over the last years, we have witnessed tremendous improvements in face analysis algorithms developed by the industry and by academia as well. Some applications, that might have been considered science fiction in the past, have become reality now. We can observe that nowadays tools are far from perfect, however, they can deal with very challenging images such as pictures taken in an unconstrained environment. In this paper, we show how easy is to build very effective applications with open source tools. For instance, it is possible to analyze the facial expressions of a public figure and his/her interactions in the last 24 h by processing images from Twitter given a hashtag. Obviously, the same analysis can be performed using images from a surveillance camera or from a family photo album. The recognition rate is now comparable to human vision, but computer vision can process thousands of images in a couple of hours. For these applications, it is not necessary to train complex deep learning networks, because they are already trained and available in public repositories. In our work, we show that anyone with certain computer skills can use (or misuse) this technology. The increased performance of facial analysis and its easy implementation have enormous potential for good, and –unfortunately– for ill too. For these reasons, we believe that our community should discuss the scope and limitations of this technology in terms of ethical issues such as definition of good practices, standards, and restrictions when using and teaching facial analysis.",2019,PSIVT Workshops,,10.1007/978-3-030-39770-8_2,http://dmery.sitios.ing.uc.cl/Prints/Conferences/International/2019-PSIVT.pdf
b42363adcb43f39034f09c22e8c542b79d5d197d,0,1,iCartoonFace: A Benchmark of Cartoon Person Recognition,"Cartoons receive increasingly attention and have a huge global market. Cartoon person recognition has a wealth of application scenarios. However, there is no large and high quality dataset for cartoon person recognition. It limit the development of recognition algorithms. In this paper, we propose the first large unconstrained cartoon database called iCartoonFace. We have released the dataset publicly available to promote cartoon person recognition research\footnote{The dataset can be applied by sending email to zhengyi01@qiyi.com}. The dataset contains 68,312 images of 2,639 identities. The dataset consists of persons which come from cartoon videos. The samples are extracted from public available images on website and online videos on iQiYi company. All images pass through a careful manual annotation process. We evaluated the state-of-the-art image classification and face recognition algorithms on the iCartoonFace dataset as a baseline. A dataset fusion method which utilize face feature to improve the performance of cartoon recognition task is proposed. Experimental performance show that the performance of baseline models much worse than human performance. The proposed dataset fusion method achieves a 4.74% improvement over the baseline model. In a word, state-of-the-art algorithms for classification and recognition are far from being perfect for unconstrained cartoon person recognition.",2019,ArXiv,,,
b42f105c2f48ae1f25d54963530394476c47ba52,0,1,Topological Higher-Order Neuron Model Based on Homology-Continuity Principle,"Recently, deep neural networks (DNNs) have been applied in various areas in computer vision. However, due to the simple neuron configuration, the existing DNNs rely heavily on the diversity of the data distribution and have poor generalization performance. In addition, the excessive network complexity also leads to low learning efficiency. To solve above problems, a topological higher-order neuron model was proposed, which is modeled according to the point-set topology in high-dimensional space. The proposed neuron is determined by the adaptively changed center and the direction weights with strong generalization ability. The high-dimensional feature space of the samples is constructed by a category-based covering learning method, thus achieving an optimal coverage of each category. Experiments on several different classical datasets demonstrate the generalization of the proposed neuron model, which provides a new approach for the further development of DNNs and can be widely used in computer vision.",2019,2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI),,10.1109/CCHI.2019.8901920,
b4354fd44517bdc22400c93bf9d6d6f928293730,0,1,Analysis of Deep Metric Learning Approaches,"This article aims to demonstrate the value of deep metric learning considering recent studies. Most existing studies are inspired by Siamese and Triplet networks that build a high-level representation of samples in separable linear space. The success and main idea of these networks is the ability to capture the similarity relationship between samples in the dataset. There are mainly three components: sampling method, loss function, and neural network architecture that determine the metric learning model. Improvement of these components is the main challenge for researchers. This article is a review and analysis of deep metric learning components.",2019,2019 IEEE International Conference on Advanced Trends in Information Theory (ATIT),,10.1109/ATIT49449.2019.9030440,
b446bcd7fb78adfe346cf7a01a38e4f43760f363,1,0,To appear in ICB 2018 Longitudinal Study of Child Face Recognition,"We present a longitudinal study of face recognition performance on Children Longitudinal Face (CLF) dataset containing 3, 682 face images of 919 subjects, in the age group [2, 18] years. Each subject has at least four face images acquired over a time span of up to six years. Face comparison scores are obtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source matcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTSA and FaceNet matchers. To improve the performance of the open-source FaceNet matcher for child face recognition, we were able to fine-tune it on an independent training set of 3,294 face images of 1,119 children in the age group [3, 18] years. Multilevel statistical models are fit to genuine comparison scores from the CLF dataset to determine the decrease in face recognition accuracy over time. Additionally, we analyze both the verification and open-set identification accuracies in order to evaluate state-of-the-art face recognition technology for tracing and identifying children lost at a young age as victims of child trafficking or abduction.",2017,,,,https://pdfs.semanticscholar.org/b446/bcd7fb78adfe346cf7a01a38e4f43760f363.pdf
b4e42daba36f1007fb792912424d0914a39eb9a5,0,1,FaRE: Open Source Face Recognition Performance Evaluation Package,"Biometrics-related research has been accelerated significantly by deep learning technology. However, there are limited open-source resources to help researchers evaluate their deep learning-based biometrics algorithms efficiently, especially for the face recognition tasks. In this work, we design, implement, and evaluate a computationally lightweight, maintainable, scalable, generalizable, and extendable face recognition evaluation toolbox named FaRE that supports both online and offline evaluation to provide feedback to algorithm development and accelerate biometricsrelated research. FaRE includes a set of evaluation metrics and provides various APIs for commonly-used face recognition datasets including LFW, CFP, UHDB31, and IJBseries datasets. FaRE can be easily extended to include other datasets. The package is publically available for research use at https://github.com/uh-cbl/FaRE.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803411,
b4ee64022cc3ccd14c7f9d4935c59b16456067d3,1,0,Unsupervised Cross-Domain Image Generation,"We explore the problem of general domain transfer by replicating a recent method presented at ICLR 2017. This method maps a sample from one domain to another using a generative adversarial network (GAN) in an unsupervised fashion. We attempt to replicate this method in two visual application areas digits and faces and perform additional analysis on various components of the approach. We achieve similar visual results for digits but not faces, finding that the training procedure is crucial to a successful GAN implementation.",2017,,,,https://pdfs.semanticscholar.org/b4ee/64022cc3ccd14c7f9d4935c59b16456067d3.pdf
b4f2deca8e85663134a729676edc2aca337e913e,0,1,The Mertens Unrolled Network (MU-Net): A High Dynamic Range Fusion Neural Network for Through the Windshield Driver Recognition,"Face recognition of vehicle occupants through windshields in unconstrained environments poses a number of unique challenges ranging from glare, poor illumination, driver pose and motion blur. In this paper, we further develop the hardware and software components of a custom vehicle imaging system to better overcome these challenges. After the build out of a physical prototype system that performs High Dynamic Range (HDR) imaging, we collect a small dataset of through-windshield image captures of known drivers. We then re-formulate the classical Mertens-Kautz-Van Reeth HDR fusion algorithm as a pre-initialized neural network, which we name the Mertens Unrolled Network (MU-Net), for the purpose of fine-tuning the HDR output of through-windshield images. Reconstructed faces from this novel HDR method are then evaluated and compared against other traditional and experimental HDR methods in a pre-trained state-of-the-art (SOTA) facial recognition pipeline, verifying the efficacy of our approach.",2020,ArXiv,2002.12257,10.1117/12.2566765,https://arxiv.org/pdf/2002.12257.pdf
b4f58d7fc87a9c276965e604fbe83592bae7f4e5,1,0,RoCGAN: Robust Conditional GAN,"Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli).",2020,International Journal of Computer Vision,,10.1007/s11263-020-01348-5,https://link.springer.com/content/pdf/10.1007/s11263-020-01348-5.pdf
b52cdf7828f863f5511782b9e327c810497cef9d,1,0,Using Age Information as a Soft Biometric Trait for Face Image Analysis,,2020,,,10.1007/978-3-030-32583-1_1,
b53054911382ec4c57ed3080feefcb3e85034f63,0,1,Deep Convolutional Neural Network Using Triplet Loss to Distinguish the Identical Twins,"In face recognition, distinguishing identical twins faces is a challenging task because of the high level of correlation in facial appearance.Generally, facial recognition is easy to make mistakes when it comes to twins or similar faces. To deal with the high level of correlation in similar faces, we proposed a deep convolutional neural network (CNN) using triplet loss function to differentiate the identical twins. We applied a hybrid strategy by combining the deep CNN model, which learns an embedding from facial images to Euclidean space and triplet loss function to evaluate the L2 distance between facial images into Euclidean space, Obtained L2 distance shows the level of similarity between corresponding faces. We implemented two different CNN models on our raw pixel images; additionally, we used different techniques to reduce the overfitting problem such as dropout and batch normalization, additionally L2 regularization. Our method achieves the best mean validation accuracy above 87.2%.",2019,2019 IEEE Globecom Workshops (GC Wkshps),,10.1109/GCWkshps45667.2019.9024704,
b5cd28ee3582114f0c785d37968a413a139a6a65,0,1,Optimal Strategies Against Generative Attacks,"Generative neural models have improved dramatically recently. With this progress comes the risk that such models will be used to attack systems that rely on sensor data for authentication and anomaly detection. Many such learning systems are installed worldwide, protecting critical infrastructure or private data against malfunction and cyber attacks. We formulate the scenario of such an authentication system facing generative impersonation attacks, characterize it from a theoretical perspective and explore its practical implications. In particular, we ask fundamental theoretical questions in learning, statistics and information theory: How hard is it to detect a ``fake reality''? How much data does the attacker need to collect before it can reliably generate nominally-looking artificial data? Are there optimal strategies for the attacker or the authenticator? We cast the problem as a maximin game, characterize the optimal strategy for both attacker and authenticator in the general case, and provide the optimal strategies in closed form for the case of Gaussian source distributions. Our analysis reveals the structure of the optimal attack and the relative importance of data collection for both authenticator and attacker. Based on these insights we design practical learning approaches, and show that they result in models that are more robust to various attacks on real-world data. Code will be made publicly available upon publication.",2020,ICLR,,,
b60ecfccdda9a9426d00c088d4a27c96f764c3d6,1,0,Disentanglement for Discriminative Visual Recognition,"Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization in a controllable manner remains an open issue. For instance, various factors such as identity-specific attributes, pose, illumination and expression affect the appearance of face images. Disentangling the identity-specific factors is potentially beneficial for facial expression recognition (FER). This chapter systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this chapter, these problems are casted as either a deep metric learning problem or an adversarial minimax game in the latent space. For the former choice, a generalized adaptive (N+M)-tuplet clusters loss function together with the identity-aware hard-negative mining and online positive mining scheme can be used for identity-invariant FER. The better FER performance can be achieved by combining the deep metric loss and softmax loss in a unified two fully connected layer branches framework via joint optimization. For the latter solution, it is possible to equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginal independent to the task-relevant/irrelevant semantic and latent variations. The framework achieves top performance on a serial of tasks, including lighting, makeup, disguise-tolerant face recognition and facial attributes recognition. This chapter systematically summarize the popular and practical solution for disentanglement to achieve more discriminative visual recognition.",2020,ArXiv,2006.0781,,https://arxiv.org/pdf/2006.07810.pdf
b633239f9300e8d3a29bf29a6724b606293f3f05,1,1,Building Computationally Efficient and Well-Generalizing Person Re-Identification Models with Metric Learning,"This work considers the problem of domain shift in person re-identification.Being trained on one dataset, a re-identification model usually performs much worse on unseen data. Partially this gap is caused by the relatively small scale of person re-identification datasets (compared to face recognition ones, for instance), but it is also related to training objectives. We propose to use the metric learning objective, namely AM-Softmax loss, and some additional training practices to build well-generalizing, yet, computationally efficient models. We use recently proposed Omni-Scale Network (OSNet) architecture combined with several training tricks and architecture adjustments to obtain state-of-the art results in cross-domain generalization problem on a large-scale MSMT17 dataset in three setups: MSMT17-all->DukeMTMC, MSMT17-train->Market1501 and MSMT17-all->Market1501.",2020,ArXiv,2003.07618,,https://arxiv.org/pdf/2003.07618.pdf
b63a9dc18baf645b4766b2b2ec8461c2c843275a,0,1,What comprises a good talking-head video generation?: A Survey and Benchmark,"Over the years, performance evaluation has become essential in computer vision, enabling tangible progress in many sub-fields. While talking-head video generation has become an emerging research topic, existing evaluations on this topic present many limitations. For example, most approaches use human subjects (e.g., via Amazon MTurk) to evaluate their research claims directly. This subjective evaluation is cumbersome, unreproducible, and may impend the evolution of new research. In this work, we present a carefully-designed benchmark for evaluating talking-head video generation with standardized dataset pre-processing strategies. As for evaluation, we either propose new metrics or select the most appropriate ones to evaluate results in what we consider as desired properties for a good talking-head video, namely, identity preserving, lip synchronization, high video quality, and natural-spontaneous motion. By conducting a thoughtful analysis across several state-of-the-art talking-head generation approaches, we aim to uncover the merits and drawbacks of current methods and point out promising directions for future work. All the evaluation code is available at: this https URL.",2020,ArXiv,2005.03201,,https://arxiv.org/pdf/2005.03201.pdf
b64264d20cf60635e3bf2b8a9be63e7a0c49c2f1,1,1,Benchmarking deep learning techniques for face recognition,"Abstract Recent progresses in Convolutional Neural Networks (CNNs) and GPUs have greatly advanced the state-of-the-art performance for face recognition. However, training CNNs for face recognition is complex and time-consuming. Multiple factors need to be considered: deep learning frameworks, GPU platforms, deep network models, training datasets and test datasets. The deep models under different frameworks may perform differently. Based on this concern, we compare three deep learning frameworks and benchmark the performance of different CNN models on five GPU platforms. The scalability issue is also explored. Our findings can help researchers select appropriate face recognition models, deep learning frameworks, GPU platforms, and training datasets for their face recognition tasks.",2019,J. Vis. Commun. Image Represent.,,10.1016/j.jvcir.2019.102663,
b6b23958a6c9e71224d15beb3f84da63e122a2ca,0,1,The OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems,"This paper presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning have focused mainly on synthetic datasets and use a very limited number of applications. OARF includes different data partitioning methods (horizontal, vertical and hybrid) as well as emerging applications in image, text and structured data, which represent different scenarios in federated learning. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution and learning task complexity. We have developed reference implementations, and evaluated the important aspects of federated learning, including model accuracy, communication cost, differential privacy, secure multiparty computation and vertical federated learning.",2020,ArXiv,2006.07856,,https://arxiv.org/pdf/2006.07856.pdf
b6ed5f9ffb793fa2752303ea3c646c3b99ed8aac,1,1,Towards Transferable Adversarial Attack Against Deep Face Recognition,"Face recognition has achieved great success in the last five years due to the development of deep learning methods. However, deep convolutional neural networks (DCNNs) have been found to be vulnerable to adversarial examples. In particular, the existence of transferable adversarial examples can severely hinder the robustness of DCNNs since this type of attacks can be applied in a fully black-box manner without queries on the target system. In this work, we first investigate the characteristics of transferable adversarial attacks in face recognition by showing the superiority of feature-level methods over label-level methods. Then, to further improve transferability of feature-level adversarial examples, we propose DFANet, a dropout-based method used in convolutional layers, which can increase the diversity of surrogate models and obtain ensemble-like effects. Extensive experiments on state-of-the-art face models with various training databases, loss functions and network architectures show that the proposed method can significantly enhance the transferability of existing attack methods. Finally, by applying DFANet to the LFW database, we generate a new set of adversarial face pairs that can successfully attack four commercial APIs without any queries. This TALFW database is available to facilitate research on the robustness and defense of deep face recognition.",2021,IEEE Transactions on Information Forensics and Security,2004.0579,10.1109/TIFS.2020.3036801,https://arxiv.org/pdf/2004.05790.pdf
b6f340798c0eac2b706ded8045a0e3a286479e51,1,0,Pose Agnostic Cross-spectral Hallucination via Disentangling Independent Factors,"The cross-sensor gap is one of the challenges that arise much research interests in Heterogeneous Face Recognition (HFR). Although recent methods have attempted to fill the gap with deep generative networks, most of them suffered from the inevitable misalignment between different face modalities. Instead of imaging sensors, the misalignment primarily results from geometric variations (e.g., pose and expression) on faces that stay independent from spectrum. Rather than building a monolithic but complex structure, this paper proposes a Pose Agnostic Cross-spectral Hallucination (PACH) approach to disentangle the independent factors and deal with them in individual stages. In the first stage, an Unsupervised Face Alignment (UFA) network is designed to align the near-infrared (NIR) and visible (VIS) images in a generative way, where 3D information is effectively utilized as the pose guidance. Thus the task of the second stage becomes spectrum transform with paired data. We develop a Texture Prior Synthesis (TPS) network to accomplish complexion control and consequently generate more realistic VIS images than existing methods. Experiments on three challenging NIR-VIS datasets verify the effectiveness of our approach in producing visually appealing images and achieving state-of-the-art performance in cross-spectral HFR.",2019,ArXiv,1909.04365,,https://arxiv.org/pdf/1909.04365.pdf
b6f758be954d34817d4ebaa22b30c63a4b8ddb35,1,0,A Proximity-Aware Hierarchical Clustering of Faces,"In this paper, we propose an unsupervised face clustering algorithm called “Proximity-Aware Hierarchical Clustering” (PAHC) that exploits the local structure of deep representations. In the proposed method, a similarity measure between deep features is computed by evaluating linear SVM margins. SVMs are trained using nearest neighbors of sample data, and thus do not require any external training data. Clus- ters are then formed by thresholding the similarity scores. We evaluate the clustering performance using three challenging un- constrained face datasets, including Celebrity in Frontal-Profile (CFP), IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3) datasets. Experimental results demonstrate that the proposed approach can achieve significant improvements over state-of-the-art methods. Moreover, we also show that the proposed clustering algorithm can be applied to curate a set of large-scale and noisy training dataset while maintaining sufficient amount of images and their variations due to nuisance factors. The face verification performance on JANUS CS3 improves significantly by finetuning a DCNN model with the curated MS-Celeb-1M dataset which contains over three million face images.",2017,2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017),1703.04835,10.1109/FG.2017.134,https://arxiv.org/pdf/1703.04835.pdf
b77d6d31a10061bac5bc2e500b6f83d9b33e8be6,1,0,2D-3D Heterogeneous Face Recognition Based on Deep Coupled Spectral Regression,"As one of the major branches in Face Recognition (FR), 2D-3D Heterogeneous FR (HFR), where face comparison is achieved across the texture and shape modalities, has become more important. This paper proposes a novel deep learning based end-to-end approach, namely Deep Coupled Spectral Regression (DCSR), for such an issue. It jointly makes use of both the advantages of CNN based deep features and CSR based common subspace. Specifically, from 2D texture and 3D depth face maps, DCSR extracts more powerful features by a deep network with the cross-modality triplet loss, which show much better uniqueness and robustness than the hand-crafted ones. Further, DCSR learns the shared space between different modalities with the constraints of sample labels, and is thereby more discriminative than the widely used unsupervised methods. More importantly, the two steps above are integrated through a couple layer to explicitly optimize the weights of deep features and projection directions rather than a simple combination. Experiments are carried out on the FRGC v2.0 database, and the results reported clearly demonstrate the competency of our proposed method. Its generalization ability is also validated by additional experiments conducted on the CASIA NIR-VIS 2.0 database.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00037,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Zheng_2D-3D_Heterogeneous_Face_Recognition_Based_on_Deep_Coupled_Spectral_Regression_CVPRW_2019_paper.pdf
b79bbf9ced4ebad03816bcba419bf5e23d4b367d,0,1,Fine-Grained Facial Expression Recognition in the Wild,"Over the past decades, researches on facial expression recognition have been restricted within six basic expressions (anger, fear, disgust, happiness, sadness and surprise). However, these six words can not fully describe the richness and diversity of human beings’ emotions. To enhance the recognitive capabilities for computers, in this paper, we focus on fine-grained facial expression recognition in the wild and build a brand new benchmark FG-Emotions to push the research frontiers on this topic, which extends the original six classes to more elaborate thirty-three classes. Our FG-Emotions contains 10,371 images and 1,491 video clips annotated with corresponding fine-grained facial expression categories and landmarks. FG-Emotions also provides several features (e.g., LBP features and dense trajectories features) to facilitate related research. Moreover, on top of FG-Emotions, we propose a new end-to-end Multi-Scale Action Unit (AU)-based Network (MSAU-Net) for facial expression recognition with image which learns a more powerful facial representation by directly focusing on locating facial action units and utilizing “zoom in” operation to aggregate distinctive local features. As for recognition with video, we further extend the MSAU-Net to a two-stream model (TMSAU-Net) by adding a module with attention mechanism and a temporal stream branch to jointly learn spatial and temporal features. (T)MSAU-Net consistently outperforms existing state-of-the-art solutions on our FG-Emotions and several other datasets, and serves as a strong baseline to drive the future research towards fine-grained facial expression recognition in the wild.",2021,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.3007327,https://pdfs.semanticscholar.org/b79b/bf9ced4ebad03816bcba419bf5e23d4b367d.pdf
b7b37f61acedbae95b5476b8e9f3992085e864a3,0,1,Face identity disentanglement via latent space mapping,"Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.",2020,ACM Trans. Graph.,2005.07728,10.1145/3414685.3417826,https://arxiv.org/pdf/2005.07728.pdf
b831fa963d043bc0e98119b12725fcba120f8df0,1,1,A Secure Visual-thermal Fused Face Recognition System Based on Non-Linear Hashing,"In this paper, we propose a secure visual-thermal fused face recognition system using non-linear hashing. To extract features from both thermal and visible facial images, a deep neural network model pre-trained by visible images, namely InsightFace, is utilized in extracting deep features from both thermal and visible images. Next, we investigate into the effectiveness of using nonlinear hashing in protecting deep features extracted from both thermal and visible face images. To further boost the accuracy performance of the facial recognition system under unfavorable environment, feature- and score-level fusion of thermal and visible images for face matching are studied. The performance of different application scenarios are tested on the EURECOM VIS-TH face dataset. Experiment results suggest that: 1) feature- and score-level fusion techniques are effective in achieving higher accuracy under unfavorable situation; 2) non-linear hashing offers additional layer of protection, namely, privacy preservation, to face image. We also found that the deep model trained by using visible images is applicable to thermal images for feature extraction, which is particularly useful because there is no large thermal dataset available to train deep neural network.",2019,2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP),,10.1109/MMSP.2019.8901814,http://www.eurecom.fr/en/publication/6130/download/sec-publi-6130.pdf
b834c68b8b64766b69bf7fbd76f797d60757a01d,0,1,ACFD: Asymmetric Cartoon Face Detector,"Cartoon face detection is a more challenging task than human face detection due to many difficult scenarios is involved. Aiming at the characteristics of cartoon faces, such as huge differences within the intra-faces, in this paper, we propose an asymmetric cartoon face detector, named ACFD. Specifically, it consists of the following modules: a novel backbone VoVNetV3 comprised of several asymmetric one-shot aggregation modules (AOSA), asymmetric bi-directional feature pyramid network (ABi-FPN), dynamic anchor match strategy (DAM) and the corresponding margin binary classification loss (MBC). In particular, to generate features with diverse receptive fields, multi-scale pyramid features are extracted by VoVNetV3, and then fused and enhanced simultaneously by ABi-FPN for handling the faces in some extreme poses and have disparate aspect ratios. Besides, DAM is used to match enough high-quality anchors for each face, and MBC is for the strong power of discrimination. With the effectiveness of these modules, our ACFD achieves the 1st place on the detection track of 2020 iCartoon Face Challenge under the constraints of model size 200MB, inference time 50ms per image, and without any pretrained models.",2020,ArXiv,2007.00899,,https://arxiv.org/pdf/2007.00899.pdf
b84f2dd6ba857b70dbc329bf0f893be740a86ce7,1,0,Feature-Level Frankenstein: Eliminating Variations for Discriminative Recognition,"Recent successes of deep learning-based recognition rely on maintaining the content related to the main-task label. However, how to explicitly dispel the noisy signals for better generalization remains an open issue. We systematically summarize the detrimental factors as task-relevant/irrelevant semantic variations and unspecified latent variation. In this paper, we cast these problems as an adversarial minimax game in the latent space. Specifically, we propose equipping an end-to-end conditional adversarial network with the ability to decompose an input sample into three complementary parts. The discriminative representation inherits the desired invariance property guided by prior knowledge of the task, which is marginally independent to the task-relevant/irrelevant semantic and latent variations. Our proposed framework achieves top performance on a serial of tasks, including digits recognition, lighting, makeup, disguise-tolerant face recognition, and facial attributes recognition.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00073,http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Feature-Level_Frankenstein_Eliminating_Variations_for_Discriminative_Recognition_CVPR_2019_paper.pdf
b85782754bda7fc238ae6d6784e140bd24291a52,0,1,Normalization Before Shaking Toward Learning Symmetrically Distributed Representation Without Margin in Speech Emotion Recognition,"Regularization is crucial to the success of many practical deep learning models, in particular in a more often than not scenario where there are only a few to a moderate number of accessible training samples. In addition to weight decay, data augmentation and dropout, regularization based on multi-branch architectures, such as Shake-Shake regularization, has been proven successful in many applications and attracted more and more attention. However, beyond model-based representation augmentation, it is unclear how Shake-Shake regularization helps to provide further improvement on classification tasks, let alone the baffling interaction between batch normalization and shaking. In this work, we present our investigation on Shake-Shake regularization, drawing connections to the vicinal risk minimization principle and discriminative feature learning in verification tasks. Furthermore, we identify a strong resemblance between batch normalized residual blocks and batch normalized recurrent neural networks, where both of them share a similar convergence behavior, which could be mitigated by a proper initialization of batch normalization. Based on the findings, our experiments on speech emotion recognition demonstrate simultaneously an improvement on the classification accuracy and a reduction on the generalization gap both with statistical significance.",2018,ArXiv,1808.00876,,https://arxiv.org/pdf/1808.00876.pdf
b87fb8efda4980871de4aee2e5a2569a7c5d06bc,0,1,A Lightweight and Robust Face Recognition Network on Noisy Condition,"Recently, deep learning has a significant breakthrough in face recognition research. Using the state-of-art convolutional neural network (CNN) model is continually improving the accuracy of recognition. However, it is difficult that the large CNN models deploy on mobile phones or embedded devices with limited computation resources and memory. At the same time, these face recognition networks show low performance in the complex environment, such as noise, shadow, illumination and so on. To address these problems, we propose a lightweight and robust face recognition network (LD-MobileFaceNet) to improve the traditional MobileFaceNet in noisy environment. In this paper, an efficient and flexible denoising block is proposed, which is an independent module to apply in MobileFaceNet. The proposed denoising block uses non-local means algorithm to denoise features that are extracted by convolutional layers. With the residual connection and the 1 × 1 convolution, it can remain more information and be combined with any layers in MobileFaceNet. Furthermore, we set fewer bottleneck layers, replace PReLU with swish nonlinearity to compensate for the loss accuracy. The experimental results demonstrate that LD-MobileFaceNet with swish is 21.35% more accurate on noisy LFW dataset while reducing parameters by 25 % compared to MobileFaceNet.",2019,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),,10.1109/APSIPAASC47483.2019.9023149,
b88b44e82fd47257e05fe2adaec33b538cd82fe2,0,1,Deep Discriminative Embedding with Ranked Weight for Speaker Verification,,2020,ICONIP,,10.1007/978-3-030-63823-8_10,
b8b4c07ff91cb0626ff40109691de81cabe490fe,0,1,Analysis and Learning of Capsule Networks Robust for Small Image Deformation,"The Capsule Network (CapsNet) is a deep learning model proposed for image classification that is robust to pose of change of objects in images. A capsule is a vector representing the position, size and presence of an object. However, with CapsNet, the number of capsules increases, depending on the number of classification classes, and learning is computationally expensive. Thus, we propose a method for reducing computational costs by enabling a single capsule to represent multiple object classes. To learn the distance between classes, we incorporate the ArcFace distance learning method in the error function. In a preliminary experiment, the distribution of capsules was visualised by principal component analysis to demonstrate the validity of the proposed method. Using the MNIST and CIFAR-10 datasets, as well as an the affine transformed dataset, we compare the accuracy and learning time of the original CapsNet and proposed method. The results demonstrate that accuracy is improved by 2.74% on the CIFAR-10 dataset, and the learning time is reduced by more than 19% in both datasets.",2020,2020 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN48605.2020.9206651,http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_WCCI_2020/IJCNN/Papers/N-20740.pdf
b8f16451ed290059c6cd21c29ca40f6d5dfd555a,0,1,An end-to-end face recognition method with alignment learning,"Abstract Many effective methods have been proposed for face recognition in the past decade and the face recognition accuracy is also gradually improved, but these algorithms usually need to perform face alignment process based on the prior knowledge of facial structure before extracting facial features. The face recognition system usually consists of face detection, face alignment, facial feature extraction, etc., which are independent of each other, and it is difficult to design and train the end-to-end face recognition model. In this paper, an end-to-end face recognition method based on spatial transformation layer is proposed. Specifically, the spatial transformation layer is placed in front of the feature extraction layer of the face recognition network, and the face region is aligned by alignment learning which requires neither prior knowledge nor artificially defined geometric transformation. The face identity category information allows the convolutional neural network to automatically learn the most appropriate face alignment. Simulation experiments on CASIA-WebFace, LFW (Labeled Face in the Wild) and YTF (Youtube Face) face database have shown that the suggested alignment learning algorithm in this paper can realize the end-to-end face recognition and can effectively improve the face recognition rate as well.",2020,,,10.1016/j.ijleo.2020.164238,
b9044152085e29e41d3bd131702a821ac4832804,1,0,Deep Learning with Maxout Activations for Visual Recognition and Verification,"Visual recognition is one of the most active research topics in computer vision due to its potential applications in self-driving cars, healthcare, social media, manufacturing, etc. For image classification tasks, deep convolutional neural networks have achieved state-of-the-art results, and many activation functions have been proposed to enhance the classification performance of these networks. We explore the performance of multiple maxout activation variants on image classification, facial recognition and verification tasks using convolutional neural networks. Our experiments compare rectified linear unit, leaky rectified linear unit, scaled exponential linear unit, and hyperbolic tangent to four maxout variants. Throughout the experiments, we find that maxout networks train relatively slower than networks comprised of traditional activation functions. We found that on average, across all datasets, rectified linear units perform better than any maxout activation when the number of convolutional filters is increased six times.",2019,2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),,10.1109/IRI.2019.00033,
b9d0935cd0017ab0705d783b985891abed6412f1,0,1,Two-Stream Aural-Visual Affect Analysis in the Wild,"Human affect recognition is an essential part of natural human-computer interaction. However, current methods are still in their infancy, especially for in-the-wild data. In this work, we introduce our submission to the Affective Behavior Analysis in-the-wild (ABAW) 2020 competition. We propose a two-stream aural-visual analysis model to recognize affective behavior from videos. Audio and image streams are first processed separately and fed into a convolutional neural network. Instead of applying recurrent architectures for temporal analysis we only use temporal convolutions. Furthermore, the model is given access to additional features extracted during face-alignment. At training time, we exploit correlations between different emotion representations to improve performance. Our model achieves promising results on the challenging Aff-Wild2 database.",2020,ArXiv,2002.03399,10.1109/FG47880.2020.00056,https://arxiv.org/pdf/2002.03399.pdf
b9e5c79339f5be61d127b46c8e25cf4dd5d0c5b5,1,1,Multiple-Identity Image Attacks Against Face-based Identity Verification,"Facial verification systems are vulnerable to poisoning attacks that make use of multiple-identity images (MIIs)---face images stored in a database that resemble multiple persons, such that novel images of any of the constituent persons are verified as matching the identity of the MII. Research on this mode of attack has focused on defence by detection, with no explanation as to why the vulnerability exists. New quantitative results are presented that support an explanation in terms of the geometry of the representations spaces used by the verification systems. In the spherical geometry of those spaces, the angular distance distributions of matching and non-matching pairs of face representations are only modestly separated, approximately centred at 90 and 40-60 degrees, respectively. This is sufficient for open-set verification on normal data but provides an opportunity for MII attacks. Our analysis considers ideal MII algorithms, demonstrating that, if realisable, they would deliver faces roughly 45 degrees from their constituent faces, thus classed as matching them. We study the performance of three methods for MII generation---gallery search, image space morphing, and representation space inversion---and show that the latter two realise the ideal well enough to produce effective attacks, while the former could succeed but only with an implausibly large gallery to search. Gallery search and inversion MIIs depend on having access to a facial comparator, for optimisation, but our results show that these attacks can still be effective when attacking disparate comparators, thus securing a deployed comparator is an insufficient defence.",2019,ArXiv,1906.08507,,https://arxiv.org/pdf/1906.08507.pdf
bac45d1b6e39f5fc85e9f576682f1f07b18e254e,0,1,A deep learning framework for face verification without alignment,"Most of the CNN (convolutional neural networks) methods require alignment, which will affect the efficiency of verification. This paper proposes a deep face verification framework without alignment. First and foremost, the framework consists of two training stages and one testing stage. In the first training stage, the CNN is fully trained on the large face dataset. In the second training stage, embedding triplet is adopted to fine-tune the models. Furthermore, in the testing stage, SIFT (scale invariant feature transform) descriptors are extracted from intermediate pooling results for cascading verification, which effectively improves the accuracy of face verification without alignment. Last but not least, two CNN architectures are designed for different scenarios. The CNN1 (convolutional neural networks 1), with fewer layers and parameters, requires a small amount of memory and computation in training and testing, so it is suitable for real-time system. The CNN2 (convolutional neural networks 2), with more layers and parameters, has excellent face verification. Through the long-term training on WEB-face dataset and experiments on the LFW (labled faces in the wild), YTB (YouTube) datasets, the results show that the proposed method has superior performance compared with some state-of-the-art methods.",2020,,,10.1007/s11554-020-01037-z,
bac6e2672475ffda5b8198a167b9bea8b188a3a5,0,1,Masked Face Recognition for Secure Authentication,"With the recent world-wide COVID-19 pandemic, using face masks have become an important part of our lives. People are encouraged to cover their faces when in public area to avoid the spread of infection. The use of these face masks has raised a serious question on the accuracy of the facial recognition system used for tracking school/office attendance and to unlock phones. Many organizations use facial recognition as a means of authentication and have already developed the necessary datasets in-house to be able to deploy such a system. Unfortunately, masked faces make it difficult to be detected and recognized, thereby threatening to make the in-house datasets invalid and making such facial recognition systems inoperable. This paper addresses a methodology to use the current facial datasets by augmenting it with tools that enable masked faces to be recognized with low false-positive rates and high overall accuracy, without requiring the user dataset to be recreated by taking new pictures for authentication. We present an open-source tool, MaskTheFace to mask faces effectively creating a large dataset of masked faces. The dataset generated with this tool is then used towards training an effective facial recognition system with target accuracy for masked faces. We report an increase of 38% in the true positive rate for the Facenet system. We also test the accuracy of re-trained system on a custom real-world dataset MFR2 and report similar accuracy.",2020,ArXiv,2008.11104,,https://arxiv.org/pdf/2008.11104.pdf
bae0a603d88f47b0ebdb1e325031c36f63dba738,1,0,Self-Supervised Learning of Face Representations for Video Face Clustering,"Analyzing the story behind TV series and movies often requires understanding who the characters are and what they are doing. With improving deep face models, this may seem like a solved problem. However, as face detectors get better, clustering/identification needs to be revisited to address increasing diversity in facial appearance. In this paper, we address video face clustering using unsupervised methods. Our emphasis is on distilling the essential information, identity, from the representations obtained using deep pre-trained face networks. We propose a self-supervised Siamese network that can be trained without the need for video/track based supervision, and thus can also be applied to image collections. We evaluate our proposed method on three video face clustering datasets. The experiments show that our methods outperform current state-of-the-art methods on all datasets. Video face clustering is lacking a common benchmark as current works are often evaluated with different metrics and/or different sets of face tracks. The datasets and code are available at https://github.com/vivoutlaw/SSIAM.",2019,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),1903.01,10.1109/FG.2019.8756609,https://arxiv.org/pdf/1903.01000.pdf
baf52a7826e7a29ba68d39b572529d1dd58f1de4,0,1,A Deep Image Compression Framework for Face Recognition,"Face recognition technology has advanced rapidly and has been widely used in various applications. Due to the huge amount of data of face images in large-scale face recognition tasks and the large computing resource cost required correspondingly, there is a requirement for a face image compression approach that is highly suitable for face recognition tasks. In this paper, we propose a deep convolutional autoencoder compression network for face recognition tasks. In compression process, deep features are extracted from the original image by a Compression Network (CompNet) to produce a compact representation of the original image, which is then encoded and saved by an existing codec PNG. In reconstruction process, this compact representation is utilized by a Reconstruction Network (RecNet) to generate a restored image of the original one. In order to improve the face recognition accuracy when the compression framework is used in a face recognition system, we combine the CompNet and RecNet with an existing face recognition network for joint optimization. We test the proposed scheme and find that after joint optimization, the Labeled Faces in the Wild (LFW) dataset compressed by our compression framework has higher face verification accuracy than that compressed by JPEG2000, and is much higher than that compressed by JPEG.",2019,2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI),1907.01714,10.1109/CCHI.2019.8901914,https://arxiv.org/pdf/1907.01714.pdf
bb1eda8def02ae0481bab46054b14e9e2b47d208,0,1,Ranked List Loss for Deep Metric Learning,"The objective of deep metric learning (DML) is to learn embeddings that can capture semantic similarity information among data points. Existing pairwise or tripletwise loss functions used in DML are known to suffer from slow convergence due to a large proportion of trivial pairs or triplets as the model improves. To improve this, rankingmotivated structured losses are proposed recently to incorporate multiple examples and exploit the structured information among them. They converge faster and achieve state-of-the-art performance. In this work, we present two limitations of existing ranking-motivated structured losses and propose a novel ranked list loss to solve both of them. First, given a query, only a fraction of data points is incorporated to build the similarity structure. Consequently, some useful examples are ignored and the structure is less informative. To address this, we propose to build a setbased similarity structure by exploiting all instances in the gallery. The samples are split into a positive set and a negative set. Our objective is to make the query closer to the positive set than to the negative set by a margin. Second, previous methods aim to pull positive pairs as close as possible in the embedding space. As a result, the intraclass data distribution might be dropped. In contrast, we propose to learn a hypersphere for each class in order to preserve the similarity structure inside it. Our extensive experiments show that the proposed method achieves state-of-the-art performance on three widely used benchmarks.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1903.03238,10.1109/CVPR.2019.00535,https://pureadmin.qub.ac.uk/ws/files/168412256/RankedNoise.pdf
bb8100a12bf0ff334f24e730c8f5e9162ccb5b92,1,0,Challenge report: Recognizing Families In the Wild Data Challenge,"This paper is a brief report to our submission to the Recognizing Families In the Wild Data Challenge (4th Edition), in conjunction with FG 2020 Forum. Automatic kinship recognition has attracted many researchers' attention for its full application, but it is still a very challenging task because of the limited information that can be used to determine whether a pair of faces are blood relatives or not. In this paper, we studied previous methods and proposed our method. We try many methods, like deep metric learning-based, to extract deep embedding feature for every image, then determine if they are blood relatives by Euclidean distance or method based on classes. Finally, we find some tricks like sampling more negative samples and high resolution that can help get better performance. Moreover, we proposed a symmetric network with a binary classification based method to get our best score in all tasks.",2020,ArXiv,2006.00154,,https://arxiv.org/pdf/2006.00154.pdf
bbcec0e050148bff21c3d7a1f0879beb7fbeb0d9,0,1,"Deep learning achieves perfect anomaly detection on 108, 308 retinal images including unlearned diseases","Optical coherence tomography (OCT) scanning is useful in detecting various retinal diseases. However, there are not enough ophthalmologists who can diagnose retinal OCT images in much of the world. To provide OCT screening inexpensively and extensively, an automated diagnosis system is indispensable. Although many machine learning techniques have been presented for assisting ophthalmologists in diagnosing retinal OCT images, there is no technique that can diagnose independently without relying on an ophthalmologist, i.e., there is no technique that does not overlook any anomaly, including unlearned diseases. As long as there is a risk of overlooking a disease with a technique, ophthalmologists must double-check even those images that the technique classifies as normal. Here, we show that our deep-learning-based binary classifier (normal or abnormal) achieved a perfect classification on 108,308 two-dimensional retinal OCT images, i.e., true positive rate = 1.000000 and true negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000. Although the test set included three types of diseases, two of these were not used for training. However, all test images were correctly classified. Furthermore, we demonstrated that our scheme was able to cope with differences in patient race. No conventional approach has achieved the above performances. Our work has a sufficient possibility of raising automated diagnosis techniques for retinal OCT images from ""assistant for ophthalmologists"" to ""independent diagnosis system without ophthalmologists"".",2020,ArXiv,2001.05859,10.2139/ssrn.3581363,https://arxiv.org/pdf/2001.05859.pdf
bc2a5d3d40fc622a122f6dbf9363407258f9681a,1,0,A novel classification-selection approach for the self updating of template-based face recognition systems,"Abstract The boosting on the need of security notably increased the amount of possible facial recognition applications, especially due to the success of the Internet of Things (IoT) paradigm. However, although handcrafted and deep learning-inspired facial features reached a significant level of compactness and expressive power, the facial recognition performance still suffers from intra-class variations such as ageing, facial expressions, lighting changes, and pose. These variations cannot be captured in a single acquisition and require multiple acquisitions of long duration, which are expensive and need a high level of collaboration from the users. Among others, self-update algorithms have been proposed in order to mitigate these problems. Self-updating aims to add novel templates to the users’ gallery among the inputs submitted during system operations. Consequently, computational complexity and storage space tend to be among the critical requirements of these algorithms. The present paper deals with the above problems by a novel template-based self-update algorithm, able to keep over time the expressive power of a limited set of templates stored in the system database. The rationale behind the proposed approach is in the working hypothesis that a dominating mode characterises the features’ distribution given the client. Therefore, the key point is to select the best templates around that mode. We propose two methods, which are tested on systems based on handcrafted features and deep-learning-inspired autoencoders at the state-of-the-art. Three benchmark data sets are used. Experimental results confirm that, by effective and compact feature sets which can support our working hypothesis, the proposed classification-selection approaches overcome the problem of manual updating and, in case, stringent computational requirements.",2020,Pattern Recognit.,1911.12688,10.1016/j.patcog.2019.107121,https://arxiv.org/pdf/1911.12688.pdf
bc41ca07f25c16d98697dd5cb348245e4c9089d4,0,1,A Deep Marginal-Contrastive Defense against Adversarial Attacks on 1D Models,"Deep learning algorithms have been recently targeted by attackers due to their vulnerability. Several research studies have been conducted to address this issue and build more robust deep learning models. Non-continuous deep models are still not robust against adversarial, where most of the recent studies have focused on developing attack techniques to evade the learning process of the models. One of the main reasons behind the vulnerability of such models is that a learning classifier is unable to slightly predict perturbed samples. To address this issue, we propose a novel objective/loss function, the socalled marginal contrastive, which enforces the features to lie under a specified margin to facilitate their prediction using deep convolutional networks (i.e., Char-CNN). Extensive experiments have been conducted on continuous cases (e.g., UNSW NB15 dataset) and discrete ones (i.e, eight-large-scale datasets [32]) to prove the effectiveness of the proposed method. The results revealed that the regularization of the learning process based on the proposed loss function can improve the performance of Char-CNN.",2020,,2012.04734,,https://arxiv.org/pdf/2012.04734.pdf
bc45cd9964702333569f3136ce7eb9e8581fbb1a,0,1,Occluded Face Recognition in the Wild by Identity-Diversity Inpainting,"Face recognition has achieved advanced development by using convolutional neural network (CNN) based recognizers. Existing recognizers typically demonstrate powerful capacity in recognizing un-occluded faces, but often suffer from accuracy degradation when directly identifying occluded faces. This is mainly due to insufficient visual and identity cues caused by occlusions. On the other hand, generative adversarial network (GAN) is particularly suitable when it needs to reconstruct visually plausible occlusions by face inpainting. Motivated by these observations, this paper proposes identity-diversity inpainting to facilitate occluded face recognition. The core idea is integrating GAN with an optimized pre-trained CNN recognizer which serves as the third player to compete with the generator by distinguishing diversity within the same identity class. To this end, a collect of identity-centered features is applied in the recognizer as supervision to enable the inpainted faces clustering towards their identity centers. In this way, our approach can benefit from GAN for reconstruction and CNN for representation, and simultaneously addresses two challenging tasks, face inpainting and face recognition. Experimental results compared with 4 state-of-the-arts prove the efficacy of the proposed approach.",2020,IEEE Transactions on Circuits and Systems for Video Technology,,10.1109/TCSVT.2020.2967754,
bc4b82d4ce44f99a127d4b745d5266a0d64192c0,0,1,Face Detection Based on Receptive Field Enhanced Multi-Task Cascaded Convolutional Neural Networks,"With the continuous development of deep learning, face detection methods have made the greatest progress. For real-time detection, cascade CNN based on the lightweight model is still the dominant structure that predicts face in a coarse-to-fine manner with strong generalization ability. Compared to other methods, it is not required for a fixed size of the input. However, MTCNN still has poor performance in detecting tiny targets. To improve model generalization ability, we propose a Receptive Field Enhanced Multi-Task Cascaded CNN. This network takes advantage of the Inception-V2 block and receptive field block to enhance the feature discriminability and robustness for small targets. The experimental results show that the performance of our network is improved by 1.08% on the AFW, 2.84% on the PASCAL FACE, 1.31% on the FDDB, and 2.3%, 2.1%, and 6.6% on the three sub-datasets of the WIDER FACE benchmark in comparison with MTCNN respectively. Furthermore, our structure uses 16% fewer parameters.",2020,IEEE Access,,10.1109/ACCESS.2020.3023782,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09195457.pdf
bc543035c4e0fff4d7eec1f4cc6cfe2725dbd269,1,1,Class-Balanced Training for Deep Face Recognition,"The performance of deep face recognition depends heavily on the training data. Recently, larger and larger datasets have been developed for the training of deep models. However, most face recognition training sets suffer from the class imbalance problem, and most studies ignore the benefit of optimizing dataset structures. In this paper, we study how class-balanced training can promote face recognition performance. A medium-scale face recognition training set BUPT-CBFace is built by exploring the optimal data structure from massive data. This publicly available dataset is characterized by the uniformly distributed sample size per class, as well as the balance between the number of classes and the number of samples in one class. Experimental results show that deep models trained with BUPT-CBFace can not only achieve comparable results to larger-scale datasets such as MS-Celeb-1M but also alleviate the problem of recognition bias.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00420,
bc6b963d47b466edc15d232938186d414ed38864,0,1,Deepfakes Detection with Automatic Face Weighting,"Altered and manipulated multimedia is increasingly present and widely distributed via social media platforms. Advanced video manipulation tools enable the generation of highly realistic-looking altered multimedia. While many methods have been presented to detect manipulations, most of them fail when evaluated with data outside of the datasets used in research environments. In order to address this problem, the Deepfake Detection Challenge (DFDC) provides a large dataset of videos containing realistic manipulations and an evaluation system that ensures that methods work quickly and accurately, even when faced with challenging data. In this paper, we introduce a method based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) that extracts visual and temporal features from faces present in videos to accurately detect manipulations. The method is evaluated with the DFDC dataset, providing competitive results compared to other techniques.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2004.12027,10.1109/CVPRW50498.2020.00342,https://arxiv.org/pdf/2004.12027.pdf
bc6edab6ad1b1158a899c674baf5e26cf60294f5,1,1,ClusterFace: Clustering-Driven Deep Face Recognition,"Recent years, image-based 2D face recognition has achieved human-level performance with the big breakthrough of deep learning paradigm. However, almost all of the existing deep face recognition methods depend on millions and millions of labeled 2D face images from different individual for supervised deep learning. In this case, face labelling becomes the pain point of deep face recognition. To solve this issue, we propose a novel clustering driven unsupervised deep face recognition framework, namely ClusterFace. In particular, our framework firstly assume that we already have a well-trained deep face model and a large number of face images without any labels. Then, all these face images are represented by this deep face model and then unsupervised clustered into different clusters using a certain clustering algorithm. Finally, these clustering-based face labelling results are employed to train a new deep CNN model for face recognition. Experimental results demonstrated that the proposed framework with a simple Mini-batch K-Means clustering algorithm can achieve surprising state-of-the-art performance (99.41%) on the LFW dataset. We also presented an intuitional explanation the reason of achieving good performance of our framework and also demonstrated its robustness to the choice of the number of clusters and the amount of unlabeled face images.",2018,CCBR,,10.1007/978-3-319-97909-0_41,
bc8aee2f6b7d9d7b126d1e8dbcd33ea355aa3b37,1,1,SeqFace: Make full use of sequence information for face recognition,"Deep convolutional neural networks (CNNs) have greatly improved the Face Recognition (FR) performance in recent years. Almost all CNNs in FR are trained on the carefully labeled datasets containing plenty of identities. However, such high-quality datasets are very expensive to collect, which restricts many researchers to achieve state-of-the-art performance. In this paper, we propose a framework, called SeqFace, for learning discriminative face features. Besides a traditional identity training dataset, the designed SeqFace can train CNNs by using an additional dataset which includes a large number of face sequences collected from videos. Moreover, the label smoothing regularization (LSR) and a new proposed discriminative sequence agent (DSA) loss are employed to enhance discrimination power of deep face features via making full use of the sequence data. Our method achieves excellent performance on Labeled Faces in the Wild (LFW), YouTube Faces (YTF), only with a single ResNet. The code and models are publicly available on-line (this https URL).",2018,ArXiv,1803.06524,,https://arxiv.org/pdf/1803.06524.pdf
bc91183f2c4193d7189d11e20f288811bc34da79,1,0,Discriminant Deep Feature Learning based on joint supervision Loss and Multi-layer Feature Fusion for heterogeneous face recognition,"Abstract Heterogeneous face recognition (HFR) is still a challenging problem in computer vision community due to large appearance difference between near infrared (NIR) and visible light (VIS) modalities. Recently, breakthroughs have been made for traditional face recognition by applying deep learning on a huge amount of labeled VIS face samples. However, the same deep learning approach cannot be simply applied to HFR task due to large domain difference as well as insufficient pairwise images in different modalities during training. In general, the pooling layer of deep network can play the role of feature reduction, but also lead to the loss of useful face information, resulting in a decrease in the performance of HFR problem. It is important to eliminate modal-related information and retain more facial identity information. In this paper, we propose a novel method called Discriminant Deep Feature Learning Based on Joint Supervision Loss and Multi-layer Feature Fusion (DDFLJM) for HFR task. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new loss function called Scatter Loss (SL), which embeds both inter- and intra-class information for effectively training the deep model. To make full use of the various layers of the deep network, a Dimension Reduction Block (DRB) is designed to effectively extract the auxiliary features on multiple mid-level layers. An orthogonality constraint is introduced to the DRB block to reduce spectrum variations of two different modalities. The proposed SL is applied to multiple layers of network for joint supervision training, which enables multiple layers of the network to obtain discriminative identity features. Moreover, a Modified Gate Two-stream Neural Network (MGTNN) is adopted to fuse multiple-layer features. Extensive experiments are carried out on two challenging NIR-VIS HFR datasets CASIA NIR-VIS 2.0 and Oulu-CASIA NIR-VIS, demonstrating the superiority of the proposed method.",2019,Comput. Vis. Image Underst.,,10.1016/J.CVIU.2019.04.003,
bc9ac5ddc52f1987ec13f0a561653d5a1131b4e5,1,1,Evaluación del rendimiento de Facenet,"Este trabajo presenta una evaluacion de la variacion del rendimiento de la red neuronal para reconocimiento de caras Facenet cuando varian algunas condiciones de la imagen de entrada. Facenet fue introducida en 2015 por un equipo de Google como evolucion del modelo Inception y fue entrenada usando un esquema de tripletas (imagen base, imagen de la misma identidad, imagen de distinta identidad) y una funcion de perdida que favorecia distancias pequenas entre los descriptores generados para imagenes de la misma identidad y distancias grandes entre los descriptores de imagenes de distinta identidad. Se presentaba Facenet como una red con resultados asombrosamente invariantes ante aspectos como la posicion, la iluminacion, la oclusion o la edad del individuo que variaba de manera significativa el estado del arte en reconocimiento de caras. La evaluacion se ha llevado a cabo utilizando el dataset de estudio Multi-Pie para evaluar la variacion en rendimiento de Facenet cuando se modifican las condiciones de iluminacion, gesto y posicion de camara, y un conjunto de imagenes generadas sinteticamente a partir de una seleccion de imagenes del dataset VGGFace2 para evaluar la variacion del rendimiento cuando se modifican aspectos como la resolucion de la imagen, el area de recorte de la cara, la tonalidad, la saturacion, la intensidad o el angulo de giro de la imagen. Se ha podido valorar tambien la variacion de rendimiento en relacion con el genero de la persona.---ABSTRACT---This works presents a performance evaluation of the face recognition neural network Facenet when some conditions of the input image are modified. Facenet was introduced in 2015 by a team from Google as an a evolution of the Inception model and was trained using a scheme of triplets (anchor image, same identity image, different identity image) and a loss function that favored small distances between descriptors generated from same identity images and larger distances between descriptors generated from images of different identity. Facenet was presented as a network with incredible invariance to aspects such as position, lighting, occlusion or the age of the individual. It significantly varied the state of the art in face recognition. The evaluation has been carried out using the Multi-Pie studio dataset to evaluate the variation in performance of Facenet when the lighting, gesture or camera position conditions are modified, and using a set of synthetically generated images from a selection of images from the VGGFace2 dataset to evaluate the performance variation when aspects such as the resolution of the image, the cropping of the face, the hue, the saturation, the intensity or the angle of rotation of the image are modified. It has also been possible to assess the performance variation regarding the gender of the person.",2019,,,,https://pdfs.semanticscholar.org/bc9a/c5ddc52f1987ec13f0a561653d5a1131b4e5.pdf
bcfba69c2fadf2efea83be12fda2601f8d4681af,1,0,Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss,"Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains.",2019,NeurIPS,1906.07413,,https://arxiv.org/pdf/1906.07413.pdf
bd09a441161fa9dcb9dc7cb12069b92dd29e7871,1,1,Does Face Recognition Accuracy Get Better With Age? Deep Face Matchers Say No,"Previous studies generally agree that face recognition accuracy is higher for older persons than for younger persons. But most previous studies were before the wave of deep learning matchers, and most considered accuracy only in terms of the verification rate for genuine pairs. This paper investigates accuracy for age groups 16-29, 30-49 and 50-70, using three modern deep CNN matchers, and considers differences in the impostor and genuine distributions as well as verification rates and ROC curves. We find that accuracy is lower for older persons and higher for younger persons. In contrast, a pre deep learning matcher on the same dataset shows the traditional result ofhigher accuracy for older persons, although its overall accuracy is much lower than that of the deep learning matchers. Comparing the impostor and genuine distributions, we conclude that impostor scores have a larger effect than genuine scores in causing lower accuracy for the older age group. We also investigate the effects of training data across the age groups. Our results show that fine-tuning the deep CNN models on additional images ofolder persons actually lowers accuracy for the older age group. Also, we fine-tune and train from scratch two models using age-balanced training datasets, and these results also show lower accuracy for older age group. These results argue that the lower accuracy for the older age group is not due to imbalance in the original training data.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1911.06396,10.1109/WACV45572.2020.9093357,https://arxiv.org/pdf/1911.06396.pdf
bd0e357ef2abd28498cccafbaacdfa307f50ad48,0,1,Supervised Joint Domain Learning for Vehicle Re-Identification,"Vehicle Re-Identification (Re-ID), which aims at matching vehicle identities across different cameras, is a critical technique for traffic analysis in a smart city. It suffers from varying image quality and challenging visual appearance characteristics. A solution for enhancing the feature robustness is by training Convolutional Neural Networks on multiple datasets simultaneously. However, the larger set of training data does not guarantee performance improvement due to misaligned feature distribution between domains. To mitigate the domain gap, we propose a Joint Domain ReIdentification Network (JDRN) to improve the feature by disentangling domain-invariant information and encourage a shared feature space between domains. With our JDRN, we perform favorably against state-of-the-arts methods on the public VeRi-776 dataset and obtain promising results on the 2019 AI City Challenge.",2019,CVPR Workshops,,,http://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Liu_Supervised_Joint_Domain_Learning_for_Vehicle_Re-Identification_CVPRW_2019_paper.pdf
bd844896eb0b0270afb1ee1622460643676c84a3,0,1,Lightweight Deep Embeddings Fusion Methods for Face Verification on Mobile Devices,"With the emergence of deep learning technology, face verification performance on the LFW benchmark outperformed human-level accuracy. In recent, face verification technique is applied to mobile devices to authenticate owner of them. Face verification on mobile devices needs to be considered with both performance and computational cost which is trade-off. In this paper, we propose lightweight network architectures and fusion methods for efficient face verification task on mobile devices. The proposed lightweight networks achieved comparable accuracy on both LFW and YTF datasets with only about 9x fewer size of model and 2x shorter inference time than Inception-resent-v1 which is a deeper network architecture. In addition, proposed score-level fusion method shows improvement of 2.38% on VR@FAR=1e-6 on LFW BLUFR and that of 0.28% on accuracy on YTF than single lightweight network.",2019,2019 International Conference on Information and Communication Technology Convergence (ICTC),,10.1109/ictc46691.2019.8939565,
bda44f9825d7c6888de3cb3b4a0fb332f267d17f,1,0,Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning,"We investigate learning a ConvNet classifier with class-imbalanced data. We found that a ConvNet over-fits significantly to the minor classes that do not have sufficient training instances, even if it is trained using vanilla empirical risk minimization (ERM). We conduct a series of analysis and argue that feature deviation between the training and test instances serves as the main cause. We propose to incorporate class-dependent temperatures (CDT) in learning a ConvNet: CDT forces the minor-class instances to have larger decision values in training, so as to compensate for the effect of feature deviation in testing. We validate our approach on several benchmark datasets and achieve promising results. Our studies further suggest that class-imbalanced data affects traditional machine learning and recent deep learning in very different ways. We hope that our insights can inspire new ways of thinking in resolving class-imbalanced deep learning.",2020,ArXiv,2001.01385,,https://arxiv.org/pdf/2001.01385.pdf
bdef93491b2eec2e71b9ba14fd304ff5eebf5188,0,1,Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems,"Federated learning (FL) is a distributed deep learning method that enables multiple participants, such as mobile and IoT devices, to contribute a neural network while their private training data remains in local devices. This distributed approach is promising in the mobile systems where have a large corpus of decentralized data and require high privacy. However, unlike the common datasets, the data distribution of the mobile systems is imbalanced which will increase the bias of model. In this article, we demonstrate that the imbalanced distributed training data will cause an accuracy degradation of FL applications. To counter this problem, we build a self-balancing FL framework named Astraea, which alleviates the imbalances by 1) Z-score-based data augmentation, and 2) Mediator-based multi-client rescheduling. The proposed framework relieves global imbalance by adaptive data augmentation and downsampling, and for averaging the local imbalance, it creates the mediator to reschedule the training of clients based on Kullback–Leibler divergence (KLD) of their data distribution. Compared with FedAvg, the vanilla FL algorithm, Astraea shows +4.39 and +6.51 percent improvement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10 datasets, respectively. Meanwhile, the communication traffic of Astraea is reduced by 75 percent compared to FedAvg.",2021,IEEE Transactions on Parallel and Distributed Systems,,10.1109/tpds.2020.3009406,
be4432bb2b074213a6e47005e6a3a99b7604e438,1,0,Virtual Big Data for GAN Based Data Augmentation,"Researchers deal with the class imbalanced problem in many real-world applications and GAN based data augmentation is considered as an efficient approach to address this problem. GANs need a huge training data to generate efficient augmented data. However, the required sufficient training data is not available in many research areas. In this paper, we introduce a new concept called virtual big data to address this problem. We prove that, virtual big data can provide the GANs sufficient training data to generate efficient augmented data with less mode collapse and vanishing generator gradients problems. We show that, the curse of dimensionality which is considered as a negative factor in machine learning can play a positive role to solve vanishing generator gradients via making discriminator less perfect. First, we transform the training data from n dimensional space into m dimensional space where, $\mathrm{m}=\mathrm{c}*\mathrm{n}$ and c is concatenation factor. To do so, c different training instances are selected and concatenated to each other to form a $\mathrm{c}*\mathrm{n}$ dimensional instance. Increasing the dimension of training data from n to $\mathrm{c}*\mathrm{n}$ is key to increase the number of training instances from N to $\mathrm{C}(\mathrm{N}, \mathrm{c})$. Transformed training data are called virtual big data since they differ original training instances in terms of size and dimension. Our experiments show that, V-GAN, a GAN trained by virtual big data can outperform standard GANs when it comes to deal with extremely scarce training data. Furthermore, V-GAN can outperform traditional oversampling techniques in terms of precision, F1 score and Area Under Curve (AUC) score.",2019,2019 IEEE International Conference on Big Data (Big Data),,10.1109/BigData47090.2019.9006268,
be67eef9f44e95305370bb5cb35977ce3bfd466e,1,1,Regularization and Iterative Initialization of Softmax for Fast Training of Convolutional Neural Networks,"A softmax regularizer is proposed, a simple and elegant constraint on softmax weight distribution in the training process. Since the direct estimation of feature centers is neither memory efficient nor robust, the proposed regularizer utilizes the relations between feature centers and the classifier weights by adding constraints on the distances between softmax weight vectors. This apparently enlarges the distances between softmax weights to benefit the separation of different classes, and provides extra gradients for the optimization of softmax in order to speed up the training process.Furthermore, we argue that the massive amount of softmax parameters is the main cause that makes the network converge slowly, especial in the class classification tasks with large class number such as face recognition. Motivated by the analysis of the relations between deep features and softmax weights, a fast training process is presented, which splits the training into multiple stages and alternates training and initializing softmax weights for fast convergence when the class number is large. Since the softmax weights can be initialized with estimated deep feature centers, the scale of training data can be gradually increased along the stages. By this procedure, the total training computation cost can be reduced. To validate its effectiveness, our approach is applied on both face recognition and image classification tasks. It obtains comparable performance with the state-of-the-art methods while boasting a faster training process.",2019,2019 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN.2019.8852459,
be716a691df9f131f43c9dc2656fcca15a601502,0,1,Cancelable fusion-based face recognition,"Biometric recognition refers to the automated process of recognizing individuals using their biometric patterns. Recent advancements in deep learning and computer vision indicate that generic descriptors which are extracted using convolutional neural networks (CNNs) could represent complex image characteristics. This paper presents a number of cancelable fusion-based face recognition (FR) methods; region-based, multi-biometric and hybrid-features. The former included methods incorporate the use of CNNs to extract deep features (DFs). A fusion network combines the DFs to obtain a discriminative facial descriptor. Cancelabilitiy is provided using bioconvolving as an encryption method. In the region-based method, the DFs are extracted from different face regions. The multi-biometric method uses different biometric traits to train multiple CNNs. The hybrid-features method merges the merits of deep-learned features and hand-crafted features to obtain a more representative output. Also, an efficient CNN model is proposed. Experimental results on various datasets prove that; (a) the proposed CNN model achieves remarkable results compared to other state-of-the-art CNNs, (b) region-based method is superior to multi-biometric and hybrid-features methods and (c) the utilization of bio-convolving method increases the system security with a slight degradation in the recognition accuracy.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-07848-y,
be913bd97a8071e7550140249d3b9e73d1d58d63,0,1,Densely Connected Time Delay Neural Network for Speaker Verification,"Time delay neural network (TDNN) has been widely used in speaker verification tasks. Recently, two TDNN-based models, including extended TDNN (E-TDNN) and factorized TDNN (F-TDNN), are proposed to improve the accuracy of vanilla TDNN. But E-TDNN and F-TDNN increase the number of parameters due to deeper networks, compared with vanilla TDNN. In this paper, we propose a novel TDNN-based model, called densely connected TDNN (D-TDNN), by adopting bottleneck layers and dense connectivity. D-TDNN has fewer parameters than existing TDNN-based models. Furthermore, we propose an improved variant of D-TDNN, called D-TDNN-SS, to employ multiple TDNN branches with short-term and longterm contexts. D-TDNN-SS can integrate the information from multiple TDNN branches with a newly designed channel-wise selection mechanism called statistics-and-selection (SS). Experiments on VoxCeleb datasets show that both D-TDNN and D-TDNN-SS can outperform existing models to achieve stateof-the-art accuracy with fewer parameters, and D-TDNN-SS can achieve better accuracy than D-TDNN.",2020,INTERSPEECH,,10.21437/interspeech.2020-1275,https://pdfs.semanticscholar.org/bd15/87fba28e963769393dbbd19f92ee9a155915.pdf
bebaf8501a365d0c3131f5baeeb5ffd712ffd251,0,1,Investigation of Specaugment for Deep Speaker Embedding Learning,"SpecAugment is a newly proposed data augmentation method for speech recognition. By randomly masking bands in the log Mel spectogram this method leads to impressive performance improvements. In this paper, we investigate the usage of SpecAugment for speaker verification tasks. Two different models, namely 1-D convolutional TDNN and 2-D convolutional ResNet34, trained with either Softmax or AAM-Softmax loss, are used to analyze SpecAugment’s effectiveness. Experiments are carried out on the Voxceleb and NIST SRE 2016 dataset. By applying SpecAugment to the original clean data in an on-the-fly manner without complex off-line data augmentation methods, we obtained 3.72% and 11.49% EER for NIST SRE 2016 Cantonese and Tagalog, respectively. For Voxceleb1 evaluation set, we obtained 1.47% EER.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053481,
bec253e82076dc363b8fd72d5c8fadf8f5b7e475,0,1,Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose,"Real-world talking faces often accompany with natural head movement. However, most existing talking face video generation methods only consider facial animation with fixed head pose. In this paper, we address this problem by proposing a deep neural network model that takes an audio signal A of a source person and a very short video V of a target person as input, and outputs a synthesized high-quality talking face video with personalized head pose (making use of the visual information in V), expression and lip synchronization (by considering both A and V). The most challenging issue in our work is that natural poses often cause in-plane and out-of-plane head rotations, which makes synthesized talking face video far from realistic. To address this challenge, we reconstruct 3D face animation and re-render it into synthesized frames. To fine tune these frames into realistic ones with smooth background transition, we propose a novel memory-augmented GAN module. By first training a general mapping based on a publicly available dataset and fine-tuning the mapping using the input short video of target person, we develop an effective strategy that only requires a small number of frames (about 300 frames) to learn personalized talking behavior including head pose. Extensive experiments and two user studies show that our method can generate high-quality (i.e., personalized head movements, expressions and good lip synchronization) talking face videos, which are naturally looking with more distinguishing head movement effects than the state-of-the-art methods.",2020,,2002.10137,,https://arxiv.org/pdf/2002.10137.pdf
bec93d27463fc009f378801f026f1e954c32c21b,1,0,Disentangled Spectrum Variations Networks for NIR–VIS Face Recognition,"Surveillance cameras often capture near infrared images since it provides a low-cost and effective solution to acquire high-quality images under low-light environments. However, visual versus near infrared (VIS-NIR) heterogeneous face recognition (HFR) is still a challenging issue in computer vision community due to the gap between sensing patterns of different spectrums as well as the lack of sufficient training samples. To solve the above problem, in this paper, we present an effective Disentangled Spectrum Variations Networks (DSVNs) for VIS-NIR HFR. Two key strategies are introduced to the DSVNs for disentangling spectrum variations between two domains: Spectrum-adversarial Discriminative Feature Learning (SaDFL) and Step-wise Spectrum Orthogonal Decomposition (SSOD). The SaDFL consists of Identity-Discriminative subnetwork (IDNet) and Auxiliary Spectrum Adversarial subnetwork (ASANet). On the one hand, the IDNet is composed of a generator <inline-formula><tex-math notation=""LaTeX"">$G_H$</tex-math></inline-formula> and a discriminator <inline-formula><tex-math notation=""LaTeX"">$D_U$</tex-math></inline-formula> for extracting identity-discriminative feature. On the other hand, the ASANet is built by a generator <inline-formula><tex-math notation=""LaTeX"">$G_H$</tex-math></inline-formula> and a discriminator <inline-formula><tex-math notation=""LaTeX"">$D_M$</tex-math></inline-formula> for eliminating modality-variant spectrum information under the guidance of the discriminator <inline-formula><tex-math notation=""LaTeX"">$D_M$</tex-math></inline-formula>. The identity-label and modality-label HFR datasets are used to train the DSVNs with triplet loss. Both IDNet and ASANet can jointly enhance the domain-invariant feature representations via an adversarial learning. Furthermore, to disentangle spectrum variations effectively as well as making identity information and modality information unrelated to each other, we present a new topology of connection block called Disentangled Spectrum Variations (DSV). An orthogonality constraint is imposed to DSV at the convolution level for channel-wise orthogonal decomposition between the modality-invariant identity information and modality-variant spectrum information. In particular, the SSOD is built by stacking multiple modularized mirco-block DSV, and thereby enjoys the benefits of disentangling spectrum variation step by step. Moreover, we investigate the similarity calculation method to further improve the HFR performance. To sum up, the designed DSVNs leads to a purification of identity information as well as an elimination of modality information. Extensive experiments are carried out on two challenging NIR-VIS HFR datasets CASIA NIR-VIS 2.0 and Oulu-CASIA NIR-VIS, demonstrating the superiority of the proposed method.",2020,IEEE Transactions on Multimedia,,10.1109/TMM.2019.2938685,
bf01a87c2bf132702fc45365a9596985b4ac3358,0,1,Feature Ensemble Networks with Re-Ranking for Recognizing Disguised Faces in the Wild,"Recognizing a person's face images with intentional/unintentional disguising effects such as make-up, plastic surgery, artificial wearables (hats, eye-glasses) is a challenging task. We propose a Feature EnsemBle Network (FEBNet) for recognizing Disguised Faces in the Wild (DFW). FEBNet encompasses multiple base networks (SE-ResNet50, Inception-ResNet-V1) pretrained on large-scale face recognition datasets (MS-Celeb-1M, VGGFace2) and fine-tuned on DFW training dataset. During the fine-tuning phase, we propose to use two novel objective functions, namely, 1) Category loss, 2) Impersonator Triplet loss along with two prevalent objective functions: Identity loss, Inter-person Triplet loss. To further improve the performance, we apply a state-of-the-art re-ranking strategy as a post-processing step. Extensive ablation studies and evaluation results show that FEBNet significantly outperforms the baseline models.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00066,http://openaccess.thecvf.com/content_ICCVW_2019/papers/DFW/Subramaniam_Feature_Ensemble_Networks_with_Re-Ranking_for_Recognizing_Disguised_Faces_in_ICCVW_2019_paper.pdf
bf037ae7e27d60514428126bb840f3674fac981f,1,1,Continual Representation Learning for Biometric Identification,"With the explosion of digital data in recent years, continuously learning new tasks from a stream of data without forgetting previously acquired knowledge has become increasingly important. In this paper, we propose a new continual learning (CL) setting, namely ``continual representation learning'', which focuses on learning better representation in a continuous way. We also provide two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant. In contrast to requiring the model to recognize more learned classes, we aim to learn feature representation that can be better generalized to not only previously unseen images but also unseen classes/identities. For the new setting, we propose a novel approach that performs the knowledge distillation over a large number of identities by applying the neighbourhood selection and consistency relaxation strategies to improve scalability and flexibility of the continual learning model. We demonstrate that existing CL methods can improve the representation in the new setting, and our method achieves better results than the competitors.",2020,ArXiv,2006.04455,,https://arxiv.org/pdf/2006.04455.pdf
bf064d345a232ac53706405b0e5810974b1a4e6f,1,1,FAN: Feature Adaptation Network for Surveillance Face Recognition and Normalization,"This paper studies face recognition (FR) and normalization in surveillance imagery. Surveillance FR is a challenging problem that has great values in law enforcement. Despite recent progress in conventional FR, less effort has been devoted to surveillance FR. To bridge this gap, we propose a Feature Adaptation Network (FAN) to jointly perform surveillance FR and normalization. Our face normalization mainly acts on the aspect of image resolution, closely related to face super-resolution. However, previous face super-resolution methods require paired training data with pixel-to-pixel correspondence, which is typically unavailable between real low- and high-resolution faces. Our FAN can leverage both paired and unpaired data as we disentangle the features into identity and non-identity components and adapt the distribution of the identity features, which breaks the limit of current face super-resolution methods. We further propose a random scale augmentation scheme to learn resolution robust identity features, with advantages over previous fixed scale augmentation. Extensive experiments on LFW, WIDER FACE, QUML-SurvFace and SCface datasets have demonstrated the superiority of our proposed method compared to the state of the arts on surveillance face recognition and normalization.",2019,ArXiv,1911.1168,,https://arxiv.org/pdf/1911.11680.pdf
bf0e3b0fba3a6cc2ef15d2fc91a8eae7056a032a,0,1,Adaptive Convolution Local and Global Learning for Class-Level Joint Representation of Facial Recognition With a Single Sample Per Data Subject,"Due to the absence of training samples and intraclass variation, the extraction of discriminative facial features and construction of powerful classifiers have bottlenecks in improving the performance of facial recognition (FR) with a single sample per data subject (SSPDS). In this paper, we propose to learn regional adaptive convolution features that are locally and globally discriminative to facial identity and robust to facial variation. Then, a novel class-level joint representation framework is presented to exploit the distinctiveness and class-level commonality of different facial features. In the proposed class-level joint representation with regional adaptive convolution features (CJR-RACF), both discriminative facial features that are robust to facial variations and powerful representations for classification with generic facial variations have been fully exploited. Furthermore, the gallery discrimination is extracted by our proposed weight-embedded supervision in the training phase (denoted by CJR-RACFw), which is conducive to more specific features for FR with SSPDS. CJR-RACF and CJR-RACFw have been evaluated on several popular databases, including the large-scale CMU Multi-PIE, LFW, Megaface, and VGGFace datasets. Experimental results demonstrate the much higher robustness and effectiveness of the proposed methods compared to the state-of-the-art methods.",2020,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.2965301,
bf6c7328f93a3b90a7bb1a3957babe0087d74ad8,1,1,Feature Transfer Learning for Deep Face Recognition with Under-Represented Data,"Despite the large volume of face recognition datasets, there is a significant portion of subjects, of which the samples are insufficient and thus under-represented. Ignoring such significant portion results in insufficient training data. Training with under-represented data leads to biased classifiers in conventionally-trained deep networks. In this paper, we propose a center-based feature transfer framework to augment the feature space of under-represented subjects from the regular subjects that have sufficiently diverse samples. A Gaussian prior of the variance is assumed across all subjects and the variance from regular ones are transferred to the under-represented ones. This encourages the under-represented distribution to be closer to the regular distribution. Further, an alternating training regimen is proposed to simultaneously achieve less biased classifiers and a more discriminative feature representation. We conduct ablative study to mimic the under-represented datasets by varying the portion of under-represented classes on the MS-Celeb-1M dataset. Advantageous results on LFW, IJB-A and MS-Celeb-1M demonstrate the effectiveness of our feature transfer and training strategy, compared to both general baselines and state-of-the-art methods. Moreover, our feature transfer successfully presents smooth visual interpolation, which conducts disentanglement to preserve identity of a class while augmenting its feature space with non-identity variations such as pose and lighting.",2018,,,,
bfab573b064217d0533f2d1e8651746cf295c182,0,1,Single Camera Training for Person Re-identification,"Person re-identification (ReID) aims at finding the same person in different cameras. Training such systems usually requires a large amount of cross-camera pedestrians to be annotated from surveillance videos, which is labor-consuming especially when the number of cameras is large. Differently, this paper investigates ReID in an unexplored single-camera-training (SCT) setting, where each person in the training set appears in only one camera. To the best of our knowledge, this setting was never studied before. SCT enjoys the advantage of low-cost data collection and annotation, and thus eases ReID systems to be trained in a brand new environment. However, it raises major challenges due to the lack of cross-camera person occurrences, which conventional approaches heavily rely on to extract discriminative features. The key to dealing with the challenges in the SCT setting lies in designing an effective mechanism to complement cross-camera annotation. We start with a regular deep network for feature extraction, upon which we propose a novel loss function named multi-camera negative loss (MCNL). This is a metric learning loss motivated by probability, suggesting that in a multi-camera system, one image is more likely to be closer to the most similar negative sample in other cameras than to the most similar negative sample in the same camera. In experiments, MCNL significantly boosts ReID accuracy in the SCT setting, which paves the way of fast deployment of ReID systems with good performance on new target scenes.",2020,AAAI,1909.10848,10.1609/AAAI.V34I07.6985,https://arxiv.org/pdf/1909.10848.pdf
bfb0e34fdd1597527abf5f9d6608811a57231edd,0,1,Biased Feature Learning for Occlusion Invariant Face Recognition,"To address the challenges posed by unknown occlusions, we propose a Biased Feature Learning (BFL) framework for occlusion-invariant face recognition. We first construct an extended dataset using a multi-scale data augmentation method. For model training, we modify the label loss to adjust the impact of normal and occluded samples. Further, we propose a biased guidance strategy to manipulate the optimization of a network so that the feature embedding space is dominated by non-occluded faces. BFL not only enhances the robustness of a network to unknown occlusions but also maintains or even improves its performance for normal faces. Experimental results demonstrate its superiority as well as the generalization capability with different network architectures and loss functions.",2020,IJCAI,,10.24963/ijcai.2020/93,https://pdfs.semanticscholar.org/0690/b3b0bfa6f01717c50beb295ddc69cefa803d.pdf
bfe5e4d55af4b9aa7f7fe3dcc08cdd2a7bbfae6c,1,0,Proximity-Aware Hierarchical Clustering of unconstrained faces,"Abstract In this paper, we propose an unsupervised face clustering algorithm called “Proximity-Aware Hierarchical Clustering” (PAHC) that exploits the local structure of deep representations. In the proposed method, a similarity measure between deep features is computed by evaluating linear SVM margins, which are learned using nearest neighbors of sample data. Clusters are then formed by applying agglomerative hierarchical clustering (AHC). We evaluate the clustering performance using four unconstrained face datasets, including Celebrity in Frontal-Profile (CFP), Labeled Faces in the Wild (LFW), IARPA JANUS Benchmark A (IJB-A), and IARPA JANUS Benchmark B (IJB-B) datasets. Experimental results demonstrate that the proposed approach can achieve improved performance over state-of-the-art methods. Moreover, we show the proposed clustering algorithm has the potential to be applied to actively learn robust deep face representations by first harvesting sufficient number of unseen face images through curation of large-scale dataset, e.g. the MS-Celeb-1 M dataset. By training DCNNs on the curated MS-Celeb-1 M dataset which contains over three million face images, improved representation for face images are learned.",2018,Image Vis. Comput.,,10.1016/j.imavis.2018.06.007,
c00dc60a7fd011a9c013e14f2a1be413a552ce7e,1,0,A Set of Distinct Facial Traits Learned by Machines Is Not Predictive of Appearance Bias in the Wild.,"We seek to determine whether state-of-the-art, black box face processing technology can learn to make biased trait judgments from human first impression biases. Using features extracted with FaceNet, a widely used face recognition framework, we train a transfer learning model on human subjects' first impressions of personality traits in other faces as measured by social psychologists. We measure the extent to which this appearance bias can be embedded in state-of-the-art face recognition models and benchmark learning performance for subjective perceptions of personality traits from faces. In particular, we find that features extracted with FaceNet can be used to predict human appearance biases for deliberately manipulated faces but not for randomly generated faces scored by humans. Additionally, in contrast to prior work in social psychology, the model does not find a significant signal correlating politicians' vote shares with perceived competence bias. With Local Interpretable Model-Agnostic Explanations (LIME), we provide several explanations for this discrepancy. Our results suggest that some signals of appearance bias documented in social psychology are not embedded by the machine learning techniques we investigate.",2020,,2002.05636,,https://arxiv.org/pdf/2002.05636.pdf
c021c909786d2ee2afacfb6235cd5df2d5df06bd,0,1,How Old Are You? Face Age Translation with Identity Preservation Using GANs,"We present a novel framework to generate images of different age while preserving identity information, which is known as face aging. Different from most recent popular face aging networks utilizing Generative Adversarial Networks(GANs) application, our approach do not simply transfer a young face to an old one. Instead, we employ the edge map as intermediate representations, firstly edge maps of young faces are extracted, a CycleGAN-based network is adopted to transfer them into edge maps of old faces, then another pix2pixHD-based network is adopted to transfer the synthesized edge maps, concatenated with identity information, into old faces. In this way, our method can generate more realistic transfered images, simultaneously ensuring that face identity information be preserved well, and the apparent age of the generated image be accurately appropriate. Experimental results demonstrate that our method is feasible for face age translation.",2019,ArXiv,1909.04988,,https://arxiv.org/pdf/1909.04988.pdf
c05160e194c54d294d1fa05dec1f0c5312528580,1,0,CNN-based System for Low Resolution Face Recognition,"Since the publication of the AlexNet in 2012, Deep Convolutional Neural Network models became the most promising and powerful technique for image representation. Specifically, the ability of their inner layers to extract high level abstractions of the input images, called deep features vectors, has been employed. Such vectors live in a high dimensional space in which an inner product and thus a metric is defined. The latter allows to carry out similarity measurements among them. This property is particularly useful in order to accomplish tasks such as Face Recognition. Indeed, in order to identify a person it is possible to compare deep features, used as face descriptors, from different identities by means of their similarities. Surveillance systems, among others, utilize this technique. To be precise, deep features extracted from probe images are matched against a database of descriptors from known identities. A critical point is that the database typically contains features extracted from high resolution images while the probes, taken by surveillance cameras, can be at a very low resolution. Therefore, it is mandatory to have a neural network which is able to extract deep features that are robust with respect to resolution variations. In this paper we discuss a CNN-based pipeline that we built for the task of Face Recognition among images with different resolution. The entire system relies on the ability of a CNN to extract deep features that can be used to perform a similarity search in order to fulfill the face recognition task.",2019,SEBD,,,http://ceur-ws.org/Vol-2400/paper-37.pdf
c053e59f0df43980ce7dd6d781f07df0f5b58a1f,1,0,Joint Pixel and Feature-level Domain Adaptation in the Wild,"Recent developments in deep domain adaptation have allowed knowledge transfer from a labeled source domain to an unlabeled target domain at the level of intermediate features or input pixels. We propose that advantages may be derived by combining them, in the form of different insights that lead to a novel design and complementary properties that result in better performance. At the feature level, inspired by insights from semi-supervised learning in a domain adversarial neural network, we propose a novel regularization in the form of domain adversarial entropy minimization. Next, we posit that insights from computer vision are more amenable to injection at the pixel level and specifically address the key challenge of adaptation across different semantic levels. In particular, we use 3D geometry and image synthesization based on a generalized appearance flow to preserve identity across higher-level pose transformations, while using an attribute-conditioned CycleGAN to translate a single source into multiple target images that differ in lower-level properties such as lighting. We validate on a novel problem of car recognition in unlabeled surveillance images using labeled images from the web, handling explicitly specified, nameable factors of variation through pixel-level and implicit, unspecified factors through feature-level adaptation. Extensive experiments achieve state-of-the-art results, demonstrating the effectiveness of complementing feature and pixel-level information via our proposed domain adaptation method.",2018,ArXiv,1803.00068,,https://arxiv.org/pdf/1803.00068.pdf
c0598983ab1e2f00154ae13d294c38ef0350e4fe,0,1,A Unified Deep Learning Framework for Short-Duration Speaker Verification in Adverse Environments,"Speaker verification (SV) has recently attracted considerable research interest due to the growing popularity of virtual assistants. At the same time, there is an increasing requirement for an SV system: it should be robust to short speech segments, especially in noisy and reverberant environments. In this paper, we consider one more important requirement for practical applications: the system should be robust to an audio stream containing long non-speech segments, where a voice activity detection (VAD) is not applied. To meet these two requirements, we introduce feature pyramid module (FPM)-based multi-scale aggregation (MSA) and self-adaptive soft VAD (SAS-VAD). We present the FPM-based MSA to deal with short speech segments in noisy and reverberant environments. Also, we use the SAS-VAD to increase the robustness to long non-speech segments. To further improve the robustness to acoustic distortions (i.e., noise and reverberation), we apply a masking-based speech enhancement (SE) method. We combine SV, VAD, and SE models in a unified deep learning framework and jointly train the entire network in an end-to-end manner. To the best of our knowledge, this is the first work combining these three models in a deep learning framework. We conduct experiments on Korean indoor (KID) and VoxCeleb datasets, which are corrupted by noise and reverberation. The results show that the proposed method is effective for SV in the challenging conditions and performs better than the baseline ${i}$ -vector and deep speaker embedding systems.",2020,IEEE Access,2010.02477,10.1109/ACCESS.2020.3025941,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09203835.pdf
c0608626b2780a54667ba683679edcff13998cd2,1,0,Data-Specific Adaptive Threshold for Face Recognition and Authentication,"Many face recognition systems boost the performance using deep learning models, but only a few researches go into the mechanisms for dealing with online registration. Although we can obtain discriminative facial features through the state-of-the-art deep model training, how to decide the best threshold for practical use remains a challenge. We develop a technique of adaptive threshold mechanism to improve the recognition accuracy. We also design a face recognition system along with the registering procedure to handle online registration. Furthermore, we introduce a new evaluation protocol to better evaluate the performance of an algorithm for real-world scenarios. Under our proposed protocol, our method can achieve a 22% accuracy improvement on the LFW dataset.",2019,2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),1810.1116,10.1109/MIPR.2019.00034,https://arxiv.org/pdf/1810.11160.pdf
c061bf63685dd928edcd8aaea68a827f77a187d8,0,1,Driver Glance Classification In-the-wild: Towards Generalization Across Domains and Subjects,"Distracted drivers are dangerous drivers. Equipping advanced driver assistance systems (ADAS) with the ability to detect driver distraction can help prevent accidents and improve driver safety. In order to detect driver distraction, an ADAS must be able to monitor their visual attention. We propose a model that takes as input a patch of the driver’s face along with a crop of the eye-region and classifies their glance into 6 coarse regions-of-interest (ROIs) in the vehicle. We demonstrate that an hourglass network, trained with an additional reconstruction loss, allows the model to learn stronger contextual feature representations than a traditional encoder-only classification module. To make the system robust to subject-specific variations in appearance and behavior, we design a personalized hourglass model tuned with an auxiliary input representing the driver’s baseline glance behavior. Finally, we present a weakly supervised multi-domain training regimen that enables the hourglass to jointly learn representations from different domains (varying in camera type, angle), utilizing unlabeled samples and thereby reducing annotation cost.",2020,ArXiv,2012.02906,,https://arxiv.org/pdf/2012.02906.pdf
c0aeb12a679555dfd833ca91ad8d876eba6cd829,1,1,Fine-grained Attention-based Video Face Recognition,"This paper aims to learn a compact representation of a video for video face recognition task. We make the following contributions: first, we propose a meta attention-based aggregation scheme which adaptively and fine-grained weighs the feature along each feature dimension among all frames to form a compact and discriminative representation. It makes the best to exploit the valuable or discriminative part of each frame to promote the performance of face recognition, without discarding or despising low quality frames as usual methods do. Second, we build a feature aggregation network comprised of a feature embedding module and a feature aggregation module. The embedding module is a convolutional neural network used to extract a feature vector from a face image, while the aggregation module consists of cascaded two meta attention blocks which adaptively aggregate the feature vectors into a single fixed-length representation. The network can deal with arbitrary number of frames, and is insensitive to frame order. Third, we validate the performance of proposed aggregation scheme. Experiments on publicly available datasets, such as YouTube face dataset and IJB-A dataset, show the effectiveness of our method, and it achieves competitive performances on both the verification and identification protocols.",2019,ArXiv,1905.01796,,https://arxiv.org/pdf/1905.01796.pdf
c10707234c57e3d9b416e07ccb7ccdedd429b78e,1,1,MarginDistillation: distillation for margin-based softmax,"The usage of convolutional neural networks (CNNs) in conjunction with a margin-based softmax approach demonstrates a state-of-the-art performance for the face recognition problem. Recently, lightweight neural network models trained with the margin-based softmax have been introduced for the face identification task for edge devices. In this paper, we propose a novel distillation method for lightweight neural network architectures that outperforms other known methods for the face recognition task on LFW, AgeDB-30 and Megaface datasets. The idea of the proposed method is to use class centers from the teacher network for the student network. Then the student network is trained to get the same angles between the class centers and the face embeddings, predicted by the teacher network.",2020,ArXiv,2003.02586,,https://arxiv.org/pdf/2003.02586.pdf
c12016bf259a2820725ac782c7f836c16a433d67,1,1,FAN-Face: a Simple Orthogonal Improvement to Deep Face Recognition,"It is known that facial landmarks provide pose, expression and shape information. In addition, when matching, for example, a profile and/or expressive face to a frontal one, knowledge of these landmarks is useful for establishing correspondence which can help improve recognition. However, in prior work on face recognition, facial landmarks are only used for face cropping in order to remove scale, rotation and translation variations. This paper proposes a simple approach to face recognition which gradually integrates features from different layers of a facial landmark localization network into different layers of the recognition network. To this end, we propose an appropriate feature integration layer which makes the features compatible before integration. We show that such a simple approach systematically improves recognition on the most difficult face recognition datasets, setting a new state-of-theart on IJB-B, IJB-C and MegaFace datasets.",2020,AAAI,,10.1609/AAAI.V34I07.6953,https://pdfs.semanticscholar.org/d847/93f84108d3d6f812c1caf64b27bed5474533.pdf
c127a5c73d747456651f1b274d8bb81e1339c818,1,0,Compact Deep Aggregation for Set Retrieval,"The objective of this work is to learn a compact embedding of a set of descriptors that is suitable for efficient retrieval and ranking, whilst maintaining discriminability of the individual descriptors. We focus on a specific example of this general problem – that of retrieving images containing multiple faces from a large scale dataset of images. Here the set consists of the face descriptors in each image, and given a query for multiple identities, the goal is then to retrieve, in order, images which contain all the identities, all but one, etc.",2018,ECCV Workshops,2003.11794,10.1007/978-3-030-11018-5_36,https://arxiv.org/pdf/2003.11794.pdf
c143f5f88d53bc35009c67a797040065bcf00f19,1,0,SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning,"We propose a new discrimination-aware learning method to improve both accuracy and fairness of face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments include two popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by gender and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present a strong algorithmic discrimination. We finally propose a discrimination-aware learning method, SensitiveLoss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory effects by automatic systems.",2020,ArXiv,2004.11246,,https://arxiv.org/pdf/2004.11246.pdf
c1482491f553726a8349337351692627a04d5dbe,1,0,When Follow is Just One Click Away: Understanding Twitter Follow Behavior in the 2016 U.S. Presidential Election,"Motivated by the two paradoxical facts that the marginal cost of following one extra candidate is close to zero and that the majority of Twitter users choose to follow only one or two candidates, we study the Twitter follow behaviors observed in the 2016 U.S. presidential election. Specifically, we complete the following tasks: (1) analyze Twitter follow patterns of the presidential election on Twitter, (2) use negative binomial regression to study the effects of gender and occupation on the number of candidates that one follows, and (3) use multinomial logistic regression to investigate the effects of gender, occupation and celebrities on the choice of candidates to follow.",2017,SocInfo,1702.00048,10.1007/978-3-319-67217-5_25,https://arxiv.org/pdf/1702.00048.pdf
c19fecbc377695d623601c70e74cfe58b7cb867a,1,1,Face Quality Estimation and Its Correlation to Demographic and Non-Demographic Bias in Face Recognition,"Face quality assessment aims at estimating the utility of a face image for the purpose of recognition. It is a key factor to achieve high face recognition performances. Currently, the high performance of these face recognition systems come with the cost of a strong bias against demographic and non-demographic sub-groups. Recent work has shown that face quality assessment algorithms should adapt to the deployed face recognition system, in order to achieve highly accurate and robust quality estimations. However, this could lead to a bias transfer towards the face quality assessment leading to discriminatory effects e.g. during enrolment. In this work, we present an in-depth analysis of the correlation between bias in face recognition and face quality assessment. Experiments were conducted on two publicly available datasets captured under controlled and uncontrolled circumstances with two popular face embeddings. We evaluated four state-of-the-art solutions for face quality assessment towards biases to pose, ethnicity, and age. The experiments showed that the face quality assessment solutions assign significantly lower quality values towards subgroups affected by the recognition bias demonstrating that these approaches are biased as well. This raises ethical questions towards fairness and discrimination which future works have to address.",2020,ArXiv,2004.01019,,https://arxiv.org/pdf/2004.01019.pdf
c1be90f540623bc517607376212659d8988b0ef8,0,1,Channel Invariant Speaker Embedding Learning with Joint Multi-Task and Adversarial Training,"Using deep neural network to extract speaker embedding has significantly improved the speaker verification task. However, such embeddings are still vulnerable to channel variability. Previous works have used adversarial training to suppress channel information to extract channel-invariant embedding and achieved a significant improvement. Inspired by the successful joint multi-task and adversarial training with phonetic information for phonetic-invariant speaker embedding learning, in this paper, a similar methodology is developed to suppress the channel variability. By treating the recording devices or environments as the channel variability, two individual experiments are carried out, and consistent performance improvement is observed in both cases. The best performance is obtained by sequentially applying multi-task training at the statistics pooling layer and adversarial training at the embedding layer, achieving 10.77% and 9.37% relative improvements in terms of EER compared to the baselines, for the recording environments or devices level, respectively.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053905,
c1dcea2a38a00ae6f51aea72f0be4acd65125ccc,1,0,Recognizing Multi-Modal Face Spoofing With Face Recognition Networks,"Detecting spoofing attacks plays a vital role for deploying automatic face recognition for biometric authentication in applications such as access control, face payment, device unlock, etc. In this paper we propose a new anti-spoofing network architecture that takes advantage of multi-modal image data and aggregates intra-channel features at multiple network layers. We also transfer strong facial features learned for face recognition and show their benefits for detecting spoofing attacks. Finally, to increase the generalization ability of our method to unseen attacks, we use an ensemble of models trained separately for distinct types of spoofing attacks. The proposed method achieves state-of-the-art result on the largest multi-modal anti-spoofing dataset CASIA-SURF[26].",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2019.00204,http://openaccess.thecvf.com/content_CVPRW_2019/papers/CFS/Parkin_Recognizing_Multi-Modal_Face_Spoofing_With_Face_Recognition_Networks_CVPRW_2019_paper.pdf
c22a7886aaff93058cb6df505dcc0b4b38753bd9,0,1,AeMFace: Additive E-Margin Loss for Deep Face Recognition,"With the rapid development of deep learning, deep face recognition models have far exceeded the traditional models. Recent models like CenterLoss, SphereFace, CosFace and ArcFace mainly design an appropriate loss following by the powerful Con-volutional Neural Networks, resulting in more powerful ability on identifying intra-class and inter-class examples. Inspired by that, we propose an Additive E-Margin Loss to obtain highly discriminative features for face recognition. Thus, the resulting AeMFace model can process the weight vector with L2 norm by introducing the E-Margin to reduce the power operations, leading to comparable accuracy and better convergence performance during the training process. Moreover, we study the effectiveness of $L_{2}$ regularization of feature vectors and show the further accuracy improvement on face recognition. The experimental results on three large-scale benchmarks demonstrate the effectiveness of our proposed model.",2019,"2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)",,10.1109/ICSIDP47821.2019.9173230,
c23713c482b40d182b16c48716e06b228a584fd8,0,1,Hybrid Video and Image Hashing for Robust Face Retrieval,"Video face retrieval (VFR) is an appealing and practical computer vision task, which aims to search particular character from masses of videos like in TV-Series. The challenges of this task mainly lie in two aspects, i.e. faces in such videos contain complex appearance variations with uncontrollable shooting environment and searching from big data usually requires high efficiency in both space and time. To fulfill this task, current works typically proceed in a learning to hash manner by fusing single-frame features within a video to obtain the video representation and further embedding it into Hamming space to yield video binary codes. The feature fusion stage has inevitably discarded too much frame information and leads to less discriminative video codes. In this paper, we propose Hybrid Video and Image Hashing (HVIH) to learn more effective binary codes for face videos. Specifically, we fully exploit the dense frame features rather than simply discarding them after the video level fusion and jointly optimize binary codes for the video and its composed frames in adapted supervised manners. To achieve more robust video representation, we introduce a module of video center alignment to ensure the binary codes location of the video and its frames to be as compact and consistent as possible in the Hamming space, which naturally facilitates both tasks of video-to-video retrieval and image-to-video retrieval. Extensive experiments on two challenging video face databases demonstrate the superiority of our approach over the state-of-the-art.",2020,,,,https://pdfs.semanticscholar.org/c237/13c482b40d182b16c48716e06b228a584fd8.pdf
c2981caccc49e67c4f4a9cc66911faf954c01e9a,0,1,Spatially and Temporally Efficient Non-local Attention Network for Video-based Person Re-Identification,"Video-based person re-identification (Re-ID) aims at matching video sequences of pedestrians across non-overlapping cameras. It is a practical yet challenging task of how to embed spatial and temporal information of a video into its feature representation. While most existing methods learn the video characteristics by aggregating image-wise features and designing attention mechanisms in Neural Networks, they only explore the correlation between frames at high-level features. In this work, we target at refining the intermediate features as well as high-level features with non-local attention operations and make two contributions. (i) We propose a Non-local Video Attention Network (NVAN) to incorporate video characteristics into the representation at multiple feature levels. (ii) We further introduce a Spatially and Temporally Efficient Non-local Video Attention Network (STE-NVAN) to reduce the computation complexity by exploring spatial and temporal redundancy presented in pedestrian videos. Extensive experiments show that our NVAN outperforms state-of-the-arts by 3.8% in rank-1 accuracy on MARS dataset and confirms our STE-NVAN displays a much superior computation footprint compared to existing methods.",2019,BMVC,1908.01683,,https://arxiv.org/pdf/1908.01683.pdf
c2cb8ecbe281793401b605634805c47daba6b944,0,1,FastReID: A Pytorch Toolbox for Real-world Person Re-identification,"We present FastReID, as a widely used object reidentification (re-id) software system in JD AI Research. High modular and extensible design makes it easy for the researcher to achieve new research ideas. Friendly manageable system configuration and engineering deployment functions allow practitioners to quickly deploy models into productions. We have implemented some state-of-the-art algorithms, including person re-id, partial re-id, crossdomain re-id and vehicle re-id, and plan to release these pre-trained models on multiple benchmark datasets. FastReID is by far the most complete and high-performance toolbox supports single and multiple GPU servers, you can reproduce our project results very easily and are very welcome to use it, the code and models are available at https://github.com/JDAI-CV/fast-reid.",2020,,,,
c2cc1411c3e08eb452664532544919fc044ae39e,0,1,Rethinking of Pedestrian Attribute Recognition: Realistic Datasets with Efficient Method,"Despite various methods are proposed to make progress in pedestrian attribute recognition, a crucial problem on existing datasets is often neglected, namely, a large number of identical pedestrian identities in train and test set, which is not consistent with practical application. Thus, images of the same pedestrian identity in train set and test set are extremely similar, leading to overestimated performance of state-of-the-art methods on existing datasets. To address this problem, we propose two realistic datasets PETA\textsubscript{$zs$} and RAPv2\textsubscript{$zs$} following zero-shot setting of pedestrian identities based on PETA and RAPv2 datasets. Furthermore, compared to our strong baseline method, we have observed that recent state-of-the-art methods can not make performance improvement on PETA, RAPv2, PETA\textsubscript{$zs$} and RAPv2\textsubscript{$zs$}. Thus, through solving the inherent attribute imbalance in pedestrian attribute recognition, an efficient method is proposed to further improve the performance. Experiments on existing and proposed datasets verify the superiority of our method by achieving state-of-the-art performance.",2020,ArXiv,2005.11909,,https://arxiv.org/pdf/2005.11909.pdf
c2fa427e42e68a9a800e8fa1709d511ac21a0716,0,1,ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill Identification,"Identifying prescription medications is a frequent task for patients and medical professionals; however, this is an error-prone task as many pills have similar appearances (e.g. white round pills), which increases the risk of medication errors. In this paper, we introduce ePillID, the largest public benchmark on pill image recognition, composed of 13k images representing 8184 appearance classes (two sides for 4092 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting. We present our experimental setup and evaluation results of various baseline models on the benchmark. The best baseline using a multi-head metric-learning approach with bilinear features performed remarkably well; however, our error analysis suggests that they still fail to distinguish particularly confusing classes.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2005.14288,10.1109/CVPRW50498.2020.00463,https://arxiv.org/pdf/2005.14288.pdf
c35f0f804caa5d75d8a2a65335ad7bbdbac97665,0,1,Sphere Margins Softmax for Face Recognition,"In conjunction with the cross-entropy loss, softmax function has gained great popularity in supervised deep convolutional neural networks (DCNNs). Although original softmax loss reduces training difficulty and makes multi classification problems easier to converge, the module does not explicitly encourage compactness within class and separability between classes, from this point of view it is not particularly suitable for face recognition tasks. In this paper, we reformulate the softmax loss with sphere margins (SM-Softmax) by normalizing both weights and extracted features of the last fully connected layer and have quantitatively adjustable angular margin by hyperparameter m1 and m2. Extensive experiments on CASIA-WebFace and Labeled Face in the Wild (LFW) validate that our SM-Softmax gives better results than the present state-of-the-art methods while adopting the same experimental configuration (benchmark datasets and network structure) and the convergence speed in the early stage is accelerated significantly.",2020,2020 39th Chinese Control Conference (CCC),,10.23919/CCC50068.2020.9188526,
c36dac0cd7043d696c5a04786a927510ce02b9a8,0,1,High-Fidelity 3D Digital Human Creation from RGB-D Selfies,"We present a fully automatic system that can produce high-fidelity, photo-realistic 3D digital human characters with a consumer RGB-D selfie camera. The system only needs the user to take a short selfie RGB-D video while rotating his/her head, and can produce a high quality reconstruction in less than 30 seconds. Our main contribution is a new facial geometry modeling and reflectance synthesis procedure that significantly improves the state-of-the-art. Specifically, given the input video a two-stage frame selection algorithm is first employed to select a few high-quality frames for reconstruction. A novel, differentiable renderer based 3D Morphable Model (3DMM) fitting method is then applied to recover facial geometries from multiview RGB-D data, which takes advantages of extensive data generation and perturbation. Our 3DMM has much larger expressive capacities than conventional 3DMM, allowing us to recover more accurate facial geometry using merely linear bases. For reflectance synthesis, we present a hybrid approach that combines parametric fitting and CNNs to synthesize high-resolution albedo/normal maps with realistic hair/pore/wrinkle details. Results show that our system can produce faithful 3D characters with extremely realistic details. Code and the constructed 3DMM is publicly available.",2020,ArXiv,2010.05562,,https://arxiv.org/pdf/2010.05562.pdf
c377cf19cad0a025f4c2396a324bfc7fcd9d3c17,0,1,Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach for Feature Embedding,"Unsupervised Deep Distance Metric Learning (UDML) aims to learn sample similarities in the embedding space from an unlabeled dataset. Traditional UDML methods usually use the triplet loss or pairwise loss which requires the mining of positive and negative samples w.r.t. anchor data points. This is, however, challenging in an unsupervised setting as the label information is not available. In this paper, we propose a new UDML method that overcomes that challenge. In particular, we propose to use a deep clustering loss to learn centroids, i.e., pseudo labels, that represent semantic classes. During learning, these centroids are also used to reconstruct the input samples. It hence ensures the representativeness of centroids - each centroid represents visually similar samples. Therefore, the centroids give information about positive (visually similar) and negative (visually dissimilar) samples. Based on pseudo labels, we propose a novel unsupervised metric loss which enforces the positive concentration and negative separation of samples in the embedding space. Experimental results on benchmarking datasets show that the proposed approach outperforms other UDML methods.",2020,ArXiv,2009.04091,,https://arxiv.org/pdf/2009.04091.pdf
c377ff19a7b89e646cf925dc40950bf75cb70928,0,1,Learning from Adversarial Features for Few-Shot Classification,"Many recent few-shot learning methods concentrate on designing novel model architectures. In this paper, we instead show that with a simple backbone convolutional network we can even surpass state-of-the-art classification accuracy. The essential part that contributes to this superior performance is an adversarial feature learning strategy that improves the generalization capability of our model. In this work, adversarial features are those features that can cause the classifier uncertain about its prediction. In order to generate adversarial features, we firstly locate adversarial regions based on the derivative of the entropy with respect to an averaging mask. Then we use the adversarial region attention to aggregate the feature maps to obtain the adversarial features. In this way, we can explore and exploit the entire spatial area of the feature maps to mine more diverse discriminative knowledge. We perform extensive model evaluations and analyses on miniImageNet and tieredImageNet datasets demonstrating the effectiveness of the proposed method.",2019,ArXiv,1903.10225,,https://arxiv.org/pdf/1903.10225.pdf
c3818de44c58d5d7a73a1fc5a338886f80013ed1,0,1,EDGE20: A Cross Spectral Evaluation Dataset for Multiple Surveillance Problems,"Surveillance-related datasets that have been released in recent years focus only on one specific problem at a time (e.g., pedestrian detection, face detection, or face recognition), while most of them were collected using visible spectrum (VIS) cameras. Even though some cross-spectral datasets were presented in the past, they were acquired in a constrained setup, which limited the performance of methods for the aforementioned problems under a cross-spectral setting. This work introduces a new dataset, named EDGE20, that can be used in addressing the problems of pedestrian detection, face detection, and face recognition in images captured using trail cameras under the VIS and NIR spectra. Data acquisition was performed in an outdoor environment, during both day and night, under unconstrained acquisition conditions. The collection of images is accompanied by a rich set of annotations, consisting of person and facial bounding boxes, unique subject identifiers, and labels that characterize facial images as frontal, profile, or back faces. Moreover, the performance of several state-of-the-art methods was evaluated for each of the scenarios covered by our dataset. The baseline results we obtained highlight the difficulty of current methods in the tasks of cross-spectral pedestrian detection, face detection, and face recognition due to unconstrained conditions, including low resolution, pose variation, illumination variation, occlusions, and motion blur.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093573,http://openaccess.thecvf.com/content_WACV_2020/papers/Le_EDGE20_A_Cross_Spectral_Evaluation_Dataset_for_Multiple_Surveillance_Problems_WACV_2020_paper.pdf
c3af4819cd8e5d0f2a6f0a099399d49e26fb9a4f,0,1,Learning to Measure Change: Fully Convolutional Siamese Metric Networks for Scene Change Detection,"A critical challenge problem of scene change detection is that noisy changes generated by varying illumination, shadows and camera viewpoint make variances of a scene difficult to define and measure since the noisy changes and semantic ones are entangled. Following the intuitive idea of detecting changes by directly comparing dissimilarities between a pair of features, we propose a novel fully Convolutional siamese metric Network(CosimNet) to measure changes by customizing implicit metrics. To learn more discriminative metrics, we utilize contrastive loss to reduce the distance between the unchanged feature pairs and to enlarge the distance between the changed feature pairs. Specifically, to address the issue of large viewpoint differences, we propose Thresholded Contrastive Loss (TCL) with a more tolerant strategy to punish noisy changes. We demonstrate the effectiveness of the proposed approach with experiments on three challenging datasets: CDnet, PCD2015, and VL-CMU-CD. Our approach is robust to lots of challenging conditions, such as illumination changes, large viewpoint difference caused by camera motion and zooming. In addition, we incorporate the distance metric into the segmentation framework and validate the effectiveness through visualization of change maps and feature distribution. The source code is available at this https URL.",2018,ArXiv,1810.09111,,https://arxiv.org/pdf/1810.09111.pdf
c40a4a61e82433b62e599ba86b83c04d864e13fb,0,1,Single-Side Domain Generalization for Face Anti-Spoofing,"Existing domain generalization methods for face anti-spoofing endeavor to extract common differentiation features to improve the generalization. However, due to large distribution discrepancies among fake faces of different domains, it is difficult to seek a compact and generalized feature space for the fake faces. In this work, we propose an end-to-end single-side domain generalization framework (SSDG) to improve the generalization ability of face anti-spoofing. The main idea is to learn a generalized feature space, where the feature distribution of the real faces is compact while that of the fake ones is dispersed among domains but compact within each domain. Specifically, a feature generator is trained to make only the real faces from different domains undistinguishable, but not for the fake ones, thus forming a single-side adversarial learning. Moreover, an asymmetric triplet loss is designed to constrain the fake faces of different domains separated while the real ones aggregated. The above two points are integrated into a unified framework in an end-to-end training manner, resulting in a more generalized class boundary, especially good for samples from novel domains. Feature and weight normalization is incorporated to further improve the generalization ability. Extensive experiments show that our proposed approach is effective and outperforms the state-of-the-art methods on four public databases. The code is released online.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.14043,10.1109/cvpr42600.2020.00851,https://arxiv.org/pdf/2004.14043.pdf
c4696d47e0d43a6761742e15c262a224466c4b85,1,1,Correlation Congruence for Knowledge Distillation,"Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1904.01802,10.1109/ICCV.2019.00511,https://arxiv.org/pdf/1904.01802.pdf
c4cadcbd7a619a88236fd868f0ff3e1e12f00a8c,0,1,Type I Attack For Generative Models,"Generative models are popular tools with a wide range of applications. Nevertheless, it is as vulnerable to adversarial samples as classifiers. The existing attack methods mainly focus on generating adversarial examples by adding imperceptible perturbations to input, which leads to wrong result. However, we focus on another aspect of attack, i.e., cheating models by significant changes. The former induces Type II error and the latter causes Type I error. In this paper, we propose Type I attack to generative models such as VAE and GAN. One example given in VAE is that we can change an original image significantly to a meaningless one but their reconstruction results are similar. To implement the Type I attack, we destroy the original one by increasing the distance in input space while keeping the output similar because different inputs may correspond to similar features for the property of deep neural network. Experimental results show that our attack method is effective to generate Type I adversarial examples for generative models on large-scale image datasets.",2020,2020 IEEE International Conference on Image Processing (ICIP),2003.01872,10.1109/ICIP40778.2020.9191032,https://arxiv.org/pdf/2003.01872.pdf
c4f652cb5e65ee0e802dcae1c3f4048bac540ef4,1,0,Improving Cross-Dataset Performance of Face Presentation Attack Detection Systems Using Face Recognition Datasets,"Presentation attack detection (PAD) is now considered critically important for any face-recognition (FR) based access-control system. Current deep-learning based PAD systems show excellent performance when they are tested in intra-dataset scenarios. Under cross-dataset evaluation the performance of these PAD systems drops significantly. This lack of generalization is attributed to domain-shift. Here, we propose a novel PAD method that leverages the large variability present in FR datasets to induce invariance to factors that cause domain-shift. Evaluation of the proposed method on several datasets, including datasets collected using mobile devices, shows performance improvements in cross-dataset evaluations.1",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP40776.2020.9053922,http://publications.idiap.ch/downloads/papers/2020/Mohammadi_InfoVAE_ICASSP_2020.pdf
c573b4510ca4b4e30d5084988ad6a9184c522696,0,1,Meta-Learning for Short Utterance Speaker Recognition with Imbalance Length Pairs,"In practical settings, a speaker recognition system needs to identify a speaker given a short utterance, while the enrollment utterance may be relatively long. However, existing speaker recognition models perform poorly with such short utterances. To solve this problem, we introduce a meta-learning framework for imbalance length pairs. Specifically, we use a Prototypical Networks and train it with a support set of long utterances and a query set of short utterances of varying lengths. Further, since optimizing only for the classes in the given episode may be insufficient for learning discriminative embeddings for unseen classes, we additionally enforce the model to classify both the support and the query set against the entire set of classes in the training set. By combining these two learning schemes, our model outperforms existing state-of-the-art speaker verification models learned with a standard supervised learning framework on short utterance (1-2 seconds) on the VoxCeleb datasets. We also validate our proposed model for unseen speaker identification, on which it also achieves significant performance gains over the existing approaches. The codes are available at this https URL.",2020,INTERSPEECH,2004.02863,10.21437/interspeech.2020-1283,https://arxiv.org/pdf/2004.02863.pdf
c58ac6a18515dda8aa444a09815196abbfb82429,1,1,The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances,"Face recognition is one of the most fundamental and long-standing topics in computer vision community. With the recent developments of deep convolutional neural networks and large-scale datasets, deep face recognition has made remarkable progress and been widely used in the real-world applications. Given a natural image or video frame as input, an end-to-end deep face recognition system outputs the face feature for recognition. To achieve this, the whole system is generally built with three key elements: face detection, face preprocessing, and face representation. The face detection locates faces in the image or frame. Then, the face preprocessing is proceeded to calibrate the faces to a canonical view and crop them to a normalized pixel size. Finally, in the stage of face representation, the discriminative features are extracted from the preprocessed faces for recognition. All of the three elements are fulfilled by deep convolutional neural networks. In this paper, we present a comprehensive survey about the recent advances of every element of the end-to-end deep face recognition, since the thriving deep learning techniques have greatly improved the capability of them. To start with, we introduce an overview of the end-to-end deep face recognition, which, as mentioned above, includes face detection, face preprocessing, and face representation. Then, we review the deep learning based advances of each element, respectively, covering many aspects such as the up-to-date algorithm designs, evaluation metrics, datasets, performance comparison, existing challenges, and promising directions for future research. We hope this survey could bring helpful thoughts to one for better understanding of the big picture of end-to-end face recognition and deeper exploration in a systematic way.",2020,ArXiv,2009.1329,,https://arxiv.org/pdf/2009.13290.pdf
c5b324f7f9abdffc1be83f640674beda81b74315,1,0,Towards Open-Set Identity Preserving Face Synthesis,"We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1803.11182,10.1109/CVPR.2018.00702,https://arxiv.org/pdf/1803.11182.pdf
c5bfbda61a29cd1206ea6670094c7f38c589c51b,0,1,ReadNet: Towards Accurate ReID with Limited and Noisy Samples,"Person re-identification (ReID) is an essential cross-camera retrieval task to identify pedestrians. However, the photo number of each pedestrian usually differs drastically, and thus the data limitation and imbalance problem hinders the prediction accuracy greatly. Additionally, in real-world applications, pedestrian images are captured by different surveillance cameras, so the noisy camera related information, such as the lights, perspectives and resolutions, result in inevitable domain gaps for ReID algorithms. These challenges bring difficulties to current deep learning methods with triplet loss for coping with such problems. To address these challenges, this paper proposes ReadNet, an adversarial camera network (ACN) with an angular triplet loss (ATL). In detail, ATL focuses on learning the angular distance among different identities to mitigate the effect of data imbalance, and guarantees a linear decision boundary as well, while ACN takes the camera discriminator as a game opponent of feature extractor to filter camera related information to bridge the multi-camera gaps. ReadNet is designed to be flexible so that either ATL or ACN can be deployed independently or simultaneously. The experiment results on various benchmark datasets have shown that ReadNet can deliver better prediction performance than current state-of-the-art methods.",2020,ArXiv,2005.0574,,https://arxiv.org/pdf/2005.05740.pdf
c61c1a6d9e9702378d3e90496bee5103c3615069,0,1,Large-Scale Training System for 100-Million Classification at Alibaba,"In the last decades, extreme classification has become an essential topic for deep learning. It has achieved great success in many areas, especially in computer vision and natural language processing (NLP). However, it is very challenging to train a deep model with millions of classes due to the memory and computation explosion in the last output layer. In this paper, we propose a large-scale training system to address these challenges. First, we build a hybrid parallel training framework to make the training process feasible. Second, we propose a novel softmax variation named KNN softmax, which reduces both the GPU memory consumption and computation costs and improves the throughput of training. Then, to eliminate the communication overhead, we propose a new overlapping pipeline and a gradient sparsification method. Furthermore, we design a fast continuous convergence strategy to reduce total training iterations by adaptively adjusting learning rate and updating model parameters. With the help of all the proposed methods, we gain 3.9× throughput of our training system and reduce almost 60% of training iterations. The experimental results show that using an in-house 256 GPUs cluster, we could train a classifier of 100 million classes on Alibaba Retail Product Dataset in about five days while achieving a comparable accuracy with the naive softmax training process.",2020,KDD,,10.1145/3394486.3403342,
c67d1c2b1da0e83f29b013d43e95c96ad7d1c069,0,1,Survey of Occluded and Unoccluded Face Recognition,,2021,,,10.1007/978-981-15-4409-5_88,
c68cd22de315a14587120e98bb02fdcf51edec46,0,1,Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness,"Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, e.g., CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey inappropriate supervisory signals, which encourage the learned feature points to spread over the space sparsely in training. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly induce dense feature regions in order to benefit robustness. Namely, the MMC loss encourages the model to concentrate on learning ordered and compact representations, which gather around the preset optimal centers for different classes. We empirically demonstrate that applying the MMC loss can significantly improve robustness even under strong adaptive attacks, while keeping state-of-the-art accuracy on clean inputs with little extra computation compared to the SCE loss.",2020,ICLR,1905.10626,,https://arxiv.org/pdf/1905.10626.pdf
c6b28d6bc3b99a2a2b62585c0d5585ee61cfca1a,1,1,Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective,"This paper considers learning deep features from long-tailed data. We observe that in the deep feature space, the head classes and the tail classes present different distribution patterns. The head classes have a relatively large spatial span, while the tail classes have a significantly small spatial span, due to the lack of intra-class diversity. This uneven distribution between head and tail classes distorts the overall feature space, which compromises the discriminative ability of the learned features. In response, we seek to expand the distribution of the tail classes during training, so as to alleviate the distortion of the feature space. To this end, we propose to augment each instance of the tail classes with certain disturbances in the deep feature space. With the augmentation, a specified feature vector becomes a set of probable features scattered around itself, which is analogical to an atomic nucleus surrounded by the electron cloud. Intuitively, we name it as ``feature cloud''. The intra-class distribution of the feature cloud is learned from the head classes, and thus provides higher intra-class variation to the tail classes. Consequentially, it alleviates the distortion of the learned feature space, and improves deep representation learning on long tailed data. Extensive experimental evaluations on person re-identification and face recognition tasks confirm the effectiveness of our method.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.10826,10.1109/CVPR42600.2020.00304,https://arxiv.org/pdf/2002.10826.pdf
c6b854413e89627a6d99e8832bd18bfbcd1d3346,0,1,On Role and Location of Normalization before Model-based Data Augmentation in Residual Blocks for Classification Tasks,"Regularization is crucial to the success of many practical deep learning models, in particular in frequent scenarios where there are only a few to a moderate number of accessible training samples. In addition to weight decay, noise injection and dropout, regularization based on multi-branch architectures, such as Shake-Shake regularization, has been proven successful in many applications and attracted more and more attention. However, beyond model-based representation augmentation, it is unclear how Shake-Shake regularization helps to provide further improvement on classification tasks, let alone the baffling interaction between batch normalization and shaking. In this work, we present our investigation on Shake-Shake regularization. One of our findings illustrates the phenomenon that batch normalization in residual blocks is indispensable when shaking is applied to model branches, along with which we also empirically demonstrate the most effective location to place a batch normalization layer in a shaking regularized residual block. Based on these findings, we believe our work is beneficial to future studies on the research topic of refining control for model-based representation augmentation.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683668,
c6bcf6b53628d36a9be8fa1fbec50cb55935319c,1,1,Deep learning face representation by fixed erasing in facial landmarks,"Face verification (FV) is a challenging problem, because occlusion, posture, illumination, aging will affect the accuracy of FV. Deep convolutional neural networks (DCNNs) have been widely used in many computer vision tasks. Due to the strong feature learning ability, DCNNs improve the accuracy of FV while they also bring some problems such as overfitting. Erasing has been proven to be an effective method for reducing overfitting, while the erasing position is seldom considered. We analyse the effect of different erasing positions and propose a novel data augmentation method especially for FV, called Fixed Erasing(FE). We randomly erase some face images with random size of rectangles centered on fixed facial landmarks such as the centers of eyes, tip of noses and the corners of mouths. Our method can alleviate the risk of overfitting and make the models learn more robust features. And our method can be easily merged with most DCNN-based FV models through a few lines of code. Extensive experiments demonstrate that FE can improve the performance of most recently proposed FV models on several popular benchmarks.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-07892-8,
c70a0607d4e17435b177545a0fcca14f46378cdc,1,1,Deep-learned faces: a survey,"Deep learning technology has enabled successful modeling of complex facial features when high-quality images are available. Nonetheless, accurate modeling and recognition of human faces in real-world scenarios “on the wild” or under adverse conditions remains an open problem. Consequently, a plethora of novel deep network architectures addressing issues related to low-quality images, varying pose, illumination changes, emotional expressions, etc., have been proposed and studied over the last few years.This survey presents a comprehensive analysis of the latest developments in the field. A conventional deep face recognition system entails several main components: deep network, optimization loss function, classification algorithm, and train data collection. Aiming at providing a complete and comprehensive study of such complex frameworks, this paper first discusses the evolution of related network architectures. Next, a comparative analysis of loss functions, classification algorithms, and face datasets is given. Then, a comparative study of state-of-the-art face recognition systems is presented. Here, the performance of the systems is discussed using three benchmarking datasets with increasing degrees of complexity. Furthermore, an experimental study was conducted to compare several openly accessible face recognition frameworks in terms of recognition accuracy and speed.",2020,EURASIP J. Image Video Process.,,10.1186/s13640-020-00510-w,
c717bc959a204751c279879435394f13c95aa531,0,1,Two-stream Deep Residual Learning with Fisher Criterion for Human Action Recognition,"Action recognition is one of the most important areas in the computer vision community. Many previous work use two-stream CNN model to obtain both spatial and temporal clues for predicting task. However, two stream are trained separately and combined later by late fusion. This strategy has overlooked the spatial-temporal features interaction. In this paper, we propose new two-stream CNN architectures that are able to learn the relation between two kinds of features. Furthermore, they can be trained end-to-end with standard back propagation algorithm. We also introduce a Fisher loss that makes features more discriminative. The experiments show that Fisher loss yields higher accuracy than using only the softmax loss.",2018,SoICT 2018,,10.1145/3287921.3287972,
c71b0ed402437470f229b3fdabb88ad044c092ea,1,0,Dynamic Conditional Networks for Few-Shot Learning,"This paper proposes a novel Dynamic Conditional Convolutional Network (DCCN) to handle conditional few-shot learning, i.e, only a few training samples are available for each condition. DCCN consists of dual subnets: DyConvNet contains a dynamic convolutional layer with a bank of basis filters; CondiNet predicts a set of adaptive weights from conditional inputs to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input. The filter bank is shared between all conditions thus only a low-dimension weight vector needs to be learned. This significantly facilitates the parameter learning across different conditions when training data are limited. We evaluate DCCN on four tasks which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. Extensive experiments demonstrate the superiority of the proposed model in the conditional few-shot learning setting.",2018,ECCV,,10.1007/978-3-030-01267-0_2,http://openaccess.thecvf.com/content_ECCV_2018/papers/Fang_Zhao_Dynamic_Conditional_Networks_ECCV_2018_paper.pdf
c738defac63395e68d5c7a27eca28fcdd6b3b876,0,1,Deep Metric Learning with Spherical Embedding,"Deep metric learning has attracted much attention in recent years, due to seamlessly combining the distance metric learning and deep neural network. Many endeavors are devoted to design different pair-based angular loss functions, which decouple the magnitude and direction information for embedding vectors and ensure the training and testing measure consistency. However, these traditional angular losses cannot guarantee that all the sample embeddings are on the surface of the same hypersphere during the training stage, which would result in unstable gradient in batch optimization and may influence the quick convergence of the embedding learning. In this paper, we first investigate the effect of the embedding norm for deep metric learning with angular distance, and then propose a spherical embedding constraint (SEC) to regularize the distribution of the norms. SEC adaptively adjusts the embeddings to fall on the same hypersphere and performs more balanced direction update. Extensive experiments on deep metric learning, face recognition, and contrastive self-supervised learning show that the SEC-based angular space learning strategy significantly improves the performance of the state-of-the-art.",2020,NeurIPS,2011.02785,,https://arxiv.org/pdf/2011.02785.pdf
c73dfd45676773fc8c8f6c26318dfd4c9ff326c4,1,1,"DynFace: A Multi-label, Dynamic-Margin-Softmax Face Recognition Model","Convolutional neural networks (CNN), more recently, have greatly increased the performance of face recognition due to its high capability in learning discriminative features. Many of the initial face recognition algorithms reported high performance in the small size Labeled Faces in the Wild (LFW) dataset but fail to deliver same results on larger or different datasets. Ongoing research tries to boost the performance of Face Recognition methods by modifying either the neural network structure or the loss function. This paper proposes two novel additions to the typical softmax CNN used for face recognition: a fusion of facial attributes at feature level and a dynamic margin softmax loss. The new network DynFace was extensively evaluated on extended LFW and much larger MegaFace, comparing its performance against known algorithms. The DynFace achieved state-of-art accuracy at high speed. Results obtained during the carefully designed test experiments, are presented in the end of this paper.",2019,CVC,,10.1007/978-3-030-17795-9_39,
c76251049b370f8258d6bbb944c696c30b8bbb85,1,0,Clothing Change Aware Person Identification,"We develop a person identification approach - Clothing Change Aware Network (CCAN) for the task of clothing assisted person identification. CCAN concerns approaches that go beyond face recognition and particularly tackles the role of clothing to identification. Person identification is a rather challenging task when clothing appears changed under complex background information. With a pair of two person images as input, CCAN simultaneously performs a verification task to detect change in clothing and an identification task to predict person identity. When clothing from the pair of input images is detected to be different, CCAN automatically understates clothing information while emphasizing face, and vice versa. In practice, CCAN outperforms the way of equally stacking face and full body context features, and shows leading results on the People in Photo Album (PIPA) dataset.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00285,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w41/Xue_Clothing_Change_Aware_CVPR_2018_paper.pdf
c76acce3f868cfd4ea57ff5597031afe4983fe7b,0,1,Low Bandwidth Video-Chat Compression using Deep Generative Models,"To unlock video chat for hundreds of millions of people hindered by poor connectivity or unaffordable data costs, we propose to authentically reconstruct faces on the receiver's device using facial landmarks extracted at the sender's side and transmitted over the network. In this context, we discuss and evaluate the benefits and disadvantages of several deep adversarial approaches. In particular, we explore quality and bandwidth trade-offs for approaches based on static landmarks, dynamic landmarks or segmentation maps. We design a mobile-compatible architecture based on the first order animation model of Siarohin et al. In addition, we leverage SPADE blocks to refine results in important areas such as the eyes and lips. We compress the networks down to about 3MB, allowing models to run in real time on iPhone 8 (CPU). This approach enables video calling at a few kbits per second, an order of magnitude lower than currently available alternatives.",2020,ArXiv,2012.00328,,https://arxiv.org/pdf/2012.00328.pdf
c789d21ae9a5ddf591382a1e27a84b0d03acda81,1,0,Static and Dynamic Fusion for Multi-modal Cross-ethnicity Face Anti-spoofing,"Regardless of the usage of deep learning and handcrafted methods, the dynamic information from videos and the effect of cross-ethnicity are rarely considered in face anti-spoofing. In this work, we propose a static-dynamic fusion mechanism for multi-modal face anti-spoofing. Inspired by motion divergences between real and fake faces, we incorporate the dynamic image calculated by rank pooling with static information into a conventional neural network (CNN) for each modality (i.e., RGB, Depth and infrared (IR)). Then, we develop a partially shared fusion method to learn complementary information from multiple modalities. Furthermore, in order to study the generalization capability of the proposal in terms of cross-ethnicity attacks and unknown spoofs, we introduce the largest public cross-ethnicity Face Anti-spoofing (CASIA-CeFA) dataset, covering 3 ethnicities, 3 modalities, 1607 subjects, and 2D plus 3D attack types. Experiments demonstrate that the proposed method achieves state-of-the-art results on CASIA-CeFA, CASIA-SURF, OULU-NPU and SiW.",2019,ArXiv,1912.0234,,https://arxiv.org/pdf/1912.02340.pdf
c78fe1ec99e1846f5e10b630cd8936be5afbf52c,0,1,Real-world Attack on MTCNN Face Detection System,"Recent studies proved that deep learning approaches achieve remarkable results on face detection task. On the other hand, the advances gave rise to a new problem associated with the security of the deep convolutional neural network models unveiling potential risks of DCNNs based applications. Even minor input changes in the digital domain can result in the network being fooled. It was shown then that some deep learning-based face detectors are prone to adversarial attacks not only in a digital domain but also in the real world. In the paper, we investigate the security of the well-known cascade CNN face detection system - MTCNN and introduce an easily reproducible and a robust way to attack it. We propose different face attributes printed on an ordinary white and black printer and attached either to the medical face mask or to the face directly. Our approach is capable of breaking the MTCNN detector in a realworld scenario.",2019,"2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)",1910.06261,10.1109/SIBIRCON48586.2019.8958122,https://arxiv.org/pdf/1910.06261.pdf
c7c8d150ece08b12e3abdb6224000c07a6ce7d47,1,0,DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification,"MeshFace photos have been widely used in many Chinese business organizations to protect ID face photos from being misused. The occlusions incurred by random meshes severely degenerate the performance of face verification systems, which raises the MeshFace verification problem between MeshFace and daily photos. Previous methods cast this problem as a typical low-level vision problem, i.e., blind inpainting. They recover perceptually pleasing clear ID photos from MeshFaces by enforcing pixel level similarity between the recovered ID images and the ground-truth clear ID images and then perform face verification on them. Essentially, face verification is conducted on a compact feature space rather than the image pixel space. Therefore, this paper argues that pixel level similarity and feature level similarity jointly offer the key to improve the verification performance. Based on this insight, we offer a novel feature oriented blind face inpainting framework. Specifically, we implement this by establishing a novel DeMeshNet, which consists of three parts. The first part addresses blind inpainting of the MeshFaces by implicitly exploiting extra supervision from the occlusion position to enforce pixel level similarity. The second part explicitly enforces a feature level similarity in the compact feature space, which can explore informative supervision from the feature space to produce better inpainting results for verification. The last part copes with face alignment within the net via a customized spatial transformer module when extracting deep facial features. All three parts are implemented within an end-to-end network that facilitates efficient optimization. Extensive experiments on two MeshFace data sets demonstrate the effectiveness of the proposed DeMeshNet as well as the insight of this paper.",2018,IEEE Transactions on Information Forensics and Security,1611.05271,10.1109/TIFS.2017.2763119,https://arxiv.org/pdf/1611.05271.pdf
c7d70e168bc43e3df1d625068b60a10779f5f7fe,0,1,Families In Wild Multimedia (FIW-MM): A Multi-Modal Database for Recognizing Kinship,"Kinship is a soft biometric that researchers found detectable in media, and although it is difficult, it comes with an abundance of practical applications. There is continual interest to solve the problem shown by the consistent performance improvements kinship recognition problems based on the large-scale still-image Families In the Wild (FIW) database-- systems are at levels unforeseeable a decade ago: approaching ever closer to being acceptable for real-world use. Biometric tasks have shown to benefit from multi-modal data, as knowledge from the additional modalities complements that of still images. To narrow the gap between research-and-reality, and to enhance the power of kinship recognition systems, we extend FIW with multimedia data (i.e., video, audio, and contextual transcripts). Specifically, we introduce the first large-scale dataset for recognizing kinship in multimedia, the FIW-MM database. We leverage automated machinery to collect, annotate, and prepare the data with minimal human input and no financial cost. This large-scale, multimedia corpus allows problem formulations to follow more realistic template-based protocols. We show significant improvements in benchmarks for multiple kin-based tasks by using the added modalities. We provide insights by highlighting edge cases to inspire future research and areas of improvement. In addition, the richness, depth, and multi-modal data of FIW-MM can support a large number of multimedia tasks (e.g., recognition, generative modeling, speech understanding, and genetics/nature-based studies). FIW-MM provides the data required to increase the potential of systems built to automatically detect kinship in multimedia. Furthermore, it allows experts from diverse fields to collaborate in ways not possible prior.",2020,ArXiv,2007.14509,,https://arxiv.org/pdf/2007.14509.pdf
c7e374f550bffff491a22f2803cf38472b549a9c,0,1,Scalable Similarity-Consistent Deep Metric Learning for Face Recognition,"With the development of deep learning, deep metric learning (DML) has achieved great improvements in face recognition. Specifically, the widely used softmax losses in the training process often bring large intra-class variations, and feature normalization is only exploited in the testing process to compute the pair similarities. To bridge the gap, we impose the intra-class cosine similarity between the features and weight vectors in softmax loss larger than a margin in the training step and extend it from four aspects. First, we explore the effect of a hard sample mining strategy. To alleviate the human labor of adjusting the margin hyper-parameter, a self-adaptive margin updating strategy is proposed. Then, a normalized version is given to take full advantage of the cosine similarity constraint. Furthermore, we enhance the former constraints to consider the intra-class and inter-class constraints simultaneously in the exponential feature projection space. The extensive experiments on the labeled face in the wild (LFW), youtube faces (YTF), and IARPA Janus benchmark A (IJB-A) datasets demonstrate that the proposed methods outperform the mainstream DML methods and approach the state-of-the-art performance.",2019,IEEE Access,,10.1109/ACCESS.2019.2931913,
c7fc277c042e64aa667ac596b65fb4a03146df89,0,1,A Genetic Algorithm Enabled Similarity-Based Attack on Cancellable Biometrics,"Cancellable biometrics (CB) as a means for biometric template protection approach refers to an irreversible yet similarity preserving transformation on the original template. With similarity preserving property, the matching between template and query instance can be performed in the transform domain without jeopardizing accuracy performance. Unfortunately, this trait invites a class of attack, namely similarity-based attack (SA). SA produces a preimage, an inverse of transformed template, which can be exploited for impersonation and cross-matching. In this paper, we propose a Genetic Algorithm enabled similarity-based attack framework (GASAF) to demonstrate that CB schemes whose possess similarity preserving property are highly vulnerable to similarity-based attack. Besides that, a set of new metrics is designed to measure the effectiveness of the similarity-based attack. We conduct the experiment on two representative CB schemes, i.e. BioHashing and Bloom-filter. The experimental results attest the vulnerability under this type of attack.",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",1905.03021,10.1109/BTAS46853.2019.9185997,https://arxiv.org/pdf/1905.03021.pdf
c808c784237f167c78a87cc5a9d48152579c27a4,1,0,Know You at One Glance: A Compact Vector Representation for Low-Shot Learning,"Low-shot face recognition is a very challenging yet important problem in computer vision. The feature representation of the gallery face sample is one key component in this problem. To this end, we propose an Enforced Softmax optimization approach built upon Convolutional Neural Networks (CNNs) to produce an effective and compact vector representation. The learned feature representation is very helpful to overcome the underlying multi-modality variations and remain the primary key features as close to the mean face of the identity as possible in the high-dimensional feature space, thus making the gallery basis more robust under various conditions, and improving the overall performance for low-shot learning. In particular, we sequentially leverage optimal dropout, selective attenuation, ℓ2 normalization, and model-level optimization to enhance the standard Softmax objective function for to produce a more compact vectorized representation for low-shot learning. Comprehensive evaluations on the MNIST, Labeled Faces in the Wild (LFW), and the challenging MS-Celeb-1M Low-Shot Learning Face Recognition benchmark datasets clearly demonstrate the superiority of our proposed method over state-of-the-arts. By further introducing a heuristic voting strategy for robust multi-view combination, and our proposed method has won the Top-1 place in the MS-Celeb-1M Low-Shot Learning Challenge.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.227,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Cheng_Know_You_at_ICCV_2017_paper.pdf
c80f98023476e0e904db1cdd9669c2d8e5e1f7a0,0,1,Google Landmarks Dataset v2 – A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,"While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -- while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by real-world applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2004.01804,10.1109/cvpr42600.2020.00265,https://arxiv.org/pdf/2004.01804.pdf
c85147d279ba2e3360b2a45a68a5c4f1a1f8f625,1,0,Selecting active frames for action recognition with 3D convolutional network,"Recent applications of Convolutional Neural Networks, especially 3-Dimensional Convoltutional Neural Networks (3DCNNs) for human action recognition  (HAR) in videos have widely used. In this paper, we  use a multi-stream framework which is a combination  from separated networks with different kind of input  generated from unique video dataset. To achieve the  high results, firstly, we proposed a method to extract  the active frames (called Selected Active Frames -  SAF) from a videos to build datasets for 3DCNNs in  video classifying problem. Second, we deploy a new  approach called Vote fusion which considered as an  effective fusion method for ensembling multi-stream  networks. From the various datasets generated from  videos, we extract frames by our method and feed  into 3DCNNs for feature extraction, then we carry out  training and then fuse the results of softmax layers  of these streams. We evaluate the proposed methods  on solving action recognition problem. These method  are carried on three well-known datasets (HMFB51,  UCF101, and KTH). The results are also compared to  the state-of-the-art results to illustrate the efficiency  and effectiveness in our approach",2018,,,,https://pdfs.semanticscholar.org/c851/47d279ba2e3360b2a45a68a5c4f1a1f8f625.pdf
c8a5760b7a87f5584e5c309997d080f73c192506,0,1,Face recognition in unconstrained environment with CNN,"In recent years, convolutional neural networks have proven to be a highly efficient approach for face recognition. In this paper, we develop such a framework to learn a robust face verification in an unconstrained environment using aggressive data augmentation. Our objective is to learn a deep face representation from large-scale data with massive noisy and occluded face. Besides, we add an adaptive fusion of softmax loss and center loss as supervision signals, which are helpful to improve the performance and to conduct the final classification. The experiment results show that the suggested system achieves comparable performances with other state-of-the-art methods on the Labeled Faces in the Wild and YouTube face verification tasks.",2020,The Visual Computer,,10.1007/s00371-020-01794-9,
c8ba7c30cc80662f5dbc02ea0865201f41e3c652,0,1,MIPGAN - Generating Robust and High Quality Morph Attacks Using Identity Prior Driven GAN,"Face morphing attacks target to circumvent Face Recognition Systems (FRS) by employing face images derived from multiple data subjects (e.g., accomplices and malicious actors). Morphed images can verify against contributing data subjects with a reasonable success rate, given they have a high degree of identity resemblance. The success of the morphing attacks is directly dependent on the quality of the generated morph images. We present a new approach for generating robust attacks extending our earlier framework for generating face morphs. We present a new approach using an Identity Prior Driven Generative Adversarial Network, which we refer to as \textit{MIPGAN (Morphing through Identity Prior driven GAN)}. The proposed MIPGAN is derived from the StyleGAN with a newly formulated loss function exploiting perceptual quality and identity factor to generate a high quality morphed face image with minimal artifacts and with higher resolution. We demonstrate the proposed approach's applicability to generate robust morph attacks by evaluating it against a commercial Face Recognition System (FRS) and demonstrate the success rate of attacks. Extensive experiments are carried out to assess the FRS's vulnerability against the proposed morphed face generation technique on three types of data such as digital images, re-digitized (printed and scanned) images, and compressed images after re-digitization from newly generated \textit{MIPGAN Face Morph Dataset}. The obtained results demonstrate that the proposed approach of morph generation profoundly threatens the FRS.",2020,ArXiv,2009.01729,,https://arxiv.org/pdf/2009.01729.pdf
c9fadb44c9b3321ffbe50341cf25684768c90163,0,1,ResidualDenseNetwork: A Simple Approach for Video Person Identification,"Video identification is an important task in the practical application and industry. Based on the iQIYI-VID-2019 dataset, ACM International Conference on Multimedia and iQIYI co-hosted the celebrity video identification challenge. We take part in the competition, propose a new feature fusion method and design a residual dense network which can improve video identification performance in the complex scenes. Only with face features, we achieve 0.9035 in mean Average Precision(mAP) which win the second place on the leadboard. At the same time, it is the best score only with official features. It is worth mention that the flops of our model is only 0.5G and the time required to predict the entire test dataset is only 2 sim 5 minutes. Our method takes accuracy and speed into account, which has a strong practical significance.",2019,ACM Multimedia,,10.1145/3343031.3356050,
ca150904327f3cf5792c4cd1440a8f0fe926a3da,0,1,Clova Baseline System for the VoxCeleb Speaker Recognition Challenge 2020,"This report describes our submission to the VoxCeleb Speaker Recognition Challenge (VoxSRC) at Interspeech 2020. We perform a careful analysis of speaker recognition models based on the popular ResNet architecture, and train a number of variants using a range of loss functions. Our results show significant improvements over most existing works without the use of model ensemble or post-processing. We release the training code and pre-trained models as unofficial baselines for this year's challenge.",2020,ArXiv,2009.14153,,https://arxiv.org/pdf/2009.14153.pdf
ca1d36108557a634ddfda943f20b1bcc31f9c670,1,1,"Face Recognition: Too Bias, or Not Too Bias?","We reveal critical insights into problems of bias in state-of-the-art facial recognition (FR) systems using a novel Balanced Faces In the Wild (BFW) dataset: data balanced for gender and ethnic groups. We show variations in the optimal scoring threshold for face-pairs across different subgroups. Thus, the conventional approach of learning a global threshold for all pairs results in performance gaps between subgroups. By learning subgroup-specific thresholds, we reduce performance gaps, and also show a notable boost in overall performance. Furthermore, we do a human evaluation to measure bias in humans, which supports the hypothesis that an analogous bias exists in human perception. For the BFW database, source code, and more, visit https://github.com/visionjo/facerec-bias-bfw.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2002.06483,10.1109/CVPRW50498.2020.00008,https://arxiv.org/pdf/2002.06483.pdf
ca20dcea9e7d958fbebed62bb1e0726f5c33553f,1,1,Deformable face net for pose invariant face recognition,"Abstract Unconstrained face recognition still remains a challenging task due to various factors such as pose, expression, illumination, partial occlusion, etc. In particular, the most significant appearance variations are stemmed from poses which leads to severe performance degeneration. In this paper, we propose a novel Deformable Face Net (DFN) to handle the pose variations for face recognition. The deformable convolution module attempts to simultaneously learn face recognition oriented alignment and identity-preserving feature extraction. The displacement consistency loss (DCL) is proposed as a regularization term to enforce the learnt displacement fields for aligning faces to be locally consistent both in the orientation and amplitude since faces possess strong structure. Moreover, the identity consistency loss (ICL) and the pose-triplet loss (PTL) are designed to minimize the intra-class feature variation caused by different poses and maximize the inter-class feature distance under the same poses. The proposed DFN can effectively handle pose invariant face recognition (PIFR). Extensive experiments show that the proposed DFN outperforms the state-of-the-art methods, especially on the datasets with large poses.",2020,Pattern Recognit.,,10.1016/j.patcog.2019.107113,http://vipl.ict.ac.cn/uploadfile/upload/2020051314412028.pdf
ca3c6c11291045f5b0994acc2a253a0b4b5e3673,1,0,Dual-Attention GAN for Large-Pose Face Frontalization,"Face frontalization provides an effective and efficient way for face data augmentation and further improves the face recognition performance in extreme pose scenario. Despite recent advances in deep learning-based face synthesis approaches, this problem is still challenging due to significant pose and illumination discrepancy. In this paper, we present a novel Dual-Attention Generative Adversarial Network (DA-GAN) for photo-realistic face frontalization by capturing both contextual dependencies and local consistency during GAN training. Specifically, a self-attention-based generator is introduced to integrate local features with their long-range dependencies yielding better feature representations, and hence generate faces that preserve identities better, especially for larger pose angles. Moreover, a novel face-attention-based discriminator is applied to emphasize local features of face regions, and hence reinforce the realism of synthetic frontal faces. Guided by semantic segmentation, four independent discriminators are used to distinguish between different aspects of a face (\ie skin, keypoints, hairline, and frontalized face). By introducing these two complementary attention mechanisms in generator and discriminator separately, we can learn a richer feature representation and generate identity preserving inference of frontal views with much finer details (i.e., more accurate facial appearance and textures) comparing to the state-of-the-art. Quantitative and qualitative experimental results demonstrate the effectiveness and efficiency of our DA-GAN approach.",2020,ArXiv,2002.07227,,https://arxiv.org/pdf/2002.07227.pdf
ca45746d158e9d58bdb8a62b6d10163a23cf5b6f,1,0,UMDFaces: An annotated face dataset for training deep networks,"Recent progress in face detection (including keypoint detection), and recognition is mainly being driven by (i) deeper convolutional neural network architectures, and (ii) larger datasets. However, most of the large datasets are maintained by private companies and are not publicly available. The academic computer vision community needs larger and more varied datasets to make further progress. In this paper, we introduce a new face dataset, called UMDFaces, which has 367,888 annotated faces of 8,277 subjects. We also introduce a new face recognition evaluation protocol which will help advance the state-of-the-art in this area. We discuss how a large dataset can be collected and annotated using human annotators and deep networks. We provide human curated bounding boxes for faces. We also provide estimated pose (roll, pitch and yaw), locations of twenty-one key-points and gender information generated by a pre-trained neural network. In addition, the quality of keypoint annotations has been verified by humans for about 115,000 images. Finally, we compare the quality of the dataset with other publicly available face datasets at similar scales.",2017,2017 IEEE International Joint Conference on Biometrics (IJCB),1611.01484,10.1109/BTAS.2017.8272731,https://arxiv.org/pdf/1611.01484.pdf
ca4ea97847c35bf1d619ec9fd61415afcf8ae618,0,1,Supplementary for Domain-aware Visual Bias Eliminating for Generalized Zero-Shot Learning,"Both f att(·) and f c att(·) give the improvements based on feature selections for the proposed DVBE. Some learned attention cases of f att(·) and f c att(·) are given. Specifically, we visualize the inferred attention maps in Figure 1, where we find that the spatial attention f att(·) focuses on localizing the foreground region, while the channel attention f c att(·) tends to localize local part regions. This proves the effectiveness of complementary feature selections from differnet attentions.",2020,,,,http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Min_Domain-Aware_Visual_Bias_CVPR_2020_supplemental.pdf
ca65412378fa31effddaa1711a89db1c87f6f082,1,0,Cross-Resolution Learning for Face Recognition,"Convolutional Neural Networks have reached extremely high performances on the Face Recognition task. Largely used datasets, such as VGGFace2, focus on gender, pose and age variations trying to balance them to achieve better results. However, the fact that images have different resolutions is not usually discussed and resize to 256 pixels before cropping is used. While specific datasets for very low resolution faces have been proposed, less attention has been payed on the task of cross-resolution matching. Such scenarios are of particular interest for forensic and surveillance systems in which it usually happens that a low-resolution probe has to be matched with higher-resolution galleries. While it is always possible to either increase the resolution of the probe image or to reduce the size of the gallery images, to the best of our knowledge an extensive experimentation of cross-resolution matching was missing in the recent deep learning based literature. In the context of low- and cross-resolution Face Recognition, the contributions of our work are: i) we proposed a training method to fine-tune a state-of-the-art model in order to make it able to extract resolution-robust deep features; ii) we tested our models on the benchmark datasets IJB-B/C considering images at both full and low resolutions in order to show the effectiveness of the proposed training algorithm. To the best of our knowledge, this is the first work testing extensively the performance of a FR model in a cross-resolution scenario; iii) we tested our models on the low resolution and low quality datasets QMUL-SurvFace and TinyFace and showed their superior performances, even though we did not train our model on low-resolution faces only and our main focus was cross-resolution; iv) we showed that our approach can be more effective with respect to preprocessing faces with super resolution techniques.",2020,Image Vis. Comput.,1912.02851,10.1016/j.imavis.2020.103927,https://arxiv.org/pdf/1912.02851.pdf
cabcdbefe93dcc74157e6274b43b341a7b4ed022,0,1,Domain Adaptive Attention Model for Unsupervised Cross-Domain Person Re-Identification,"Person re-identification (Re-ID) across multiple datasets is a challenging yet important task due to the possibly large distinctions between different datasets and the lack of training samples in practical applications. This work proposes a novel unsupervised domain adaption framework which transfers discriminative representations from the labeled source domain (dataset) to the unlabeled target domain (dataset). We propose to formulate the domain adaption task as an one-class classification problem with a novel domain similarity loss. Given the feature map of any image from a backbone network, a novel domain adaptive attention model (DAAM) first automatically learns to separate the feature map of an image to a domain-shared feature (DSH) map and a domain-specific feature (DSP) map simultaneously. Specially, the residual attention mechanism is designed to model DSP feature map for avoiding negative transfer. Then, a DSH branch and a DSP branch are introduced to learn DSH and DSP feature maps respectively. To reduce domain divergence caused by that the source and target datasets are collected from different environments, we force to project the DSH feature maps from different domains to a new nominal domain, and a novel domain similarity loss is proposed based on one-class classification. In addition, a novel unsupervised person Re-ID loss is proposed to take full use of unlabeled target data. Extensive experiments on the Market-1501 and DukeMTMC-reID benchmarks demonstrate state-of-the-art performance of the proposed method. Code will be released to facilitate further studies on the cross-domain person re-identification task.",2019,ArXiv,1905.10529,,https://arxiv.org/pdf/1905.10529.pdf
cb22980c53e97539ed5bf64ef2b2afd541e8b0e8,1,0,Robust Deep Face Recognition with Label Noise,"In the last few years, rapid development of deep learning method has boosted the performance of face recognition systems. However, face recognition still suffers from a diverse variation of face images, especially for the problem of face identification. The high expense of labelling data makes it hard to get massive face data with accurate identification information. In real-world applications, the collected data are mixed with severe label noise, which significantly degrades the generalization ability of deep learning models. In this paper, to alleviate the impact of the label noise, we propose a robust deep face recognition (RDFR) method by automatic outlier removal. The noisy faces are automatically recognized and removed, which can boost the performance of the learned deep models. Experiments on large-scale face datasets LFW, CCFD, and COX show that RDFR can effectively remove the label noise and improve the face recognition performance.",2017,ICONIP,,10.1007/978-3-319-70096-0_61,
cb2470aade8e5630dcad5e479ab220db94ecbf91,1,0,Exploring Facial Differences in European Countries Boundary by Fine-Tuned Neural Networks,"Travel Agents and retailers are always curious about where their customers come from, as this would help them increase their sale and optimize their marketing models. In this study, we build a system to predict where people come from in Europe by analyzing their faces. The countries that have been chosen for the study are Russia, Italy, Germany, Spain, and France. In the first stage of the study, we implement different neural network classifiers on the dataset of people's faces that we collected from Twitter. The highest accuracy achieved is 53.2%, while human accuracy is only 26.96%. In the second stage of the study, we analyze 11 different facial features that might differentiate people in those five countries. The study lays the groundwork for future work to find out differences/similarities of people's faces around the world regardless of their current geographic location.",2018,2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),,10.1109/MIPR.2018.00062,
cb89a8ddb9db73f73c6dd2931ea999bad741f4e7,1,0,Catastrophic Interference in Disguised Face Recognition,"It is commonly known the natural tendency of artificial neural networks to completely and abruptly forget previously known information when learning new information. We explore this behaviour in the context of Face Verification on the recently proposed Disguised Faces in the Wild dataset (DFW). We empirically evaluate several commonly used DCNN architectures on Face Recognition and distill some insights about the effect of sequential learning on distinct identities from different datasets, showing that the catastrophic forgetness phenomenon is present even in feature embeddings fine-tuned on different tasks from the original domain.",2019,IbPRIA,,10.1007/978-3-030-31321-0_6,
cba5f93bb2f25a7e87c90adc8b974b7d26ad0926,0,1,Attribute-guided Feature Extraction and Augmentation Robust Learning for Vehicle Re-identification,"Vehicle re-identification is one of the core technologies of intelligent transportation systems and smart cities, but large intra-class diversity and inter-class similarity poses great challenges for existing method. In this paper, we propose a multi-guided learning approach which utilizing the information of attributes and meanwhile introducing two novel random augments to improve the robustness during training. What’s more, we propose an attribute constraint method and group re-ranking strategy to refine matching results. Our method achieves mAP of 66.83% and rank-1 accuracy 76.05% in the CVPR 2020 AI City Challenge.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2005.06184,10.1109/CVPRW50498.2020.00317,https://arxiv.org/pdf/2005.06184.pdf
cbb93f58c06daa1b7be7bb9109e880b20f3714ab,0,1,Adapting End-to-end Neural Speaker Verification to New Languages and Recording Conditions with Adversarial Training,"In this article we propose a novel approach for adapting speaker embeddings to new domains based on adversarial training of neural networks. We apply our embeddings to the task of text-independent speaker verification, a challenging, real-world problem in biometric security. We further the development of end-to-end speaker embedding models by combing a novel 1-dimensional, self-attentive residual network, an angular margin loss function and adversarial training strategy. Our model is able to learn extremely compact, 64-dimensional speaker embeddings that deliver competitive performance on a number of popular datasets using simple cosine distance scoring. One the NIST-SRE 2016 task we are able to beat a strong i-vector baseline, while on the Speakers in the Wild task our model was able to outperform both i-vector and x-vector baselines, showing an absolute improvement of 2.19% over the latter. Additionally, we show that the integration of adversarial training consistently leads to a significant improvement over an unadapted model.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1811.03055,10.1109/ICASSP.2019.8682611,https://arxiv.org/pdf/1811.03055.pdf
cbbd46338ce7749caba2c7c292e55ccc215a505a,1,1,Face Segmentor-Enhanced Deep Feature Learning for Face Recognition,"In this paper, we propose a Face Segmentor-Enhanced Network (FSENet) for face recognition to exploit facial localized property. Most existing methods emphasize the holistic characteristics on entire face images, which have limit discriminative ability due to large intra-class variations and inter-class fine-grain. To address this, we present a face segmentor to parse the face into local components and explore their internal correlations, which strengthens the discriminability to discern identities. Specifically, we introduce a semantic parsing module to assign each pixel with a semantic part label. This module generates a set of parsing maps, where each of them represents the pixel-wise occurrence probability of a certain facial component. We then segment facial regions masked by the parsing maps to achieve local features. We further build the structure correlation of facial part features to boost personalized attribute. We finally incorporate holistic and local information to improve the discriminative power of the face descriptor. Extensive experiments on popular public-domain datasets including Labeled Face in the Wild (LFW), Youtube Faces (YTF), IARPA IJB-A, IJB-B and IJB-C, and the MegaFace Challenge show that our method achieves promising performance.",2019,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2019.2936624,http://boyuan.global-optimization.com/Mypaper/TBIOM-2019.pdf
cbcc32d88b627a251eee8c1dd17c579cb93c9f79,1,1,NIR-to-VIS Face Recognition via Embedding Relations and Coordinates of the Pairwise Features,"NIR-to-VIS face recognition is identifying faces of two different domains by extracting domain-invariant features. However, this is a challenging problem due to the two different domain characteristics, and the lack of NIR face dataset. In order to reduce domain discrepancy while using the existing face recognition models, we propose a ’Relation Module’ which can simply add-on to any face recognition models. The local features extracted from face image contain information of each component of the face. Based on two different domain characteristics, to use the relationships between local features is more domain-invariant than to use it as it is. In addition to these relationships, positional information such as distance from lips to chin or eye to eye, also provides domain-invariant information. In our Relation Module, Relation Layer implicitly captures relationships, and Coordinates Layer models the positional information. Also, our proposed Triplet loss with conditional margin reduces intra-class variation in training, and resulting in additional performance improvements.Different from the general face recognition models, our add-on module does not need to pre-train with the large scale dataset. The proposed module fine-tuned only with CASIA NIR-VIS 2.0 database. With the proposed module, we achieve 14.81% rank-1 accuracy and 15.47% verification rate of 0.1% FAR improvements compare to two baseline models.",2019,2019 International Conference on Biometrics (ICB),,10.1109/ICB45273.2019.8987306,
cc423fa3ba93c927f8ca9cc9fb0b0afcf92dff6e,1,1,Deep Face Rectification for 360° Dual-Fisheye Cameras,"Rectilinear face recognition models suffer from severe performance degradation when applied to fisheye images captured by 360° back-to-back dual fisheye cameras. We propose a novel face rectification method to combat the effect of fisheye image distortion on face recognition. The method consists of a classification network and a restoration network specifically designed to handle the non-linear property of fisheye projection. The classification network classifies an input fisheye image according to its distortion level. The restoration network takes a distorted image as input and restores the rectilinear geometric structure of the face. The performance of the proposed method is tested on an end-to-end face recognition system constructed by integrating the proposed rectification method with a conventional rectilinear face recognition system. The face verification accuracy of the integrated system is 99.18% when tested on images in the synthetic Labeled Faces in the Wild (LFW) dataset and 95.70% for images in a real image dataset, resulting in an average accuracy improvement of 6.57% over the conventional face recognition system. For face identification, the average improvement over the conventional face recognition system is 4.51%.",2021,IEEE Transactions on Image Processing,,10.1109/TIP.2020.3019661,
cc989b88f3799835f16842b066a36e171b607e7f,1,1,ShuffleFaceNet: A Lightweight Face Architecture for Efficient and Highly-Accurate Face Recognition,"The recent success of convolutional neural networks has led to the development of a variety of new effective and efficient architectures. However, few of them have been designed for the specific case of face recognition. Inspired on the state-of-the-art ShuffleNetV2 model, a lightweight face architecture is presented in this paper. The proposal, named ShuffleFaceNet, introduces significant modifications in order to improve face recognition accuracy. First, the Global Average Pooling layer is replaced by a Global Depth-wise Convolution layer, and Parametric Rectified Linear Unit is used as a non-linear activation function. Under the same experimental conditions, ShuffleFaceNet achieves significantly superior accuracy than the original ShuffleNetV2, maintaining the same speed and compact storage. In addition, extensive experiments conducted on three challenging benchmark face datasets, show that our proposal improves not only state-of-the-art lightweight models but also very deep face recognition models.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00333,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Martindez-Diaz_ShuffleFaceNet_A_Lightweight_Face_Architecture_for_Efficient_and_Highly-Accurate_Face_ICCVW_2019_paper.pdf
ccab598a9894d77c6e8c057a5d48e7757ab74b46,1,0,Omni-supervised Facial Expression Recognition: A Simple Baseline,"In this paper, we target on advancing the performance in facial expression recognition (FER) by exploiting omni-supervised learning. The current state of the art FER approaches usually aim to recognize facial expressions in a controlled environment by training models with a limited number of samples. To enhance the robustness of the learned models for various scenarios, we propose to perform omni-supervised learning by exploiting the labeled samples together with a large number of unlabeled data. Particularly, we first employ MS-Celeb-1M as the facial-pool where around 5,822K unlabeled facial images are included. Then, a primitive model learned on a small number of labeled samples is adopted to select samples with high confidence from the facial-pool by conducting feature-based similarity comparison. We find the new dataset constructed in such an omni-supervised manner can significantly improve the generalization ability of the learned FER model and boost the performance consequently. However, as more training samples are used, more computation resources and training time are required, which is usually not affordable in many circumstances. To relieve the requirement of computational resources, we further adopt a dataset distillation strategy to distill the target task-related knowledge from the new mined samples and compressed them into a very small set of images. This distilled dataset is capable of boosting the performance of FER with few additional computational cost introduced. We perform extensive experiments on five popular benchmarks and a newly constructed dataset, where consistent gains can be achieved under various settings using the proposed framework. We hope this work will serve as a solid baseline and help ease future research in FER.",2020,ArXiv,2005.08551,,https://arxiv.org/pdf/2005.08551.pdf
ccbb94f8a025467d6001b60dd10d2078d8d9ab0e,0,1,Scalable Logo Recognition Using Proxies,"Logo recognition is the task of identifying and classifying logos. Logo recognition is a challenging problem as there is no clear definition of a logo and there are huge variations of logos, brands and re-training to cover every variation is impractical. In this paper, we formulate logo recognition as a few-shot object detection problem. The two main components in our pipeline are universal logo detector and few-shot logo recognizer. The universal logo detector is a class-agnostic deep object detector network which tries to learn the characteristics of what makes a logo. It predicts bounding boxes on likely logo regions. These logo regions are then classified by logo recognizer using nearest neighbor search, trained by triplet loss using proxies. We also annotated a first of its kind product logo dataset containing 2000 logos from 295K images collected from Amazon called PL2K. Our pipeline achieves 97% recall with 0.6 mAP on PL2K test dataset and state-of-the-art 0.565 mAP on the publicly available FlickrLogos-32 test set without fine-tuning.",2019,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),1811.08009,10.1109/WACV.2019.00081,https://arxiv.org/pdf/1811.08009.pdf
cda0e22c39c3969a68b568d1809ff837f573b8f0,0,1,DeepACC: Automate Chromosome Classification based on Metaphase Images using Deep Learning Framework Fused with Prior Knowledge,"Chromosome classification is an important but difficult and tedious task in karyotyping. Previous methods only classify manually segmented single chromosome, which is far from clinical practice. In this work, we propose a detection based method, DeepACC, to locate and fine classify chromosomes simultaneously based on the whole metaphase image. We firstly introduce the Additive Angular Margin Loss to enhance the discriminative power of model. To alleviate batch effects, we transform decision boundary of each class case-by-case through a siamese network which make full use of prior knowledges that chromosomes usually appear in pairs. Furthermore, we take the clinically seven group criterion as a prior knowledge and design an additional Group Inner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase images from clinical laboratory are collected and labelled to evaluate the performance. Results show that the new design brings encouraging performance gains comparing to the state-of-the-art baselines.",2020,ArXiv,2006.15528,,https://arxiv.org/pdf/2006.15528.pdf
cdb734e2790f38465ca83d40cd50ef91ec67ee90,1,1,Reducing Geographic Performance Differentials for Face Recognition,"As face recognition algorithms become more accurate and get deployed more widely, it becomes increasingly important to ensure that the algorithms work equally well for everyone. We study the geographic performance differentials—differences in false acceptance and false rejection rates across different countries—when comparing selfies against photos from ID documents. We show how to mitigate geographic performance differentials using sampling strategies despite large imbalances in the dataset. Using vanilla domain adaptation strategies to fine-tune a face recognition CNN on domain-specific doc-selfie data improves the performance of the model on such data, but, in the presence of imbalanced training data, also significantly increases the demographic bias. We then show how to mitigate this effect by employing sampling strategies to balance the training procedure.",2020,2020 IEEE Winter Applications of Computer Vision Workshops (WACVW),2002.12093,10.1109/WACVW50321.2020.9096930,https://arxiv.org/pdf/2002.12093.pdf
cdd71d9377bb682e8b683c4700cae650ade96eba,0,1,On visual BMI analysis from facial images,"Abstract Automatically assessing body mass index (BMI) from facial images is an interesting and challenging problem in computer vision. Facial feature extraction is an important step for visual BMI estimation. This work studies the visual BMI estimation problem based on the characteristics and performance of different facial representations, which has not been well studied yet. Various facial representations, including geometry based representations and deep learning based, are comprehensively evaluated and analyzed from three perspectives: the overall performance on visual BMI prediction, the redundancy in facial representations and the sensitivity to head pose changes. The experiments are conducted on two databases: a new dataset we collected, called the FIW-BMI and an existing large dataset Morph II. Our studies provide some deep insights into the facial representations for visual BMI analysis: 1) The deep model based methods perform better than geometry based methods. Among them, the VGG-Face and Arcface show more robustness than others in most cases; 2) Removing the redundancy in VGG-Face representation can increase the accuracy and efficiency in BMI estimation; 3) Large head poses lead to low performance for BMI estimation. The Arcface, VGG-Face and PIGF are more robust than the others to head pose variations.",2019,Image Vis. Comput.,,10.1016/J.IMAVIS.2019.07.003,
ce609a175def661b31511dfe0170b7b05e47c5fd,0,1,Robustness Analysis of Face Obscuration,"Face obscuration is needed by law enforcement and mass media outlets to guarantee privacy. Sharing sensitive content where obscuration or redaction techniques have failed to completely remove all identifiable traces can lead to many legal and social issues. Hence, we need to be able to systematically measure the face obscuration performance of a given technique. In this paper we propose to measure the effectiveness of eight obscuration techniques. We do so by attacking the redacted faces in three scenarios: obscured face identification, verification, and reconstruction. Threat modeling is also considered to provide a vulnerability analysis for each studied obscuration technique. Based on our evaluation, we show that the k-same based methods are the most effective.",2019,ArXiv,1905.05243,,https://arxiv.org/pdf/1905.05243.pdf
ce86cd67c07a01772dbebf241f5bc6fcdd188537,0,1,ActGAN: Flexible and Efficient One-shot Face Reenactment,"This paper introduces ActGAN – a novel end-to-end generative adversarial network (GAN) for one-shot face reenactment. Given two images, the goal is to transfer the facial expression of the source actor onto a target person in a photo-realistic fashion. While existing methods require target identity to be predefined, we address this problem by introducing a ""many-to-many"" approach, which allows arbitrary persons both for source and target without additional retraining. To this end, we employ the Feature Pyramid Network (FPN) as a core generator building block – the first application of FPN in face reenactment, producing finer results. We also introduce a solution to preserve a person’s identity between synthesized and target person by adopting the state-of-the-art approach in deep face recognition domain. The architecture readily supports reenactment in different scenarios: ""many-to-many"", ""one-to-one"", ""one-to-another"" in terms of expression accuracy, identity preservation, and overall image quality. We demonstrate that ActGAN achieves competitive performance against recent works concerning visual quality.",2020,2020 8th International Workshop on Biometrics and Forensics (IWBF),2003.1384,10.1109/IWBF49977.2020.9107944,https://arxiv.org/pdf/2003.13840.pdf
cf1359b648ce03f27908b9a16ef3f024fbef49bc,1,1,Masked Face Recognition with Latent Part Detection,"This paper focuses on a novel task named masked faces recognition (MFR), which aims to match masked faces with common faces and is important especially during the global outbreak of COVID-19. It is challenging to identify masked faces for two main reasons. Firstly, there is no large-scale training data and test data with ground truth for MFR. Collecting and annotating millions of masked faces is labor-consuming. Secondly, since most facial cues are occluded by mask, it is necessary to learn representations which are both discriminative and robust to mask wearing. To handle the first challenge, this paper collects two datasets designed for MFR: MFV with 400 pairs of 200 identities for verification, and MFI which contains 4,916 images of 669 identities for identification. As is known, a robust face recognition model needs images of millions of identities to train, and hundreds of identities is far from enough. Hence, MFV and MFI are only considered as test datasets to evaluate algorithms. Besides, a data augmentation method for training data is introduced to automatically generate synthetic masked face images from existing common face datasets. In addition, a novel latent part detection (LPD) model is proposed to locate the latent facial part which is robust to mask wearing, and the latent part is further used to extract discriminative features. The proposed LPD model is trained in an end-to-end manner and only utilizes the original and synthetic training data. Experimental results on MFV, MFI and synthetic masked LFW demonstrate that LPD model generalizes well on both realistic and synthetic masked data and outperforms other methods by a large margin.",2020,ACM Multimedia,,10.1145/3394171.3413731,
cf29bf5080d8eff9394021025160afe02653789c,0,1,Privacy Prediction of Lightweight Convolutional Neural Network,,2020,,,10.1007/978-981-15-9739-8_39,
cf4946cce852aeb2654442cb9322d0a80037e00d,1,0,Mitigating Face Recognition Bias via Group Adaptive Classifier,"Face recognition is known to exhibit bias - subjects in certain demographic group can be better recognized than other groups. This work aims to learn a fair face representation, where faces of every group could be equally well-represented. Our proposed group adaptive classifier, GAC, learns to mitigate bias by using adaptive convolution kernels and attention mechanisms on faces based on their demographic attributes. The adaptive module comprises kernel masks and channel-wise attention maps for each demographic group so as to activate different facial regions for identification, leading to more discriminative features pertinent to their demographics. We also introduce an automated adaptation strategy which determines whether to apply adaptation to a certain layer by iteratively computing the dissimilarity among demographic-adaptive parameters, thereby increasing the efficiency of the adaptation learning. Experiments on benchmark face datasets (RFW, LFW, IJB-A, and IJB-C) show that our framework is able to mitigate face recognition bias on various demographic groups as well as maintain the competitive performance.",2020,ArXiv,2006.07576,,https://arxiv.org/pdf/2006.07576.pdf
cf4e5206722ba16061982b885f8c7c86beacd27c,1,0,"Group-Level Emotion Recognition Using Hybrid Deep Models Based on Faces, Scenes, Skeletons and Visual Attentions","This paper presents a hybrid deep learning network submitted to the 6th Emotion Recognition in the Wild (EmotiW 2018) Grand Challenge [9], in the category of group-level emotion recognition. Advanced deep learning models trained individually on faces, scenes, skeletons and salient regions using visual attention mechanisms are fused to classify the emotion of a group of people in an image as positive, neutral or negative. Experimental results show that the proposed hybrid network achieves 78.98% and 68.08% classification accuracy on the validation and testing sets, respectively. These results outperform the baseline of 64% and 61%, and achieved the first place in the challenge.",2018,ICMI,,10.1145/3242969.3264990,
cf517aad72c17328486d759f22755e43a9cd1b76,1,0,Deep multi-factor forensic face recognition,"of the Dissertation Deep Multi-Factor Forensic Face Recognition by Zhengming Ding Doctor of Philosophy in Electrical and Computer Engineering Northeastern University, 2018 Dr. Yun Fu, Advisor Forensic science is any scientific field that is applied to the field of law. Due to the popularity of the digital media carriers such as images, videos, the facial recognition becomes another important forensic technique. The major issue of forensic face recognition is the unstable system performance due to internal factor, e.g., aging, and external factors, e.g., image resolution/modality, illumination, pose. In this thesis, we investigate a theoretical framework for forensic face recognition, subject to a variety of internal/external impact factors to tackle face recognition under different views, illuminations, resolutions, modalities, periods, when probe images are captured in the surveillance environments without collaborations. Specifically, we explore two scenarios as follows. First of all, we explore one dominant factor which hinders the forensic face recognition, which is view variance, e.g., pose and modality. Thus, we propose multi-view face recognition, which covers two settings in multi-view face recognition. On one hand, labeled data in multiple views are available in the training stage, which is the traditional multi-view learning setting. Specifically, we address the challenging but practical situation, in which the view information of the test data is unknown. On the other hand, some source views are labeled while target views are unlabeled, which satisfies transfer learning scenarios. Specifically, we explore the practical but challenging missing modality problem. Secondly, multiple factors are modeled as the noises as a whole. On one hand, conventional auto-encoder and its variants usually involve additive noises for training data to learn robust features, which, however, did not consider the already corrupted data. We propose a novel Deep Robust Encoder (DRE) through locality preserving low-rank dictionary to extract robust and discriminative features from corrupted data. Furthermore, we fight off one-shot face recognition, where we only have one training sample for some persons, by proposing a one-shot generative model to build a more effective face recognizer.",2018,,,10.17760/d20292360,https://pdfs.semanticscholar.org/cf51/7aad72c17328486d759f22755e43a9cd1b76.pdf
cfb7b3cd096e571337f6a5fd6a26b94a982ca6d1,0,1,"Learning from the Past: Meta-Continual Learning with Knowledge Embedding for Jointly Sketch, Cartoon, and Caricature Face Recognition","This paper deals with a challenging task of learning from different modalities by tackling the difficulty problem of jointly face recognition between abstract-like sketches, cartoons, caricatures and real-life photographs. Due to the significant variations in the abstract faces, building vision models for recognizing data from these modalities is an extremely challenging. We propose a novel framework termed as Meta-Continual Learning with Knowledge Embedding to address the task of jointly sketch, cartoon, and caricature face recognition. In particular, we firstly present a deep relational network to capture and memorize the relation among different samples. Secondly, we present the construction of our knowledge graph that relates image with the label as the guidance of our meta-learner. We then design a knowledge embedding mechanism to incorporate the knowledge representation into our network. Thirdly, to mitigate catastrophic forgetting, we use a meta-continual model that updates our ensemble model and improves its prediction accuracy. With this meta-continual model, our network can learn from its past. The final classification is derived from our network by learning to compare the features of samples. Experimental results demonstrate that our approach achieves significantly higher performance compared with other state-of-the-art approaches.",2020,ACM Multimedia,,10.1145/3394171.3413892,
cfe9ab4e98d6c4ff456332e5bae1695e7d5d0714,1,1,Adversarial Privacy-preserving Filter,"While widely adopted in practical applications, face recognition has been critically discussed regarding the malicious use of face images and the potential privacy problems, e.g., deceiving payment system and causing personal sabotage. Online photo sharing services unintentionally act as the main repository for malicious crawler and face recognition applications. This work aims to develop a privacy-preserving solution, called Adversarial Privacy-preserving Filter (APF), to protect the online shared face images from being maliciously used. We propose an end-cloud collaborated adversarial attack solution to satisfy requirements of privacy, utility and non-accessibility. Specifically, the solutions consist of three modules: (1) image-specific gradient generation, to extract image-specific gradient in the user end with a compressed probe model; (2) adversarial gradient transfer, to fine-tune the image-specific gradient in the server cloud; and (3) universal adversarial perturbation enhancement, to append image-independent perturbation to derive the final adversarial noise. Extensive experiments on three datasets validate the effectiveness and efficiency of the proposed solution. A prototype application is also released for further evaluation. We hope the end-cloud collaborated attack framework could shed light on addressing the issue of online multimedia sharing privacy-preserving issues from user side.",2020,ACM Multimedia,2007.12861,10.1145/3394171.3413906,https://arxiv.org/pdf/2007.12861.pdf
d02af879f25e0ae0cebbdafe98e26d8ed8ebd111,1,0,Female Facial Beauty Analysis Using Transfer Learning and Stacking Ensemble Model,"Automatic analysis of facial beauty has become an emerging research topic in recent years and has fascinated many researchers. One of the key challenges of facial attractiveness prediction is to obtain accurate and discriminative face representation. This study provides a new framework to analyze the attractiveness of female faces using transfer learning methodology as well as stacking ensemble model. Specifically, a pre-trained Convolutional Neural Network (CNN) originally trained on relatively similar datasets for face recognition task, namely Ms-Celeb-1M and VGGFace2, is utilized to acquire high-level and robust features of female face images. This is followed by leveraging a stacking ensemble model which combines the predictions of several base models to predict the attractiveness of a face. Extensive experiments conducted on SCUT-FBP and SCUT-FBP 5500 benchmark datasets, confirm the strong robustness of the proposed approach. Interestingly, prediction correlations of 0.89 and 0.91 are achieved by our new method for SCUT-FBP and SCUT-FBP5500 datasets, respectively. This would indicate significant advantages over the other state-of-the-art work. Moreover, our successful results would certainly support the efficacy of transfer learning when applying deep learning techniques to compute facial attractiveness.",2019,ICIAR,,10.1007/978-3-030-27272-2_22,
d06e67db6ab47498d375617185b7452084a0772f,1,1,Large-scale Multi-modal Person Identification in Real Unconstrained Environments,"Person identification (P-ID) under real unconstrained noisy environments is a huge challenge. In multiple-feature learning with Deep Convolutional Neural Networks (DCNNs) or Machine Learning method for large-scale person identification in the wild, the key is to design an appropriate strategy for decision layer fusion or feature layer fusion which can enhance discriminative power. It is necessary to extract different types of valid features and establish a reasonable framework to fuse different types of information. In traditional methods, different persons are identified based on single modal features to identify, such as face feature, audio feature, and head feature. These traditional methods cannot realize a highly accurate level of person identification in real unconstrained environments. The study aims to propose a fusion module to fuse multi-modal features for person identification in real unconstrained environments.",2019,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),1912.12134,10.1109/robio49542.2019.8961716,https://arxiv.org/pdf/1912.12134.pdf
d06f2dc67dfa0017683a46294272d69cbb41f1cd,1,0,Learning Generative Models using Transformations,"One of the fundamental problems in machine learning and statistics is learning generative models of data. Explicit generative models, which model probability densities of data, have been intensively studied in numerous applications.However, it is usually difficult to model complex data, such as natural images, by using combinations of simple parametric distributions. Implicit generative models (IGMs), which model transformations between known source distributions and target distributions to simulate the sampling process withoutspecifying densities explicitly, regain its attention with explosion of interests. With recent success of deep learning, IGMs have yielded impressive empirical performance in different applications. While there are new algorithms for learning IGMs, its theoretical properties are weakly justified and their relationships with existing methods are underexplored.The ?first thrust of this thesis is to understand statistical guarantees of learning IGMs. By connecting IGMs with two-sample test, we propose a new generic algorithm that can be built on the top of many existing approaches and bring performance improvement over the state-of-the-art. On the other hand, from the perspective of statistical analysis, IGMs, which model transformations, is fundamentally different from traditional explicit models, whichmakes the existing results not directly applicable. We then study error bounds and sample complexities of learning IGMs taking a step forward in building its rigorous foundations. In the second part, we shift our focus to different types of data that we are interested in. We develop algorithms for learning IGMs on various data rangingfrom images, text, to point clouds, by exploiting their underlying structures. Instead of modeling IGM transformations blindly via powerful functions only,such as deep neural networks, we propose to leverage human priors into algorithm designs to reduce model sizes, save computational overhead, and achieve interpretable results. In this thesis, we show an example of incorporating asimple yet fairly representative renderer developed in computer graphics into IGM transformations for generating realistic and highly structured body data, which paves a new path of learning IGMs. Finally, we study how IGMs can improve existing machine learning algorithms. From its nature of modeling sampling processes, we propose learning powerful kernels via Fourier analysis and IGM sampling. By thinking IGMs as learning transformations, we extend IGMs to broader applications in diff?erent domains. In the second example, we present how to learn proximal operators as IGM transformations to solve important linear inverse problems in computer vision. Lastly, we introduce a new way of using IGMs by treating them as auxiliarycomponents to benefit non-generative tasks while the ?final output of the interest is not the generative models. We present an application of optimizing test power in anomaly detection by constructing a lower bound of test powervia auxiliary IGMs.",2020,,,10.1184/R1/11878179.V1,https://pdfs.semanticscholar.org/d06f/2dc67dfa0017683a46294272d69cbb41f1cd.pdf
d0818dac77eee5b970736e57a478bcedfb1b15fe,1,0,KVQA: Knowledge-Aware Visual Question Answering,"Visual Question Answering (VQA) has emerged as an important problem spanning Computer Vision, Natural Language Processing and Artificial Intelligence (AI). In conventional VQA, one may ask questions about an image which can be answered purely based on its content. For example, given an image with people in it, a typical VQA question may inquire about the number of people in the image. More recently, there is growing interest in answering questions which require commonsense knowledge involving common nouns (e.g., cats, dogs, microphones) present in the image. In spite of this progress, the important problem of answering questions requiring world knowledge about named entities (e.g., Barack Obama, White House, United Nations) in the image has not been addressed in prior research. We address this gap in this paper, and introduce KVQA – the first dataset for the task of (world) knowledge-aware VQA. KVQA consists of 183K question-answer pairs involving more than 18K named entities and 24K images. Questions in this dataset require multi-entity, multi-relation, and multi-hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. To the best of our knowledge, KVQA is the largest dataset for exploring VQA over KG. Further, we also provide baseline performances using state-of-the-art methods on KVQA.",2019,AAAI,,10.1609/aaai.v33i01.33018876,https://pdfs.semanticscholar.org/d081/8dac77eee5b970736e57a478bcedfb1b15fe.pdf
d08879401204dc1a8183cb98a8c87cfba42e8ad9,1,0,Key Protected Classification for Collaborative Learning,"Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hides the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique.",2020,Pattern Recognit.,1908.10172,10.1016/j.patcog.2020.107327,https://arxiv.org/pdf/1908.10172.pdf
d0d955edbc44067e7fb469d1884eb59236dc770b,1,1,Data Uncertainty Learning in Face Recognition,"Modeling data uncertainty is important for noisy images, but seldom explored for face recognition. The pioneer work, PFE, considers uncertainty by modeling each face image embedding as a Gaussian distribution. It is quite effective. However, it uses fixed feature (mean of the Gaussian) from an existing model. It only estimates the variance and relies on an ad-hoc and costly metric. Thus, it is not easy to use. It is unclear how uncertainty affects feature learning. This work applies data uncertainty learning to face recognition, such that the feature (mean) and uncertainty (variance) are learnt simultaneously, for the first time. Two learning methods are proposed. They are easy to use and outperform existing deterministic methods as well as PFE on challenging unconstrained scenarios. We also provide insightful analysis on how incorporating uncertainty estimation helps reducing the adverse effects of noisy samples and affects the feature learning.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2003.11339,10.1109/cvpr42600.2020.00575,https://arxiv.org/pdf/2003.11339.pdf
d0df01d1a183d23be8fd638fdb2142280c737e80,1,0,Apprentissage automatique pour la détection d'anomalies dans les données ouvertes : application à la cartographie. (Satellite images analysis for anomaly detection in open geographical data),"Dans cette these nous etudions le probleme de detection d’anomalies dans les donnees ouvertes utilisees par l’entreprise Qucit ; aussi bien les donnees metiers de ses clients, que celles permettant de les contextualiser. Dans un premier temps, nous nous sommes interesses a la detection de velos defectueux au sein des donnees de trajets du systeme de velo en libre service de New York. Nous cherchons des donnees refletant une anomalie dans la realite. Des caracteristiques decrivant le comportement de chaque velo observe sont partitionnes. Les comportements anormaux sont extraits depuis ce partitionnement et compares aux rapports mensuels indiquant le nombre de velos repares ; c’est un probleme d’apprentissage a sortie agregee. Les resultats de ce premier travail se sont averes insatisfaisant en raison de la pauvrete des donnees. Ce premier volet des travaux a ensuite laisse place a une problematique tournee vers la detection de bâtiments au sein d’images satellites. Nous cherchons des anomalies dans les donnees geographiques qui ne refletent pas la realite. Nous proposons une methode de fusion de modeles de segmentation ameliorant la metrique d’erreur jusqu’a +7% par rapport a la methode standard. Nous evaluons la robustesse de notre modele face a la suppression de bâtiments dans les etiquettes, afin de determiner a quel point les omissions sont susceptibles d’en alterer les resultats. Ce type de bruit est communement rencontre au sein des donnees OpenStreetMap, regulierement utilisees par Qucit, et la robustesse observee indique qu’il pourrait etre corrige.",2018,,,,https://pdfs.semanticscholar.org/d0df/01d1a183d23be8fd638fdb2142280c737e80.pdf
d0e10059e473f57a0a89b61a0d15a09bb2624641,1,0,Deep Latent Space Learning for Cross-Modal Mapping of Audio and Visual Signals,"We propose a novel deep training algorithm for joint representation of audio and visual information which consists of a single stream network (SSNet) coupled with a novel loss function to learn a shared deep latent space representation of multimodal information. The proposed framework characterizes the shared latent space by leveraging the class centers which helps to eliminate the need of pairwise or triplet supervision. We quantitatively and qualitatively evaluate the proposed approach on VoxCeleb, a benchmarks audio-visual dataset on multitude of tasks including cross-modal verification, cross-modal matching and cross-modal retrieval. State-of-the-art performance is achieved on cross-modal verification and matching while comparable results are observed on the remaining applications. Our experiments demonstrate the effectiveness of the technique for cross-modal biometric applications.",2019,2019 Digital Image Computing: Techniques and Applications (DICTA),1909.08685,10.1109/DICTA47822.2019.8945863,https://arxiv.org/pdf/1909.08685.pdf
d146b70db066a4b775131104479ddf058023cc76,1,0,Cosmetic-Aware Makeup Cleanser,"Face verification aims at determining whether a pair of face images belongs to the same identity. Recent studies have revealed the negative impact of facial makeup on the verification performance. With the rapid development of deep generative models, this paper proposes a semanticaware makeup cleanser (SAMC) to remove facial makeup under different poses and expressions and achieve verification via generation. The intuition lies in the fact that makeup is a combined effect of multiple cosmetics and tailored treatments should be imposed on different cosmetic regions. To this end, we present both unsupervised and supervised semantic-aware learning strategies in SAMC. At image level, an unsupervised attention module is jointly learned with the generator to locate cosmetic regions and estimate the degree. At feature level, we resort to the effort of face parsing merely in training phase and design a localized texture loss to serve complements and pursue superior synthetic quality. The experimental results on four makeuprelated datasets verify that SAMC not only produces appealing de-makeup outputs at a resolution of 256*256, but also facilitates makeup-invariant face verification through image generation.",2020,ArXiv,2004.09147,,https://arxiv.org/pdf/2004.09147.pdf
d1826354e1e2635e8aa08d7faace176add535f5f,0,1,Live Face De-Identification in Video,"We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1911.08348,10.1109/ICCV.2019.00947,https://arxiv.org/pdf/1911.08348.pdf
d197d0ab852c20122ffb1833fb29199ca77ce9bb,0,1,Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video Processing,"Convolutional Neural Networks with 3D kernels (3D CNNs) currently achieve state-of-the-art results in video recognition tasks due to their supremacy in extracting spatiotemporal features within video frames. There have been many successful 3D CNN architectures surpassing the state-of-the-art results successively. However, nearly all of them are designed to operate offline creating several serious handicaps during online operation. Firstly, conventional 3D CNNs are not dynamic since their output features represent the complete input clip instead of the most recent frame in the clip. Secondly, they are not temporal resolution-preserving due to their inherent temporal downsampling. Lastly, 3D CNNs are constrained to be used with fixed temporal input size limiting their flexibility. In order to address these drawbacks, we propose dissected 3D CNNs, where the intermediate volumes of the network are dissected and propagated over depth (time) dimension for future calculations, substantially reducing the number of computations at online operation. For action classification, the dissected version of ResNet models performs 74-90% fewer computations at online operation while achieving $\sim$5% better classification accuracy on the Kinetics-600 dataset than conventional 3D ResNet models. Moreover, the advantages of dissected 3D CNNs are demonstrated by deploying our approach onto several vision tasks, which consistently improved the performance.",2020,ArXiv,2009.14639,,https://arxiv.org/pdf/2009.14639.pdf
d1b837acc371364dbbdbc6139c9729efb3458f62,1,0,3D Human Face Reconstruction and 2D Appearance Synthesis,"OF DISSERTATION 3D Human Face Reconstruction and 2D Appearance Synthesis 3D human face reconstruction has been an extensive research for decades due to its wide applications, such as animation, recognition and 3D-driven appearance synthesis. Although commodity depth sensors are widely available in recent years, image based face reconstruction are significantly valuable as images are much easier to access and store. In this dissertation, we first propose three image-based face reconstruction approaches according to different assumption of inputs. In the first approach, face geometry is extracted from multiple key frames of a video sequence with different head poses. The camera should be calibrated under this assumption. As the first approach is limited to videos, we propose the second approach then focus on single image. This approach also improves the geometry by adding fine grains using shading cue. We proposed a novel albedo estimation and linear optimization algorithm in this approach. In the third approach, we further loose the constraint of the input image to arbitrary in the wild images. Our proposed approach can robustly reconstruct high quality model even with extreme expressions and large poses. We then explore the applicability of our face reconstructions on four interesting applications: video face beautification, Generating personalized facial blendshape from image sequences, face video stylizing and video face replacement. We demonstrate great potentials of our reconstruction approaches on these real-world applications. In particular, with the recent surge of interests in VR/AR, it is increasingly common to see people wearing head-mounted displays. However, the large occlusion on face is a big obstacle for people to communicate in a face-to-face manner. Our another application is that we explore hardware/software solutions for synthesizing the face image with presence of HMDs. We design two setups(experimental and mobile) which integrate two near IR cameras and one color camera to solve this problem. With our algorithm and prototype, we can achieve photo-realistic results. We further propose a deep neutral network to solve the HMD removal problem considering it as a face inpainting problem. This approach doesn’t need special hardware and run in real-time with satisfying results.",2018,,,10.13023/ETD.2018.216,
d1cba93b3d8d8ad96eb671a56e6138331552ad4d,0,1,Action-Independent Generalized Behavioral Identity Descriptors for Look-alike Recognition in Videos,"There is a long history of exploitation of the visual similarity of look-alikes for fraud and deception. The visual similarity along with the application of physical and digital cosmetics greatly challenges the recognition ability of average humans. Face recognition systems are not an exception in this regard and are vulnerable to such similarities. In contrast to physiological face recognition, behavioral face recognition is often overlooked due to the outstanding success of the former. However, the behavior of a person can provide an additional source of discriminative information with regards to the identity of individuals when physiological attributes are not reliable. In this study, we propose a novel biometric recognition system based only on facial behavior for the differentiation of look-alikes in unconstrained recording conditions. To this end, we organized a dataset of 85, 656 utterances from 1000 look-alike pairs based on videos collected from the wild, large enough for the development of deep learning solutions. Our selection criteria assert that for these collected videos, both state-of-the-art biometric systems and human judgment fail in recognition. Furthermore, to utilize the advantage of large-scale data, we introduce a novel action-independent biometric recognition system that was trained using triplet-loss to create generalized behavioral identity embeddings. We achieve look-alike recognition equal-error-rate of 7.93% with sole reliance on the behavior descriptors extracted from facial landmark movements. The proposed method can have applications in face recognition as well as presentation attack detection and Deepfake detection.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
d23569e56365ec2e5a7498eb0721c799cfdc27a7,0,1,Efficiency Analysis of Post-quantum-secure Face Template Protection Schemes based on Homomorphic Encryption,"Since biometric characteristics are not revocable and biometric data is sensitive, privacy-preserving methods are essential to operate a biometric recognition system. More precisely, the biometric information protection standard ISO/IEC IS 24745 requires that biometric templates are stored and compared in a secure domain. Using homomorphic encryption (HE), we can ensure permanent protection since mathematical operations on the ciphertexts directly correspond to those on the plaintexts. Thus, HE allows to compute the distance between two protected templates in the encrypted domain without a degradation of biometric performance with respect to the corresponding system. In this paper, we benchmark three post-quantum-secure HE schemes, and thereby show that a face verification in the encrypted domain requires only 50 ms transaction time and a template size of 5.5 KB.",2020,2020 International Conference of the Biometrics Special Interest Group (BIOSIG),,,
d235afbd0c3f1515a1ac0f7dbb119d57addf81ce,0,1,Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars,"We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person's appearance by decomposing it into two layers. The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system.",2020,ECCV,2008.10174,10.1007/978-3-030-58610-2_31,https://arxiv.org/pdf/2008.10174.pdf
d23ac9b910161e9fd253131803619cbaeeb1f5f9,1,1,Exploring Unlabeled Faces for Novel Attribute Discovery,"Despite remarkable success in unpaired image-to-image translation, existing systems still require a large amount of labeled images. This is a bottleneck for their real-world applications; in practice, a model trained on labeled CelebA dataset does not work well for test images from a different distribution -- greatly limiting their application to unlabeled images of a much larger quantity. In this paper, we attempt to alleviate this necessity for labeled data in the facial image translation domain. We aim to explore the degree to which you can discover novel attributes from unlabeled faces and perform high-quality translation. To this end, we use prior knowledge about the visual world as guidance to discover novel attributes and transfer them via a novel normalization method. Experiments show that our method trained on unlabeled data produces high-quality translations, preserves identity, and be perceptually realistic, as good as, or better than, state-of-the-art methods trained on labeled data.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1912.03085,10.1109/cvpr42600.2020.00586,https://arxiv.org/pdf/1912.03085.pdf
d2a4361533fe6657762b38e445d19b300b572672,0,1,Application of Difficult Sample Mining based on Cosine Loss in Face Recognition,"Due to the development of deep convolutional neural networks, face recognition has made great progress, and its main goal is how to improve feature recognition capabilities. In this regard, several loss functions based on angular boundaries have been proposed to increase the feature margin between different classes. Although very good results have been achieved in this direction, there are still some problems. These loss functions only expand the feature margin from the perspective of real classification in training, and do not provide distinguishability for misclassified samples. In order to solve this problem, this paper improves on the original cosine loss function, and implements feature learning in the direction of difficult samples based on misclassified feature vectors.",2020,2020 IEEE International Conference on Mechatronics and Automation (ICMA),,10.1109/ICMA49215.2020.9233852,
d2d1a81ae7dc5a206509dbc25e2b25d66c09607d,1,1,HLT-NUS Submission for NIST 2019 Multimedia Speaker Recognition Evaluation,"This work describes the speaker verification system developed by Human Language Technology Laboratory, National University of Singapore (HLT-NUS) for 2019 NIST Multimedia Speaker Recognition Evaluation (SRE). The multimedia research has gained attention to a wide range of applications and speaker recognition is no exception to it. In contrast to the previous NIST SREs, the latest edition focuses on a multimedia track to recognize speakers with both audio and visual information. We developed separate systems for audio and visual inputs followed by a score level fusion of the systems from the two modalities to collectively use their information. The audio systems are based on x-vector based speaker embedding, whereas the face recognition systems are based on ResNet and InsightFace based face embeddings. With post evaluation studies and refinements, we obtain an equal error rate (EER) of 0.88% and an actual detection cost function (actDCF) of 0.026 on the evaluation set of 2019 NIST multimedia SRE corpus.",2020,ArXiv,2010.03905,,https://arxiv.org/pdf/2010.03905.pdf
d2edbdc2cc062db0b0db2264388dfb16681b6043,1,1,Black-Box Face Recovery from Identity Features,"In this work, we present a novel algorithm based on an it-erative sampling of random Gaussian blobs for black-box face recovery, given only an output feature vector of deep face recognition systems. We attack the state-of-the-art face recognition system (ArcFace) to test our algorithm. Another network with different architecture (FaceNet) is used as an independent critic showing that the target person can be identified with the reconstructed image even with no access to the attacked model. Furthermore, our algorithm requires a significantly less number of queries compared to the state-of-the-art solution.",2020,ArXiv,2007.13635,,https://arxiv.org/pdf/2007.13635.pdf
d305ef0945edcae6cd643450bcd8a48984ba87f1,0,1,A lightweight face recognition method based on depthwise separable convolution and triplet loss,"At present, there are two main challenges for large-scale face recognition based on deep learning. One is to design an appropriate loss function to enhance the discrimination ability. The other is that the deployment environment of face recognition often has problems such as low performance and high real-time requirements. In order to solve the above two problems, this paper designs a method combine L2 loss and triplet loss to form a loss function, and uses the Depthwise Separable Convolutions to improve the face residual network so that reduce the amount of network parameters. These methods can effectively enhance the robustness of the system, and reduce the amount of network parameters to improve real-time performance and reduce performance requirements. The experimental results show that the method used in this experiment has a recognition rate of 99.36% on the LFW standard test set, and the amount of parameters of the improved network model is about 45% less than that of the face residual network.",2020,2020 39th Chinese Control Conference (CCC),,10.23919/CCC50068.2020.9189491,
d34423178d6befb791058dd59acfe1bd449c75b0,0,1,Using Deep Learning to Recognize People by Face and Voice,"There are many ways to build a person identification system and those systems can be used for authentication and security. The latest phones for example, bring fingerprint readers to enhance the user experience. From our perspective on Neural Networkswe determine that Machine Learning is enough to guarantee someone’s identity without the need of any specific sensors other than a camera and a microphone. It is achievable with pictures of their face, sounds of their voice and Deep Learning. This work presents a study to build an application to allow biometric authentication using only multimedia and Deep Learning.",2019,,,10.5753/webmedia_estendido.2019.8143,https://pdfs.semanticscholar.org/731d/b2f8c25f74b66b358f61bbc4006226e7b4f7.pdf
d3536975f9f340c4d023f37acb76873f079c31b1,0,1,Side Information for Face Completion: A Robust PCA Approach,"Robust principal component analysis (RPCA) is a powerful method for learning low-rank feature representation of various visual data. However, for certain types as well as significant amount of error corruption, it fails to yield satisfactory results; a drawback that can be alleviated by exploiting domain-dependent prior knowledge or information. In this paper, we propose two models for the RPCA that take into account such side information, even in the presence of missing values. We apply this framework to the task of UV completion which is widely used in pose-invariant face recognition. Moreover, we construct a generative adversarial network (GAN) to extract side information as well as subspaces. These subspaces not only assist in the recovery but also speed up the process in case of large-scale data. We quantitatively and qualitatively evaluate the proposed approaches through both synthetic data and eight real-world datasets to verify their effectiveness.",2019,IEEE Transactions on Pattern Analysis and Machine Intelligence,1801.0758,10.1109/TPAMI.2019.2902556,http://jultika.oulu.fi/files/nbnfi-fe2020051229393.pdf
d35534f3f59631951011539da2fe83f2844ca245,1,0,Semantically Decomposing the Latent Spaces of Generative Adversarial Networks,"We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs.",2018,ICLR,1705.07904,,https://arxiv.org/pdf/1705.07904.pdf
d369c9d86e229d2f3eacd7e2fbf0b4a8b4ce83ff,0,1,Self-supervised classification for detecting anomalous sounds,"Representation learning, using self-supervised classification has recently been shown to give state-of-the-art accuracies for anomaly detection on computer vision datasets. Geometric transformations on images such as rotations, translations and flipping have been used in these recent works to create auxiliary classification tasks for feature learning. This paper introduces a new self-supervised classification framework for anomaly detection in audio signals. Classification tasks are set up based on differences in the metadata associated with the audio files. Synthetic augmentations such as linearly combining and warping audio-spectrograms are also used to increase the complexity of the classification task, to learn finer features. The proposed approach is validated using the publicly available DCASE 2020 challenge task 2: Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring dataset. We demonstrate the effectiveness of our approach by comparing against the baseline autoencoder model, showing an improvement of over 12.8% in the average AUC metrics using a MobileNetV2 based model. Ensembles of these models with our concurrently published Group-Masked Auto-Encoder won the top 3 positions in the DCASE 2020 challenge task 2.",2020,,,,https://assets.amazon.science/8f/33/04709ab4460da4af7f80528ab61c/self-supervised-classification-for-detecting-anomalous-sounds.pdf
d36eccfa525d4d1573c4d24e1cb2af362ccf1bea,1,0,Age Estimation Using Expectation of Label Distribution Learning,"Age estimation performance has been greatly improved by using convolutional neural network. However, existing methods have an inconsistency between the training objectives and evaluation metric, so they may be suboptimal. In addition, these methods always adopt image classification or face recognition models with a large amount of parameters, which bring expensive computation cost and storage overhead. To alleviate these issues, we design a lightweight network architecture and propose a unified framework which can jointly learn age distribution and regress age. The effectiveness of our approach has been demonstrated on apparent and real age estimation tasks. Our method achieves new state-of-the-art results using the single model with 36× fewer parameters and 2.6× reduction in inference time. Moreover, our method can achieve comparable results as the state-of-the-art even though model parameters are further reduced to 0.9M (3.8MB disk storage). We also analyze that Ranking methods are implicitly learning label distributions.",2018,IJCAI,,10.24963/ijcai.2018/99,https://pdfs.semanticscholar.org/d550/ea822a7f8b98e7016c4953df75a0f454a635.pdf
d39729b02f54da845301c7999632a826bf4566e8,1,0,Face Identification for an in-vehicle Surveillance System Using Near Infrared Camera,"Face identification is an essential topic in surveillance system research. Surveillance systems have many unconstrained conditions, e.g., brightness, occlusion, and user state variations. In this paper, we propose a multi-SVM based face recognition method using a near-infrared camera. Our method has a face identification scenario optimized for an in-vehicle surveillance system, which comprises two steps: (i) registering a driver and (ii) recognizing whether the driver is a registered. We perform feature extraction and recognition for each facial landmark. In the case of extreme exposure to light, we convert normal face images into simulated light overexposed images for learning. Thus, face classifiers for normal and extreme illumination conditions are simultaneously generated. We also create a new face dataset and evaluate our method with both our new and PolyU NIR datasets. Experimental results show that we achieve significantly higher recognition accuracy than existing methods.",2018,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),,10.1109/AVSS.2018.8639472,
d3e33d56c7f246dea87a81c4983a3bb7083da973,0,1,Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs,"Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric clues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code and models will be released at this https URL.",2020,ArXiv,2011.00844,,https://arxiv.org/pdf/2011.00844.pdf
d3e7465d822446c981a426b32b49b1e0d0393927,0,1,Enabling and Emerging Technologies for Social Distancing: A Comprehensive Survey,"Social distancing is crucial for preventing the spread of viral diseases illnesses such as COVID-19. By minimizing the closely physical contact between people, we can reduce chances of catching the virus and spreading it to the community. This paper aims to provide a comprehensive survey on how emerging technologies, e.g., wireless and networking, AI can enable, encourage, and even enforce social distancing. To that end, we provide a fundamental background of social distancing including basic concepts, measurements, models and propose practical scenarios. We then discuss enabling wireless technologies which are especially effective and can be widely adopted in practice to keep distance and monitor people. After that, emerging technologies such as machine learning, computer vision, thermal, ultrasound, etc., are introduced. These technologies open many new solutions and directions to deal with problems in social distancing, e.g., symptom prediction, detection and monitoring quarantined people, and contact tracing. Finally, we provide important open issues and challenges (e.g., privacy-preserving, cybersecurity) in implementing social distancing in practice.",2020,,2005.02816,10.1109/ACCESS.2020.3018140,https://arxiv.org/pdf/2005.02816.pdf
d40fe197fd5289aac49ce509adf0c021fa72b732,0,1,Robust Face Verification via Disentangled Representations,"We introduce a robust algorithm for face verification, i.e., deciding whether twoimages are of the same person or not. Our approach is a novel take on the idea ofusing deep generative networks for adversarial robustness. We use the generativemodel during training as an online augmentation method instead of a test-timepurifier that removes adversarial noise. Our architecture uses a contrastive loss termand a disentangled generative model to sample negative pairs. Instead of randomlypairing two real images, we pair an image with its class-modified counterpart whilekeeping its content (pose, head tilt, hair, etc.) intact. This enables us to efficientlysample hard negative pairs for the contrastive loss. We experimentally show that, when coupled with adversarial training, the proposed scheme converges with aweak inner solver and has a higher clean and robust accuracy than state-of-the-art-methods when evaluated against white-box physical attacks.",2020,ArXiv,2006.03638,,https://arxiv.org/pdf/2006.03638.pdf
d42a90ebbdfcf639a0412763851c9452a9226489,1,0,Boosting Deep Face Recognition via Disentangling Appearance and Geometry,"In this paper, we propose a framework for disentangling the appearance and geometry representations in the face recognition task. To provide supervision for this aim, we generate geometrically identical faces by incorporating spatial transformations. We demonstrate that the proposed approach enhances the performance of deep face recognition models by assisting the training process in two ways. First, it enforces the early and intermediate convolutional layers to learn more representative features that satisfy the properties of disentangled embeddings. Second, it augments the training set by altering faces geometrically. Through extensive experiments, we demonstrate that integrating the proposed approach into state-of-the-art face recognition methods effectively improves their performance on challenging datasets, such as LFW, YTF, and MegaFace. Both theoretical and practical aspects of the method are analyzed rigorously by concerning ablation studies and knowledge transfer tasks. Furthermore, we show that the knowledge leaned by the proposed method can favor other face-related tasks, such as attribute prediction.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),2001.04559,10.1109/WACV45572.2020.9093326,https://arxiv.org/pdf/2001.04559.pdf
d45fd2b4583dd218091003ab65a4d77ccec6da47,1,0,Large-Scale Bisample Learning on ID Versus Spot Face Recognition,"In real-world face recognition applications, there is a tremendous amount of data with two images for each person. One is an ID photo for face enrollment, and the other is a probe photo captured on spot. Most existing methods are designed for training data with limited breadth (a relatively small number of classes) and sufficient depth (many samples for each class). They would meet great challenges on ID versus Spot (IvS) data, including the under-represented intra-class variations and an excessive demand on computing devices. In this paper, we propose a deep learning based large-scale bisample learning (LBL) method for IvS face recognition. To tackle the bisample problem with only two samples for each class, a classification–verification–classification training strategy is proposed to progressively enhance the IvS performance. Besides, a dominant prototype softmax is incorporated to make the deep learning scalable on large-scale classes. We conduct LBL on a IvS face dataset with more than two million identities. Experimental results show the proposed method achieves superior performance to previous ones, validating the effectiveness of LBL on IvS face recognition.",2019,International Journal of Computer Vision,1806.03018,10.1007/s11263-019-01162-8,https://arxiv.org/pdf/1806.03018.pdf
d4f100ca5edfe53b562f1d170b2c48939bab0e27,1,0,ArcFace: Additive Angular Margin Loss for Deep Face Recognition,"One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1801.07698,10.1109/CVPR.2019.00482,https://arxiv.org/pdf/1801.07698.pdf
d56eb17f086da77c71398b3d6c45a44ef1a9a5d4,1,0,Post-Comparison Mitigation of Demographic Bias in Face Recognition Using Fair Score Normalization,"Current face recognition systems achieved high progress on several benchmark tests. Despite this progress, recent works showed that these systems are strongly biased against demographic sub-groups. Consequently, an easily integrable solution is needed to reduce the discriminatory effect of these biased systems. Previous work introduced fairness-enhancing solutions that strongly degrades the overall system performance. In this work, we propose a novel fair score normalization approach that is specifically designed to reduce the effect of bias in face recognition and subsequently lead to a significant overall performance boost. Our hypothesis is built on the notation of individual fairness by designing a normalization approach that leads to treating ""similar"" individuals ""similarly"". Experiments were conducted on two publicly available datasets captured under controlled and in-the-wild circumstances. The results show that our fair normalization approach enhances the overall performance by up to 14.8% under intermediate false match rate settings and up to 30.7% under high security settings. Our proposed approach significantly reduces the errors of all demographic groups, and thus reduce bias. Especially under in-the-wild conditions, we demonstrated that our fair normalization method improves the recognition performance of the effected population sub-groups by 31.6%. Unlike previous work, our proposed fairness-enhancing solution does not require demographic information about the individuals, leads to an overall performance boost, and can be easily integrated in existing biometric systems.",2020,ArXiv,2002.03592,,https://arxiv.org/pdf/2002.03592.pdf
d67dd32a6bb38e530642f97fd763aff9a24e5f69,0,1,SMOT: Single-Shot Multi Object Tracking,"We present single-shot multi-object tracker (SMOT), a new tracking framework that converts any single-shot detector (SSD) model into an online multiple object tracker, which emphasizes simultaneously detecting and tracking of the object paths. Contrary to the existing tracking by detection approaches [37,43,17] which suffer from errors made by the object detectors, SMOT adopts the recently proposed scheme of tracking by re-detection. We combine this scheme with SSD detectors by proposing a novel tracking anchor assignment module. With this design SMOTis able to generate tracklets with a constant per-frame runtime. A light-weighted linkage algorithm is then used for online tracklet linking. On three benchmarks of object tracking: Hannah, Music Videos, and MOT17, the proposed SMOT achieves state-of-the-art performance.",2020,ArXiv,2010.16031,,https://arxiv.org/pdf/2010.16031.pdf
d6ec232e30ae03c1f97981fb00c84195f9f9bff3,0,1,Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision,"The goal of this work is to train discriminative cross-modal embeddings without access to manually annotated data. Recent advances in self-supervised learning have shown that effective representations can be learnt from natural cross-modal synchrony. We build on earlier work to train embeddings that are more discriminative for uni-modal downstream tasks. To this end, we propose a novel training strategy that not only optimises metrics across modalities, but also enforces intra-class feature separation within each of the modalities. The effectiveness of the method is demonstrated on two downstream tasks: lip reading using the features trained on audio-visual synchronisation, and speaker recognition using the features trained for cross-modal biometric matching. The proposed method outperforms state-of-the-art self-supervised baselines by a signficant margin.",2020,INTERSPEECH,2004.14326,10.21437/Interspeech.2020-1113,https://arxiv.org/pdf/2004.14326.pdf
d720c07a9d82d4aff214d90bd227af1e9b134b99,1,1,Generalizing Face Representation with Unlabeled Data,"In recent years, significant progress has been made in face recognition due to the availability of large-scale labeled face datasets. However, since the faces in these datasets usually contain limited degree and types of variation, the models trained on them generalize poorly to more realistic unconstrained face datasets. While collecting labeled faces with larger variations could be helpful, it is practically infeasible due to privacy and labor cost. In comparison, it is easier to acquire a large number of unlabeled faces from different domains which would better represent the testing scenarios in real-world problems. We present an approach to use such unlabeled faces to learn generalizable face representations, which can be viewed as an unsupervised domain generalization framework. Experimental results on unconstrained datasets show that a small amount of unlabeled data with sufficient diversity can (i) lead to an appreciable gain in recognition performance and (ii) outperform the supervised baseline when combined with less than half of the labeled data. Compared with the state-of-the-art face recognition methods, our method further improves their performance on challenging benchmarks, such as IJB-B, IJB-C and IJB-S.",2020,ArXiv,2003.07936,,https://arxiv.org/pdf/2003.07936.pdf
d75123ec5fd6bae1ac7f251ba81c37404a4564ec,1,1,Hard-Mining Loss based Convolutional Neural Network for Face Recognition,"Face Recognition is one of the prominent problems in the computer vision domain. Witnessing advances in deep learning, significant work has been observed in face recognition, which touched upon various parts of the recognition framework like Convolutional Neural Network (CNN), Layers, Loss functions, etc. Various loss functions such as Cross-Entropy, Angular-Softmax and ArcFace have been introduced to learn the weights of network for face recognition. However, these loss functions are not able to priorities the hard samples as compared to easy samples. Moreover, their learning process is biased due to a number of easy examples compared to hard examples. In this paper, we address this issue by considering hard examples with more priority. In order to do so, We propose a Hard-Mining loss by by increasing the loss for harder examples and decreasing the loss for easy examples. The proposed concept is generic and can be used with any existing loss function. We test the Hard-Mining loss with different losses such as Cross-Entropy, Angular-Softmax and ArcFace. The proposed Hard-Mining loss is tested over widely used the Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) datasets while training is performed over CASIA-WebFace and MS-Celeb-1M datasets. We use the residual network (i.e., ResNet18) for the experimental analysis. The experimental results suggest that the performance of existing loss functions is boosted when used in the framework of the proposed Hard-Mining loss.",2019,ArXiv,1908.09747,,https://arxiv.org/pdf/1908.09747.pdf
d76d7102edb5668cf425af1e806375b5a01dab33,0,1,Audio-driven Talking Face Video Generation with Natural Head Pose,"Real-world talking faces often accompany with natural head movement. However, most existing talking face video generation methods only consider facial animation with fixed head pose. In this paper, we address this problem by proposing a deep neural network model that takes an audio signal A of a source person and a very short video V of a target person as input, and outputs a synthesized highquality talking face video with natural head pose (making use of the visual information in V), expression and lip synchronization (by considering both A and V). The most challenging issue in our work is that natural poses often cause in-plane and out-of-plane head rotations, which makes synthesized talking face video far from realistic. To address this challenge, we reconstruct 3D face animation and re-render it into synthesized frames. To fine tune these frames into realistic ones with smooth background transition, we propose a novel memory-augmented GAN module. Extensive experiments and three user studies show that our method can generate high-quality (i.e., natural head movements, expressions and good lip synchronization) personalized talking face videos, outperforming the state-of-the-art methods.",2020,ArXiv,,,
d7c6b3628725638188dee085b58752f754697d99,1,0,Learning an Evolutionary Embedding via Massive Knowledge Distillation,"Knowledge distillation methods aim at transferring knowledge from a large powerful teacher network to a small compact student one. These methods often focus on close-set classification problems and matching features between teacher and student networks from a single sample. However, many real-world classification problems are open-set. This paper proposes an Evolutionary Embedding Learning (EEL) framework to learn a fast and accurate student network for open-set problems via massive knowledge distillation. First, we revisit the formulation of canonical knowledge distillation and make it suitable for the open-set problems with massive classes. Second, by introducing an angular constraint, a novel correlated embedding loss (CEL) is proposed to match embedding spaces between the teacher and student network from a global perspective. Lastly, we propose a simple yet effective paradigm towards a fast and accurate student network development for knowledge distillation. We show the possibility to implement an accelerated student network without sacrificing accuracy, compared with its teacher network. The experimental results are quite encouraging. EEL achieves better performance with other state-of-the-art methods for various large-scale open-set problems, including face recognition, vehicle re-identification and person re-identification.",2020,International Journal of Computer Vision,,10.1007/s11263-019-01286-x,
d7c85c1b702c22a9883e9bad05bd0cc4d55dd9cf,0,1,A Unit Softmax with Laplacian Smoothing Stochastic Gradient Descent for Deep Convolutional Neural Networks,"Several techniques were designed during last few years to improve the performance of deep architecture by means of appropriate loss functions or activation functions. Arguably, softmax is the traditionally convenient to train Deep Convolutional Neural Networks (DCNNs) for classification task. However, the modern deep learning architectures have exposed its limitation towards feature discriminability. In this paper, we offered a supervision signal for discriminative image features through a modification in softmax to boost up the power of loss function. Amending the original softmax loss and motivated by the A-softmax loss for face recognition, we fixed the angular margin to introduce a unit margin softmax loss. The improved alternative form of softmax is trainable, easy to optimize and stable for usage along with Stochastic Gradient Descent (SGD) and Laplacian Smoothing Stochastic Gradient Descent (LS-SGD) and applicable to classify the digits in image. Experimental results demonstrate a state-of-the-art performance on famous database of handwritten digits the Modified National Institute of Standards and Technology (MNIST) database.",2019,,,10.1007/978-981-15-5232-8_14,
d7cbedbee06293e78661335c7dd9059c70143a28,1,1,MobileFaceNets: Efficient CNNs for Accurate Real-time Face Verification on Mobile Devices,"We present a class of extremely efficient CNN models, MobileFaceNets, which use less than 1 million parameters and are specifically tailored for high-accuracy real-time face verification on mobile and embedded devices. We first make a simple analysis on the weakness of common mobile networks for face verification. The weakness has been well overcome by our specifically designed MobileFaceNets. Under the same experimental conditions, our MobileFaceNets achieve significantly superior accuracy as well as more than 2 times actual speedup over MobileNetV2. After trained by ArcFace loss on the refined MS-Celeb-1M, our single MobileFaceNet of 4.0MB size achieves 99.55% accuracy on LFW and 92.59% TAR@FAR1e-6 on MegaFace, which is even comparable to state-of-the-art big CNN models of hundreds MB size. The fastest one of MobileFaceNets has an actual inference time of 18 milliseconds on a mobile phone. For face verification, MobileFaceNets achieve significantly improved efficiency over previous state-of-the-art mobile CNNs.",2018,CCBR,1804.07573,10.1007/978-3-319-97909-0_46,https://arxiv.org/pdf/1804.07573.pdf
d7de3f4d5bd36df1c3d05d7c9da8da3b923b2ced,0,1,In defence of metric learning for speaker recognition,"The objective of this paper is 'open-set' speaker recognition of unseen speakers, where ideal embeddings should be able to condense information into a compact utterance-level representation that has small intra-speaker and large inter-speaker distance.  A popular belief in speaker recognition is that networks trained with classification objectives outperform metric learning methods. In this paper, we present an extensive evaluation of most popular loss functions for speaker recognition on the VoxCeleb dataset. We demonstrate that the vanilla triplet loss shows competitive performance compared to classification-based losses, and those trained with our proposed metric learning objective outperform state-of-the-art methods.",2020,INTERSPEECH,2003.11982,10.21437/Interspeech.2020-1064,https://arxiv.org/pdf/2003.11982.pdf
d82035fa2fccdc2bac4bdd1ad5adbcdff4a1daaa,0,1,Transfer Learning in 2.5D Face Image for Occlusion Presence and Gender Classification,,2019,,,10.4018/978-1-5225-7862-8.CH006,
d8356103e26cdf83a0259cc209e4e9b543541dcb,1,1,Effective Methods for Lightweight Image-Based and Video-Based Face Recognition,"Face recognition has achieved significant advances with the rise of deep convolutional neural networks (CNNs) and the development of large annotated datasets. However, how to design deep models in lightweight face recognition is still a challenge when aiming at mobile and embedded devices. In this paper, we focus on recent efficient CNN architectures, speedup skills and reduction methods to design models for lightweight face recognition. We combine octave convolution with MobileNet and ResNet for those models sensitive to computation complexity, replace feature output layer for those models sensitive to memory and explore network scaling for more powerful representation. Further, we extract a subset from the whole training dataset to speed up the performance evaluation of different models. We provide a scaling method on MobileFaceNet to boost the performance with the limit of computational cost, and propose a simple supplementary method for average pooling which throws up those noise frames based on the cluster information in video face recognition. With the upper bound of 1G FLOPs computation complexity and 20MB model size, our best model achieves 99.80% accuracy on LFW, 98.48% on AgeDB, 98% on CFP-FP and 97.67% TAR@FAR 10^6 on MegaFace.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00328,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Ma_Effective_Methods_for_Lightweight_Image-Based_and_Video-Based_Face_Recognition_ICCVW_2019_paper.pdf
d83f8a61c11c5b1a92ef412c8f1cf2ec7bd4a026,0,1,The JD AI Speaker Verification System for the FFSVC 2020 Challenge,"This paper presents the development of our systems for the Interspeech 2020 Far-Field Speaker Verification Challenge (FFSVC). Our focus is the task 2 of the challenge, which is to perform far-field text-independent speaker verification using a single microphone array. The FFSVC training set provided by the challenge is augmented by pre-processing the far-field data with both beamforming, voice channel switching, and a combination of weighted prediction error (WPE) and beamforming. Two open-access corpora, CHData in Mandarin and VoxCeleb2 in English, are augmented using multiple methods and mixed with the augmented FFSVC data to form the final training data. Four different model structures are used to model speaker characteristics: ResNet, extended time-delay neural network (ETDNN), Transformer, and factorized TDNN (FTDNN), whose output values are pooled across time using the self-attentive structure, the statistic pooling structure, and the GVLAD structure. The final results are derived by fusing the adaptively normalized scores of the four systems with a two-stage fusion method, which achieves a minimum of the detection cost function (minDCF) of 0.3407 and an equal error rate (EER) of 2.67% on the development set of the challenge.",2020,INTERSPEECH,,10.21437/interspeech.2020-3062,https://isca-speech.org/archive/Interspeech_2020/pdfs/3062.pdf
d8526863f35b29cbf8ac2ae756eaae0d2930ffb1,1,0,Face Generation for Low-Shot Learning Using Generative Adversarial Networks,"Recently, low-shot learning has been proposed for handling the lack of training data in machine learning. Despite of the importance of this issue, relatively less efforts have been made to study this problem. In this paper, we aim to increase the size of training dataset in various ways to improve the accuracy and robustness of face recognition. In detail, we adapt a generator from the Generative Adversarial Network (GAN) to increase the size of training dataset, which includes a base set, a widely available dataset, and a novel set, a given limited dataset, while adopting transfer learning as a backend. Based on extensive experimental study, we conduct the analysis on various data augmentation methods, observing how each affects the identification accuracy. Finally, we conclude that the proposed algorithm for generating faces is effective in improving the identification accuracy and coverage at the precision of 99% using both the base and novel set.",2017,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),,10.1109/ICCVW.2017.229,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w27/Choe_Face_Generation_for_ICCV_2017_paper.pdf
d8b997237a30f7fd87a824c065597b759f5be72f,0,1,Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation,"Recent deep learning based face recognition methods have achieved great performance, but it still remains challenging to recognize very low-resolution query face like 28x28 pixels when CCTV camera is far from the captured subject. Such face with very low-resolution is totally out of detail information of the face identity compared to normal resolution in a gallery and hard to find corresponding faces therein. To this end, we propose a Resolution Invariant Model (RIM) for addressing such cross-resolution face recognition problems, with three distinct novelties. First, RIM is a novel and unified deep architecture, containing a Face Hallucination sub-Net (FHN) and a Heterogeneous Recognition sub-Net (HRN), which are jointly learned end to end. Second, FHN is a well-designed tri-path Generative Adversarial Network (GAN) which simultaneously perceives facial structure and geometry prior information, i.e. landmark heatmaps and parsing maps, incorporated with an unsupervised cross-domain adversarial training strategy to super-resolve very low-resolution query image to its 8x larger ones without requiring them to be well aligned. Third, HRN is a generic Convolutional Neural Network (CNN) for heterogeneous face recognition with our proposed residual knowledge distillation strategy for learning discriminative yet generalized feature representation. Quantitative and qualitative experiments on several benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts. Codes and models will be released upon acceptance.",2019,ArXiv,1905.10777,,https://arxiv.org/pdf/1905.10777.pdf
d931210ba35b348969b3389b62e045f07ea9bcdc,1,0,Led 3 D : A Lightweight and Efficient Deep Approach to Recognizing Low-quality 3 D Faces,"Due to the intrinsic invariance to pose and illumination changes, 3D Face Recognition (FR) has a promising potential in the real world. 3D FR using high-quality faces, which are of high resolutions and with smooth surfaces, have been widely studied. However, research on that with low-quality input is limited, although it involves more applications. In this paper, we focus on 3D FR using lowquality data, targeting an efficient and accurate deep learning solution. To achieve this, we work on two aspects: (1) designing a lightweight yet powerful CNN; (2) generating finer and bigger training data. For (1), we propose a MultiScale Feature Fusion (MSFF) module and a Spatial Attention Vectorization (SAV) module to build a compact and discriminative CNN. For (2), we propose a data processing system including point-cloud recovery, surface refinement, and data augmentation (with newly proposed shape jittering and shape scaling). We conduct extensive experiments on Lock3DFace and achieve state-of-the-art results, outperforming many heavy CNNs such as VGG-16 and ResNet34. In addition, our model can operate at a very high speed (136 fps) on Jetson TX2, and the promising accuracy and efficiency reached show its great applicability on edge/mobile devices.",2019,,,,https://pdfs.semanticscholar.org/9f4e/1d6136ffb732c75e19de3e4df9b82a8916ce.pdf
d95cda3f29834e7f78c309de62f1358860deeaec,1,1,Achieving Better Kinship Recognition Through Better Baseline,"Recognizing blood relations using face images can be seen as an application of face recognition systems with additional restrictions. These restrictions proved to be difficult to deal with, however, recent advancements in face verification show that there is still much to gain using more data and novel ideas. As a result face recognition is a great source domain from which we can transfer the knowledge to get better performance in kinship recognition as a source domain. We present a new baseline for an automatic kinship recognition task and relatives search based on RetinaFace[1] for face registration and ArcFace[2] face verification model. With the approach described above as the foundation, we constructed a pipeline that achieved state-of-the-art performance on two tracks in the recent Recognizing Families In the Wild Data Challenge.",2020,ArXiv,2006.11739,10.1109/FG47880.2020.00137,https://arxiv.org/pdf/2006.11739.pdf
d9670fac9a5e8550716b522b7a6c64b2a4f67794,1,0,Dynamic Region-Aware Convolution,"We propose a new convolution called Dynamic Region-Aware Convolution (DRConv), which can automatically assign multiple filters to corresponding spatial regions where features have similar representation. In this way, DRConv outperforms standard convolution in modeling semantic variations. Standard convolution can increase the number of channels to extract more visual elements but results in high computational cost. More gracefully, our DRConv transfers the increasing channel-wise filters to spatial dimension with learnable instructor, which significantly improves representation ability of convolution and maintains translation-invariance like standard convolution. DRConv is an effective and elegant method for handling complex and variable spatial information distribution. It can substitute standard convolution in any existing networks for its plug-and-play property. We evaluate DRConv on a wide range of models (MobileNet series, ShuffleNetV2, etc.) and tasks (Classification, Face Recognition, Detection and Segmentation.). On ImageNet classification, DRConv-based ShuffleNetV2-0.5x achieves state-of-the-art performance of 67.1% at 46M multiply-adds level with 6.3% relative improvement.",2020,ArXiv,2003.12243,,https://arxiv.org/pdf/2003.12243.pdf
d9836c34ad6538a540dd1fa524457bd5fa347d24,0,1,Revisiting Training Strategies and Generalization Performance in Deep Metric Learning,"Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets. Code and a publicly accessible WandB-repo are available at this https URL.",2020,ICML,2002.08473,,https://arxiv.org/pdf/2002.08473.pdf
d99ebe7df34bc5eeb7cc5251b80d0adb4c5ee358,1,1,Robust Video Facial Authentication With Unsupervised Mode Disentanglement,"Deep learning-based video facial authentication has limitations when it comes to real-world applications, due to large mode variations such as illumination, pose, and eyeglasses variations in real-life situations. Many of existing mode-invariant facial authentication methods need labels of each mode. However, the label information could not be always available in practice. To alleviate this problem, we develop an unsupervised mode disentangling method for video facial authentication. By matching both disentangled identity features and dynamic features of two facial videos, our proposed method shows significant face verification and identification performances on three publicly available datasets, KAIST-MPMI, UVA-NEMO, and YTF.",2020,2020 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP40778.2020.9191052,
d9b4627cfe77ee87f46d798f75680f4ff5ad8942,0,1,Doubly Stochastic Neighbor Embedding on Spheres,"Recently, Stochastic Neighbor Embedding (SNE) methods have widely been applied in data visualization. These methods minimize the divergence between the pairwise similarities of high- and low-dimensional data. Despite their popularity, the current SNE methods experience the ""crowding problem"" when the data include highly imbalanced similarities. This implies that the data points with higher total similarity tend to get crowded around the display center. To solve this problem, we normalize the similarity matrix to be doubly stochastic such that all the data points have equal total similarities. A fast normalization method is proposed. Furthermore, we show empirically and theoretically that the doubly stochasticity constraint often leads to approximately spherical embeddings. This suggests replacing a flat space with spheres as the embedding space. The spherical embedding eliminates the discrepancy between the center and the periphery in visualization and thus resolves the ""crowding problem"". We compared the proposed method with the state-of-the-art SNE method on three real-world datasets. The results indicate that our method is more favorable in terms of visualization quality.",2019,Pattern Recognit. Lett.,1609.01977,10.1016/j.patrec.2019.08.026,https://arxiv.org/pdf/1609.01977.pdf
d9da5523b3b82b28f2dd5d7ca83c6d57bc680bf5,1,0,PPCU Sam: Open-source face recognition framework,"Abstract In recent years by the popularization of AI, an increasing number of enterprises deployed machine learning algorithms in real life settings. This trend shed light on leaking spots of the Deep Learning bubble, namely the catastrophic decrease in quality when the distribution of the test data shifts from the training data. It is of utmost importance that we treat the promises of novel algorithms with caution and discourage reporting near perfect experimental results by fine-tuning on fixed test sets and finding metrics that hide weak points of the proposed methods. To support the wider acceptance of computer vision solutions we share our findings through a case-study in which we built a face-recognition system from scratch using consumer grade devices only, collected a database of 100k images from 150 subjects and carried out extensive validation of the most prominent approaches in single-frame face recognition literature. We show that the reported worst-case score, 74.3% true-positive ratio drops below 46.8% on real data. To overcome this barrier, after careful error analysis of the single-frame baselines we propose a low complexity solution to cover the failure cases of the single-frame recognition methods which yields an increased stability in multi-frame recognition during test time. We validate the effectiveness of the proposal by an extensive survey among our users which evaluates to 89.5% true-positive ratio.",2019,KES,,10.1016/j.procs.2019.09.367,
da13b4b500ec16326fcd49144b9db6a9088a7a3f,1,0,Video Recordings of Male Face and Neck Movements for Facial Recognition and Other Purposes,"Facial recognition is made more difficult by unusual facial positions and movement. However, for many applications, the ability to accurately recognize moving subjects with movement-distorted facial features is required. This dataset includes videos of multiple subjects, taken under multiple lighting brightness and temperature conditions, which can be used to train and evaluate the performance of facial recognition systems.",2019,Data,,10.3390/data4030130,https://pdfs.semanticscholar.org/607d/ca6c6f651ed857d5bb42cd9e4168a604be23.pdf
da57bcc1a6c3b12eece51eded72035d21af10584,1,1,VarGNet: Variable Group Convolutional Neural Network for Efficient Embedded Computing,"In this paper, we propose a novel network design mechanism for efficient embedded computing. Inspired by the limited computing patterns, we propose to fix the number of channels in a group convolution, instead of the existing practice that fixing the total group numbers. Our solution based network, named Variable Group Convolutional Network (VarGNet), can be optimized easier on hardware side, due to the more unified computing schemes among the layers. Extensive experiments on various vision tasks, including classification, detection, pixel-wise parsing and face recognition, have demonstrated the practical value of our VarGNet.",2019,ArXiv,1907.05653,,https://arxiv.org/pdf/1907.05653.pdf
da80ff11d362ca1e609cf16a57b078c8879d8f43,0,1,Personal Privacy Protection via Irrelevant Faces Tracking and Pixelation in Video Live Streaming,"To date, the privacy-protection intended pixelation tasks are still labor-intensive and yet to be studied. With the prevailing of video live streaming, establishing an online face pixelation mechanism during streaming is an urgency. In this paper, we develop a new method called Face Pixelation in Video Live Streaming (FPVLS) to generate automatic personal privacy filtering during unconstrained streaming activities. Simply applying multi-face trackers will encounter problems in target drifting, computing efficiency, and over-pixelation. Therefore, for fast and accurate pixelation of irrelevant people’s faces, FPVLS is organized in a frame-to-video structure of two core stages. On individual frames, FPVLS utilizes image-based face detection and embedding networks to yield face vectors. In the raw trajectories generation stage, the proposed Positioned Incremental Affinity Propagation (PIAP) clustering algorithm leverages face vectors and positioned information to quickly associate the same person’s faces across frames. Such frame-wise accumulated raw trajectories are likely to be intermittent and unreliable on video level. Hence, we further introduce the trajectory refinement stage that merges a proposal network with the two-sample test based on the Empirical Likelihood Ratio (ELR) statistic to refine the raw trajectories. A Gaussian filter is laid on the refined trajectories for final pixelation. On the video live streaming dataset we collected, FPVLS obtains satisfying accuracy, real-time efficiency, and contains the over-pixelation problems.",2021,IEEE Transactions on Information Forensics and Security,,10.1109/TIFS.2020.3029913,
da94ecc33f8339abb6e1ceb3a17f27f4fcb16211,0,1,A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-Output Classification Problems,"This article describes one objective function for learning semantically coherent feature embeddings in multi-output classification problems, i.e., when the response variables have dimension higher than one. Such coherent embeddings can be used simultaneously for different tasks, such as identity retrieval and soft biometrics labelling. We propose a generalization of the triplet loss that: 1) defines a metric that considers the number of agreeing labels between pairs of elements; 2) introduces the concept of similar classes, according to the values provided by the metric; and 3) disregards the notion of anchor, sampling four arbitrary elements at each time, from where two pairs are defined. The distances between elements in each pair are imposed according to their semantic similarity (i.e., the number of agreeing labels). Likewise the triplet loss, our proposal also privileges small distances between positive pairs. However, the key novelty is to additionally enforce that the distance between elements of any other pair corresponds inversely to their semantic similarity. The proposed loss yields embeddings with a strong correspondence between the classes centroids and their semantic descriptions. In practice, it is a natural choice to jointly infer coarse (soft biometrics) + fine (ID) labels, using simple rules such as k-neighbours. Also, in opposition to its triplet counterpart, the proposed loss appears to be agnostic with regard to demanding criteria for mining learning instances (such as the semi-hard pairs). Our experiments were carried out in five different datasets (BIODI, LFW, IJB-A, Megaface and PETA) and validate our assumptions, showing results that are comparable to the state-of-the-art in both the identity retrieval and soft biometrics labelling tasks.",2021,IEEE Transactions on Information Forensics and Security,2002.11644,10.1109/TIFS.2020.3023304,https://arxiv.org/pdf/2002.11644.pdf
daba1d26df01ba007895f4b24938e9c01a596ccf,1,1,DotFAN: A Domain-transferred Face Augmentation Network for Pose and Illumination Invariant Face Recognition,"The performance of a convolutional neural network (CNN) based face recognition model largely relies on the richness of labelled training data. Collecting a training set with large variations of a face identity under different poses and illumination changes, however, is very expensive, making the diversity of within-class face images a critical issue in practice. In this paper, we propose a 3D model-assisted domain-transferred face augmentation network (DotFAN) that can generate a series of variants of an input face based on the knowledge distilled from existing rich face datasets collected from other domains. DotFAN is structurally a conditional CycleGAN but has two additional subnetworks, namely face expert network (FEM) and face shape regressor (FSR), for latent code control. While FSR aims to extract face attributes, FEM is designed to capture a face identity. With their aid, DotFAN can learn a disentangled face representation and effectively generate face images of various facial attributes while preserving the identity of augmented faces. Experiments show that DotFAN is beneficial for augmenting small face datasets to improve their within-class diversity so that a better face recognition model can be learned from the augmented dataset.",2020,ArXiv,2002.09859,,https://arxiv.org/pdf/2002.09859.pdf
db04cf426357724df54519f8b8af1ca8b56dc38e,0,1,Synthesizing Coupled 3 D Face Modalities by Trunk-Branch Generative Adversarial Networks,"Generating realistic 3D faces is of high importance for computer graphics and computer vision applications. Generally, research on 3D face generation revolves around linear statistical models of the facial surface. Nevertheless, these models cannot represent faithfully either the facial texture or the normals of the face, which are very crucial for photo-realistic face synthesis. Recently, it was demonstrated that Generative Adversarial Networks (GANs) can be used for generating high-quality textures of faces. Nevertheless, the generation process either omits the geometry and normals, or independent processes are used to produce 3D shape information. In this paper, we present the first methodology that generates high-quality texture, shape, and normals jointly, which can be used for photo-realistic synthesis. To do so, we propose a novel GAN that can generate data from different modalities while exploiting their correlations. Furthermore, we demonstrate how we can condition the generation on the expression and create faces with various facial expressions. The qualitative results shown in this pre-print is compressed due to size limitations, full resolution results and the accompanying video can be found at the project page: https://github.com/barisgecer/TBGAN.",2019,,,,https://pdfs.semanticscholar.org/db04/cf426357724df54519f8b8af1ca8b56dc38e.pdf
db058667181438b4884f7791c32218e4042b955f,0,1,Semantics-Guided Representation Learning with Applications to Visual Synthesis,"Learning interpretable and interpolatable latent representations has been an emerging research direction, allowing researchers to understand and utilize the derived latent space for further applications such as visual synthesis or recognition. While most existing approaches derive an interpolatable latent space and induces smooth transition in image appearance, it is still not clear how to observe desirable representations which would contain semantic information of interest. In this paper, we aim to learn meaningful representations and simultaneously perform semantic-oriented and visually-smooth interpolation. To this end, we propose an angular triplet-neighbor loss (ATNL) that enables learning a latent representation whose distribution matches the semantic information of interest. With the latent space guided by ATNL, we further utilize spherical semantic interpolation for generating semantic warping of images, allowing synthesis of desirable visual data. Experiments on MNIST and CMU Multi-PIE datasets qualitatively and quantitatively verify the effectiveness of our method.",2020,ArXiv,2010.10772,,https://arxiv.org/pdf/2010.10772.pdf
db366fdece9500485d45c494b200be3130b4f2a9,1,1,Contrastive Learning with Hallucinating Data for Long-Tailed Face Recognition,,2020,ICONIP,,10.1007/978-3-030-63830-6_18,
db54b44fb8fd4a668eff2748c474893ab2645b66,1,1,Differentiable Kernel Evolution,"This paper proposes a differentiable kernel evolution (DKE) algorithm to find a better layer-operator for the convolutional neural network. Unlike most of the other neural architecture searching (NAS) technologies, we consider the searching space in a fundamental scope: kernel space, which encodes the assembly of basic multiply-accumulate (MAC) operations into a conv-kernel. We first deduce a strict form of the generalized convolutional operator by some necessary constraints and construct a continuous searching space for its extra freedom-of-degree, namely, the connection of each MAC. Then a novel unsupervised greedy evolution algorithm called \textit{gradient agreement guided searching} (GAGS) is proposed to learn the optimal location for each MAC in the spatially continuous searching space. We leverage DKE on multiple kinds of tasks such as object classification, face/object detection, large-scale fine-grained and recognition, with various kinds of backbone architecture. Not to mention the consistent performance gain, we found the proposed DKE can further act as an auto-dilated operator, which makes it easy to boost the performance of miniaturized neural networks in multiple tasks.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00192,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Differentiable_Kernel_Evolution_ICCV_2019_paper.pdf
db7892d6bd1e27f4e65ba8d87fb214d2ba337300,0,1,Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery Detection,"Although current deep learning-based face forgery detectors achieve impressive performance in constrained scenarios, they are vulnerable to samples created by unseen manipulation methods. Some recent works show improvements in generalisation but rely on cues that are easily corrupted by common post-processing operations such as compression. In this paper, we propose LipForensics, a detection approach capable of both generalising to novel manipulations and withstanding various distortions. LipForensics targets high-level semantic irregularities in mouth movements, which are common in many generated videos. It consists in first pretraining a spatio-temporal network to perform visual speech recognition (lipreading), thus learning rich internal representations related to natural mouth motion. A temporal network is subsequently finetuned on fixed mouth embeddings of real and forged data in order to detect fake videos based on mouth movements without overfitting to low-level, manipulation-specific artefacts. Extensive experiments show that this simple approach significantly surpasses the state-of-the-art in terms of generalisation to unseen manipulations and robustness to perturbations, as well as shed light on the factors responsible for its performance.",2020,,2012.07657,,https://arxiv.org/pdf/2012.07657.pdf
dbab3abcefa4f93e001b49c3e4cc14bccaa17f89,0,1,Graph Representation for Face Analysis in Image Collections,"Given an image collection of a social event with a huge number of pictures, it is very useful to have tools that can be used to analyze how the individuals --that are present in the collection-- interact with each other. In this paper, we propose an optimal graph representation that is based on the `connectivity' of them. The connectivity of a pair of subjects gives a score that represents how `connected' they are. It is estimated based on co-occurrence, closeness, facial expressions, and the orientation of the head when they are looking to each other. In our proposed graph, the nodes represent the subjects of the collection, and the edges correspond to their connectivities. The location of the nodes is estimated according to their connectivity (the closer the nodes, the more connected are the subjects). Finally, we developed a graphical user interface in which we can click onto the nodes (or the edges) to display the corresponding images of the collection in which the subject of the nodes (or the connected subjects) are present. We present relevant results by analyzing a wedding celebration, a sitcom video, a volleyball game and images extracted from Twitter given a hashtag. We believe that this tool can be very helpful to detect the existing social relations in an image collection.",2019,ArXiv,1911.1197,,https://arxiv.org/pdf/1911.11970.pdf
dbcfb8263d4085e4b83fbbe998968eab5510953c,0,1,PerSeg : segmenting salient objects from bag of single image perturbations,"Salient object segmentation is an important computer vision problem having applications in numerous areas such as video surveillance, scene parsing, autonomous navigation etc. For images, this task is quite challenging due to clutter/texture present in the background, low resolution and/or low contrast of the object(s) of interest etc. In case of videos, additional issues such as object deformation, camera motion and presence of multiple moving objects make the foreground object segmentation a significantly difficult and open problem. However, motion pattern can also act as an important cue to identify the foreground objects against the background. This is exploited by the recent approaches via aggregation of temporally perturbed information from a series of consecutive frames. Unfortunately for images, this additional cue is not available. In this paper, we propose to emulate the effect of such perturbations by constructing a bag of multiple augmentations applied on a single input image. Saliency features are estimated independently from each perturbed image in this bag, which are further combined using a novel aggregation strategy based on a convolutional gated recurrent encoder-decoder unit. Through extensive experiments on the benchmark datasets, we show better or very competitive performance when compared with the state-of-the-art methods. We further observe that even with a bag constructed using simple affine transformations, we achieve impressive performances, proving the robustness of the proposed framework.",2019,Multimedia Tools and Applications,,10.1007/s11042-019-08388-1,
dc1d7673d6e1ac4afcf639b3b7723c306863c039,1,0,Improved Face Verification with Simple Weighted Feature Combination,"Since the appearance of deep learning, face verification (FV) has made great progress with large scale datasets, well-designed networks, new loss functions, fusion of models and metric learning methods. However, incorporating all these methods obviously takes a lot of time both at training and testing stages. In this paper, we just select training images randomly without any clean and alignment procedure. Then we propose a simple weighted average method which combines features of the last two layers with different weights on the modified VGGNet, named as CB-VGG. It is significantly reducing the complexity of time that one model can be treated as two models. LMNN is used as a post-processing procedure to improve the discrimination of the combined features. Our experiments show relatively competitive results on LFW, CFP, and CACD datasets.",2017,CCCV,,10.1007/978-981-10-7302-1_2,
dc7f8535dcfe089609233ecbfdbd279f5a772d6f,0,1,CAN-GAN: Conditioned-attention normalized GAN for face age synthesis,"Abstract This work aims to freely translate an input face to an aging face with robust identity preservation, satisfying aging effect and authentic visual appearance. Witnessing the success of GAN in image synthesis, researchers employ GAN to address the problem of face aging synthesis. However, most GAN-based methods hold that the aging changing of all facial regions is equal, which ignores the fact that different facial regions have distinct aging speeds and aging patterns. To this end, we propose a novel Conditioned-Attention Normalization GAN (CAN-GAN) for age synthesis by leveraging the aging difference between two age groups to capture facial aging regions with different attention factors. In particular, a new Conditioned-Attention Normalization (CAN) layer is designed to enhance the aging-relevant information of face, while smoothing the aging-irrelevant information of face by attention map. Since different facial attributes contribute to the discrimination of age groups with divers degrees, we further present a Contribution-Aware Age Classifier (CAAC) that finely measures the importance of face vector’s elements in terms of the age classification. Qualitative and quantitative experiments on several commonly-used datasets show the advance of CAN-GAN compared with the other competitive methods.",2020,Pattern Recognit. Lett.,,10.1016/j.patrec.2020.08.021,
dca689c34332ed72d26be8235e2e7bbb97d9ab3e,1,1,Study on Face Identification Technology for its implementation in the Schengen Information System,.............................................................................................................. 5 Acknowledgements ................................................................................................ 6 Executive Summary ............................................................................................... 8 Policy Context ................................................................................................... 8 Key Conclusions .............................................................................................. 10 List of recommendations................................................................................... 1,2019,,,10.2760/661464,http://publications.jrc.ec.europa.eu/repository/bitstream/JRC116530/sis_face-jrc_science_for_policy_report_22.07.2019_final.pdf
dcd546b3a8475b9e7e4e67376fb196a23ed923a9,1,0,RD-GAN: Few/Zero-Shot Chinese Character Style Transfer via Radical Decomposition and Rendering,,2020,ECCV,,10.1007/978-3-030-58539-6_10,
dd4717b997a718714c301873e89c052bb468c536,1,1,Unified Representation Learning for Cross Model Compatibility,"We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification.",2020,ArXiv,2008.04821,,https://arxiv.org/pdf/2008.04821.pdf
dd8084b2878ca95d8f14bae73e1072922f0cc5da,1,0,"Model Distillation with Knowledge Transfer in Face Classification, Alignment and Verification","Knowledge distillation is a potential solution for model compression. The idea is to make a small student network imitate the target of a large teacher network, then the student network can be competitive to the teacher one. Most previous studies focus on model distillation in the classification task, where they propose different architectures and initializations for the student network. However, only the classification task is not enough, and other related tasks such as regression and retrieval are barely considered. To solve the problem, in this paper, we take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification. By selecting appropriate initializations and targets in the knowledge transfer, the distillation can be easier in non-classification tasks. Experiments on the CelebA and CASIA-WebFace datasets demonstrate that the student network can be competitive to the teacher one in alignment and verification, and even surpasses the teacher network under specific compression rates. In addition, to achieve stronger knowledge transfer, we also use a common initialization trick to improve the distillation performance of classification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M datasets show the effectiveness of this simple trick.",2017,ArXiv,1709.02929,,https://arxiv.org/pdf/1709.02929.pdf
dd85ff7f6a148ff97161f57ce2abd78ceb739c60,0,1,Visual Diver Face Recognition for Underwater Human-Robot Interaction,"This paper presents a deep-learned facial recognition method for underwater robots to identify scuba divers. Specifically, the proposed method is able to recognize divers underwater with faces heavily obscured by scuba masks and breathing apparatus. Our contribution in this research is towards robust facial identification of individuals under significant occlusion of facial features and image degradation from underwater optical distortions. With the ability to correctly recognize divers, autonomous underwater vehicles (AUV) will be able to engage in collaborative tasks with the correct person in human-robot teams and ensure that instructions are accepted from only those authorized to command the robots. We demonstrate that our proposed framework is able to learn discriminative features from real-world diver faces through different data augmentation and generation techniques. Experimental evaluations show that this framework achieves a 3-fold increase in prediction accuracy compared to the state-of-the-art (SOTA) algorithms and is well-suited for embedded inference on robotic platforms.",2020,ArXiv,2011.09556,,https://arxiv.org/pdf/2011.09556.pdf
dd929f0266fe8827b6f88105b4d791b51b15d746,0,1,Validating Seed Data Samples for Synthetic Identities – Methodology and Uniqueness Metrics,"This work explores the identity attribute of synthetic face samples derived from Generative Adversarial Networks. The goal is to determine if individual samples are unique in terms of identity, firstly with respect to the seed dataset that trains the GAN model and secondly with respect to other synthetic face samples. Two approaches are introduced to enable the comparative analysis of large sets of synthetic face samples. The first of these uses ROC curves to determine identity uniqueness using a number of large publicly available datasets of real facial samples to provide reference ROCs as a baseline. The second approach uses a thresholding technique utilizing again large publicly available datasets as a reference. For this approach, new metrics are introduced, and a technique is provided to remove the most connected data samples within a large synthetic dataset. The remaining synthetic samples can be considered as unique as data samples gathered from different real individuals. Several StyleGAN models are used to create the synthetic datasets, and variations in key model parameters are explored. It is concluded that the resulting synthetic data samples exhibit excellent uniqueness when compared with the original training dataset, but significantly less uniqueness when comparisons are made within the synthetic dataset. Nevertheless, it is possible to remove the most highly connected synthetic data samples. Thus, in some cases, up to 92% of the data samples in a 20k synthetic dataset can be shown to exhibit similar uniqueness to data samples taken from real public datasets.",2020,IEEE Access,,10.1109/ACCESS.2020.3016097,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09165737.pdf
ddb29dfe9365a15a4928f9a879ccfba00b1d2e9d,0,1,EyesGAN: Synthesize human face from human eyes,"Abstract Face recognition recently has achieved remarkable success in many fields, especially in mobile payment, authentication, criminal investigation, and city management. However, face occlusion is still the key problem in person identification, such as in the field of anti-terrorism, criminal cases and public security. To solve this problem, an improved end-to-end deep generative adversarial network (named EyesGAN) has been proposed to synthesize human face from human eyes in this paper, which can be used as a potential scheme for masked face recognition. BicycleGAN is chosen as the baseline and effective improvements have been achieved. First, the self-attentional mechanism is introduced so that the improved model can more effectively learn about the internal mapping between human eyes and face. Second, the perceptual loss is applied to guide the model cyclic training and help with updating the network parameters so that the synthesized face can be of higher-similarity to the ground truth face. Third, EyesGAN has been designed by getting the utmost out of the performance of the perceptual loss and the self-attentional mechanism in GANs. A dataset of eyes-to-face synthesis has been reconstructed based on the public face datasets for training and testing. Finally, the faces synthesized by EyesGAN have been quantitatively and qualitatively compared with the results of existing methods. Extensive experiments demenstrate that our proposed method has performed better than the state-of-the-art methods in terms of Average Euclidean Distance, Average Cosine Similarity, Synthesis Accuracy Percentage, Frechet Inception Distance, and Baidu face recognition rate (the accuracy achieved 96.1 % on 615 test data of CelebA database). In this paper, the feasibility of synthesizing human face from human eyes has been explored, and the attention map shows that our network can predict other parts of the face from eyes.",2020,Neurocomputing,,10.1016/j.neucom.2020.04.121,
ddb2aecf8777007414b1eb341c6c19ec799280d3,1,0,Frame Attention Networks for Facial Expression Recognition in Videos,"The video-based facial expression recognition aims to classify a given video into several basic emotions. How to integrate facial features of individual frames is crucial for this task. In this paper, we propose the Frame Attention Networks (FAN)1, to automatically highlight some discriminative frames in an end-to-end framework. The network takes a video with a variable number of face images as its input and produces a fixed-dimension representation. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which embeds face images into feature vectors. The frame attention module learns multiple attention weights which are used to adaptively aggregate the feature vectors to form a single discriminative video representation. We conduct extensive experiments on CK+ and AFEW8.0 datasets. Our proposed FAN shows superior performance compared to other CNN based methods and achieves state-of-the-art performance on CK+.",2019,2019 IEEE International Conference on Image Processing (ICIP),1907.00193,10.1109/ICIP.2019.8803603,https://arxiv.org/pdf/1907.00193.pdf
ddb35477ddcf8d66a2d695265e7f26d4ce1946c2,0,1,"Information Retrieval Technology: 15th Asia Information Retrieval Societies Conference, AIRS 2019, Hong Kong, China, November 7–9, 2019, Proceedings","This book constitutes the refereed proceedings of the 15th Information Retrieval Technology Conference, AIRS 2019, held in Hong Kong, China, in November 2019.The 14 full papers presented together with 3 short papers were carefully reviewed and selected from 27 submissions. The scope of the conference covers applications, systems, technologies and theory aspects of information retrieval in text, audio, image, video and multimedia data.",2020,AIRS,,10.1007/978-3-030-42835-8,
ddfde808af8dc8b737d115869d6cca780d050884,1,1,Minimum Margin Loss for Deep Face Recognition,"Face recognition has achieved great progress owing to the fast development of the deep neural network in the past a few years. As the baton in a deep neural network, a number of the loss functions have been proposed which significantly improve the state-of-the-art methods. In this paper, we proposed a new loss function called Minimum Margin Loss (MML) which aims at enlarging the margin of those over-close class centre pairs so as to enhance the discriminative ability of the deep features. MML supervises the training process together with the Softmax loss and the Centre loss, and also makes up the defect of Softmax + Centre loss. The experimental results on LFW and YTF datasets show that the proposed method achieves the state-of-the-art performance, which demonstrates the effectiveness of the proposed MML.",2020,Pattern Recognit.,1805.06741,10.1016/j.patcog.2019.107012,https://arxiv.org/pdf/1805.06741.pdf
de00fffe4b64aef3797e05e74b5d3d07065b20ee,0,1,Advances in Speaker Recognition for Telephone and Audio-Visual Data: the JHU-MIT Submission for NIST SRE19,"We present a condensed description of the joint effort of JHUCLSP, JHU-HLTCOE and MIT-LL for NIST SRE19. NIST SRE19 consisted of a Tunisian Arabic Telephone Speech challenge (CTS) and an audio-visual (AV) evaluation based on Internet video content. The audio-visual evaluation included the regular audio condition but also novel visual (face recognition) and multi-modal conditions. For CTS and AV-audio conditions, successful systems were based on x-Vector embeddings with very deep encoder networks, i.e, 2D residual networks (ResNet34) and Factorized TDNN (F-TDNN). For CTS, PLDA back-end domain-adapted using SRE18 eval labeled data provided significant gains w.r.t. NIST SRE18 results. For AVaudio, cosine scoring with x-Vector fine-tuned to full-length recordings outperformed PLDA based systems. In CTS, the best fusion attained EER=2.19% and Cprimary=0.205, which are around 50% and 30% better than SRE18 CTS results respectively. The best single system was HLTCOE wide ResNet with EER=2.68% and Cprimary=0.258. In AV-audio, our primary fusion attained EER=1.48% and Cprimary=0.087, which was just slightly better than the best single system (EER=1.78%, Cprimary=0.101). For the AV-video condition, our systems were based on pre-trained face detectors–MT-CNN and RetinaFace– and face recognition embeddings–ResNets trained with additive angular margin softmax. We focused on selecting the best strategies to select the enrollment faces and how to cluster and combine the embeddings of the faces of the multiple subjects in the test recording. Our primary fusion attained EER=1.87% and Cprimary=0.052. For the multi-modal condition, we just added the calibrated scores of the individual audio and video systems. Thus, we assumed complete independence between audio and video modalities. The multi-modal fusion provided impressive improvement with EER=0.44% and Cprimary=0.018.",2020,,,10.21437/odyssey.2020-39,https://www.isca-speech.org/archive/Odyssey_2020/pdfs/88.pdf
de5fca5622f2baa014b84a0cc0ca7f6209e5bcc0,1,0,Enhanced Knowledge Distillation for Face Recognition,"Face recognition have been widely used in different industries due to the advancement of deep convolutional neural networks. Although deep learning has greatly promoted the development of face recognition technology, its computing-intensive and memory-intensive features make it difficult to deploy the model on some embedded devices or mobile computing platforms. Many solutions which include Knowledge Distillation have been proposed to increase the calculation speed of model and reduce the storage space required for calculations, in this paper, we propose a novel Two Stage Knowledge Distillation which enhances the performance of knowledge distillation in face recognition and low resolution face recognition. After experimenting on the several major face datasets, our method turns out to have better results compared to the traditional optimization methods.",2019,"2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",,10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00207,
dec0c26855da90876c405e9fd42830c3051c2f5f,1,0,Supplementary Material : Learning Compositional Visual Concepts with Mutual Consistency,,2018,,,,https://pdfs.semanticscholar.org/dec0/c26855da90876c405e9fd42830c3051c2f5f.pdf
dec43c7511acfc02f5f22fbe4e19ed2aed49b015,0,1,"CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French","Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French. The proposed dataset, called CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes), is the largest of its kind with 40,000 total labelled sentences. It covers a diverse set topics and speakers, and carries supervision of 20 labels including sentiment (and subjectivity), emotions, and attributes. Our evaluations on a state-of-the-art multimodal model demonstrates that CMU-MOSEAS enables further research for multilingual studies in multimodal language.",2020,EMNLP,,10.18653/v1/2020.emnlp-main.141,https://pdfs.semanticscholar.org/dec4/3c7511acfc02f5f22fbe4e19ed2aed49b015.pdf
decbcd2604042e3286c28631ba3218d568889ed9,0,1,Large Margin Few-Shot Learning,"The key issue of few-shot learning is learning to generalize. This paper proposes a large margin principle to improve the generalization capacity of metric based methods for few-shot learning. To realize it, we develop a unified framework to learn a more discriminative metric space by augmenting the classification loss function with a large margin distance loss function for training. Extensive experiments on two state-of-the-art few-shot learning methods, graph neural networks and prototypical networks, show that our method can improve the performance of existing models substantially with very little computational overhead, demonstrating the effectiveness of the large margin principle and the potential of our method.",2018,ArXiv,1807.02872,,https://arxiv.org/pdf/1807.02872.pdf
df53cb2445521a7350cd98a207534795a2a503df,1,0,Photorealistic Face Completion with Semantic Parsing and Face Identity-Preserving Features,"Tremendous progress on deep learning has shown exciting potential for a variety of face completion tasks. However, most learning-based methods are limited to handle general or structure specified face images (e.g., well-aligned faces). In this article, we propose a novel face completion algorithm, called Learning and Preserving Face Completion Network (LP-FCN), which simultaneously parses face images and extracts face identity-preserving (FIP) features. By tackling these two tasks in a mutually boosting way, the LP-FCN can guide an identity preserving inference and ensure pixel faithfulness of completed faces. In addition, we adopt a global discriminator and a local discriminator to distinguish real images from synthesized ones. By training with a combined identity preserving, semantic parsing and adversarial loss, the LP-FCN encourages the completion results to be semantically valid and visually consistent for more complicated image completion tasks. Experiments show that our approach obtains similar visual quality, but achieves better performance on unaligned faces completion and fine detailed synthesis against the state-of-the-art methods.",2019,TOMM,,10.1145/3300940,
df867f4d1f33307f35e182a77be5aa7240f6bdba,1,1,A Graph Based Unsupervised Feature Aggregation for Face Recognition,"In most of the testing dataset, the images are collected from video clips or different environment conditions, which implies that the mutual information between pairs are significantly important. To address this problem and utilize this information, in this paper, we propose a graph-based unsupervised feature aggregation method for face recognition. Our method uses the inter-connection between pairs with a directed graph approach thus refine the pair-wise scores. First, based on the assumption that all features follow Gaussian distribution, we derive a iterative updating formula of features. Second, in discrete conditions, we build a directed graph where the affinity matrix is obtained from pair-wise similarities, and filtered by a pre-defined threshold along with K-nearest neighbor. Third, the affinity matrix is used to obtain a pseudo center matrix for the iterative update process. Besides evaluation on face recognition testing dataset, our proposed method can further be applied to semi-supervised learning to handle the unlabelled data for improving the performance of the deep models. We verified the effectiveness on 5 different datasets: IJB-C, CFP, YTF, TrillionPair and IQiYi Video dataset.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00332,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Cheng_A_Graph_Based_Unsupervised_Feature_Aggregation_for_Face_Recognition_ICCVW_2019_paper.pdf
dfbb6ed457122236b14197242e09f90506f1d4f4,1,1,Comparative Analysis of Different Loss Functions for Deep Face Recognition,"Face Recognition has been one the fastest emerging field in the last ten years. Convolutional neural network (CNN) or Deep convolutional neural network (DCNN) have significantly developed the extraordinary state-of-the-art solution for Face Recognition. This promising development results from the enhanced learning and representation of the discriminative features. The learning depends extensively on the loss function employed in the model. The loss function plays a vital role in the training of CNN and its job is to evaluate the performance of the model, i.e. bad performance results in a huge loss and vice versa. The gradients of this loss function are further used in the back propagation of errors which in turn enables the model to improve its learning from the given data. The objective of this paper is to have a comparative analysis of different loss functions available for the Deep Face Recognition.",2019,ACAI 2019,,10.1145/3377713.3377779,
dfbeb3ca7a01fe80c49b76baa50bf092f71eef4a,0,1,A Survey of Deep Learning-Based Object Detection,"Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in people’s life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning algorithms for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline thoroughly and deeply, in this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at first. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.",2019,IEEE Access,1907.09408,10.1109/ACCESS.2019.2939201,https://arxiv.org/pdf/1907.09408.pdf
e070d25182ab02fa937543e99dd1e4ac812719f3,0,1,FM2u-Net: Face Morphological Multi-Branch Network for Makeup-Invariant Face Verification,"It is challenging in learning a makeup-invariant face verification model, due to (1) insufficient makeup/non-makeup face training pairs, (2) the lack of diverse makeup faces, and (3) the significant appearance changes caused by cosmetics. To address these challenges, we propose a unified Face Morphological Multi-branch Network (FMMu-Net) for makeup-invariant face verification, which can simultaneously synthesize many diverse makeup faces through face morphology network (FM-Net) and effectively learn cosmetics-robust face representations using attention-based multi-branch learning network (AttM-Net). For challenges (1) and (2), FM-Net (two stacked auto-encoders) can synthesize realistic makeup face images by transferring specific regions of cosmetics via cycle consistent loss. For challenge (3), AttM-Net, consisting of one global and three local (task-driven on two eyes and mouth) branches, can effectively capture the complementary holistic and detailed information. Unlike DeepID2 which uses simple concatenation fusion, we introduce a heuristic method AttM-FM, attached to AttM-Net, to adaptively weight the features of different branches guided by the holistic information. We conduct extensive experiments on makeup face verification benchmarks (M-501, M-203, and FAM) and general face recognition datasets (LFW and IJB-A). Our framework FMMu-Net achieves state-of-the-art performances.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00577,https://pdfs.semanticscholar.org/e070/d25182ab02fa937543e99dd1e4ac812719f3.pdf
e09df55b9aaf6e81b210815106d5ea075e3aaad0,0,1,Modality-Agnostic Attention Fusion for visual search with text feedback,"Image retrieval with natural language feedback offers the promise of catalog search based on fine-grained visual features that go beyond objects and binary attributes, facilitating real-world applications such as e-commerce. Our Modality-Agnostic Attention Fusion (MAAF) model combines image and text features and outperforms existing approaches on two visual search with modifying phrase datasets, Fashion IQ and CSS, and performs competitively on a dataset with only single-word modifications, Fashion200k. We also introduce two new challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which provide new settings with rich language inputs, and we show that our approach without modification outperforms strong baselines. To better understand our model, we conduct detailed ablations on Fashion IQ and provide visualizations of the surprising phenomenon of words avoiding ""attending"" to the image region they refer to.",2020,ArXiv,2007.00145,,https://arxiv.org/pdf/2007.00145.pdf
e0ce152ab54b7d9a58abc9396fe0723c0c9fe854,1,0,Robust Distance Metric Learning via Bayesian Inference,"Distance metric learning (DML) has achieved great success in many computer vision tasks. However, most existing DML algorithms are based on point estimation, and thus are sensitive to the choice of training examples and tend to be over-fitting in the presence of label noise. In this paper, we present a robust DML algorithm based on Bayesian inference. In particular, our method is essentially a Bayesian extension to a previous classic DML method—large margin nearest neighbor classification and we use stochastic variational inference to estimate the posterior distribution of the transformation matrix. Furthermore, we theoretically show that the proposed algorithm is robust against label noise in the sense that an arbitrary point with label noise has bounded influence on the learnt model. With some reasonable assumptions, we derive a generalization error bound of this method in the presence of label noise. We also show that the DML hypothesis class in which our model lies is probably approximately correct-learnable and give the sample complexity. The effectiveness of the proposed method1 is demonstrated with state of the art performance on three popular data sets with different types of label noise.1A MATLAB implementation of this method is made available at http://parnec.nuaa.edu.cn/xtan/Publication.htm",2018,IEEE Transactions on Image Processing,,10.1109/TIP.2017.2782366,http://parnec.nuaa.edu.cn/pubs/xiaoyang%20tan/journal/2017/TIP_WD_2017.pdf
e0fa271bd71d9ddb339a917b2ece0767ae6ab538,1,1,BWCFace: Open-set Face Recognition using Body-worn Camera,"With computer vision reaching an inflection point in the past decade, face recognition technology has become pervasive in policing, intelligence gathering, and consumer applications. Recently, face recognition technology has been deployed on bodyworn cameras to keep officers safe, enabling situational awareness and providing evidence for trial. However, limited academic research has been conducted on this topic using traditional techniques on datasets with small sample size. This paper aims to bridge the gap in the state-of-the-art face recognition using bodyworn cameras (BWC). To this aim, the contribution of this work is two-fold: (1) collection of a dataset called BWCFace consisting of a total of 178K facial images of 132 subjects captured using the body-worn camera in in-door and daylight conditions, and (2) open-set evaluation of the latest deep-learning-based Convolutional Neural Network (CNN) architectures combined with five different loss functions for face identification, on the collected dataset. Experimental results on our BWCFace dataset suggest a maximum of 33.89% Rank-1 accuracy obtained when facial features are extracted using SENet-50 trained on a large scale VGGFace2 facial image dataset. However, performance improved up to a maximum of 99.00% Rank-1 accuracy when pretrained CNN models are fine-tuned on a subset of identities in our BWCFace dataset. Equivalent performances were obtained across body-worn camera sensor models used in existing face datasets. The collected BWCFace dataset and the pretrained/ fine-tuned algorithms are publicly available to promote further research and development in this area. A downloadable link of this dataset and the algorithms is available by contacting the authors.",2020,ArXiv,2009.11458,,https://arxiv.org/pdf/2009.11458.pdf
e1451715d7f80b70eb406e3a8f1c4aad77ec565b,1,1,Triplet Distillation for Deep Face Recognition,"Convolutional neural networks (CNNs) have achieved a great success in face recognition, which unfortunately comes at the cost of massive computation and storage consumption. Many compact face recognition networks are thus proposed to resolve this problem. Triplet loss is effective to further improve the performance of those compact models. However, it normally employs a fixed margin to all the samples, which neglects the informative similarity structures between different identities. In this paper, we propose an enhanced version of triplet loss, named triplet distillation, which exploits the capability of a teacher model to transfer the similarity information to a small model by adaptively varying the margin between positive and negative pairs. Experiments on LFW, AgeDB, and CPLFW datasets show the merits of our method compared to the original triplet loss.",2020,ICIP,1905.04457,10.1109/ICIP40778.2020.9190651,https://arxiv.org/pdf/1905.04457.pdf
e17fb1e36cdc3553c54a1070da2e3966608ff59f,0,1,Distribution Distillation Loss: Generic Approach for Improving Face Recognition from Hard Samples,"Large facial variations are the main challenge in face recognition. To this end, previous variation-specific methods make full use of task-related prior to design special network losses, which are typically not general among different tasks and scenarios. In contrast, the existing generic methods focus on improving the feature discriminability to minimize the intra-class distance while maximizing the interclass distance, which perform well on easy samples but fail on hard samples. To improve the performance on those hard samples for general tasks, we propose a novel Distribution Distillation Loss to narrow the performance gap between easy and hard samples, which is a simple, effective and generic for various types of facial variations. Specifically, we first adopt state-of-the-art classifiers such as ArcFace to construct two similarity distributions: teacher distribution from easy samples and student distribution from hard samples. Then, we propose a novel distribution-driven loss to constrain the student distribution to approximate the teacher distribution, which thus leads to smaller overlap between the positive and negative pairs in the student distribution. We have conducted extensive experiments on both generic large-scale face benchmarks and benchmarks with diverse variations on race, resolution and pose. The quantitative results demonstrate the superiority of our method over strong baselines, e.g., Arcface and Cosface.",2020,ArXiv,,,https://arxiv.org/pdf/2002.03662.pdf
e1f25c2ff4738f000ff11281a5fc1f7a9f939d3c,1,0,The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks,"This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its first introduction by~\cite{fredrikson2014privacy}, such attacks have raised serious concerns given that training data usually contain privacy sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear regression and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. Here we present a novel attack method, termed the \emph{generative model-inversion attack}, which can invert deep neural networks with high success rates. Rather than reconstructing private training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we theoretically prove that a model's predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin---highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identification accuracy over the existing work by about $75\%$ for reconstructing face images from a state-of-the-art face recognition classifier. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1911.07135,10.1109/cvpr42600.2020.00033,https://arxiv.org/pdf/1911.07135.pdf
e2043fb5b1fbb2e257c1ddf03a093d2a22cbf1cb,1,1,Overview of Deep Learning in Facial Recognition,,2020,,,10.1007/978-3-030-37830-1_6,
e28c5a6c4ef336b7abd3b4cb15e53d431e96dc05,1,0,Learning Flow-based Feature Warping for Face Frontalization with Illumination Inconsistent Supervision,"Despite recent advances in deep learning-based face frontalization methods, photo-realistic and illumination preserving frontal face synthesis is still challenging due to large pose and illumination discrepancy during training. We propose a novel Flow-based Feature Warping Model (FFWM) which can learn to synthesize photo-realistic and illumination preserving frontal images with illumination inconsistent supervision. Specifically, an Illumination Preserving Module (IPM) is proposed to learn illumination preserving image synthesis from illumination inconsistent image pairs. IPM includes two pathways which collaborate to ensure the synthesized frontal images are illumination preserving and with fine details. Moreover, a Warp Attention Module (WAM) is introduced to reduce the pose discrepancy in the feature level, and hence to synthesize frontal images more effectively and preserve more details of profile images. The attention mechanism in WAM helps reduce the artifacts caused by the displacements between the profile and the frontal images. Quantitative and qualitative experimental results show that our FFWM can synthesize photo-realistic and illumination preserving frontal images and performs favorably against the state-of-the-art results.",2020,ECCV,2008.06843,10.1007/978-3-030-58610-2_33,https://arxiv.org/pdf/2008.06843.pdf
e2e5093f8164b3c3bb5ac0d223c2100e9f334f6c,1,1,Unequal-Training for Deep Face Recognition With Long-Tailed Noisy Data,"Large-scale face datasets usually exhibit a massive number of classes, a long-tailed distribution, and severe label noise, which undoubtedly aggravate the difficulty of training. In this paper, we propose a training strategy that treats the head data and the tail data in an unequal way, accompanying with noise-robust loss functions, to take full advantage of their respective characteristics. Specifically, the unequal-training framework provides two training data streams: the first stream applies the head data to learn discriminative face representation supervised by Noise Resistance loss; the second stream applies the tail data to learn auxiliary information by gradually mining the stable discriminative information from confusing tail classes. Consequently, both training streams offer complementary information to deep feature learning. Extensive experiments have demonstrated the effectiveness of the new unequal-training framework and loss functions. Better yet, our method could save a significant amount of GPU memory. With our method, we achieve the best result on MegaFace Challenge 2 (MF2) given a large-scale noisy training data set.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.00800,http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhong_Unequal-Training_for_Deep_Face_Recognition_With_Long-Tailed_Noisy_Data_CVPR_2019_paper.pdf
e36fdb50844132fc7925550398e68e7ae95467de,1,0,Face Verification with Disguise Variations via Deep Disguise Recognizer,"The performance of current automatic face recognition algorithms is hindered by different covariates such as facial aging, disguises, and pose variations. Specifically, disguises are employed for intentional or unintentional modifications in the facial appearance for hiding one's own identity or impersonating someone else's identity. In this paper, we utilize deep learning based transfer learning approach for face verification with disguise variations. We employ Residual Inception network framework with center loss for learning inherent face representations. The training for the Inception-ResNet model is performed using a large-scale face database which is followed by inductive transfer learning to mitigate the impact of facial disguises. To evaluate the performance of the proposed Deep Disguise Recognizer (DDR) framework, Disguised Faces in the Wild and IIIT-Delhi Disguise Version 1 face databases are used. Experimental evaluation reveals that for the two databases, the proposed DDR framework yields 90.36% and 66.9% face verification accuracy at the false accept rate of 10%.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00010,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w1/Kohli_Face_Verification_With_CVPR_2018_paper.pdf
e3d76f1920c5bf4a60129516abb4a2d8683e48ae,1,0,I Know That Person: Generative Full Body and Face De-identification of People in Images,"We propose a model for full body and face deidentification of humans in images. Given a segmentation of the human figure, our model generates a synthetic human image with an alternative appearance that looks natural and fits the segmentation outline. The model is usable with various levels of segmentation, from simple human figure blobs to complex garment-level segmentations. The level of detail in the de-identified output depends on the level of detail in the input segmentation. The model de-identifies not only primary biometric identifiers (e.g. the face), but also soft and non-biometric identifiers including clothing, hairstyle, etc. Quantitative and perceptual experiments indicate that our model produces de-identified outputs that thwart human and machine recognition, while preserving data utility and naturalness.",2017,2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2017.173,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w16/papers/Kalafatic_I_Know_That_CVPR_2017_paper.pdf
e3dacba4399fc66f31779060db730ea423f341d8,1,1,AdvHat: Real-world adversarial attack on ArcFace Face ID system,"In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.",2019,ArXiv,1908.08705,,https://arxiv.org/pdf/1908.08705.pdf
e43d8e1a03a7265566409fd8cc9246c58ff03ebb,1,0,Global and Local Consistent Wavelet-Domain Age Synthesis,"Age synthesis is a challenging task due to the complicated and non-linear transformation in the human aging process. Aging information is usually reflected in local facial parts, such as wrinkles at the eye corners. However, these local facial parts contribute less in previous GAN-based methods for age synthesis. To address this issue, we propose a wavelet-domain global and local consistent age generative adversarial network (WaveletGLCA-GAN), in which one global specific network and three local specific networks are integrated together to capture both global topology information and local texture details of human faces. Different from the most existing methods that modeling age synthesis in image domain, we adopt wavelet transform to depict the textual information in frequency domain. Moreover, five types of losses are adopted: 1) adversarial loss aims to generate realistic wavelets; 2) identity preserving loss aims to better preserve identity information; 3) age preserving loss aims to enhance the accuracy of age synthesis; 4) pixel-wise loss aims to preserve the background information of the input face; and 5) the total variation regularization aims to remove ghosting artifacts. Our method is evaluated on three face aging datasets, including CACD2000, Morph, and FG-NET. Qualitative and quantitative experiments show the superiority of the proposed method over other state-of-the-arts.",2019,IEEE Transactions on Information Forensics and Security,1809.07764,10.1109/TIFS.2019.2907973,https://arxiv.org/pdf/1809.07764.pdf
e4695c8efb8fdb5923e868888ddb4b614a49c9e6,1,0,Data Augmentation-Based Joint Learning for Heterogeneous Face Recognition,"Heterogeneous face recognition (HFR) is the process of matching face images captured from different sources. HFR plays an important role in security scenarios. However, HFR remains a challenging problem due to the considerable discrepancies (i.e., shape, style, and color) between cross-modality images. Conventional HFR methods utilize only the information involved in heterogeneous face images, which is not effective because of the substantial differences between heterogeneous face images. To better address this issue, this paper proposes a data augmentation-based joint learning (DA-JL) approach. The proposed method mutually transforms the cross-modality differences by incorporating synthesized images into the learning process. The aggregated data augments the intraclass scale, which provides more discriminative information. However, this method also reduces the interclass diversity (i.e., discriminative information). We develop the DA-JL model to balance this dilemma. Finally, we obtain the similarity score between heterogeneous face image pairs through the log-likelihood ratio. Extensive experiments on a viewed sketch database, forensic sketch database, near-infrared image database, thermal-infrared image database, low-resolution photo database, and image with occlusion database illustrate that the proposed method achieves superior performance in comparison with the state-of-the-art methods.",2019,IEEE Transactions on Neural Networks and Learning Systems,,10.1109/TNNLS.2018.2872675,
e47f4a127f41c055fb7893ddc295932ead783c63,1,0,Adversarial Discriminative Heterogeneous Face Recognition,"The gap between sensing patterns of different face modalities remains a challenging problem in heterogeneous face recognition (HFR). This paper proposes an adversarial discriminative feature learning framework to close the sensing gap via adversarial learning on both raw-pixel space and compact feature space. This framework integrates cross-spectral face hallucination and discriminative feature learning into an end-to-end adversarial network. In the pixel space, we make use of generative adversarial networks to perform cross-spectral face hallucination. An elaborate two-path model is introduced to alleviate the lack of paired images, which gives consideration to both global structures and local textures. In the feature space, an adversarial loss and a high-order variance discrepancy loss are employed to measure the global and local discrepancy between two heterogeneous distributions respectively. These two losses enhance domain-invariant feature learning and modality independent noise removing. Experimental results on three NIR-VIS databases show that our proposed approach outperforms state-of-the-art HFR methods, without requiring of complex network or large-scale training dataset.",2018,AAAI,1709.03675,,https://arxiv.org/pdf/1709.03675.pdf
e4b391b15790256a8184fea28eede93a9c56b9d8,0,1,Multi-label Classification of Common Bengali Handwritten Graphemes: Dataset and Challenge,"Latin has historically led the state-of-the-art in handwritten optical character recognition (OCR) research. Adapting existing systems from Latin to alpha-syllabary languages is particularly challenging due to a sharp contrast between their orthographies. The segmentation of graphical constituents corresponding to characters becomes significantly hard due to a cursive writing system and frequent use of diacritics in the alpha-syllabary family of languages. We propose a labeling scheme based on graphemes (linguistic segments of word formation) that makes segmentation inside alpha-syllabary words linear and present the first dataset of Bengali handwritten graphemes that are commonly used in an everyday context. The dataset is open-sourced as a part of the BengaliAI Handwritten Grapheme Classification Challenge on Kaggle to benchmark vision algorithms for multi-label grapheme classification. From competition proceedings, we see that deep learning methods can generalize to a large span of uncommon graphemes even when they are absent during training. Dataset and starter codes at this http URL.",2020,ArXiv,2010.0017,,https://arxiv.org/pdf/2010.00170.pdf
e4c52a81246b2d4458d29ba231a6c211c19e370b,1,1,Learn to Propagate Reliably on Noisy Affinity Graphs,"Recent works have shown that exploiting unlabeled data through label propagation can substantially reduce the labeling cost, which has been a critical issue in developing visual recognition models. Yet, how to propagate labels reliably, especially on a dataset with unknown outliers, remains an open question. Conventional methods such as linear diffusion lack the capability of handling complex graph structures and may perform poorly when the seeds are sparse. Latest methods based on graph neural networks would face difficulties on performance drop as they scale out to noisy graphs. To overcome these difficulties, we propose a new framework that allows labels to be propagated reliably on large-scale real-world data. This framework incorporates (1) a local graph neural network to predict accurately on varying local structures while maintaining high scalability, and (2) a confidence-based path scheduler that identifies outliers and moves forward the propagation frontier in a prudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our confidence guided framework can significantly improve the overall accuracies of the propagated labels, especially when the graph is very noisy.",2020,ArXiv,2007.08802,,https://arxiv.org/pdf/2007.08802.pdf
e4e847c78932c9594ee13455f67c360baf0228c2,1,0,Evaluation of maxout activations in deep learning across several big data domains,"This study investigates the effectiveness of multiple maxout activation function variants on 18 datasets using Convolutional Neural Networks. A network with maxout activation has a higher number of trainable parameters compared to networks with traditional activation functions. However, it is not clear if the activation function itself or the increase in the number of trainable parameters is responsible in yielding the best performance for different entity recognition tasks. This paper investigates if an increase in the number of convolutional filters on traditional activation functions performs equal-to or better-than maxout networks. Our experiments compare the Rectified Linear Unit, Leaky Rectified Linear Unit, Scaled Exponential Linear Unit, and Hyperbolic Tangent activations to four maxout function variants. We observe that maxout networks train relatively slower than networks with traditional activation functions, e.g. Rectified Linear Unit. In addition, we found that on average, across all datasets, the Rectified Linear Unit activation function performs better than any maxout activation when the number of convolutional filters is increased. Furthermore, adding more filters enhances the classification accuracy of the Rectified Linear Unit networks, without adversely affecting their advantage over maxout activations with respect to network-training speed.",2019,Journal of Big Data,,10.1186/s40537-019-0233-0,
e516d0108fc22d88452dc4df13e3b29802fe3492,0,1,Gaussian Affinity for Max-Margin Class Imbalanced Learning,"Real-world object classes appear in imbalanced ratios. This poses a significant challenge for classifiers which get biased towards frequent classes. We hypothesize that improving the generalization capability of a classifier should improve learning on imbalanced datasets. Here, we introduce the first hybrid loss function that jointly performs classification and clustering in a single formulation. Our approach is based on an `affinity measure' in Euclidean space that leads to the following benefits: (1) direct enforcement of maximum margin constraints on classification boundaries, (2) a tractable way to ensure uniformly spaced and equidistant cluster centers, (3) flexibility to learn multiple class prototypes to support diversity and discriminability in feature space. Our extensive experiments demonstrate the significant performance improvements on visual classification and verification tasks on multiple imbalanced datasets. The proposed loss can easily be plugged in any deep architecture as a differentiable block and demonstrates robustness against different levels of data imbalance and corrupted labels.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,10.1109/ICCV.2019.00657,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hayat_Gaussian_Affinity_for_Max-Margin_Class_Imbalanced_Learning_ICCV_2019_paper.pdf
e57f731f5181ad8215ad931e5483caa0168e6362,0,1,ArcGrad: Angular Gradient Margin Loss for Classification,"The cosine-based softmax loss functions greatly enhance intra-class compactness and perform well on the tasks of face recognition and object classification. Outperformance, however, depends on the careful hyperparameter selection. Adaptively Scaling Cosine Logits (AdaCos) tries to propose a parameter-free version by leveraging an adaptive scaling parameter. Nevertheless, the application of AdaCos is limited in specific domains because of improper approximation.In this paper, to promote intra-class compactness and interclass separability, we propose an Angular Gradient Margin Loss (ArcGrad) that generates a gradient margin by maximizing the angular gradient. Our work suggests that the margin parameter on cosine-based methods is not necessary, and the scaling parameter is inversely proportional to the margin. Furthermore, a stable and large gradient promotes better feature representation. In experiments, we test our method, as well as other methods enhancing discriminative information, on CIFAR and 15 datasets from UCI. Experimental results show ArcGrad consistently outperforms both on large and small scale problems and has the superiority in discriminative information and time-consumption.",2020,2020 International Joint Conference on Neural Networks (IJCNN),,10.1109/IJCNN48605.2020.9207251,
e58dd160a76349d46f881bd6ddbc2921f08d1050,1,0,Merge or Not? Learning to Group Faces via Imitation Learning,"Given a large number of unlabeled face images, face grouping aims at clustering the images into individual identities present in the data. This task remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation. In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections. Often, a user needs to correct the erroneous grouping manually. In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. This is achieved through imitation learning (a.k.a apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL). In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short- and long-term rewards. Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.",2018,AAAI,1707.03986,,https://arxiv.org/pdf/1707.03986.pdf
e5e5621486f11538aa1016dbdb3b50f60f6f63d2,0,1,"img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation","We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation without face detection or landmark localization. We observe that estimating the 6DoF rigid transformation of a face is a simpler problem than facial landmark detection, often used for 3D face alignment. In addition, 6DoF offers more information than face bounding box labels. We leverage these observations to make multiple contributions: (a) We describe an easily trained, efficient, Faster R-CNN–based model which regresses 6DoF pose for all faces in the photo, without preliminary face detection. (b) We explain how pose is converted and kept consistent between the input photo and arbitrary crops created while training and evaluating our model. (c) Finally, we show how face poses can replace detection bounding box training labels. Tests on AFLW2000-3D and BIWI show that our method runs at real-time and outperforms state of the art (SotA) face pose estimators. Remarkably, our method also surpasses SotA models of comparable complexity on the WIDER FACE detection benchmark, despite not been optimized on bounding box labels. ∗ Joint first authorship. All experiments reported in this paper were performed at the University of Notre Dame.",2020,,2012.07791,,https://arxiv.org/pdf/2012.07791.pdf
e6b45d5a86092bbfdcd6c3c54cda3d6c3ac6b227,1,1,Pairwise Relational Networks for Face Recognition,"Existing face recognition using deep neural networks is difficult to know what kind of features are used to discriminate the identities of face images clearly. To investigate the effective features for face recognition, we propose a novel face recognition method, called a pairwise relational network (PRN), that obtains local appearance patches around landmark points on the feature map, and captures the pairwise relation between a pair of local appearance patches. The PRN is trained to capture unique and discriminative pairwise relations among different identities. Because the existence and meaning of pairwise relations should be identity dependent, we add a face identity state feature, which obtains from the long short-term memory (LSTM) units network with the sequential local appearance patches on the feature maps, to the PRN. To further improve accuracy of face recognition, we combined the global appearance representation with the pairwise relational feature. Experimental results on the LFW show that the PRN using only pairwise relations achieved 99.65% accuracy and the PRN using both pairwise relations and face identity state feature achieved 99.76% accuracy. On the YTF, both the PRN using only pairwise relations and the PRN using pairwise relations and the face identity state feature achieved the state-of-the-art (95.7% and 96.3%). The PRN also achieved comparable results to the state-of-the-art for both face verification and face identification tasks on the IJB-A, and the state-of-the-art on the IJB-B.",2018,ECCV,1808.04976,10.1007/978-3-030-01216-8_39,https://arxiv.org/pdf/1808.04976.pdf
e6c6e7597585b3ac38b4b90f11a786dd1f0ad383,1,0,Investigating Bias in Deep Face Analysis: The KANFace Dataset and Empirical Study,"Deep learning-based methods have pushed the limits of the state-of-the-art in face analysis. However, despite their success, these models have raised concerns regarding their bias towards certain demographics. This bias is inflicted both by limited diversity across demographics in the training set, as well as the design of the algorithms. In this work, we investigate the demographic bias of deep learning models in face recognition, age estimation, gender recognition and kinship verification. To this end, we introduce the most comprehensive, large-scale dataset of facial images and videos to date. It consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The data are manually annotated in terms of identity, exact age, gender and kinship. The performance of state-of-the-art models is scrutinized and demographic bias is exposed by conducting a series of experiments. Lastly, a method to debias network embeddings is introduced and tested on the proposed benchmarks.",2020,Image Vis. Comput.,2005.07302,10.1016/j.imavis.2020.103954,https://arxiv.org/pdf/2005.07302.pdf
e6f34d1e415d37a44bfc52c02afa3f09a97b4f1d,1,0,ECML: An Ensemble Cascade Metric Learning Mechanism towards Face Verification,"Face verification can be regarded as a two-class fine-grained visual-recognition problem. Enhancing the feature's discriminative power is one of the key problems to improve its performance. Metric-learning technology is often applied to address this need while achieving a good tradeoff between underfitting, and overfitting plays a vital role in metric learning. Hence, we propose a novel ensemble cascade metric-learning (ECML) mechanism. In particular, hierarchical metric learning is executed in a cascade way to alleviate underfitting. Meanwhile, at each learning level, the features are split into nonoverlapping groups. Then, metric learning is executed among the feature groups in the ensemble manner to resist overfitting. Considering the feature distribution characteristics of faces, a robust Mahalanobis metric-learning method (RMML) with a closed-form solution is additionally proposed. It can avoid the computation failure issue on an inverse matrix faced by some well-known metric-learning approaches (e.g., KISSME). Embedding RMML into the proposed ECML mechanism, our metric-learning paradigm (EC-RMML) can run in the one-pass learning manner. The experimental results demonstrate that EC-RMML is superior to state-of-the-art metric-learning methods for face verification. The proposed ECML mechanism is also applicable to other metric-learning approaches.",2020,IEEE transactions on cybernetics,2007.0572,10.1109/TCYB.2020.2996207,https://arxiv.org/pdf/2007.05720.pdf
e71db960a43bf26d9de3515652f04579952ab6ff,1,0,Online Multi-modal Person Search in Videos,"The task of searching certain people in videos has seen increasing potential in real-world applications, such as video organization and editing. Most existing approaches are devised to work in an offline manner, where identities can only be inferred after an entire video is examined. This working manner precludes such methods from being applied to online services or those applications that require real-time responses. In this paper, we propose an online person search framework, which can recognize people in a video on the fly. This framework maintains a multimodal memory bank at its heart as the basis for person recognition, and updates it dynamically with a policy obtained by reinforcement learning. Our experiments on a large movie dataset show that the proposed method is effective, not only achieving remarkable improvements over online schemes but also outperforming offline methods.",2020,ECCV,2008.03546,10.1007/978-3-030-58610-2_11,https://arxiv.org/pdf/2008.03546.pdf
e730b89ca182e3f3e5f1c4e1d8300bf3f8f564d5,0,1,A New Loss Function for CNN Classifier Based on Predefined Evenly-Distributed Class Centroids,"With the development of convolutional neural networks (CNNs) in recent years, the network structure has become more and more complex and varied, and has achieved very good results in pattern recognition, image classification, object detection and tracking. For CNNs used for image classification, in addition to the network structure, more and more researches focus on the improvement of the loss function, so as to enlarge the inter-class feature differences, and reduce the intra-class feature variations as soon as possible. Besides the traditional Softmax, typical loss functions include L-Softmax, AM-Softmax, ArcFace, and Center loss, etc. Based on the concept of predefined evenly-distributed class centroids (PEDCC) in CSAE network, this paper proposes a PEDCC-based loss function called PEDCC-Loss, which can make the inter-class distance maximal and intra-class distance small enough in latent feature space. Multiple experiments on image classification and face recognition have proved that our method achieve the best recognition accuracy, and network training is stable and easy to converge. Code is available in https://github.com/ZLeopard/PEDCC-Loss",2020,IEEE Access,1904.06008,10.1109/ACCESS.2019.2960065,https://arxiv.org/pdf/1904.06008.pdf
e733c35f24584e539c5c97e35de1dc5c45ed4121,1,1,Face Image Quality Assessment: A Literature Survey,"The performance of face analysis and recognition systems depends on the quality of the acquired face data, which is influenced by numerous factors. Automatically assessing the quality of face data in terms of biometric utility can thus be useful to filter out low quality data. This survey provides an overview of the face quality assessment literature in the framework of face biometrics, with a focus on face recognition based on visible wavelength face images as opposed to e.g. depth or infrared quality assessment. A trend towards deep learning based methods is observed, including notable conceptual differences among the recent approaches. Besides image selection, face image quality assessment can also be used in a variety of other application scenarios, which are discussed herein. Open issues and challenges are pointed out, i.a. highlighting the importance of comparability for algorithm evaluations, and the challenge for future work to create deep learning approaches that are interpretable in addition to providing accurate utility predictions.",2020,ArXiv,2009.01103,,https://arxiv.org/pdf/2009.01103.pdf
e76432d26daefdb8ab40e11521a9dbd0396e0d63,0,1,Towards Palmprint Verification On Smartphones,"With the rapid development of mobile devices, smartphones have gradually become an indispensable part of people's lives. Meanwhile, biometric authentication has been corroborated to be an effective method for establishing a person's identity with high confidence. Hence, recently, biometric technologies for smartphones have also become increasingly sophisticated and popular. But it is noteworthy that the application potential of palmprints for smartphones is seriously underestimated. Studies in the past two decades have shown that palmprints have outstanding merits in uniqueness and permanence, and have high user acceptance. However, currently, studies specializing in palmprint verification for smartphones are still quite sporadic, especially when compared to face- or fingerprint-oriented ones. In this paper, aiming to fill the aforementioned research gap, we conducted a thorough study of palmprint verification on smartphones and our contributions are twofold. First, to facilitate the study of palmprint verification on smartphones, we established an annotated palmprint dataset named MPD, which was collected by multi-brand smartphones in two separate sessions with various backgrounds and illumination conditions. As the largest dataset in this field, MPD contains 16,000 palm images collected from 200 subjects. Second, we built a DCNN-based palmprint verification system named DeepMPV+ for smartphones. In DeepMPV+, two key steps, ROI extraction and ROI matching, are both formulated as learning problems and then solved naturally by modern DCNN models. The efficiency and efficacy of DeepMPV+ have been corroborated by extensive experiments. To make our results fully reproducible, the labeled dataset and the relevant source codes have been made publicly available at this https URL.",2020,ArXiv,2003.13266,,https://arxiv.org/pdf/2003.13266.pdf
e78b38f093537c07a839052ac246a923b1bfc17a,0,1,"3rd Place Solution to ""Google Landmark Retrieval 2020""","Image retrieval is a fundamental problem in computer vision. This paper presents our 3rd place detailed solution to the Google Landmark Retrieval 2020 challenge. We focus on the exploration of data cleaning and models with metric learning. We use a data cleaning strategy based on embedding clustering. Besides, we employ a data augmentation method called Corner-Cutmix, which improves the model's ability to recognize multi-scale and occluded landmark images. We show in detail the ablation experiments and results of our method.",2020,ArXiv,2008.1048,,https://arxiv.org/pdf/2008.10480.pdf
e7b41f1c3567b7cd45a12d727df93929ccb8aacb,1,0,Unsupervised Large-Scale World Locations Dataset,"Deep Learning systems require vasts amounts of data to be trained. Due to this, it is unfeasible to rely on humanlycurated datasets. Moreover, it is also needed to have access to large data collections (usually Internet-based and very noisy). To solve this challenge, this paper presents a) a novel approach to generate unsupervised large-scale classname-annotated landmark datasets and b) a system to reduce the noise in such datasets without supervision. To evaluate the robustness of the generated dataset, this paper compares the presented dataset with the Google Landmark dataset on a Landmark Recognitoin task, showing similar results on the Oxford5K and Paris6K sets. The noise filtering system is evaluated demonstrating successful results. The combination of the unsupervised dataset generation and the unsupervised noise filtering systems presented in this paper have the potential to drastically increase currently available landmark datasets and therefore its potential applications.",2018,,,,https://pdfs.semanticscholar.org/e7b4/1f1c3567b7cd45a12d727df93929ccb8aacb.pdf
e7d2e0d31378ace9deeddbdc2b3623602e1c3e29,0,1,Deep in-situ learning for object recognition,"T his dissertation is about in-situ object recognition, meaning that specific objects (instances) can be learned from a few training examples that depict them within the place where such objects are commonly present or being used. Learning to recognize objects in-situ opposes to conventional approaches in deep learning of relying on largescale class-level datasets of grouped instances, utilizing complex image acquisition setups or utilizing synthetic data. We aim for a scalable, robust, and real-time system based on Convolutional Neural Networks (CNNs) that learn discriminative features from images depicting objects from an egocentric point of view. We are particularly interested in learning objects from a few examples taken directly by an agent or by a demonstrator, and where the CNN does not need a finetuning process for learning additional instances, motivated by the computational limitations in most autonomous platforms. We hope our approach will be helpful for robotic tasks such as object manipulation, human-robot interaction, semantic mapping, scene understanding, autonomous navigation, and contribute to FARSCOPE’s vision on advancing the state-of-the-art of autonomy in robots and intelligent systems.",2019,,,,https://research-information.bris.ac.uk/files/218781900/Final_Copy_2019_11_28_Lagunes_Fortiz_M_A_PhD.pdf
e7da564afdf8162489cbace970f26b239913d1f1,1,1,R³ Adversarial Network for Cross Model Face Recognition,"In this paper, we raise a new problem, namely cross model face recognition (CMFR), which has considerable economic and social significance. The core of this problem is to make features extracted from different models comparable. However, the diversity, mainly caused by different application scenarios, frequent version updating, and all sorts of service platforms, obstructs interaction among different models and poses a great challenge. To solve this problem, from the perspective of Bayesian modelling, we propose R3 Adversarial Network (R3AN) which consists of three paths: Reconstruction, Representation and Regression. We also introduce adversarial learning into the reconstruction path for better performance. Comprehensive experiments on public datasets demonstrate the feasibility of interaction among different models with the proposed framework. When updating the gallery, R3AN conducts the feature transformation nearly 10 times faster than ResNet-101. Meanwhile, the transformed feature distribution is very close to that of target model, and its error rate is incredibly reduced by approximately 75% compared with a naive transformation model. Furthermore, we show that face feature can be deciphered into original face image roughly by the reconstruction path, which may give valuable hints for improving the original face recognition models.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/CVPR.2019.01010,http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_R3_Adversarial_Network_for_Cross_Model_Face_Recognition_CVPR_2019_paper.pdf
e81e44772c95e1283f0806c6101c161418ebb883,0,1,Margin Matters: Towards More Discriminative Deep Neural Network Embeddings for Speaker Recognition,"Recently, speaker embeddings extracted from a speaker discriminative deep neural network (DNN) yield better performance than the conventional methods such as i-vector. In most cases, the DNN speaker classifier is trained using cross entropy loss with softmax. However, this kind of loss function does not explicitly encourage inter-class separability and intra-class compactness. As a result, the embeddings are not optimal for speaker recognition tasks. In this paper, to address this issue, three different margin based losses which not only separate classes but also demand a fixed margin between classes are introduced to deep speaker embedding learning. It could be demonstrated that the margin is the key to obtain more discriminative speaker embeddings. Experiments are conducted on two public text independent tasks: VoxCeleb1 and Speaker in The Wild (SITW). The proposed approach can achieve the state-of-the-art performance, with 25% ~ 30% equal error rate (EER) reduction on both tasks when compared to strong baselines using cross entropy loss with softmax, obtaining 2.238% EER on VoxCeleb1 test set and 2.761% EER on SITW core-core test set, respectively.",2019,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),1906.07317,10.1109/APSIPAASC47483.2019.9023039,https://arxiv.org/pdf/1906.07317.pdf
e893875be22be4800b39292d914b56a515390994,0,1,Resolution Invariant Face Recognition Using a Distillation Approach,"Modern face recognition systems extract face representations using deep neural networks (DNNs) and give excellent identification and verification results, when tested on high resolution (HR) images. However, the performance of such an algorithm degrades significantly for low resolution (LR) images. A straight forward solution could be to train a DNN, using simultaneously, high and low resolution face images. This approach yields a definite improvement at lower resolutions but suffers a performance degradation for high resolution images. To overcome this shortcoming, we propose to train a network using both HR and LR images under the guidance of a fixed network, pretrained on HR face images. The guidance is provided by minimising the KL-divergence between the output Softmax probabilities of the pretrained (i.e., Teacher) and trainable (i.e., Student) network as well as by sharing the Softmax weights between the two networks. The resulting solution is tested on down-sampled images from FaceScrub and MegaFace datasets and shows a consistent performance improvement across various resolutions. We also tested our proposed solution on standard LR benchmarks such as TinyFace and SCFace. Our algorithm consistently outperforms the state-of-the-art methods on these datasets, confirming the effectiveness and merits of the proposed method.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2020.3007356,http://epubs.surrey.ac.uk/858113/1/TBIOM2020.pdf
e89cf011bb543137b961807924e0b765d536aa98,1,1,iQIYI-VID: A Large Dataset for Multi-modal Person Identification,"Person identification in the wild is very challenging due to great variation in poses, face quality, clothes, makeup and so on. Traditional research, such as face recognition, person re-identification, and speaker recognition, often focuses on a single modal of information, which is inadequate to handle all the situations in practice. Multi-modal person identification is a more promising way that we can jointly utilize face, head, body, audio features, and so on. In this paper, we introduce iQIYI-VID, the largest video dataset for multi-modal person identification. It is composed of 600K video clips of 5,000 celebrities. These video clips are extracted from 400K hours of online videos of various types, ranging from movies, variety shows, TV series, to news broadcasting. All video clips pass through a careful human annotation process, and the error rate of labels is lower than 0.2%. We evaluated the state-of-art models of face recognition, person re-identification, and speaker recognition on the iQIYI-VID dataset. Experimental results show that these models are still far from being perfect for task of person identification in the wild. We further demonstrate that a simple fusion of multi-modal features can improve person identification considerably. We have released the dataset online to promote multi-modal person identification research.",2018,ArXiv,1811.07548,,https://arxiv.org/pdf/1811.07548.pdf
e8a90ec963168efb31470cd998400b29d6f74023,1,1,Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network,"With the broad use of face recognition, its weakness gradually emerges that it is able to be attacked. So, it is important to study how face recognition networks are subject to attacks. In this paper, we focus on a novel way to do attacks against face recognition network that misleads the network to identify someone as the target person not misclassify inconspicuously. Simultaneously, for this purpose, we introduce a specific attentional adversarial attack generative network to generate fake face images. For capturing the semantic information of the target person, this work adds a conditional variational autoencoder and attention modules to learn the instance-level correspondences between faces. Unlike traditional two-player GAN, this work introduces face recognition networks as the third player to participate in the competition between generator and discriminator which allows the attacker to impersonate the target person better. The generated faces which are hard to arouse the notice of onlookers can evade recognition by state-of-the-art networks and most of them are recognized as the target person.",2018,ArXiv,1811.12026,10.1007/s11042-020-09604-z,https://arxiv.org/pdf/1811.12026.pdf
e8f20892d6df40ee7d42f36255a01fae67805233,1,1,RetinaFace: Single-stage Dense Face Localisation in the Wild,"Though tremendous strides have been made in uncontrolled face detection, accurate and efficient face localisation in the wild remains an open challenge. This paper presents a robust single-stage face detector, named RetinaFace, which performs pixel-wise face localisation on various scales of faces by taking advantages of joint extra-supervised and self-supervised multi-task learning. Specifically, We make contributions in the following five aspects: (1) We manually annotate five facial landmarks on the WIDER FACE dataset and observe significant improvement in hard face detection with the assistance of this extra supervision signal. (2) We further add a self-supervised mesh decoder branch for predicting a pixel-wise 3D shape face information in parallel with the existing supervised branches. (3) On the WIDER FACE hard test set, RetinaFace outperforms the state of the art average precision (AP) by 1.1% (achieving AP equal to 91.4%). (4) On the IJB-C test set, RetinaFace enables state of the art methods (ArcFace) to improve their results in face verification (TAR=89.59% for FAR=1e-6). (5) By employing light-weight backbone networks, RetinaFace can run real-time on a single CPU core for a VGA-resolution image. Extra annotations and code have been made available at: this https URL.",2019,ArXiv,1905.00641,,https://arxiv.org/pdf/1905.00641.pdf
e912fd2e53de28c4278b5459f91f3dec9018ce6d,0,1,Arc Loss: Softmax with Additive Angular Margin for Answer Retrieval,"Answer retrieval is a crucial step in question answering. To determine the best Q–A pair in a candidate pool, traditional approaches adopt triplet loss (i.e., pairwise ranking loss) for a meaningful distributed representation. Triplet loss is widely used to push away a negative answer from a certain question in a feature space and leads to a better understanding of the relationship between questions and answers. However, triplet loss is inefficient because it requires two steps: triplet generation and negative sampling. In this study, we propose an alternative loss function, namely, arc loss, for more efficient and effective learning than that by triplet loss. We evaluate the proposed approach on a commonly used QA dataset and demonstrate that it significantly outperforms the triplet loss baseline.",2019,AIRS,,10.1007/978-3-030-42835-8_4,
e9434796a49affff2de82c60bc01687ec196b877,1,0,SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses for Mobile Platforms,"Virtual applications through mobile platforms are one of the most critical and ever-growing fields in AI, where ubiquitous and real-time person authentication has become critical after the breakthrough of all services provided via mobile devices. In this context, face verification technologies can provide reliable and robust user authentication, given the availability of cameras in these devices, as well as their widespread use in everyday applications. The rapid development of deep Convolutional Neural Networks has resulted in many accurate face verification architectures. However, their typical size (hundreds of megabytes) makes them infeasible to be incorporated in downloadable mobile applications where the entire file typically may not exceed 100 Mb. Accordingly, we address the challenge of developing a lightweight face recognition network of just a few megabytes that can operate with sufficient accuracy in comparison to much larger models. The network also should be able to operate under different poses, given the variability naturally observed in uncontrolled environments where mobile devices are typically used. In this paper, we adapt the lightweight SqueezeNet model, of just 4.4MB, to effectively provide cross-pose face recognition. After trained on the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23% on the difficult frontal vs. profile comparison, and0.54% on profile vs. profile images. Under less extreme variations involving frontal images in any of the enrolment/query images pair, EER is pushed down to<0.3%, and the FRR at FAR=0.1%to less than 1%. This makes our light model suitable for face recognition where at least acquisition of the enrolment image can be controlled. At the cost of a slight degradation in performance, we also test an even lighter model (of just 2.5MB) where regular convolutions are replaced with depth-wise separable convolutions.",2020,ArXiv,2007.08566,,https://arxiv.org/pdf/2007.08566.pdf
e9997c90e61600fa65f33a28cdfc6e524373e714,1,1,Exploring Factors for Improving Low Resolution Face Recognition,"State-of-the-art deep face recognition approaches report near perfect performance on popular benchmarks, e.g., Labeled Faces in the Wild. However, their performance deteriorates significantly when they are applied on low quality images, such as those acquired by surveillance cameras. A further challenge for low resolution face recognition for surveillance applications is the matching of recorded low resolution probe face images with high resolution reference images, which could be the case in watchlist scenarios. In this paper, we have addressed these problems and investigated the factors that would contribute to the identification performance of the state-of-the-art deep face recognition models when they are applied to low resolution face recognition under mismatched conditions. We have observed that the following factors affect performance in a positive way: appearance variety and resolution distribution of the training dataset, resolution matching between the gallery and probe images, and the amount of information included in the probe images. By leveraging this information, we have utilized deep face models trained on MS-Celeb-1M and fine-tuned on VGGFace2 dataset and achieved state-of-the-art accuracies on the SCFace and ICB-RW benchmarks, even without using any training data from the datasets of these benchmarks.",2019,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),1907.10104,10.1109/CVPRW.2019.00290,https://arxiv.org/pdf/1907.10104.pdf
e9efef2cce71cef9d9467d806e1fad05bb60eb1b,0,1,A New Bearing-Fault Classification Method Based on Deep Learning*,"Bearing fault signal detection plays a vital role in the industrial field, which directly affects the performance and safety of mechanical equipment. The application of CNN (convolutional neural network) for fault signal detection is an emerging method, but this method does not work well on one-dimensional data such as bearing fault signals, mainly because the features of the one-dimensional signal are not distinct compared to the image signal. Secondly, due to the limitation of the special situation, the data of the bearing signal is less, which makes it hard for the deep learning model to fit and converge well. To solve the above problems, this paper proposes a CNN based on improved softmax-loss (ISM-CNN). The constructed CNN can learn more subtle features from the bearing signals, thereby improving the accuracy of bearing signal classification. Besides, the algorithm proposed in this paper expands the training data set to a certain extent, so that the parameters of the ISM-CNN can be better fitted. We validate the effectiveness of the proposed algorithm on the CWRU open dataset and give ablation experiments to prove it. In the 97-category complex bearing signal generation scenario, the proposed algorithm achieves 95% accuracy.",2020,2020 15th International Conference on Computer Science & Education (ICCSE),,10.1109/ICCSE49874.2020.9201672,
e9f8cdd089b926c5dcdeeeda78db7d52a298d598,1,1,A2-LINK: Recognizing Disguised Faces via Active Learning and Adversarial Noise Based Inter-Domain Knowledge,"Face recognition in the unconstrained environment is an ongoing research challenge. Although several covariates of face recognition such as pose and low resolution have received significant attention, “disguise” is considered an onerous covariate of face recognition. One of the primary reasons for this is the scarcity of large and representative labeled databases, along with the lack of algorithms that work well for multiple covariates in such environments. In order to address the problem of face recognition in the presence of disguise, the paper proposes an active learning framework termed as A2-LINK. Starting with a face recognition machine-learning model, A2-LINK intelligently selects training samples from the target domain to be labeled and, using hybrid noises such as adversarial noise, fine-tunes a model that works well both in the presence and absence of disguise. Experimental results demonstrate the effectiveness and generalization of the proposed framework on the DFW and DFW2019 datasets with state-of-the-art deep learning featurization models such as LCSSE, ArcFace, and DenseNet.",2020,"IEEE Transactions on Biometrics, Behavior, and Identity Science",,10.1109/TBIOM.2020.2998912,http://iab-rubric.org/papers/2020_TBIOM_A2LINK.pdf
ea222e3b19ea901218d9969a9ad835e5e85172e1,0,1,HyNet: Local Descriptor with Hybrid Similarity Measure and Triplet Loss,"Recent works show that local descriptor learning benefits from the use of L2 normalisation, however, an in-depth analysis of this effect lacks in the literature. In this paper, we investigate how L2 normalisation affects the back-propagated descriptor gradients during training. Based on our observations, we propose HyNet, a new local descriptor that leads to state-of-the-art results in matching. HyNet introduces a hybrid similarity measure for triplet margin loss, a regularisation term constraining the descriptor norm, and a new network architecture that performs L2 normalisation of all intermediate feature maps and the output descriptors. HyNet surpasses previous methods by a significant margin on standard benchmarks that include patch matching, verification, and retrieval, as well as outperforming full end-to-end methods on 3D reconstruction tasks.",2020,ArXiv,,,https://arxiv.org/pdf/2006.10202.pdf
ea4fd6b3527e913a038c8ad66e3a2ee1e0adf0c7,0,1,The JHU-MIT System Description for NIST SRE19 AV,"This document represents the SRE19 AV submission by the team composed of JHU-CLSP, JHU-HLTCOE and MIT Lincoln Labs. All the developed systems for the audio and video conditions consisted of Neural network embeddings with some flavor of PLDA/cosine back-end. Primary fusions obtained Actual DCF of 0.250 on SRE18 VAST eval, 0.183 on SRE19 AV dev audio, 0.140 on SRE19 AV dev video and 0.054 on SRE19 AV multi-modal.",2019,,,,https://www.ll.mit.edu/sites/default/files/publication/doc/JHU-MIT-system-description-nist-villalba-124945.pdf
ea71c3c87da30d999cbf55dd3d0888751c7f61cf,1,1,Learning Domain-Invariant Discriminative Features for Heterogeneous Face Recognition,"Heterogeneous face recognition (HFR), referring to matching face images across different domains, is a challenging problem due to the vast cross-domain discrepancy and insufficient pairwise cross-domain training data. This article proposes a quadruplet framework for learning domain-invariant discriminative features (DIDF) for HFR, which integrates domain-level and class-level alignment in one unified network. The domain-level alignment reduces the cross-domain distribution discrepancy. The class-level alignment based on a special quadruplet loss is developed to further diminish the intra-class variations and enlarge the inter-class separability among instances, thus handling the misalignment and adversarial equilibrium problems confronted by the domain-level alignment. With a bidirectional cross-domain data selection strategy, the quadruplet loss-based method prominently enriches the training set and further eliminates the cross-modality shift. Benefiting from the joint supervision and mutual reinforcement of these two components, the domain invariance and class discrimination of identity features are guaranteed. Extensive experiments on the challenging CASIA NIR-VIS 2.0 database, the Oulu-CASIA NIR&VIS database, the BUAA-VisNir database, and the IIIT-D viewed sketch database demonstrate the effectiveness and preferable generalization capability of the proposed method.",2020,IEEE Access,,10.1109/ACCESS.2020.3038906,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09262951.pdf
ea836d4292736b3226d4128bc7101c91ec8418bd,0,1,Identity Extraction from Clusters of Multi-modal Observations,"In this paper, we present a method for identity extraction from TV News Broadcasts. We define the identity as a set of multi-modal observations. In our case it is the face of a person and a name of a person. The method is based on agglomerative clustering of observations. The resulting clusters represent individual identities, that appeared in the broadcasts. To evaluate the accuracy of our system, we hand labelled approximately one year worth of TV News broadcasts. This resulted in total of \(10\,301\) multi-modal observations and 2563 unique identities. Our method achieved a coverage measure of 90.69 % and precision measure of 94.69 %. Given the simplicity of the proposed algorithm, these results are very satisfactory. Furthermore, the designed system is modular and new modalities can be easily added.",2019,SPECOM,,10.1007/978-3-030-26061-3_18,
eac286903fcf745d0273dc493161a55dc7449ee0,0,1,Precise Adjacent Margin Loss for Deep Face Recognition,"Softmax loss is arguably one of the most widely used loss functions in CNNs. In recent years some Softmax variants have been proposed to enhance the discriminative ability of the learned features by adding additional margin constraints, which significantly improved the state-of-the-art performance of face recognition. However, the ‘margin’ referenced in these losses does not represent the real margin between the different classes in the training set. Furthermore, they impose a margin on all possible combinations of class pairs, which is unnecessary. In this paper we propose the Precise Adjacent Margin loss (PAM loss), which gives an accurate definition of ‘margin’ and has precise operations appropriate for different cases. PAM loss has better geometrical interpretation than the existing margin-based losses. Extensive experiments are conducted on LFW, YTF, MegaFace and FaceScrub datasets, and results show that the proposed method has state-of-the-art performance.",2019,2019 IEEE International Conference on Image Processing (ICIP),,10.1109/ICIP.2019.8803751,
eae968f13f9a5cacd49e2140e8bb4a207e788e7c,0,1,Canonical Correlation Analysis Feature Fusion With Patch of Interest: A Dynamic Local Feature Matching for Face Sketch Image Retrieval,"An automatic photo retrieval system based on a face sketch has very useful application as to narrow down potential suspects in criminal investigations. This is true when there is no other evident except the face sketch that is rendered based on the recollection of a victim or eyewitness. Among the noticeable difficulties in matching the sketch and photo due to its modality difference are the generated sketch has some tendency of shape exaggeration, the sketch has very less accurate details and the real-world photo may expose to lighting variation unlike the sketch. In this paper, we attempt to address these complications by matching the sketch and photos using dynamic local feature of Difference of Gaussian Oriented Gradient Histogram (DoGOGH) on some selected patches. To avoid discriminative power degradation due to a large number of gallery images, two stage matching blocks are introduced in a cascaded fashion. The front block matches the feature such that it short lists $k$ most similar photos for the second block. In this front block, Histogram of Oriented Gradient (HOG) and Gabor Wavelet (GW) features are fused by maximizing the correlation between the two using Canonical Correlation Analysis (CCA). Based on the short listed photos, the following block re-matched the sketch and photos using dynamically extracted local feature on its Patch of Interest (PoI). Eventually, the matching scores from the blocks are fused before getting rank-1 accuracy. The experimental results on two baseline datasets indicate that the proposed method outperforms the state-of-the-art methods. The extended evaluation on semi-forensic and forensic sketch datasets demonstrate its usage feasibility.",2020,IEEE Access,,10.1109/access.2020.3009744,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09142218.pdf
eaeea227a426a90e73bfe6527db6fab23632a492,1,0,Supplementary Material of Modeling the Uncertainty of Contextual-Connections between Tracklets for Unconstrained Video-based Face Recognition,"where Tgt and Ttt are corresponding temperature factors, k ∈ {0, 1}, αp is the positive penalty and αn is the negative penalty. Directly looking for the label assignment that minimizes E(x,y) is a combinatorial optimization problem which is intractable. Instead, similar to [9], we use mean field method to approximate the distribution P (X,Y) ∝ exp(−E(X,Y)) by the product of independent marginals",2019,,,,https://pdfs.semanticscholar.org/eaee/a227a426a90e73bfe6527db6fab23632a492.pdf
eb1cd4d3a94e44e7af8e7a0d564d497bda548389,1,0,Transductive semi-supervised metric learning for person re-identification,"Abstract Semi-supervised learning is important and has become more widespread because obtaining labeled data is expensive and labor-intensive. In this paper, we focus on the challenging semi-supervised person Re-identification (ReID) task, which is a metric learning problem based on the assumption that unlabeled data is open-set. To address this problem, we propose the Transductive Semi-Supervised Metric Learning (TSSML) framework. In TSSML, we propose a graph-based transductive hard mining method for deeply mining hard triplets in unlabeled data and a degree-based relationship confidence scoring method for further reducing incorrect triplets. Moreover, we investigate the feature consistency loss (FCL) and adopt the curriculum learning strategy to improve the representation learning for semi-supervised ReID. Extensive experiments have been conducted on three large-scale ReID datasets and demonstrate the effectiveness of our TSSML framework.",2020,Pattern Recognit.,,10.1016/j.patcog.2020.107569,
eb25dc0749814be6ffa531cb3e91fe6106a41547,0,1,Locally Linear Attributes of ReLU Neural Networks,"A ReLU neural network determines/is a continuous piecewise linear map from an input space to an output space. The weights in the neural network determine a decomposition of the input space into convex polytopes and on each of these polytopes the network can be described by a single affine mapping. The structure of the decomposition, together with the affine map attached to each polytope, can be analyzed to investigate the behavior of the associated neural network.",2020,ArXiv,2012.0194,,https://arxiv.org/pdf/2012.01940.pdf
eb526174fa071345ff7b1fad1fad240cd943a6d7,1,0,Deeply vulnerable: a study of the robustness of face recognition to presentation attacks,"The vulnerability of deep-learning-based face-recognition (FR) methods, to presentation attacks (PA), is studied in this study. Recently, proposed FR methods based on deep neural networks (DNN) have been shown to outperform most other methods by a significant margin. In a trustworthy face-verification system, however, maximising recognition-performance alone is not sufficient – the system should also be capable of resisting various kinds of attacks, including PA. Previous experience has shown that the PA vulnerability of FR systems tends to increase with face-verification accuracy. Using several publicly available PA datasets, the authors show that DNN-based FR systems compensate for variability between bona fide and PA samples, and tend to score them similarly, which makes such FR systems extremely vulnerable to PAs. Experiments show the vulnerability of the studied DNN-based FR systems to be consistently higher than 90%, and often higher than 98%.",2018,IET Biom.,,10.1049/iet-bmt.2017.0079,https://pdfs.semanticscholar.org/eb52/6174fa071345ff7b1fad1fad240cd943a6d7.pdf
eb68b70f3953f560216a5e009b6a1ee877cf952d,0,1,DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning,"Visual Similarity plays an important role in many computer vision applications. Deep metric learning (DML) is a powerful framework for learning such similarities which not only generalize from training data to identically distributed test distributions, but in particular also translate to unknown test classes. However, its prevailing learning paradigm is class-discriminative supervised training, which typically results in representations specialized in separating training classes. For effective generalization, however, such an image representation needs to capture a diverse range of data characteristics. To this end, we propose and study multiple complementary learning tasks, targeting conceptually different data relationships by only resorting to the available training samples and labels of a standard DML setting. Through simultaneous optimization of our tasks we learn a single model to aggregate their training signals, resulting in strong generalization and state-of-the-art performance on multiple established DML benchmark datasets.",2020,ECCV,2004.13458,10.1007/978-3-030-58598-3_35,https://arxiv.org/pdf/2004.13458.pdf
eb7d94d5143f6fd7bf11c266fd59a6d2139414cc,1,1,How (Not) to Measure Bias in Face Recognition Networks,"Within the last years Face Recognition (FR) systems have achieved human-like (or better) performance, leading to extensive deployment in large-scale practical settings. Yet, especially for sensible domains such as FR we expect algorithms to work equally well for everyone, regardless of somebody’s age, gender, skin colour and/or origin. In this paper, we investigate a methodology to quantify the amount of bias in a trained Convolutional Neural Network (CNN) model for FR that is not only intuitively appealing, but also has already been used in the literature to argue for certain debiasing methods. It works by measuring the “blindness” of the model towards certain face characteristics in the embeddings of faces based on internal cluster validation measures. We conduct experiments on three openly available FR models to determine their bias regarding race, gender and age, and validate the computed scores by comparing their predictions against the actual drop in face recognition performance for minority cases. Interestingly, we could not link a crisp clustering in the embedding space to a strong bias in recognition rates—it is rather the opposite. We therefore offer arguments for the reasons behind this observation and argue for the need of a less näıve clustering approach to develop a working measure for bias in FR models.",2020,ANNPR,,10.1007/978-3-030-58309-5_10,https://digitalcollection.zhaw.ch/bitstream/11475/20277/3/2020_Gluege-etal_Bias-in-face-recognition-networks_ANNPR.pdf
eb995c2d7e30f412632afbe9585c1c14d2a69ea0,1,1,Adversarial Learning With Margin-Based Triplet Embedding Regularization,"The Deep neural networks (DNNs) have achieved great success on a variety of computer vision tasks, however, they are highly vulnerable to adversarial attacks. To address this problem, we propose to improve the local smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification objective, so that the obtained models learn to resist adversarial examples. The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way. Experimental results on MNIST, CASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the robustness of the network against both feature and label adversarial attacks in simple object classification and deep face recognition.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1909.09481,10.1109/ICCV.2019.00665,https://arxiv.org/pdf/1909.09481.pdf
eb9f4b80623341813c1af9e0f421641a40f341bc,0,1,Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network,"Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g. ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3\% to 82.78\%, mCE from 76.0\% to 48.9\% and mFR from 57.7\% to 32.3\% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at this https URL",2020,ArXiv,2001.06268,,https://arxiv.org/pdf/2001.06268.pdf
ebb0ca72775413d0f7c9cbd3c088a87a9fe586e2,1,0,Improved Techniques for Model Inversion Attacks,"Model inversion (MI) attacks in the whitebox setting are aimed at reconstructing training data from model parameters. Such attacks have triggered increasing concerns about privacy, especially given a growing number of online model repositories. However, existing MI attacks against deep neural networks (DNNs) have large room for performance improvement. A natural question is whether the underperformance is because the target model does not memorize much about its training data or it is simply an artifact of imperfect attack algorithm design? This paper shows that it is the latter. We present a variety of new techniques that can significantly boost the performance of MI attacks against DNNs. Recent advances to attack DNNs are largely attributed to the idea of training a general generative adversarial network (GAN) with potential public data and using it to regularize the search space for reconstructed images. We propose to customize the training of a GAN to the inversion task so as to better distill knowledge useful for performing attacks from public data. Moreover, unlike previous work that directly searches for a single data point to represent a target class, we propose to model private data distribution in order to better reconstruct representative data points. Our experiments show that the combination of these techniques can lead to state-of-the-art attack performance on a variety of datasets and models, even when the public data has a large distributional shift from the private data.",2020,ArXiv,2010.04092,,https://arxiv.org/pdf/2010.04092.pdf
ebb3d5c70bedf2287f9b26ac0031004f8f617b97,1,0,"Deep Learning for Understanding Faces: Machines May Be Just as Good, or Better, than Humans","Recent developments in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on various object detection/recognition problems. This has been made possible due to the availability of large annotated data and a better understanding of the nonlinear mapping between images and class labels, as well as the affordability of powerful graphics processing units (GPUs). These developments in deep learning have also improved the capabilities of machines in understanding faces and automatically executing the tasks of face detection, pose estimation, landmark localization, and face recognition from unconstrained images and videos. In this article, we provide an overview of deep-learning methods used for face recognition. We discuss different modules involved in designing an automatic face recognition system and the role of deep learning for each of them. Some open issues regarding DCNNs for face recognition problems are then discussed. This article should prove valuable to scientists, engineers, and end users working in the fields of face recognition, security, visual surveillance, and biometrics.",2018,IEEE Signal Processing Magazine,,10.1109/MSP.2017.2764116,
ebc92b41c4a2148ad2ae3f9a3b2ad4480e376db4,1,0,Active Learning for Imbalanced Datasets,"Active learning increases the effectiveness of labeling when only subsets of unlabeled datasets can be processed manually. To our knowledge, existing algorithms are designed under the assumption that datasets are balanced. However, many real-life datasets are actually imbalanced and we propose two adaptations of active learning to tackle imbalance. First, we modify acquisition functions to select samples by taking advantage of a deep model pretrained on a source domain. Second, we introduce a balancing step in the acquisition process to reduce the imbalance of the labeled subset. Evaluation is done with four imbalanced datasets using existing active learning methods and their modifications introduced here. Results show that our adaptations are useful as long as knowledge from the source domain is transferable to target domains.",2020,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),,10.1109/WACV45572.2020.9093475,http://openaccess.thecvf.com/content_WACV_2020/papers/Aggarwal_Active_Learning_for_Imbalanced_Datasets_WACV_2020_paper.pdf
ebdf99ae25257ef350da565feeaed3cf073d44af,0,1,Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot Learning,"In recent years, few-shot learning problems have received a lot of attention. While methods in most previous works were trained and tested on datasets in one single domain, cross-domain few-shot learning is a brand-new branch of few-shot learning problems, where models handle datasets in different domains between training and testing phases. In this paper, to solve the problem that the model is pre-trained (meta-trained) on a single dataset while fine-tuned on datasets in four different domains, including common objects, satellite images, and medical images, we propose a novel large margin fine-tuning method (LMM-PQS), which generates pseudo query images from support images and fine-tunes the feature extraction modules with a large margin mechanism inspired by methods in face recognition. According to the experiment results, LMM-PQS surpasses the baseline models by a significant margin and demonstrates that our approach is robust and can easily adapt pre-trained models to new domains with few data.",2020,ArXiv,2005.09218,,https://arxiv.org/pdf/2005.09218.pdf
ebff807c5bafcc037e5fa8e2b8963f063f1fdc40,0,1,Adversarial Margin Maximization Networks,"The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability. Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts. In addition, research works also show that DNNs are vulnerable to adversarial examples---maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions. In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view. We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy. It encourages a large margin in the input space, just like the support vector machines. With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner. Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts. Code and models for reproducing our results will be made publicly available.",2019,IEEE transactions on pattern analysis and machine intelligence,1911.05916,10.1109/TPAMI.2019.2948348,https://arxiv.org/pdf/1911.05916.pdf
ec7ff1cefcd86523f98652150686de7ae1531287,0,1,x-Vector DNN Refinement with Full-Length Recordings for Speaker Recognition,"State-of-the-art text-independent speaker recognition systems for long recordings (a few minutes) are based on deep neural network (DNN) speaker embeddings. Current implementations of this paradigm use short speech segments (a few seconds) to train the DNN. This introduces a mismatch between training and inference when extracting embeddings for long duration recordings. To address this, we present a DNN refinement approach that updates a subset of the DNN parameters with full recordings to reduce this mismatch. At the same time, we also modify the DNN architecture to produce embeddings optimized for cosine distance scoring. This is accomplished using a largemargin strategy with angular softmax. Experimental validation shows that our approach is capable of producing embeddings that achieve record performance on the SITW benchmark.",2019,INTERSPEECH,,10.21437/interspeech.2019-2205,https://pdfs.semanticscholar.org/ec7f/f1cefcd86523f98652150686de7ae1531287.pdf
eccbf6cb023ca027524877ab9b2a21e7ed1dd3d1,0,1,ASFD: Automatic and Scalable Face Detector,"In this paper, we propose a novel Automatic and Scalable Face Detector (ASFD), which is based on a combination of neural architecture search techniques as well as a new loss design. First, we propose an automatic feature enhance module named Auto-FEM by improved differential architecture search, which allows efficient multi-scale feature fusion and context enhancement. Second, we use Distance-based Regression and Margin-based Classification (DRMC) multi-task loss to predict accurate bounding boxes and learn highly discriminative deep features. Third, we adopt compound scaling methods and uniformly scale the backbone, feature modules, and head networks to develop a family of ASFD, which are consistently more efficient than the state-of-the-art face detectors. Extensive experiments conducted on popular benchmarks, e.g. WIDER FACE and FDDB, demonstrate that our ASFD-D6 outperforms the prior strong competitors, and our lightweight ASFD-D0 runs at more than 120 FPS with Mobilenet for VGA-resolution images.",2020,ArXiv,2003.11228,,https://arxiv.org/pdf/2003.11228.pdf
ed267e4b9dec7ace26e54878ecddc2262940662c,0,1,SEN: A Novel Feature Normalization Dissimilarity Measure for Prototypical Few-Shot Learning Networks,,2020,ECCV,,10.1007/978-3-030-58592-1_8,
ed2ce0ed9470ae983c8ee1dda0765597e7d2f0a5,1,0,Deep Learning Features for Face Age Estimation: Better Than Human?,"Deep convolutional neural networks have the ability to infer highly representative features from data. We decided to use this power for the purpose of estimating the human face age from a single colour image. We trained the Support Vector Machine regression model on raw feature vectors from the FaceNet deep neural network pretrained for face recognition. Our proposed method is a simple but effective FaceNet extension which does not need large scale data. In order to measure the accuracy of our approach, we proposed a test procedure on the FACES database for which we achieved the mean absolute error of 5.18 and the mean error of 0.09 years. Then, we conducted an experiment employing 78 students and showed that our method outperforms human for faces in the regular upright orientation. For vertically inverted faces, we reported an age underestimation trend in responses of students and our method.",2018,BDAS,,10.1007/978-3-319-99987-6_29,
ed43a7ab9a72c60cc37f69e3e22a52405a0e31ba,0,1,A Hierarchical Framwork with Improved Loss for Large-scale Multi-modal Video Identification,"This paper introduces our solution for iQIYI Celebrity Video Identification Challenge. After analyzing the iQIYI-VID-2019 dataset, we find the distribution of the dataset is very unbalanced and there are many unlabeled samples in the validation set and the test set. For these challenge, we propose a hierarchical system which combines different models and fuses base classifiers. For the false detections and low-quality features in the dataset, we use a simple and reasonable strategy to fuse features. In order to detect videos more accurately, we choose an improved loss function for the learning of base classifiers. Experiment results show that our framework performs well and evaluation conducted by the organizers shows that our final result gets the ninth place online and mAP 88.08%.",2019,ACM Multimedia,,10.1145/3343031.3356074,
ed7181956b28cae5de142c01e0e6884ce3491c2c,0,1,An Attention-Based Speaker Naming Method for Online Adaptation in Non-Fixed Scenarios,"A speaker naming task, which finds and identifies the active speaker in a certain movie or drama scene, is crucial for dealing with high-level video analysis applications such as automatic subtitle labeling and video summarization. Modern approaches have usually exploited biometric features with a gradient-based method instead of rule-based algorithms. In a certain situation, however, a naive gradient-based method does not work efficiently. For example, when new characters are added to the target identification list, the neural network needs to be frequently retrained to identify new people and it causes delays in model preparation. In this paper, we present an attention-based method which reduces the model setup time by updating the newly added data via online adaptation without a gradient update process. We comparatively analyzed with three evaluation metrics(accuracy, memory usage, setup time) of the attention-based method and existing gradient-based methods under various controlled settings of speaker naming. Also, we applied existing speaker naming models and the attention-based model to real video to prove that our approach shows comparable accuracy to the existing state-of-the-art models and even higher accuracy in some cases.",2019,ArXiv,1912.00649,,https://arxiv.org/pdf/1912.00649.pdf
ed9384cef417f69c070d7bca07958f6fd1c5e8d3,1,0,SqueezeFacePoseNet: Lightweight Face Verification Across Different Poses for Mobile Platforms,"Virtual applications through mobile platforms are one of the most critical and ever-growing fields in AI, where ubiquitous and real-time person authentication has become critical after the breakthrough of all services provided via mobile devices. In this context, face verification technologies can provide reliable and robust user authentication, given the availability of cameras in these devices, as well as their widespread use in everyday applications. The rapid development of deep Convolutional Neural Networks has resulted in many accurate face verification architectures. However, their typical size (hundreds of megabytes) makes them infeasible to be incorporated in downloadable mobile applications where the entire file typically may not exceed 100 Mb. Accordingly, we address the challenge of developing a lightweight face recognition network of just a few megabytes that can operate with sufficient accuracy in comparison to much larger models. The network also should be able to operate under different poses, given the variability naturally observed in uncontrolled environments where mobile devices are typically used. In this paper, we adapt the lightweight SqueezeNet model, of just 4.4MB, to effectively provide cross-pose face recognition. After trained on the MS-Celeb-1M and VGGFace2 databases, our model achieves an EER of 1.23% on the difficult frontal vs. profile comparison, and0.54% on profile vs. profile images. Under less extreme variations involving frontal images in any of the enrolment/query images pair, EER is pushed down to<0.3%, and the FRR at FAR=0.1%to less than 1%. This makes our light model suitable for face recognition where at least acquisition of the enrolment image can be controlled. At the cost of a slight degradation in performance, we also test an even lighter model (of just 2.5MB) where regular convolutions are replaced with depth-wise separable convolutions.",2020,ArXiv,2007.08566,,https://arxiv.org/pdf/2007.08566.pdf
ed9b09325bd810b78821b8cbd14cd1abdfc67119,0,1,The xx205 System for the VoxCeleb Speaker Recognition Challenge 2020,"This report describes the systems submitted to the first and second tracks of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020, which ranked second in both tracks. Three key points of the system pipeline are explored: (1) investigating multiple CNN architectures including ResNet, Res2Net and dual path network (DPN) to extract the x-vectors, (2) using a composite angular margin softmax loss to train the speaker models, and (3) applying score normalization and system fusion to boost the performance. Measured on the VoxSRC-20 Eval set, the best submitted systems achieve an EER of $3.808\%$ and a MinDCF of $0.1958$ in the close-condition track 1, and an EER of $3.798\%$ and a MinDCF of $0.1942$ in the open-condition track 2, respectively.",2020,ArXiv,2011.002,,https://arxiv.org/pdf/2011.00200.pdf
ede3c2c5d74fe06fc952a543ed40832dd55bce0b,1,0,Density-Aware Feature Embedding for Face Clustering,"Clustering has many applications in research and industry. However, traditional clustering methods, such as K-means, DBSCAN and HAC, impose oversimplifying assumptions and thus are not well-suited to face clustering. To adapt to the distribution of realistic problems, a natural approach is to use Graph Convolutional Networks (GCNs) to enhance features for clustering. However, GCNs can only utilize local information, which ignores the overall characterisitcs of the clusters. In this paper, we propose a Density-Aware Feature Embedding Network (DA-Net) for the task of face clustering, which utilizes both local and non-local information, to learn a robust feature embedding. Specifically, DA-Net uses GCNs to aggregate features locally, and then incorporates non-local information using a density chain, which is a chain of faces from low density to high density. This density chain exploits the non-uniform distribution of face images in the dataset. Then, an LSTM takes the density chain as input to generate the final feature embedding. Once this embedding is generated, traditional clustering methods, such as density-based clustering, can be used to obtain the final clustering results. Extensive experiments verify the effectiveness of the proposed feature embedding method, which can achieve state-of-the-art performance on public benchmarks.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,10.1109/cvpr42600.2020.00673,http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Density-Aware_Feature_Embedding_for_Face_Clustering_CVPR_2020_paper.pdf
ee02bed42e11e88907c69e899d192019900f1a14,0,1,Angular Visual Hardness,"Recent convolutional neural networks (CNNs) have led to impressive performance but often suffer from poor calibration. They tend to be overconfident, with the model confidence not always reflecting the underlying true ambiguity and hardness. In this paper, we propose angular visual hardness (AVH), a score given by the normalized angular distance between the sample feature embedding and the target classifier to measure sample hardness. We validate this score with an in-depth and extensive scientific study, and observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models improve on the classification of harder examples. We observe that the training dynamics of AVH is vastly different compared to the training loss. Specifically, AVH quickly reaches a plateau for all samples even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. We also find that AVH has a statistically significant correlation with human visual hardness. Finally, we demonstrate the benefit of AVH to a variety of applications such as self-training for domain adaptation and domain generalization.",2020,ICML,1912.02279,,https://arxiv.org/pdf/1912.02279.pdf
ee0c1cefb180f408ef3765657bff06d31d325832,1,1,Deep Polynomial Neural Networks,"Deep Convolutional Neural Networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose $\Pi$-Nets, a new class of DCNNs. $\Pi$-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that $\Pi$-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art results in three challenging tasks, i.e. image generation, face verification and 3D mesh representation learning.",2020,ArXiv,2006.13026,,https://arxiv.org/pdf/2006.13026.pdf
ee2772acc36b473fbe684d24bc12a39dba9329a2,1,0,"Do They All Look the Same? Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning","We study to what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of ob- taining large numbers of facial images with nationality la- bels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 49% . Further, we train mul- tiple facial attribute classifiers to identify the most distinc- tive features for each group. We find that Chinese, Japanese and Koreans do exhibit substantial differences in certain at- tributes, such as bangs, smiling, and bushy eyebrows. Along the way, we uncover several gender-related cross-country patterns as well. Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find potential applications in tourism, e-commerce, social media marketing, criminal justice and even counter- terrorism.",2018,2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),1610.01854,10.1109/MIPR.2018.00015,https://arxiv.org/pdf/1610.01854.pdf
ee93a0fc2166f8416cf5d424c4826cb634ebed1d,0,1,Dividing and Conquering Cross-Modal Recipe Retrieval: from Nearest Neighbours Baselines to SoTA,"We propose a novel non-parametric method for cross-modal retrieval which is applied on top of precomputed image and text embeddings. By combining our method with standard approaches for building image and text encoders, trained independently with a self-supervised classification objective, we create a baseline model which outperforms most existing methods on a challenging image-to-recipe task. We also use our method for comparing image and text encoders trained using different modern approaches, thus addressing the issues hindering the developments of novel methods for cross-modal recipe retrieval. We demonstrate how to use the insights from model comparison and extend our baseline model with standard triplet loss that improves SoTA on the Recipe1M dataset by a large margin, while using only precomputed features and with much less complexity than existing methods.",2019,ArXiv,1911.12763,,https://arxiv.org/pdf/1911.12763.pdf
eecd9a8df7b1fa7b1288c17e6dcef88e0dd822d7,1,0,An Arable Field for Benchmarking of Metaheuristic Algorithms for Capacitated Coverage Path Planning Problems,"This study specifies an agricultural field (Latitude = 56°30′0.8″ N, Longitude = 9°35′27.88″ E) and provides the absolute optimal route for covering that field. The calculated absolute optimal solution for this field can be used as the basis for benchmarking of metaheuristic algorithms used for finding the most efficient route in the field. The problem of finding the most efficient route that covers a field can be formulated as a Traveling Salesman Problem (TSP), which is an NP-hard problem. This means that the optimal solution is infeasible to calculate, except for very small fields. Therefore, a range of metaheuristic methods has been developed that provide a near-optimal solution to a TSP in a “reasonable” time. The main challenge with metaheuristic methods is that the quality of the solutions can normally not be compared to the absolute optimal solution since this “ground truth” value is unknown. Even though the selected benchmarking field requires only eight tracks, the solution space consists of more than 1.3 billion solutions. In this study, the absolute optimal solution for the capacitated coverage path planning problem was determined by calculating the non-working distance of the entire solution space and determining the solution with the shortest non-working distance. This was done for four scenarios consisting of low/high bin capacity and short/long distance between field and storage depot. For each scenario, the absolute optimal solution and its associated cost value (minimum non-working distance) were compared to the solutions of two metaheuristic algorithms; Simulated Annealing Algorithm (SAA) and Ant Colony Optimization (ACO). The benchmarking showed that neither algorithm could find the optimal solution for all scenarios, but they found near-optimal solutions, with only up to 6 pct increasing non-working distance. SAA performed better than ACO, concerning quality, stability, and execution time.",2020,,,10.3390/AGRONOMY10101454,
ef2005ed94e553a1e94555216419c44440d3280d,0,1,On the Reliability of Cancelable Biometrics: Revisit the Irreversibility,"Over the years, many biometric template protection schemes, primarily based on the notion of ""cancelable biometrics"" have been proposed. A cancelable biometric algorithm needs to satisfy four biometric template protection criteria, i.e., irreversibility, revocability, unlinkability, and performance preservation. However, a systematic analysis of irreversibility has been often neglected. In this paper, the common distance correlation characteristic of cancelable biometrics is analyzed. Next, a similarity-based attack is formulated to break the irreversibility of cancelable biometric under the Kerckhoffs's assumption where the cancelable biometrics algorithm and parameter are known to the attackers. The irreversibility based on the mutual information is also redefined, and a framework to measure the information leakage from the distance correlation characteristic is proposed. The results achieved on face, iris, and fingerprint prove that it is theoretically hard to meet full irreversibility. To have a good biometric system, a balance has to be achieved between accuracy and security.",2019,ArXiv,,,
ef5085b455892471d6c6f3d94a798c557bace0f9,1,0,Beware the evolving 'intelligent' web service! An integration architecture tactic to guard AI-first components,"Intelligent services provide the power of AI to developers via simple RESTful API endpoints, abstracting away many complexities of machine learning. However, most of these intelligent services-such as computer vision-continually learn with time. When the internals within the abstracted 'black box' become hidden and evolve, pitfalls emerge in the robustness of applications that depend on these evolving services. Without adapting the way developers plan and construct projects reliant on intelligent services, significant gaps and risks result in both project planning and development. Therefore, how can software engineers best mitigate software evolution risk moving forward, thereby ensuring that their own applications maintain quality? Our proposal is an architectural tactic designed to improve intelligent service-dependent software robustness. The tactic involves creating an application-specific benchmark dataset baselined against an intelligent service, enabling evolutionary behaviour changes to be mitigated. A technical evaluation of our implementation of this architecture demonstrates how the tactic can identify 1,054 cases of substantial confidence evolution and 2,461 cases of substantial changes to response label sets using a dataset consisting of 331 images that evolve when sent to a service.",2020,ArXiv,2005.13186,,https://arxiv.org/pdf/2005.13186.pdf
efe63af7e8921c276bcf62e06ba5979b7fe679f4,0,1,BioMetricNet: deep unconstrained face verification through learning of metrics regularized onto Gaussian distributions,"We present BioMetricNet: a novel framework for deep unconstrained face verification which learns a regularized metric to compare facial features. Differently from popular methods such as FaceNet, the proposed approach does not impose any specific metric on facial features; instead, it shapes the decision space by learning a latent representation in which matching and non-matching pairs are mapped onto clearly separated and well-behaved target distributions. In particular, the network jointly learns the best feature representation, and the best metric that follows the target distributions, to be used to discriminate face images. In this paper we present this general framework, first of its kind for facial verification, and tailor it to Gaussian distributions. This choice enables the use of a simple linear decision boundary that can be tuned to achieve the desired trade-off between false alarm and genuine acceptance rate, and leads to a loss function that can be written in closed form. Extensive analysis and experimentation on publicly available datasets such as Labeled Faces in the wild (LFW), Youtube faces (YTF), Celebrities in Frontal-Profile in the Wild (CFP), and challenging datasets like cross-age LFW (CALFW), cross-pose LFW (CPLFW), In-the-wild Age Dataset (AgeDB) show a significant performance improvement and confirms the effectiveness and superiority of BioMetricNet over existing state-of-the-art methods.",2020,ArXiv,2008.06021,,https://arxiv.org/pdf/2008.06021.pdf
effc2b3f5cd09120eadc2e9b28116d4227326e04,1,0,Morphing Detection Using a General- Purpose Face Recognition System,"Image morphing has proven to be very successful at deceiving facial recognition systems. Such a vulnerability can be critical when exploited in an automatic border control scenario. Recent works on this topic rely on dedicated algorithms which require additional software modules deployed alongside an existing facial recognition system. In this work, we address the problem of morphing detection by using state-of-the-art facial recognition algorithms based on hand-crafted features and deep convolutional neural networks. We show that a general-purpose face recognition system combined with a simple linear classifier can be successfully used as a morphing detector. The proposed method reuses an existing feature extraction pipeline instead of introducing additional modules. It requires neither fine-tuning nor modifications to the existing recognition system and can be trained using only a small dataset. The proposed approach achieves state-of-the-art performance on our morphing datasets using a 5-fold cross-validation.",2018,2018 26th European Signal Processing Conference (EUSIPCO),,10.23919/EUSIPCO.2018.8553375,https://www.eurasip.org/Proceedings/Eusipco/Eusipco2018/papers/1570437948.pdf
f026f2578658683d8a9e5ed98af6b5cc75a371cb,1,0,Learning Compositional Visual Concepts with Mutual Consistency,"Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,1711.06148,10.1109/CVPR.2018.00903,https://arxiv.org/pdf/1711.06148.pdf
f02f0f6fcd56a9b1407045de6634df15c60a85cd,1,0,Learning Low-shot facial representations via 2D warping,"Face recognition has seen a significant improvement by using the deep convolutional neural networks. In this work, we mainly study the influence of the 2D warping module for one-shot face recognition. To achieve this, we first propose a 2D-Warping Layer to generate new features for the novel classes during the training, then fine-tuning the network by adding the recent proposed fisher loss to learn more discriminative features. We evaluate the proposed method on two popular databases for unconstrained face recognition, the Labeled Faces in the Wild (LFW) and the Youtube Faces (YTF) database. In both cases, the proposed method achieves competitive results with the accuracy of 99.25\% for LFW and 94.3\% for YTF, separately. Moreover, the experimental results on MS-Celeb-1M one-shot faces dataset show that with the proposed method, the model achieves comparable results of 77.92\% coverage rate at precision = 99\% for the novel classes while still keeps top-1 accuracy of 99.80\% for the normal classes.",2017,ArXiv,1712.05015,,https://arxiv.org/pdf/1712.05015.pdf
f0928c32a9b483dc0a0e6b34fa356ba525622edb,1,1,Landmark Detection in Low Resolution Faces with Semi-Supervised Learning,"Landmark detection algorithms trained on high resolution images perform poorly on datasets containing low resolution images. This deters the performance of algorithms relying on quality landmarks, for example, face recognition. To the best of our knowledge, there does not exist any dataset consisting of low resolution face images along with their annotated landmarks, making supervised training infeasible. In this paper, we present a semi-supervised approach to predict landmarks on low resolution images by learning them from labeled high resolution images. The objective of this work is to show that predicting landmarks directly on low resolution images is more effective than the current practice of aligning images after rescaling or superresolution. In a two-step process, the proposed approach first learns to generate low resolution images by modeling the distribution of target low resolution images. In the second stage, the roles of generated images and real low resolution images are switched and the model learns to predict landmarks for real low resolution images from generated low resolution images. With extensive experimentation, we study the impact of each of the design choices and also show that prediction of landmarks directly on low resolution images improves the performance of important tasks such as face recognition in low resolution images.",2019,ArXiv,1907.13255,,https://arxiv.org/pdf/1907.13255.pdf
f0933301a517158e8b80987000d7f302680c45f9,1,1,GroupFace: Learning Latent Groups and Constructing Group-Based Representations for Face Recognition,"In the field of face recognition, a model learns to distinguish millions of face images with fewer dimensional embedding features, and such vast information may not be properly encoded in the conventional model with a single branch. We propose a novel face-recognition-specialized architecture called GroupFace that utilizes multiple group-aware representations, simultaneously, to improve the quality of the embedding feature. The proposed method provides self-distributed labels that balance the number of samples belonging to each group without additional human annotations, and learns the group-aware representations that can narrow down the search space of the target identity. We prove the effectiveness of the proposed method by showing extensive ablation studies and visualizations. All the components of the proposed method can be trained in an end-to-end manner with a marginal increase of computational complexity. Finally, the proposed method achieves the state-of-the-art results with significant improvements in 1:1 face verification and 1:N face identification tasks on the following public datasets: LFW, YTF, CALFW, CPLFW, CFP, AgeDB-30, MegaFace, IJB-B and IJB-C.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2005.10497,10.1109/CVPR42600.2020.00566,https://arxiv.org/pdf/2005.10497.pdf
f15b7c317f106816bf444ac4ffb6c280cd6392c7,1,0,Deep Disguised Faces Recognition,"Recently, deep learning based approaches have yielded a significant improvement in face recognition in the wild. However,"" disguised face"" recognition is still a challenging task that needs to be investigated, and the Disguised Faces in the Wild (DFW) competition is designed for this task. In this paper, we propose a two-stage training approach to utilize the small-scale training data provided by the DFW competition. Specifically, in the first stage, we train Deep Convolutional Neural Networks (DCNNs) for generic face recognition. In the second stage, we use Principal Components Analysis (PCA) based on the DFW training set to find the best transformation matrix for identity representation of disguised faces. We evaluate our model on the DFW testing dataset and it shows better performance over the state-of-the-art generic face recognition methods. It also achieves the best results on the DFW competition - Phase 1.",2018,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW.2018.00012,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w1/Zhang_Deep_Disguised_Faces_CVPR_2018_paper.pdf
f1a956a83fefb0726f9c6d128b115c9f68a05732,0,1,Frequency and Temporal Convolutional Attention for Text-Independent Speaker Recognition,"Majority of the recent approaches for text-independent speaker recognition apply attention or similar techniques for aggregation of frame-level feature descriptors generated by a deep neural network (DNN) front-end. In this paper, we propose methods of convolutional attention for independently modelling temporal and frequency information in a convolutional neural network (CNN) based front-end. Our system utilizes convolutional block attention modules (CBAMs) [1] appropriately modified to accommodate spectrogram inputs. The proposed CNN front-end fitted with the proposed convolutional attention modules outperform the no-attention and spatial-CBAM baselines by a significant margin on the VoxCeleb [2], [3] speaker verification benchmark. Our best model achieves an equal error rate of 2.031% on the VoxCeleb1 test set, which is a considerable improvement over comparable state of the art results. For a more thorough assessment of the effects of frequency and temporal attention in real-world conditions, we conduct ablation experiments by randomly dropping frequency bins and temporal frames from the input spectrograms, concluding that instead of modelling either of the entities, simultaneously modelling temporal and frequency attention translates to better real-world performance.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1910.07364,10.1109/ICASSP40776.2020.9054440,https://arxiv.org/pdf/1910.07364.pdf
f1d02db201273b31305a853c6d470976bb49fb6e,1,1,Out-of-Distribution Detection for Reliable Face Recognition,"In real applications, face recognition systems are always faced with non-face inputs and low-quality faces due to the complicated conditions like mis-detections by face detectors. However, in deep learning based methods, these outliers are always ignored during training phase and the models tend to make unreasonable decisions on these images. For example, matching a texture-rich patch to an old-man face overconfidently. We formulate this challenge on the task of out-of-distribution detection (OOD), where a network must determine whether or not an input is outside of the set on which the network can safely perform. In this paper, we propose to detect out-of-distribution samples based on uncertainty prediction and the L2-norm of features, so as to effectively filter out non-face and low-quality faces. We demonstrate that the proposed method can reliably detect out-of-distribution samples and improve the performance of face recognition, without the need of labelled OOD data.",2020,IEEE Signal Processing Letters,,10.1109/LSP.2020.2988140,
f22e2eb59c4cf3f124c14bbdd0c7c93fb1c74cd6,0,1,Primate Face Identification in the Wild,"Ecological imbalance owing to rapid urbanization and deforestation has adversely affected the population of several wild animals. This loss of habitat has skewed the population of several non-human primate species like chimpanzees and macaques and has constrained them to co-exist in close proximity of human settlements, often leading to human-wildlife conflicts while competing for resources. For effective wildlife conservation and conflict management, regular monitoring of population and of conflicted regions is necessary. However, existing approaches like field visits for data collection and manual analysis by experts is resource intensive, tedious and time consuming, thus necessitating an automated, non-invasive, more efficient alternative like image based facial recognition. The challenge in individual identification arises due to unrelated factors like pose, lighting variations and occlusions due to the uncontrolled environments, that is further exacerbated by limited training data. Inspired by human perception, we propose to learn representations that are robust to such nuisance factors and capture the notion of similarity over the individual identity sub-manifolds. The proposed approach, Primate Face Identification (PFID), achieves this by training the network to distinguish between positive and negative pairs of images. The PFID loss augments the standard cross entropy loss with a pairwise loss to learn more discriminative and generalizable features, thus making it appropriate for other related identification tasks like open-set, closed set and verification. We report state-of-the-art accuracy on facial recognition of two primate species, rhesus macaques and chimpanzees under the four protocols of classification, verification, closed-set identification and open-set recognition.",2019,PRICAI,1907.02642,10.1007/978-3-030-29894-4_32,https://arxiv.org/pdf/1907.02642.pdf
f24ae8a7699182a40b471f03ca2c48d22018f410,0,1,"Benanza: Automatic μBenchmark Generation to Compute ""Lower-bound"" Latency and Inform Optimizations of Deep Learning Models on GPUs","As Deep Learning (DL) models have been increasingly used in latency-sensitive applications, there has been a growing interest in improving their response time. An important venue for such improvement is to profile the execution of these models and characterize their performance to identify possible optimization opportunities. However, the current profiling tools lack the highly desired abilities to characterize ideal performance, identify sources of inefficiency, and quantify the benefits of potential optimizations. Such deficiencies have led to slow characterization/optimization cycles that cannot keep up with the fast pace at which new DL models are introduced.We propose Benanza, a sustainable and extensible benchmarking and analysis design that speeds up the characterization/optimization cycle of DL models on GPUs. Benanza consists of four major components: a model processor that parses models into an internal representation, a configurable benchmark generator that automatically generates micro-benchmarks given a set of models, a database of benchmark results, and an analyzer that computes the ""lower-bound"" latency of DL models using the benchmark data and informs optimizations of model execution. The ""lower-bound"" latency metric estimates the ideal model execution on a GPU system and serves as the basis for identifying optimization opportunities in frameworks or system libraries. We used Benanza to evaluate 30 ONNX models in MXNet, ONNX Runtime, and PyTorch on 7 GPUs ranging from Kepler to the latest Turing, and identified optimizations in parallel layer execution, cuDNN convolution algorithm selection, framework inefficiency, layer fusion, and using Tensor Cores.",2020,2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS),1911.06922,10.1109/IPDPS47924.2020.00053,https://arxiv.org/pdf/1911.06922.pdf
f24d59f2351eb2578cf55d969b6be42223866ac1,1,0,Rough Sets,s of Invited Talks Fuzzy Rough Sets: Achievements and Opportunities,2019,Lecture Notes in Computer Science,,10.1007/978-3-030-22815-6,https://biblio.ugent.be/publication/8612581/file/8612583.pdf
f25fc4105b78c925d5d195e240c0059dd366e574,1,1,Recognizing Families through Images with Pretrained Encoder,"Kinship verification and kinship retrieval are emerging tasks in computer vision. Kinship verification aims at determining whether two facial images are from related people or not, while kinship retrieval is the task of retrieving possible related facial images to a person from a gallery of images. They introduce unique challenges because of the hidden relations and features that carry inherent characteristics between the facial images. We employ 3 methods, FaceNet, Siamese VGG-Face, and a combination of FaceNet and VGG-Face models as feature extractors, to achieve the 9th standing for kinship verification and the 5th standing for kinship retrieval in the Recognizing Family in The Wild 2020 competition. We then further experimented using StyleGAN2 as another encoder, with no improvement in the result.",2020,ArXiv,2005.11811,,https://arxiv.org/pdf/2005.11811.pdf
f26fd03feb0fa85737b5cc16fc29a72634ea2e0b,0,1,Toward Driver Face Recognition in the Intelligent Traffic Monitoring Systems,"This paper models the driver face recognition problem under the intelligent traffic monitoring systems as severe illumination variation face recognition with single sample problem. Firstly, in the point of view of numerical value sign, the current illumination invariant unit is derived from the subtraction of two pixels in the face local region, which may be positive or negative, we propose a generalized illumination robust (GIR) model based on positive and negative illumination invariant units to tackle severe illumination variations. Then, the GIR model can be used to generate several GIR images based on the local edge-region or the local block-region, which results in the edge-region based GIR (EGIR) image or the block-region based GIR (BGIR) image. For single GIR image based classification, the GIR image utilizes the saturation function and the nearest neighbor classifier, which can develop EGIR-face and BGIR-face. For multi GIR images based classification, the GIR images employ the extended sparse representation classification (ESRC) as the classifier that can form the EGIR image based classification (GIRC) and the BGIR image based classification (BGIRC). Further, the GIR model is integrated with the pre-trained deep learning (PDL) model to construct the GIR-PDL model. Finally, the performances of the proposed methods are verified on the Extended Yale B, CMU PIE, AR, self-built Driver and VGGFace2 face databases. The experimental results indicate that the proposed methods are efficient to tackle severe illumination variations.",2020,IEEE Transactions on Intelligent Transportation Systems,,10.1109/TITS.2019.2945923,
f288b372a1a0c4c9a28a5ca9865385dacb48a6ad,1,0,Single Unit Status in Deep Convolutional Neural Network Codes for Face Identification: Sparseness Redefined,"Deep convolutional neural networks (DCNNs) trained for face identification develop representations that generalize over variable images, while retaining subject (e.g., gender) and image (e.g., viewpoint) information. Identity, gender, and viewpoint codes were studied at the ""neural unit"" and ensemble levels of a face-identification network. At the unit level, identification, gender classification, and viewpoint estimation were measured by deleting units to create variably-sized, randomly-sampled subspaces at the top network layer. Identification of 3,531 identities remained high (area under the ROC approximately 1.0) as dimensionality decreased from 512 units to 16 (0.95), 4 (0.80), and 2 (0.72) units. Individual identities separated statistically on every top-layer unit. Cross-unit responses were minimally correlated, indicating that units code non-redundant identity cues. This ""distributed"" code requires only a sparse, random sample of units to identify faces accurately. Gender classification declined gradually and viewpoint estimation fell steeply as dimensionality decreased. Individual units were weakly predictive of gender and viewpoint, but ensembles proved effective predictors. Therefore, distributed and sparse codes co-exist in the network units to represent different face attributes. At the ensemble level, principal component analysis of face representations showed that identity, gender, and viewpoint information separated into high-dimensional subspaces, ordered by explained variance. Identity, gender, and viewpoint information contributed to all individual unit responses, undercutting a neural tuning analogy for face attributes. Interpretation of neural-like codes from DCNNs, and by analogy, high-level visual codes, cannot be inferred from single unit responses. Instead, ""meaning"" is encoded by directions in the high-dimensional space.",2020,ArXiv,2002.06274,,https://arxiv.org/pdf/2002.06274.pdf
f31e2025ded882f9f9b57c8b1de9c196c69ad3ee,1,0,The Use of Computer Vision to Analyze Brand-Related User Generated Image Content,"Abstract With the increasing popularity of visual-oriented social media platforms, the prevalence of visual brand-related User Generated Content (UGC) have increased. Monitoring such content is important as this visual brand-related UGC can have a large influence on a brand's image and hence provides useful opportunities to observe brand performance (e.g., monitoring trends and consumer segments). The current research discusses the application of computer vision for marketing practitioners and researchers and examines the usability of three different pre-trained ready-to-use computer vision models (i.e., YOLOV2, Google Cloud Vision, and Clarifai) to analyze visual brand-related UGC automatically. A 3-step approach was adopted in which 1) a database of 21,738 Instagram pictures related to 24 different brands was constructed, 2) the images were processed by the three different computer vision models, and 3) a label evaluation procedure was conducted with a sample of the labels (object names) outputted by the models. The results of the label evaluation procedure are quantitatively assessed and complemented with four concrete examples of how the output of computer vision can be used to analyze visual brand-related UGC. Results show that computer vision can yield various marketing insights. Moreover, we found that the three tested computer vision models differ in applicability. Google Cloud Vision is more accurate in object detection, whereas Clarifai provides more useful labels to interpret the portrayal of a brand. YOLOV2 did not prove to be useful to analyze visual brand-related UGC. Results and implications of the findings for marketers and marketing scholars will be discussed.",2020,,,10.1016/j.intmar.2019.09.003,
f3291d682a4fb4e2521beee9c70398cd5ff38b3b,1,1,Lightweight Face Recognition Challenge,"Face representation using Deep Convolutional Neural Network (DCNN) embedding is the method of choice for face recognition. Current state-of-the-art face recognition systems can achieve high accuracy on existing in-the-wild datasets. However, most of these datasets employ quite limited comparisons during the evaluation, which does not simulate a real-world scenario, where extensive comparisons are encountered by a face recognition system. To this end, we propose two large-scale datasets (DeepGlint-Image with 1.8M images and IQIYI-Video with 0.2M videos) and define an extensive comparison metric (trillion-level pairs on the DeepGlint-Image dataset and billion-level pairs on the IQIYI-Video dataset) for an unbiased evaluation of deep face recognition models. To ensure fair comparison during the competition, we define light-model track and large-model track, respectively. Each track has strict constraints on computational complexity and model size. To the best of our knowledge, this is the most comprehensive and unbiased benchmarks for deep face recognition. To facilitate future research, the proposed datasets are released and the online test server is accessible as part of the Lightweight Face Recognition Challenge at the International Conference on Computer Vision, 2019.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00322,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Deng_Lightweight_Face_Recognition_Challenge_ICCVW_2019_paper.pdf
f375d3f576ae04e2f78db777d8d083ba8b9cb248,0,1,Improving Face Recognition from Hard Samples via Distribution Distillation Loss,"Large facial variations are the main challenge in face recognition. To this end, previous variation-specific methods make full use of task-related prior to design special network losses, which are typically not general among different tasks and scenarios. In contrast, the existing generic methods focus on improving the feature discriminability to minimize the intra-class distance while maximizing the interclass distance, which perform well on easy samples but fail on hard samples. To improve the performance on those hard samples for general tasks, we propose a novel Distribution Distillation Loss to narrow the performance gap between easy and hard samples, which is a simple, effective and generic for various types of facial variations. Specifically, we first adopt state-of-the-art classifiers such as ArcFace to construct two similarity distributions: teacher distribution from easy samples and student distribution from hard samples. Then, we propose a novel distribution-driven loss to constrain the student distribution to approximate the teacher distribution, which thus leads to smaller overlap between the positive and negative pairs in the student distribution. We have conducted extensive experiments on both generic large-scale face benchmarks and benchmarks with diverse variations on race, resolution and pose. The quantitative results demonstrate the superiority of our method over strong baselines, e.g., Arcface and Cosface.",2020,ECCV,2002.03662,10.1007/978-3-030-58577-8_9,https://arxiv.org/pdf/2002.03662.pdf
f38fc1ac7b77bbbd61dd6f18bbe1bae7536634fa,0,1,Turning a Vulnerability into an Asset: Accelerating Facial Identification with Morphing,"In recent years, morphing of facial images has arisen as an important attack vector on biometric systems. Detection of morphed images has proven challenging for automated systems and human experts alike. Likewise, in recent years, the importance of efficient (fast) biometric identification has been emphasised by the rapid rise and growth of large-scale bio-metric systems around the world.In this paper, the aforementioned, hitherto unrelated, topics within the biometrics domain are combined: the properties of morphed images are exploited for the purpose of improving the transaction times of a biometric identification system. Specifically, morphs of two or more samples are used in the pre-selection step of a two-stage biometric identification system. In a proof-of-concept experimental evaluation using two state-of-the-art open-source facial recognition frameworks it is shown, that the proposed system achieves hit rates comparable to that of an exhaustive search-based baseline, while significantly reducing the penetration rate (and thus the computational workload) associated with the biometric identification transactions.",2019,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,10.1109/ICASSP.2019.8683326,https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/11250/2600348/2/Drozdowski-MorphingPreselection-ICASSP-2019.pdf
f3bc9b570604e475466bef60a8cda6119b9bbeea,0,1,Deep Domain Adaptation for Asian Face Recognition via Ada-IBN,"Owing to the great power of convolution neural networks (CNNs) in extracting discriminative features, remarkable progress has been made in face recognition recently. However, there remains a significant gap between the performance of real-world scenario and ideal experimental condition. Due to the lack of training data in Asian face, a lot of well-known datasets, such as MS-Celeb-1M, VGG Face and CASIA-Web Face that mainly consist of Caucasians, are applied to verify in Asians directly, which results in poor performance. To address this issue from the perspective of domain adaptation, in this paper, we propose a conceptually simple but novel method named Adaptive Instance Batch Normalization (Ada-IBN). We first develop the method that transferring the knowledge from source to target domain to alleviate the discrepancy while increases the generalization capacity of CNNs. Then we design a dataset Cceleb-1K which captures identification photos and live pictures from one thousand different Asians. Most importantly, extensive experiments and analysis on our Cceleb-1k and LFW show that our Ada-IBN network achieves state-of-the-art performance in face verification task.",2019,2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),,10.1109/ICMEW.2019.00096,
f3ec74645ebdc6a2f082ad50ab59553a6c244880,0,1,WIDER Face and Pedestrian Challenge 2018: Methods and Results,"This paper presents a review of the 2018 WIDER Challenge on Face and Pedestrian. The challenge focuses on the problem of precise localization of human faces and bodies, and accurate association of identities. It comprises of three tracks: (i) WIDER Face which aims at soliciting new approaches to advance the state-of-the-art in face detection, (ii) WIDER Pedestrian which aims to find effective and efficient approaches to address the problem of pedestrian detection in unconstrained environments, and (iii) WIDER Person Search which presents an exciting challenge of searching persons across 192 movies. In total, 73 teams made valid submissions to the challenge tracks. We summarize the winning solutions for all three tracks. and present discussions on open problems and potential research directions in these topics.",2019,ArXiv,1902.06854,,https://arxiv.org/pdf/1902.06854.pdf
f41f0395f42b8655a630e82bd6e40d73d4c420ef,1,0,Plug-in Factorization for Latent Representation Disentanglement,"In this work, we propose a Factorized Disentangler-Entangler Network (FDEN) that learns to decompose a latent representation into two mutually independent factors, namely, identity and style. Given a latent representation, the proposed framework draws a set of interpretable factors aligned to identity of an observed data and learns to maximize the independency between these factors. Our work introduces an idea for a plug-in method to disentangle latent representations of already learned deep models with no affect to the model. In doing so, it brings the possibilities of extending state-of-the-art models to solve different tasks and also maintain the performance of its original task. Thus, FDEN is naturally applicable to jointly perform multiple tasks such as few-shot learning and image-to-image translation in a single framework. We show the effectiveness of our work in disentangling a latent representation in two parts. First, to evaluate the alignment of factor to an identity, we perform few-shot learning using only the aligned factor. Then, to evaluate the effectiveness of decomposition of latent representation and to show that plugin method does not affect the deep model in its performance, we perform image-to-image style transfer by mixing factors of different images. These evaluations show, qualitatively and quantitatively, that our proposed framework can indeed disentangle a latent representation.",2019,ArXiv,,,
f447caf1c5d479fea473c8192e1813b62b2a194c,1,0,Joint Deep Learning of Facial Expression Synthesis and Recognition,"Recently, deep learning based facial expression recognition (FER) methods have attracted considerable attention and they usually require large-scale labelled training data. Nonetheless, the publicly available facial expression databases typically contain a small amount of labelled data. In this paper, to overcome the above issue, we propose a novel joint deep learning of facial expression synthesis and recognition method for effective FER. More specifically, the proposed method involves a two-stage learning procedure. Firstly, a facial expression synthesis generative adversarial network (FESGAN) is pre-trained to generate facial images with different facial expressions. To increase the diversity of the training images, FESGAN is elaborately designed to generate images with new identities from a prior distribution. Secondly, an expression recognition network is jointly learned with the pre-trained FESGAN in a unified framework. In particular, the classification loss computed from the recognition network is used to simultaneously optimize the performance of both the recognition network and the generator of FESGAN. Moreover, in order to alleviate the problem of data bias between the real images and the synthetic images, we propose an intra-class loss with a novel real data-guided back-propagation (RDBP) algorithm to reduce the intra-class variations of images from the same class, which can significantly improve the final performance. Extensive experimental results on public facial expression databases demonstrate the superiority of the proposed method compared with several state-of-the-art FER methods.",2020,IEEE Transactions on Multimedia,2002.02194,10.1109/TMM.2019.2962317,https://arxiv.org/pdf/2002.02194.pdf
f4a44b94d4b184e4638cb1019526170590f99ac4,0,1,AM-LFS: AutoML for Loss Function Search,"Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1905.07375,10.1109/ICCV.2019.00850,https://arxiv.org/pdf/1905.07375.pdf
f4c44ed215390edb07415d767b101a8340ea650f,0,1,Speaker Representation Learning using Global Context Guided Channel and Time-Frequency Transformations,"In this study, we propose the global context guided channel and time-frequency transformations to model the long-range, non-local time-frequency dependencies and channel variances in speaker representations. We use the global context information to enhance important channels and recalibrate salient time-frequency locations by computing the similarity between the global context and local features. The proposed modules, together with a popular ResNet based model, are evaluated on the VoxCeleb1 dataset, which is a large scale speaker verification corpus collected in the wild. This lightweight block can be easily incorporated into a CNN model with little additional computational costs and effectively improves the speaker verification performance compared to the baseline ResNet-LDE model and the Squeeze&Excitation block by a large margin. Detailed ablation studies are also performed to analyze various factors that may impact the performance of the proposed modules. We find that by employing the proposed L2-tf-GTFC transformation block, the Equal Error Rate decreases from 4.56% to 3.07%, a relative 32.68% reduction, and a relative 27.28% improvement in terms of the DCF score. The results indicate that our proposed global context guided transformation modules can efficiently improve the learned speaker representations by achieving time-frequency and channel-wise feature recalibration.",2020,INTERSPEECH,2009.00768,10.21437/interspeech.2020-1845,https://arxiv.org/pdf/2009.00768.pdf
f5d35e4b5289a7cb9c7c619cecfb2bb391344f41,1,0,Learning a bi-level adversarial network with global and local perception for makeup-invariant face verification,"Abstract Makeup is widely used to improve facial attractiveness and is well accepted by the public. However, different makeup styles will result in significant facial appearance changes. It remains a challenging problem to match makeup and non-makeup face images. This paper proposes a learning from generation approach for makeup-invariant face verification by introducing a bi-level adversarial network (BLAN). To alleviate the negative effects from makeup, we first generate non-makeup images from makeup ones, and then use the synthesized non-makeup images for further verification. Specifically, there are two adversarial sub-networks on different levels in BLAN, with the one on pixel level for reconstructing appealing facial images and the other on feature level for preserving identity information. For the non-makeup image generation module, a two-path network that involves both global and local structures is applied to improve the synthesis quality. Moreover, we make the generator well constrained by incorporating multiple perceptual losses. All the modules are embedded in an end-to-end network and jointly reduce the sensing gap between makeup and non-makeup images. Experimental results on three benchmark makeup face datasets demonstrate that our method achieves state-of-the-art verification accuracy across makeup status and can produce photo-realistic non-makeup face images.",2019,Pattern Recognit.,,10.1016/J.PATCOG.2019.01.013,
f60070d3a4d333aa1436e4c372b1feb5b316a7ba,1,0,Face Recognition via Centralized Coordinate Learning,"Owe to the rapid development of deep neural network (DNN) techniques and the emergence of large scale face databases, face recognition has achieved a great success in recent years. During the training process of DNN, the face features and classification vectors to be learned will interact with each other, while the distribution of face features will largely affect the convergence status of network and the face similarity computing in test stage. In this work, we formulate jointly the learning of face features and classification vectors, and propose a simple yet effective centralized coordinate learning (CCL) method, which enforces the features to be dispersedly spanned in the coordinate space while ensuring the classification vectors to lie on a hypersphere. An adaptive angular margin is further proposed to enhance the discrimination capability of face features. Extensive experiments are conducted on six face benchmarks, including those have large age gap and hard negative samples. Trained only on the small-scale CASIA Webface dataset with 460K face images from about 10K subjects, our CCL model demonstrates high effectiveness and generality, showing consistently competitive performance across all the six benchmark databases.",2018,ArXiv,1801.05678,,https://arxiv.org/pdf/1801.05678.pdf
f61e157b3a84518ce56d2303f0b0012578001a65,1,0,Cascaded Algorithm Representation for Purifying Face Dataset with Labeled Noise,"Data plays an important role in the development of computational vision. Google, Microsoft and other companies, Harvard, Stanford, and other famous schools and research institutes have published a variety of datasets. Most of the datasets are acquired by engine search, and more or less there will be noise problems. The larger the scale of the datasets, the more noise problems there will be. We put forward a cascaded algorithm to purify Noisy face dataset. Take MS-Celeb-1M dataset as an example, it can automatically generate three datasets-MS1, MS2, and MS3, which are outputs of our cascaded algorithm. Through comparing evaluation results of models trained by different datasets, the model trained by MS2 performs the best on LFW and IJBA, which demonstrates the effectiveness of our algorithm.",2019,2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC),,10.1109/CyberC.2019.00032,
f631f99a67e7723d45ecde63d6d32a255d4cb6db,1,1,Self-paced Robust Deep Face Recognition with Label Noise,"Deep face recognition has achieved rapid development but still suffers from occlusions, illumination and pose variations, especially for face identification. The success of deep learning models in face recognition lies in large-scale high quality face data with accurate labels. However, in real-world applications, the collected data may be mixed with severe label noise, which significantly degrades the generalization ability of deep models. To alleviate the impact of label noise on face recognition, inspired by curriculum learning, we propose a self-paced deep learning model (SPDL) by introducing a negative \(l_1\)-norm regularizer for face recognition with label noise. During training, SPDL automatically evaluates the cleanness of samples in each batch and chooses cleaner samples for training while abandons the noisy samples. To demonstrate the effectiveness of SPDL, we use deep convolution neural network architectures for the task of robust face recognition. Experimental results show that our SPDL achieves superior performance on LFW, MegaFace and YTF when there are different levels of label noise.",2019,PAKDD,,10.1007/978-3-030-16142-2_33,
f651d18db2ad799e5c6a347f6457a4781033da12,0,1,Pedestrian Detection Based on Light-Weighted Separable Convolution for Advanced Driver Assistance Systems,,2020,Neural Process. Lett.,,10.1007/s11063-020-10367-9,
f6735789ee2a9d1410bf13e394b8c4347dbb1ae3,1,1,Facial Feature Embedded Cyclegan For Vis-Nir Translation,"Visible and near-infrared (VIS-NIR) face recognition remains a challenging task due to distinctions between spectral components of two modalities. Inspired by the CycleGAN, this paper presents a method aiming to translate between VIS and NIR face images. To achieve this, we propose a new facial feature embedded CycleGAN. Firstly, to learn the particular feature while preserving common facial representation between VIS and NIR domains, we employ a general facial feature extractor (FFE) to extract effective features. Herein the MobileFaceNet is pre-trained on a VIS face database and serves as the FFE. Secondly, the domain-invariant feature learning is enhanced by proposing a new pixel consistency loss. Lastly, we establish a new WHU VIS-NIR database including varies in face rotation and expressions to enrich the training data. Experimental results on the Oulu-CASIA and our WHU VIS-NIR databases show that the proposed FFE-based CycleGAN (FFE-CycleGAN) outperforms some state-of-the-art methods and achieves 96.5% accuracy.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1904.09464,10.1109/ICASSP40776.2020.9054007,https://arxiv.org/pdf/1904.09464.pdf
f67b1de637586ee024683c76d91fb6a7fad9de6f,0,1,Unsupervised Representation Learning for Speaker Recognition via Contrastive Equilibrium Learning,"In this paper, we propose a simple but powerful unsupervised learning method for speaker recognition, namely Contrastive Equilibrium Learning (CEL), which increases the uncertainty on nuisance factors latent in the embeddings by employing the uniformity loss. Also, to preserve speaker discriminability, a contrastive similarity loss function is used together. Experimental results showed that the proposed CEL significantly outperforms the state-of-the-art unsupervised speaker verification systems and the best performing model achieved 8.01% and 4.01% EER on VoxCeleb1 and VOiCES evaluation sets, respectively. On top of that, the performance of the supervised speaker embedding networks trained with initial parameters pre-trained via CEL showed better performance than those trained with randomly initialized parameters.",2020,ArXiv,2010.11433,,https://arxiv.org/pdf/2010.11433.pdf
f6c540889476f00c685beb83c856743c6eda02f3,1,0,People Tracking and Re-Identifying in Distributed Contexts: PoseTReID Framework and Dataset,"We introduce a generic framework which is designed for effective real-time 2D multi-person tracking in distributed people interaction spaces like malls or amusement parks where long-term people's identities are important for other studies such as behavior analysis. The framework relies on multi-person pose detector for detecting bodies' parts and a recognizer for re-identifying people. We carefully selected existing sub-modules for our contexts, and the framework can efficiently track people and re-identify them using faces even later after tracking losses or reappearances of people. Along with this paper, since all existing datasets for people tracking barely have visible faces, we also introduce our people tracking dataset which is specifically designed for distributed people interaction spaces where people's faces are visible and recognizable. The results of the proposed PoseTReID framework are very interesting in all scenarios when compared on our dataset to a recent state-of-the-art tracking method. This efficient people tracking framework in distributed and interactive contexts, which is achieved here, is an important brick towards future works of people grouping and behavior analysis.",2020,2020 12th International Conference on Information Technology and Electrical Engineering (ICITEE),,10.1109/ICITEE49829.2020.9271712,
f6ec7b5fb411f10f9e51bb05ec73c9efa7c662a1,0,1,Exponential triplet loss,"This paper introduces a novel variant of the Triplet Loss function that converges faster and gives better results. This function can separate class instances homogeneously through the whole embedding space. With Exponential Triplet Loss function we also introduce a novel type of embedding space regularization Unit-Range and Unit-Bounce that utilizes euclidean space more efficiently and resembles features of the cosine distance. We also examined factors for choosing the best embedding vector size for specific embedding spaces. Finally, we also demonstrate how new function can train models for one-shot learning and re-identification tasks.",2020,ICCDA,,10.1145/3388142.3388163,
f6ecf492033dfa9f36eb9a628e99a45c26881b8d,1,1,Circle Loss: A Unified Perspective of Pair Similarity Optimization,"This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity $s_p$ and minimize the between-class similarity $s_n$. We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed $s_n$ and $s_p$ into similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, \emph {i.e.}, learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2002.10857,10.1109/CVPR42600.2020.00643,https://arxiv.org/pdf/2002.10857.pdf
f6f3236b6b79efbb705321868a1d036adf671319,1,1,Personal Verification System Using Thai ID Card and Face Photo for Cross-Age Face,"Nowadays, the main personal verification system uses the photo in Thai ID card. However, the card can be used up to nine year. Consequently, there could be difference between the face on ID card and the current face of the cardholder. This article demonstrates the use of ArcFace + Face aging with IPCGAN to solve the problem of cross-age face comparison. Based on the experimental results, the proposed algorithm outperforms other algorithms. Moreover, to automatically extract text information from the ID card, the developed system applies OCR of Google document text detection. It was found that Thai text information obtained from the proposed system yielded 93.97% accuracy, which is higher than others.",2019,2019 23rd International Computer Science and Engineering Conference (ICSEC),,10.1109/ICSEC47112.2019.8974845,
f74917fc0e55f4f5682909dcf6929abd19d33e2e,1,0,GAN Q UALITY I NDEX ( GQI ) BY GAN-INDUCED C LASSIFIER,"We propose an objective measure, called GAN Quality Index (GQI), to evaluate GANs. The idea is to train a GAN-induced classifier from the GAN generated data and use its accuracy on a real test set as a metric to measure how well the GAN model distribution matches the real data distribution. Unlike most existing quantitative measurements of GANs, which only reflect partial characteristics of generation distribution, the accuracy of the GAN-induced classifier can be used to derive a simple yet sufficient index to measure how well the generation distribution matches the true data distribution. We demonstrate the effectiveness of GQI on CIFAR-100, Flower-102, and MS-Celeb-1M which contains 10,000 classes.",2018,,,,https://pdfs.semanticscholar.org/f749/17fc0e55f4f5682909dcf6929abd19d33e2e.pdf
f75f3da28f7909d17ab82b0a42cda8e8ec5c381c,1,1,Facial Expressions as a Vulnerability in Face Recognition,"This work explores facial expression bias as a security vulnerability of face recognition systems. Face recognition technology has experienced great advances during the last decades. However, despite the great performance achieved by state of the art face recognition systems, the algorithms are still sensitive to a large range of covariates. This work presents a comprehensive analysis of how facial expression bias impacts the performance of face recognition technologies. Our study analyzes: i) facial expression biases in the most popular face recognition databases; and ii) the impact of facial expression in face recognition performances. Our experimental framework includes four face detectors, three face recognition models, and four different databases. Our results demonstrate a huge facial expression bias in the most widely used databases, as well as a related impact of face expression in the performance of state-of-the-art algorithms. This work opens the door to new research lines focused on mitigating the observed vulnerability.",2020,ArXiv,2011.08809,,https://arxiv.org/pdf/2011.08809.pdf
f76f74c6b7cd43daaa11c1eba25633c9c6cc95e9,1,0,Deformable Face Net: Learning Pose Invariant Feature with Pose Aware Feature Alignment for Face Recognition,"Face recognition plays an important role in computer vision. It still remains a challenging task due to pose, expression, illumination, partial occlusion, etc. In this work, we propose a novel Deformable Face Net (DFN) to handle the pose variations in face recognition. The Deformable Face Net introduces deformable convolution modules to simultaneously learn face recognition oriented alignment and feature extraction. Specifically, two loss functions, namely displacement consistency loss (DCL) and identity consistency loss (ICL) are designed to minimize the intra-class feature variation caused by different poses. These two loss functions jointly learn pose-aware displacement fields for deformable convolutions in the DFN. Different from the existing methods, the DFN focuses on aligning features across different poses rather than frontalizing the input faces. Extensive experiments show that the proposed DFN outperforms the state-of-the-art methods, especially on the datasets with large poses.",2019,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),,10.1109/FG.2019.8756575,http://vipl.ict.ac.cn/uploadfile/upload/2019070910345285.pdf
f772a73bbd1d7e51ed615aeb54e3707651d603f8,0,1,How do you Perceive Differently from an AI — A Database for Semantic Distortion Measurement,"Artificial intelligence (AI) is enabling the automated analysis of large amounts of image/video data, boosting the speed of multimedia data processing remarkably. Meanwhile, Image Quality Assessment (IQA) plays an important role in developing automatic analysis methods. To ensure the effectiveness of AI, images in multimedia applications should be considered for visual examination by both human and machine. Therefore, it is significant to understand the differences between human's and AI's perception of semantic distortion. However, little work has been done due to the lack of data from human on the semantic level. In this paper, we first propose a semantic database (SID) based on the surveillance scenarios, by collecting subjective average recognition rates of 3 semantic targets (face, pedestrian, license plate) with 3 types of distortion (JPEG Compression, BPG Compression, Motion Blur). Then, we present a detailed analysis of how human and AI perceive semantic distortion differently. Experimental results show that AI is stronger in tolerance to distortion than human beings on average, while weaker at generalization and stability. It is also implied in the experiments that existing IQA methods are not effective enough at judging the semantic distortion.",2019,2019 IEEE International Symposium on Circuits and Systems (ISCAS),,10.1109/ISCAS.2019.8702774,http://home.ustc.edu.cn/~weichou/papers/19_ISCAS_SID.pdf
f79a4ebefa3f570bad71104593ff9f4ba96c12b3,0,1,Pose-variant 3D Facial Attribute Generation,"We address the challenging problem of generating facial attributes using a single image in an unconstrained pose. In contrast to prior works that largely consider generation on 2D near-frontal images, we propose a GAN-based framework to generate attributes directly on a dense 3D representation given by UV texture and position maps, resulting in photorealistic, geometrically-consistent and identity-preserving outputs. Starting from a self-occluded UV texture map obtained by applying an off-the-shelf 3D reconstruction method, we propose two novel components. First, a texture completion generative adversarial network (TC-GAN) completes the partial UV texture map. Second, a 3D attribute generation GAN (3DA-GAN) synthesizes the target attribute while obtaining an appearance consistent with 3D face geometry and preserving identity. Extensive experiments on CelebA, LFW and IJB-A show that our method achieves consistently better attribute generation accuracy than prior methods, a higher degree of qualitative photorealism and preserves face identity information.",2019,ArXiv,1907.10202,,https://arxiv.org/pdf/1907.10202.pdf
f7a7f27516a219122000b9b915289b7127dca2f2,1,0,A Light CNN for Deep Face Representation With Noisy Labels,"The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit the large amount of training data. When training data are obtained from the Internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called max-feature-map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance, meanwhile, reducing the number of parameters and computational costs. Finally, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning.",2018,IEEE Transactions on Information Forensics and Security,1511.02683,10.1109/TIFS.2018.2833032,https://arxiv.org/pdf/1511.02683.pdf
f7dfa144ea723d8a653511c562b2f16db18a5664,0,1,Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval,"Optimising a ranking-based metric, such as Average Precision (AP), is notoriously challenging due to the fact that it is non-differentiable, and hence cannot be optimised directly using gradient-descent methods. To this end, we introduce an objective that optimises instead a smoothed approximation of AP, coined Smooth-AP. Smooth-AP is a plug-and-play objective function that allows for end-to-end training of deep networks with a simple and elegant implementation. We also present an analysis for why directly optimising the ranking based metric of AP offers benefits over other deep metric learning losses. We apply Smooth-AP to standard retrieval benchmarks: Stanford Online products and VehicleID, and also evaluate on larger-scale datasets: INaturalist for fine-grained category retrieval, and VGGFace2 and IJB-C for face retrieval. In all cases, we improve the performance over the state-of-the-art, especially for larger-scale datasets, thus demonstrating the effectiveness and scalability of Smooth-AP to real-world scenarios.",2020,ECCV,2007.12163,10.1007/978-3-030-58545-7_39,https://arxiv.org/pdf/2007.12163.pdf
f7e40f631825034e474be5640e6ac9ed7a6917c7,0,1,Talking-head Generation with Rhythmic Head Motion,"When people deliver a speech, they naturally move heads, and this rhythmic head motion conveys prosodic information. However, generating a lip-synced video while moving head naturally is challenging. While remarkably successful, existing works either generate still talkingface videos or rely on landmark/video frames as sparse/dense mapping guidance to generate head movements, which leads to unrealistic or uncontrollable video synthesis. To overcome the limitations, we propose a 3D-aware generative network along with a hybrid embedding module and a non-linear composition module. Through modeling the head motion and facial expressions1 explicitly, manipulating 3D animation carefully, and embedding reference images dynamically, our approach achieves controllable, photo-realistic, and temporally coherent talking-head videos with natural head movements. Thoughtful experiments on several standard benchmarks demonstrate that our method achieves significantly better results than the state-of-the-art methods in both quantitative and qualitative comparisons. The code is available on this https URL lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.",2020,ECCV,2007.08547,10.1007/978-3-030-58545-7_3,https://arxiv.org/pdf/2007.08547.pdf
f81cbe53331ddcc03e2e6dc31c01d7c22aa9af86,0,1,Improved Large-Margin Softmax Loss for Speaker Diarisation,"Speaker diarisation systems nowadays use embeddings generated from speech segments in a bottleneck layer, which are needed to be discriminative for unseen speakers. It is well-known that large-margin training can improve the generalisation ability to unseen data, and its use in such open-set problems has been widespread. Therefore, this paper introduces a general approach to the large-margin softmax loss without any approximations to improve the quality of speaker embeddings for diarisation. Furthermore, a novel and simple way to stabilise training, when large-margin softmax is used, is proposed. Finally, to combat the effect of overlapping speech, different training margins are used to reduce the negative effect overlapping speech has on creating discriminative embeddings. Experiments on the AMI meeting corpus show that the use of large-margin softmax significantly improves the speaker error rate (SER). By using all hyper parameters of the loss in a unified way, further improvements were achieved which reached a relative SER reduction of 24.6% over the baseline. However, by training overlapping and single speaker speech samples with different margins, the best result was achieved, giving overall a 29.5% SER reduction relative to the baseline.",2020,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",1911.0397,10.1109/ICASSP40776.2020.9053373,https://arxiv.org/pdf/1911.03970.pdf
f8215e4b9d5e557daf387ac1a0e725530ea6bd6a,0,1,An Adversarial Neuro-Tensorial Approach for Learning Disentangled Representations,"Several factors contribute to the appearance of an object in a visual scene, including pose, illumination, and deformation, among others. Each factor accounts for a source of variability in the data, while the multiplicative interactions of these factors emulate the entangled variability, giving rise to the rich structure of visual object appearance. Disentangling such unobserved factors from visual data is a challenging task, especially when the data have been captured in uncontrolled recording conditions (also referred to as “in-the-wild”) and label information is not available. In this paper, we propose a pseudo-supervised deep learning method for disentangling multiple latent factors of variation in face images captured in-the-wild. To this end, we propose a deep latent variable model, where the multiplicative interactions of multiple latent factors of variation are explicitly modelled by means of multilinear (tensor) structure. We demonstrate that the proposed approach indeed learns disentangled representations of facial expressions and pose, which can be used in various applications, including face editing, as well as 3D face reconstruction and classification of facial expression, identity and pose.",2019,International Journal of Computer Vision,1711.10402,10.1007/s11263-019-01163-7,https://link.springer.com/content/pdf/10.1007%2Fs11263-019-01163-7.pdf
f83390fb79e9a34192a92fd02099a94c8a375dbc,1,0,The GAN That Warped: Semantic Attribute Editing With Unpaired Data,"Deep neural networks have recently been used to edit images with great success, in particular for faces. However, they are often limited to only being able to work at a restricted range of resolutions. Many methods are so flexible that face edits can often result in an unwanted loss of identity. This work proposes to learn how to perform semantic image edits through the application of smooth warp fields. Previous approaches that attempted to use warping for semantic edits required paired data, i.e. example images of the same subject with different semantic attributes. In contrast, we employ recent advances in Generative Adversarial Networks that allow our model to be trained with unpaired data. We demonstrate face editing at very high resolutions (4k images) with a single forward pass of a deep network at a lower resolution. We also show that our edits are substantially better at preserving the subject's identity. The robustness of our approach is demonstrated by showing plausible image editing results on the Cub200 birds dataset. To our knowledge this has not been previously accomplished, due the challenging nature of the dataset.",2020,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),1811.12784,10.1109/cvpr42600.2020.00540,https://arxiv.org/pdf/1811.12784.pdf
f87b1c1574b678c26cbd6d46ec71b181eeef44b3,0,1,Real-Time Possessing Relationship Detection for Sports Analytics,"In this paper, we propose a novel algorithm for relationship detection. This task involves the tracking of a target object and human pose. The target object is tracked with a visual object tracker. The human poses are estimated via a keypoint detector while the person identities are preserved with a simple yet effective IoU tracker. Finally, a possessing relationship inference is made based on the position information of the tracked target and humans. This algorithm meets the real-time requirement by running at over 20 FPS and we give an application illustration in sports analytics.",2020,2020 39th Chinese Control Conference (CCC),,10.23919/CCC50068.2020.9189516,
f8e710560ab550fd7ff6f26eef53b18e584f30ff,0,1,Unifying Deep Local and Global Features for Image Search,"Image retrieval is the problem of searching an image database for items that are similar to a query image. To address this task, two main types of image representations have been studied: global and local image features. In this work, our key contribution is to unify global and local features into a single deep model, enabling accurate retrieval with efficient feature extraction. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads -- requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the Revisited Oxford and Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at this https URL .",2020,ECCV,2001.05027,10.1007/978-3-030-58565-5_43,https://arxiv.org/pdf/2001.05027.pdf
f917cdd7f7330454e52205ebda6147aa9fdef9da,1,0,Perceptual Face Completion using a Local-Global Generative Adversarial Network,"Face completion is one of the most challenging problems, as the reconstruction algorithm should render the missing pixels with semantically plausible contents. Recent methods have achieved promising advances in photorealistic human face synthesis. However, these approaches are limited to deal with general or structure specified faces. In this paper, we propose a Two-Pathway Perceptual Generative Adversarial Network (TPP-GAN) for face completion by perceiving semantic representations from both global structures and local details of a face. We combine a reconstruction network and a perceptual network containing two pathway adversarial networks (local and global) into our framework to efficiently ensure the transfer of the prominent facial features to the occluded parts, which encourages a visually high-quality image completion results. Experimental results well demonstrate that our proposed framework not only generates locally semantic and globally consistent fragments, but also outperforms existing methods on unaligned faces and synthesis of part components.",2018,2018 24th International Conference on Pattern Recognition (ICPR),,10.1109/ICPR.2018.8545559,
f94a09614d0b6f5c7ac6ab4b0f6df21480f0c7a5,1,0,Voxceleb: Large-scale speaker verification in the wild,"Abstract The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million ‘real-world’ utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.",2020,Comput. Speech Lang.,,10.1016/j.csl.2019.101027,
f9728b3800fe3ed9c4fa8357f8e594352de5eb64,0,1,A comprehensive study on face recognition: methods and challenges,"ABSTRACT Face Recognition is the process of identifying and verifying the faces. Face recognition has vast importance in the field of Security, Healthcare, Banking, Criminal Identification, Payment, and Advertising. In this paper, we have reviewed various techniques and challenges for the face recognition. Illumination, pose variation, facial expressions, occlusions, aging, etc. are the key challenges to the success of face recognition. Pre-processing, Face Detection, Feature Extraction, Optimal Feature Selection, and Classification are primary steps in any face recognition system. This paper provides a detailed review of each. Feature extraction techniques can be classified as appearance-based methods or geometry-based methods, such method may be local or global. Feature extraction is the most crucial stage for the success of the face recognition system. However, deep learning methods have freed the user from handcrafting the features. In this article, we have surveyed state-of-the-art methods of last few decades and the comparative study of various feature extraction methods is provided. Article also describes the current challenges in the area.",2020,,,10.1080/13682199.2020.1738741,
f9878fe60ca3f7f8c7cf02fbb5ea327f0fc400b6,1,0,How We've Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis,"Race and gender have long sociopolitical histories of classification in technical infrastructures-from the passport to social media. Facial analysis technologies are particularly pertinent to understanding how identity is operationalized in new technical systems. What facial analysis technologies can do is determined by the data available to train and evaluate them with. In this study, we specifically focus on this data by examining how race and gender are defined and annotated in image databases used for facial analysis. We found that the majority of image databases rarely contain underlying source material for how those identities are defined. Further, when they are annotated with race and gender information, database authors rarely describe the process of annotation. Instead, classifications of race and gender are portrayed as insignificant, indisputable, and apolitical. We discuss the limitations of these approaches given the sociohistorical nature of race and gender. We posit that the lack of critical engagement with this nature renders databases opaque and less trustworthy. We conclude by encouraging database authors to address both the histories of classification inherently embedded into race and gender, as well as their positionality in embedding such classifications.",2020,Proc. ACM Hum. Comput. Interact.,,10.1145/3392866,https://cmci.colorado.edu/idlab/assets/bibliography/pdf/Scheuerman2020-cscw-databaseidentity.pdf
f9aff7c7559cc4844a3f965e3b29f4b233c9a34d,0,1,"AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint Conference, AI 2020, Canberra, ACT, Australia, November 29–30, 2020, Proceedings","Data plays a vital role in deep learning model training. In large-scale medical image analysis, data privacy and ownership make data gathering challenging in a centralized location. Hence, federated learning has been shown as successful in alleviating both problems for the last few years. In this work, we have proposed multi-diseases classification from chest-X-ray using Federated Deep Learning (FDL). The FDL approach detects pneumonia from chest-X-ray and also identify viral and bacterial pneumonia. Without submitting the chest-X-ray images to a central server, clients train the local models with limited private data at the edge server and send them to the central server for global aggregation. We have used four pre-trained models such as ResNet18, ResNet50, DenseNet121, and MobileNetV2 and applied transfer learning on them at each edge server. The learned models in the federated setting have compared with centrally trained deep learning models. It has been observed that the models trained using the ResNet18 in federated environment produce accuracy up to 98.3% for pneumonia detection and up to 87.3% accuracy for viral and bacterial pneumonia detection. We have compared the performance of adaptive learning rate based optimizers such as Adam and Adamax with Momentum based Stochastic Gradient Descent (SGD) and found out that Momentum SGD yields better results than others. Lastly, for visualization, we have used Class Activation Mapping (CAM) approaches such as Grad-CAM, Grad-CAM++, and Score-CAM to identify pneumonia affected regions in a chest-X-ray.",2020,Australasian Conference on Artificial Intelligence,,10.1007/978-3-030-64984-5,
fa03cac5aa5192822a85273852090ca20a6c47aa,1,1,Towards Interpretable Face Recognition,"Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations.",2019,2019 IEEE/CVF International Conference on Computer Vision (ICCV),1805.00611,10.1109/ICCV.2019.00944,https://arxiv.org/pdf/1805.00611.pdf
fa252339e3bd172167288330bb2fc26664bfe84e,0,1,Voxel-based 3D occlusion-invariant face recognition using game theory and simulated annealing,"A novel voxel-based occlusion-invariant 3D face recognition framework (V3DOFR) based on game theory and simulated annealing is proposed. In V3DOFR approach, 3D meshes are converted to voxel form of sizes 43, 83, and 163. After that, locality preserving projection-based embeddings are computed for removing the sparseness of voxels and generating consistent linear embedding per mesh with size 64 × 3, 128 × 3, and 256 × 3, respectively. The generator of triplets provides the triplets of sizes 64x3x3, 128x3x3, and 256x3x3. The simulated annealing is used to check the threshold value of adversarial triplet loss generated after ensembling losses of different grid sizes. The proposed framework is compared with four well-known methods using three face datasets, namely, Bosphorus, UMBDB, and KinectFaceDB. The performance evaluation has been done using four different cases of experimentations, viz. voxel based face recognition, occlusion invariant face recognition, landmarks based 3D face recognition, and 3D mesh based face recognition. Seven evaluation metrics are used to compare the proposed technique with other methods. The proposed method provides better accuracy and computation time over the other existing techniques in the majority of cases.",2020,Multimedia Tools and Applications,,10.1007/s11042-020-09331-5,
fa46285eb5218c4c543514dd7a34370ff9d99e77,1,1,Mis-classified Vector Guided Softmax Loss for Face Recognition,"Face recognition has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs), the central task of which is how to improve the feature discrimination. To this end, several margin-based (e.g., angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from three issues: 1) Obviously, they ignore the importance of informative features mining for discriminative learning; 2) They encourage the feature margin only from the ground truth class, without realizing the discriminability from other non-ground truth classes; 3) The feature margin between different classes is set to be same and fixed, which may not adapt the situations very well. To cope with these issues, this paper develops a novel loss function, which adaptively emphasizes the mis-classified feature vectors to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative face features. To the best of our knowledge, this is the first attempt to inherit the advantages of feature margin and feature mining into a unified loss function. Experimental results on several benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives. Our code is available at http://www.cbsr.ia.ac.cn/users/xiaobowang/.",2020,AAAI,1912.00833,10.1609/AAAI.V34I07.6906,https://arxiv.org/pdf/1912.00833.pdf
fa4beed5676020efe1612056b02aae51a991f8b1,0,1,Differential Detection of Facial Retouching: A Multi-Biometric Approach,"Facial retouching apps have become common tools which are frequently applied to improve one’s facial appearance, e.g. before sharing face images via social media. Beautification induced by retouching has the ability to substantially alter the appearance of face images and hence might represent a challenge for face recognition. Towards deploying secure face recognition as well as enforcing anti-photoshop legislations, a robust and reliable detection of retouched face image is needed. Published approaches consider a single image-based (no-reference) scenario where a potentially retouched face image serves as sole input to the retouching detector. However, in many cases a trusted unaltered face image of a subject examined is available which enables an image pair-based (differential) detection scheme. In this work, ICAO-compliant subsets of the FERET and FRGCv2 face databases are used to automatically create a database containing 9,078 retouched face images together with unconstrained probe images. In evaluations employing the commercial Cognitec FaceVACS and the open-source ArcFace face recognition system, it is shown that facial retouching can negatively impact face recognition performance. Further, a differential facial retouching detection system is proposed which processes pairs of a potentially retouched reference image and corresponding unaltered probe image of single subjects. Estimated differences in feature vectors obtained from texture descriptors, facial landmarks, and deep face representations are leveraged by machine learning-based classifiers of which the detection scores are fused to distinguish between retouched and unaltered face images. The proposed scheme is evaluated in a cross-database scenario where training and testing are performed on the FERET and FRGCv2 databases and vice versa. In the scenario where the used retouching algorithm is known by the detection algorithm, a competitive average D-EER of approximately 2% is achieved. Further, the scenario in which the employed retouching algorithm is not known by the detection algorithm is evaluated. In the latter scenario, the proposed approach obtains an average D-EER below 10% and is shown to outperform several state-of-the-art single image-based detection schemes.",2020,IEEE Access,,10.1109/ACCESS.2020.3000254,https://ieeexplore.ieee.org/ielx7/6287639/6514899/09109348.pdf
fa9cdb156b45f85c57c12addec26bb516f5a0bd5,0,1,RefineFace: Refinement Neural Network for High Performance Face Detection,"Face detection has achieved significant progress in recent years. However, high performance face detection still remains a very challenging problem, especially when there exists many tiny faces. In this paper, we present a single-shot refinement face detector namely RefineFace to achieve high performance. Specifically, it consists of five modules: Selective Two-step Regression (STR), Selective Two-step Classification (STC), Scale-aware Margin Loss (SML), Feature Supervision Module (FSM) and Receptive Field Enhancement (RFE). To enhance the regression ability for high location accuracy, STR coarsely adjusts locations and sizes of anchors from high level detection layers to provide better initialization for subsequent regressor. To improve the classification ability for high recall efficiency, STC first filters out most simple negatives from low level detection layers to reduce search space for subsequent classifier, then SML is applied to better distinguish faces from background at various scales and FSM is introduced to let the backbone learn more discriminative features for classification. Besides, RFE is presented to provide more diverse receptive field to better capture faces in some extreme poses. Extensive experiments conducted on WIDER FACE, AFW, PASCAL Face, FDDB, MAFA demonstrate that our method achieves state-of-the-art results and runs at 37.3 FPS with ResNet-18 for VGA-resolution images.",2020,IEEE transactions on pattern analysis and machine intelligence,1909.04376,10.1109/tpami.2020.2997456,https://arxiv.org/pdf/1909.04376.pdf
faed82f8980af7ee8dbdb8bea732422f81e638e5,0,1,TediGAN: Text-Guided Diverse Image Generation and Manipulation,"In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module is to train an image encoder to map real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity is to learn the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipulation. Our model can provide the lowest effect guarantee, and produce diverse and highquality images with an unprecedented resolution at 1024. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels with or without instance (text or real image) guidance. To facilitate text-guided multi-modal synthesis, we propose the MULTIMODAL CELEBA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",2020,ArXiv,2012.03308,,https://arxiv.org/pdf/2012.03308.pdf
faf9fadcbcc547a9cd2b8207cd2e27610c3ced87,0,1,Dual-Path Part-Level Method for Visible–Infrared Person Re-identification,"Visible–infrared cross-modality person re-identification is a realistic problem of person re-identification. Under poor illumination scenario, general methods of visible–visible person re-identification can not solve the problem well. If we directly compare the visible images of pedestrians captured under dark lighting with the visible images of pedestrians captured under normal light, this extreme color deviation will greatly reduce the recognition ability of the learned representations. In this paper, we propose a dual-path framework for visible–infrared cross-modality person re-identification based human part level features. Feature learning module contains modality-specific dual-path layers and modality-shared human part-level layers, which achieve discriminative global and local representations. In order to better optimize the proposed network, we design a global loss function and a local loss function for the global features and local features, respectively. The two loss functions are integrated together to train the network. We verify the effectiveness of our method on the challenging benchmarks: SYSU-MM01 and RegDB. Experimental results show that, compared with other cross-modality methods, our method has better effect in improving visible–infrared cross-modality person re-identification tasks.",2020,Neural Processing Letters,,10.1007/s11063-020-10239-2,
fb3ff56ab12bd250caf8254eca30cd97984a949a,1,0,Face recognition Face2vec based on deep learning: Small database case,"Object classification is a common problem in artificial intelligence and now it is usually approached by deep learning. In the paper the artificial neural network (ANN) architecture is considered. According to described ANN architecture, the ANN models are trained and tested on a relatively small Color-FERET facial image database under different conditions. The best fine-tuned ANN model provides 94% face recognition accuracy on Color-FERET frontal images and 98% face recognition accuracy within 3 attempts. However, for improving recognition system accuracy large data sets are still necessary preferably consisting of millions of images.",2017,Automatic Control and Computer Sciences,,10.3103/S0146411617010072,
fbc2f5cf943f440b1ba2374ecf82d0176a44f1eb,1,0,3D-Aided Dual-Agent GANs for Unconstrained Face Recognition,"Synthesizing realistic profile faces is beneficial for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by augmenting the number of samples with extreme poses and avoiding costly annotation work. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy betwedistributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces while preserving the identity information during the realism refinement. The dual agents are specially designed for distinguishing real versus fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose, texture as well as identity, and stabilize the training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only achieves outstanding perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A and CFP unconstrained face recognition benchmarks. In addition, the proposed DA-GAN is also a promising new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our winning entry to the NIST IJB-A face recognition competition in which we secured the $1^{st}$ places on the tracks of verification and identification.",2019,IEEE Transactions on Pattern Analysis and Machine Intelligence,,10.1109/TPAMI.2018.2858819,
fbd6b8ec70e04eb5a8ead29ce3fe4a85282dce7b,0,1,Access Control System Based on Face Recognition,"Face recognition is a technology that uses face image of someone to verify his identity by finding this person in a given photos database. It becomes very practical in access control systems as it does not require any physical interaction for gaining access as traditional ways with keys. Moreover, these systems only require a camera for recognition and are easy to install and use. This is why they are already in use by companies as access control to their offices, in home automation systems, etc.In this paper, different approaches to face recognition are studied. The first step of any face recognition system is face detection and cropping so we analyzed classical Viola-Jones face detection and MultiTask Convolutional Neural Networks (MTCNN) in terms of detection quality and processing time.The final classifier obtained is capable of matching face from the online camera to image from a given database. We also considered decreasing the vulnerability of standalone face recognition by adding a spoof detection method so that the system does not react to every approach to bypass the system like showing a photo of an allowed person shown on a phone’s screen.",2020,"2020 7th International Conference on Control, Decision and Information Technologies (CoDIT)",,10.1109/CoDIT49905.2020.9263894,
fc3371adaa34185b97a4d889316858a768af973e,1,0,Using Honeypots to Catch Adversarial Attacks on Neural Networks,,2019,,1904.08554,,https://arxiv.org/pdf/1904.08554.pdf
fca9ebaa30d69ccec8bb577c31693c936c869e72,1,0,Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross-Age Face Synthesis for Age-Invariant Face Recognition,"Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.",2019,AAAI,1809.00338,10.1609/aaai.v33i01.33019251,https://arxiv.org/pdf/1809.00338.pdf
fcad97b7cf615d9c67f0165cf19e25036511ad63,0,1,Multi-layer Attention Aggregation in Deep Neural Network,"Convolutional neural networks have achieved significant successes in image classification recently due to its high capacity in learning discriminative features. In this work, we propose Multi-layer attention aggregation(MAA) model, a convolutional architecture using attention mechanism and global aggregation module iteratively. By merging attention-aware features from every convolutional stage, MAA improves the performance of image classification. Specifically, the proposed MAA model can be applied to state-of-the-art convolutional architectures, such as ResNet, and improve its performance by increasing 30% computational cost. Furthermore, we also employ ArcFace loss in the training process to improve the performance of image classification. Applying the proposed method on ResNet, our MAA model achieves higher image classification performance including on standard benchmarks of Google-Landmarks dataset, CIFAR-10 and CIFAR-100 dataset. Note that, our method achieves 0.68% top-1 accuracy improvement on Google-Landmarks dataset, 2.27% top-1 accuracy improvement on CIFAR-100 and 1.14% top-1 accuracy improvement on CIFAR-10.",2019,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),,10.1109/ITAIC.2019.8785533,
fce737611b30ec6d6f10ec02b4f48f41ce68e05b,1,1,512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU,"Small factor and ultra-low power devices are becoming more and more smart and capable even for deep learning network inference. And as the devices are ""small"", the challenge is becoming tougher. This paper covers full development and deployment pipeline of Face Recognition with a live camera - from model training and quantization to porting to RISC-V MCU with 512 kilobytes of internal RAM. Authors provide GreenWaves GAP8 SoC overview and the approaches for DNN model optimization and inference in the extreme environment. As the project outcome, authors were able to run Face Detection and Recognition with live QVGA camera and display preview on a battery-powered board.",2019,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),,10.1109/ICCVW.2019.00305,http://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.pdf
fd3b6e034255bb1f0a64a7368c5b2f262e413ca1,0,1,Adversarial Neural Pruning,"It is well known that neural networks are susceptible to adversarial perturbations and are also computationally and memory intensive which makes it difficult to deploy them in real-world applications where security and computation are constrained. In this work, we aim to obtain both robust and sparse networks that are applicable to such scenarios, based on the intuition that latent features have a varying degree of susceptibility to adversarial perturbations. Specifically, we define vulnerability at the latent feature space and then propose a Bayesian framework to prioritize features based on their contribution to both the original and adversarial loss, to prune vulnerable features and preserve the robust ones. Through quantitative evaluation and qualitative analysis of the perturbation to latent features, we show that our sparsification method is a defense mechanism against adversarial attacks and the robustness indeed comes from our model's ability to prune vulnerable latent features that are more susceptible to adversarial perturbations.",2019,ArXiv,,,
fdca7486611a574d08aca45b83ba38486b2d83ab,0,1,Cross-View Gait Recognition by Discriminative Feature Learning,"Recently, deep learning-based cross-view gait recognition has become popular owing to the strong capacity of convolutional neural networks (CNNs). Current deep learning methods often rely on loss functions used widely in the task of face recognition, e.g., contrastive loss and triplet loss. These loss functions have the problem of hard negative mining. In this paper, a robust, effective, and gait-related loss function, called angle center loss (ACL), is proposed to learn discriminative gait features. The proposed loss function is robust to different local parts and temporal window sizes. Different from center loss which learns a center for each identity, the proposed loss function learns multiple sub-centers for each angle of the same identity. Only the largest distance between the anchor feature and the corresponding cross-view sub-centers is penalized, which achieves better intra-subject compactness. We also propose to extract discriminative spatial–temporal features by local feature extractors and a temporal attention model. A simplified spatial transformer network is proposed to localize the suitable horizontal parts of the human body. Local gait features for each horizontal part are extracted and then concatenated as the descriptor. We introduce long short-term memory (LSTM) units as the temporal attention model to learn the attention score for each frame, e.g., focusing more on discriminative frames and less on frames with bad quality. The temporal attention model shows better performance than the temporal average pooling or gait energy images (GEI). By combing the three aspects, we achieve state-of-the-art results on several cross-view gait recognition benchmarks.",2020,IEEE Transactions on Image Processing,,10.1109/TIP.2019.2926208,
fde151d986a65db775d0ebbfa20a0c6f8b28d925,1,1,Discriminative Representation Learning for Face Recognition,"A significant progress has been made on face recognition in recent years because of the rapid development of deep learning approaches. Deep learning approaches offer a powerful toolbox for tackling many aspects of face recognition, including the search for effective discriminative features. We compare several state-of-the-art face recognition methods and combine different modules from those methods to propose a special approach for discriminative representation learning. Our approach has a special architecture for representation learning combined with a latest design of classification loss function, making it a highly effective solution for uncontrolled face recognition. Experimented on the Labeled Faces in the Wild (LFW), the Celebrities in Frontal-Profile dataset (CFP), and the AgeDB datasets, our approach shows competitive performance to other state-of-the-art methods.",2020,AICV,,10.1007/978-3-030-44289-7_54,
fdfef78d9c74394562fcb5b0d9d5ba2b80c12ed4,1,1,A survey of face recognition techniques under occlusion,"The limited capacity to recognize faces under occlusions is a long-standing problem that presents a unique challenge for face recognition systems and even for humans. The problem regarding occlusion is less covered by research when compared to other challenges such as pose variation, different expressions, etc. Nevertheless, occluded face recognition is imperative to exploit the full potential of face recognition for real-world applications. In this paper, we restrict the scope to occluded face recognition. First, we explore what the occlusion problem is and what inherent difficulties can arise. As a part of this review, we introduce face detection under occlusion, a preliminary step in face recognition. Second, we present how existing face recognition methods cope with the occlusion problem and classify them into three categories, which are 1) occlusion robust feature extraction approaches, 2) occlusion aware face recognition approaches, and 3) occlusion recovery based face recognition approaches. Furthermore, we analyze the motivations, innovations, pros and cons, and the performance of representative approaches for comparison. Finally, future challenges and method trends of occluded face recognition are thoroughly discussed.",2020,ArXiv,2006.11366,,https://arxiv.org/pdf/2006.11366.pdf
feb8398f40e5d97d0aab500048b7d0f8ebc9ca61,0,1,Adversarial Neural Pruning with Latent Vulnerability Suppression,"Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.",2020,ICML,1908.04355,,https://arxiv.org/pdf/1908.04355.pdf
feb9a759386f55bbccc85bf4882bc0a39eeb25c1,0,1,On the Security Risk of Cancelable Biometrics,"Over the years, a number of biometric template protection schemes, primarily based on the notion of ""cancelable biometrics"" (CB) have been proposed. An ideal cancelable biometric algorithm possesses four criteria, i.e., irreversibility, revocability, unlinkability, and performance preservation. Cancelable biometrics employed an irreversible but distance preserving transform to convert the original biometric templates to the protected templates. Matching in the transformed domain can be accomplished due to the property of distance preservation. However, the distance preservation property invites security issues, which are often neglected. In this paper, we analyzed the property of distance preservation in cancelable biometrics, and subsequently, a pre-image attack is launched to break the security of cancelable biometrics under the Kerckhoffs's assumption, where the cancelable biometrics algorithm and parameters are known to the attackers. Furthermore, we proposed a framework based on mutual information to measure the information leakage incurred by the distance preserving transform, and demonstrated that information leakage is theoretically inevitable. The results examined on face, iris, and fingerprint revealed that the risks origin from the matching score computed from the distance/similarity of two cancelable templates jeopardize the security of cancelable biometrics schemes greatly. At the end, we discussed the security and accuracy trade-off and made recommendations against pre-image attacks in order to design a secure biometric system.",2019,,1910.0777,,https://arxiv.org/pdf/1910.07770.pdf
feea73095b1be0cbae1ad7af8ba2c4fb6f316d35,1,0,Deep Face Recognition with Center Invariant Loss,"Convolutional Neural Networks (CNNs) have been widely used for face recognition and got extraordinary performance with large number of available face images of different people. However, it is hard to get uniform distributed data for all people. In most face datasets, a large proportion of people have few face images. Only a small number of people appear frequently with more face images. These people with more face images have higher impact on the feature learning than others. The imbalanced distribution leads to the difficulty to train a CNN model for feature representation that is general for each person, instead of mainly for the people with large number of face images. To address this challenge, we proposed a center invariant loss which aligns the center of each person to enforce the learned features to have a general representation for all people. The center invariant loss penalizes the difference between each center of classes. With center invariant loss, we can train a robust CNN that treats each class equally regardless the number of class samples. Extensive experiments demonstrate the effectiveness of the proposed approach. We achieve state-of-the-art results on LFW and YTF datasets.",2017,ACM Multimedia,,10.1145/3126686.3126693,http://www1.ece.neu.edu/~yuewu/files/2017/twu024.pdf
ff31d6af28b33e89266fe850f1d8c8a5bb36acf4,1,0,Video Face Recognition: Component-wise Feature Aggregation Network (C-FAN),"We propose a new approach to video face recognition. Our component-wise feature aggregation network (C-FAN) accepts a set of face images of a subject as an input, and outputs a single feature vector as the face representation of the set for the recognition task. The whole network is trained in two steps: (i) train a base CNN for still image face recognition; (ii) add an aggregation module to the base network to learn the quality value for each feature component, which adaptively aggregates deep feature vectors into a single vector to represent the face in a video. C-FAN automatically learns to retain salient face features with high quality scores while suppressing features with low quality scores. The experimental results on three benchmark datasets, YouTube Faces [39], IJB-A [13], and IJB-S [12] show that the proposed C-FAN network is capable of generating a compact feature vector with 512 dimensions for a video sequence by efficiently aggregating feature vectors of all the video frames to achieve state of the art performance.",2019,2019 International Conference on Biometrics (ICB),1902.07327,10.1109/ICB45273.2019.8987385,https://arxiv.org/pdf/1902.07327.pdf
ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a,1,0,Towards Video Captioning with Naming: A Novel Dataset and a Multi-modal Approach,"Current approaches for movie description lack the ability to name characters with their proper names, and can only indicate people with a generic “someone” tag. In this paper we present two contributions towards the development of video description architectures with naming capabilities: firstly, we collect and release an extension of the popular Montreal Video Annotation Dataset in which the visual appearance of each character is linked both through time and to textual mentions in captions. We annotate, in a semi-automatic manner, a total of 53k face tracks and 29k textual mentions on 92 movies. Moreover, to underline and quantify the challenges of the task of generating captions with names, we present different multi-modal approaches to solve the problem on already generated captions.",2017,ICIAP,,10.1007/978-3-319-68548-9_36,http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2017_ICIAP_Naming.pdf
ffafec886d1ed59a8068b434e9caad6d8334b549,1,0,Long-Tailed Recognition Using Class-Balanced Experts,,2020,ArXiv,2004.03706,,https://arxiv.org/pdf/2004.03706.pdf
ffe3fcf48dec8866237e675d6f99076ae36f1aeb,0,1,Aging Deep Face Features: Finding Missing Children,"Given a gallery of face images of missing children, state-of-the-art face recognition systems fall short in identifying a child (probe) recovered at a later age. We propose an age-progression module that can age-progress deep face features output by any commodity face matcher. For time lapses larger than 10 years (the missing child is found after 10 or more years), the proposed age-progression module improves the closed-set identification accuracy of FaceNet from 40% to 49.56% and CosFace from 56.88% to 61.25% on a child celebrity dataset, namely ITWCC. The proposed method also outperforms state-of-the-art approaches with a rank-1 identification rate from 94.91% to 95.91% on a public aging dataset, FG-NET, and from 99.50% to 99.58% on CACD-VS. These results suggest that aging face features enhances the ability to identify young children who are possible victims of child trafficking or abduction.",2019,,,,
fffbcbf1625f4da9a2e4f71a8cdd1689aa59c56c,0,1,Face Recognition: A Novel Multi-Level Taxonomy based Survey,"In a world where security issues have been gaining growing importance, face recognition systems have attracted increasing attention in multiple application areas, ranging from forensics and surveillance to commerce and entertainment. To help understanding the landscape and abstraction levels relevant for face recognition systems, face recognition taxonomies allow a deeper dissection and comparison of the existing solutions. This paper proposes a new, more encompassing and richer multi-level face recognition taxonomy, facilitating the organization and categorization of available and emerging face recognition solutions; this taxonomy may also guide researchers in the development of more efficient face recognition solutions. The proposed multi-level taxonomy considers levels related to the face structure, feature support and feature extraction approach. Following the proposed taxonomy, a comprehensive survey of representative face recognition solutions is presented. The paper concludes with a discussion on current algorithmic and application related challenges which may define future research directions for face recognition.",2020,IET Biom.,1901.00713,10.1049/iet-bmt.2019.0001,https://arxiv.org/pdf/1901.00713.pdf